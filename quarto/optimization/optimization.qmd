---
title: "Optimization"
author: "Michael LIou"
date: "3/25/2022"
execute:
  cache: true
---

```{r setup}
#| code-summary: Libraries
#| message: false
library(CVXR)
library(tidyverse)
library(Matrix)
library(DEoptim)
```

## Convex Optimization

A convex optimization problem has the form:

$$
\begin{aligned}
\min \quad & f_0(x) \\
\text{s.t.} \quad & f_i (x) \leq 0\qquad i = 1 \dots n \\
&a_i^Tx = b_i \qquad i = 1 \dots p
\end{aligned}
$$

1. objective function $f_0(x)$ is convex
2. inequality constraints $f_i(x) \leq 0$ are convex
3. equality constraints are affine

Some useful compositions that are preserve convexity

* composition of convex w/ affine function
  - $g(x) = f(Ax + b)$
* weighted sums ($w_i \geq 0$) of convex functions $f_i$
  - $g(x) = w_1f_1(x) + w_2f_2(x) + \dots w_nf_n(x)$
  
Notable counter examples, if $f_1, f_2$ are convex,

* $f_1 - f_2$ may NOT be convex
  - $0 - x^2$
* $f_1 \times f_2$ may NOT be convex
  - $x * x^2$
* $f_1 / f_2$ may NOT be convex
  - $\frac{x^{3/2}}{x}$

### CVXR

A program for principled convex optimization

0. Define the variables
- `Betahat <- Variable(p)` where `p` is the length of the vector
1. Define objective function (`objective <- Minimize(...)`)
- `Minimize(...)`
- `Maximize(...)`
2. Define the problem
3. Solve

Useful functions for debugging the program 

- `is_dcp(problem)` - check if problem follows the DCP rules
- `is_dgp(problem)` - check if problem follows geometric programming rules

```{r}
# Tutorial
set.seed(123)

n <- 100
p <- 10
beta <- -4:5   # beta is just -4 through 5.

X <- matrix(rnorm(n * p), nrow=n)
colnames(X) <- paste0("beta_", beta)
Y <- X %*% beta + rnorm(n)
```

```{r}
betahat <- Variable(p)
objective <- Minimize(sum((Y-X %*% betahat)^2))
problem <- Problem(objective, 
                   constraints = list(betahat >= 0)) # list constraints, betahat >= 0 
result <- solve(problem)
```

```{r}
result$getValue(betahat) %>% zapsmall()
```


#### Examples

::: {.panel-tabset}

##### Largest Eigenvalue

Let $A \in \mathbb{R}^{pxp}$ be a symmetric, nonegative matrix. Then our target value is: 

```{r}
set.seed(1)
# Symmetric Matrix
p <- 10 # matrix is p x p
A <- matrix(abs(rnorm(p^2)), ncol = p) # non negative matrix
sA <- forceSymmetric(A, uplo = "U") %>% as.matrix()

sA_eig <- eigen(sA)
sA_eig$values[1] # largest eigenvalue
```

We show 3 ways to use CVXR to solve for the largest eigenvalue, starting with the simplest

1. objective is pf_eigenvalue
2. perron frobenius, geometric programming
3. perron frobenius, convex programming


```{r}
# w/ CVXR
# using objective 
X <- Variable(p,p, pos = TRUE)
objective <- Minimize(pf_eigenvalue(X))
constraints <- list(X == sA)
problem <- Problem(objective, constraints)
results <- solve(problem, gp = TRUE)

c(results$value, sA_eig$values[1])
```


```{r}
# w/ CVXR
# spectral radius w/ geometric rules, A is known

lambda <- Variable(1, name = "lambda", pos = TRUE)
u <- Variable(p, pos = TRUE)
objective <- Minimize(lambda)
constraints <- list()
for (i in 1:p) {
  constraints <- c(constraints, sA[i,, drop = F] %*% u / (lambda * u[i]) <= 1)
}
problem <- Problem(objective,constraints)
results <- solve(problem, gp = TRUE)

c(results$getValue(lambda), sA_eig$values[1]) # the same
```


In order to transform the geometric program of minimizing largest eigenvalue, we note that the inequalities can be written

$$
\begin{aligned}
\frac{\sum_j A_{ij}u_j}{\lambda u_i} &\leq 1 \qquad \text{for } i = 1 \dots n \\
\log(\sum_j \exp(u_j + \log(A_{ij}) - \log(\lambda) - u_i)) & \leq 0
\end{aligned}
$$

```{r}
# w/ CVXR
# manually transforming geometric to convex program
lambda <- Variable(1, name = "lambda", pos = TRUE)
u <- Variable(p, pos = TRUE)
objective <- Minimize(lambda)
constraints <- list()
for ( i in 1:p) {
 constraints <- c(constraints, 
                  log_sum_exp(u + log(sA[i,]) - log(lambda) - u[i]) <= 0)
}
problem <- Problem(objective, constraints)
results <- solve(problem)
# sum(vec + vec - scalar - scalar)
# curvature(log_sum_exp(u + log(sA[1,]) - log(lambda) - u[1]))

c(results$getValue(lambda), sA_eig$values[1]) # the same! nice!
```


##### Minimize $\lambda_1$

In addition to just _finding_ the maximum eigenvalue (spectral radius) of a fixed matrix, we can also let the matrix be an optimization variable and minimize the spectral radius of the matrix! Thus, if entries of the matrix $A(x)$ are a function of $x$, and we have some constraints $f(x) \leq 1$, we can figure out those entries.

:::

## DEoptim

Differential Evolution Optimization, this is a derivative free way of optimization, and seems to work quite well, at least for rosenbrock function!


### Examples

#### First Use


```{r results = F}
# Rosenbrock Function for optimization
# global minimum f(x) = 0, at x = (1, 1).
Rosenbrock <- function(x){
    x1 <- x[1]
    x2 <- x[2]
    100 * (x2 - x1^2)^2 + (1 - x1)^2
}

# specify the grid for replicability
lower <- c(-10,-10)
upper <- -lower

# Optimize over space
set.seed(1234)
DEoptim(Rosenbrock, lower, upper) # basic invocation

# invocation with control aspects
outDEoptim <- DEoptim(Rosenbrock,
                      lower,
                      upper,
                      DEoptim.control(NP = 80,
                                      itermax = 400, # number of procedure iterations
                                      F = 1.2,
                                      CR = 0.7)) 
```

```{r results = "hold"}
outDEoptim$optim$bestmem # the final parameters
outDEoptim$optim$bestval # the final value

sprintf("The found parameters are %.2f %.2f\n The minimum objective is: %.2f\n DEoptim did %.2f iterations.", 
  outDEoptim$optim$bestmem[[1]], outDEoptim$optim$bestmem[[2]], # parameters
  outDEoptim$optim$bestval, # final value
  outDEoptim$optim$iter) # final value


plot(outDEoptim)
```

