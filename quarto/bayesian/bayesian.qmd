---
title: "Bayesian"
author: "Michael Liou"
date: "12/6/2021"
execute:
  cache: true
---

```{r}
library(INLA)
library(tidyverse)
library(patchwork)
library(rjags)
library(rstanarm) # regression modeling with rstan backend
library(sjPlot)
library(effects)
library(sjPlot)
library(ggeffects)
library(rstanarm)
```

## Introduction to Bayesian Modeling

### Software Ecosystem

The software mostly is split up by the methodology that each method uses. Seems like results will vary, and syntax also will vary. Trade-offs on speed and use cases.

#### R Software

* **INLA** - integrated nested laplace approximations. Uses R formula syntax
  - not on CRAN because it uses a C library that is machine dependent
  - option to use pardiso optimizer, for which you need a license.
* **BUGS** - MCMC based sampling
  - has many different versions, seems easier to install on windows
* **JAGS** 
  - the original MCMC based solutions, has an R implementation with "rjags"
  - Stands for "Just Another Gibbs Sampler"
  - text file specification of the model
  - reimplementation of "BUGS"
* **rstan** - does some magical Hamiltonion Monte Carlo, which supp
  - **brms** - uses STAN as backend, but allows linear and nonlinear models with R formula syntax. Offers [more flexibility than rstanarm](https://m-clark.github.io/easy-bayes/comparison-to-rstanarm.html)
  - **rstanarm** - meant as a drop in replacement using STAN for many base R and lme4 models. The main difference is that these models are precompiled in STAN code, while brms will compile everytime into new STAN code, thus will be slower for basic models. However, what you trade in slowness you get flexibility.
  - **blavaan** - latent variable modeling, like SEM modeling, latent growth curve models, confirmatory factor analysis.

## INLA

Broadly speaking, INLA is for a broad class of models used for Guassian markov random fields (GMRF). INLA tends to be faster than other methods, because the software is based on Laplace Approximation. Because of that, it is also a deterministic method unlike stochastic MCMC based software like STAN or BUGS/JAGS

### Installation Issues

The [standard installation as recommended by the website](https://www.r-inla.org/download-install) is:

```{r eval=FALSE}
install.packages("INLA",repos=c(getOption("repos"),INLA="https://inla.r-inla-download.org/R/testing"), dep=TRUE) # R > 4.1
```

But I keep getting an error:

> Error in inla.call.builtin() : INLA installation error; no such file 

so i tried installing directly from github with devtools.

> /usr/local/lib/R/4.1/site-library/INLA/bin/mac/64bit/inla.run: line 125: /usr/local/lib/R/4.1/site-library/INLA/bin/mac/64bit/inla: No such file or directory

The "devel" version unfortunately gives the following error, so trying the stable version.

```{r eval=FALSE}
devtools::install_github(repo = "https://github.com/hrue/r-inla", ref = "stable", subdir = "rinla", build = FALSE)
```
The stable version also didn't work, was giving some dynamic library errors, finally tried installing manually and it seemed to work. [INLA Binaries for R 4.1](http://inla.r-inla-download.org/R/testing/bin/macosx/contrib/4.1/). [The thread that gave me idea of installing manually](https://groups.google.com/g/r-inla-discussion-group/c/X7VsO41DnGA/m/TSMD3NQeCAAJ).

In order to install it manually, 

1. Download the precompiled binary here: https://inla.r-inla-download.org/R/testing/bin/macosx/contrib/4.1/
  - back up directories if you need a different binary
2. From R, run `install.packages("pathtobinary.tgz", repos = NULL)`

#### Pardiso License

Pardiso is parallel computing library, stands for "Parallel Sparse Direct Solver", and academics can get a license, just need to set the `$PARDISO_LIC_PATH` to where you download the license 


There might be some useful documentation on how R finds its matrix libraries.

See the Advanced R sections for more on how R finds its libraries. Ultimnately, after some permissioning issues, we have a working example.

### INLA Basics

`inla()` - this is the main function that is used from the package.

* `formula` - formula object
* `data` - the dataframe
* `family` - string or vector to

### INLA Examples (simulated)

#### Linear Regression

We first simulate some data, and plot the points that 

```{r}
# create the simulated data
N <- 100 # 500, 5000, 25000, 100000
x <- rnorm(N, mean = 6, sd = 2)
y <- rnorm(N, mean = x, sd = 1)
dat_simple <- list(x = x, y = y, N = N)
```

```{r}
# Basic fit of model, assuming gaussian link
mod <- inla(y ~ x,
  family = "gaussian",
  data = dat_simple,
  control.predictor = list(link = 1)
)
```

You can plot the posterior mean for each of the observations, but this plot is not particularly useful.

```{r}
# attributes(mod)
# ?plot.inla
# methods("inla")
plot(mod, plot.prior = TRUE, single=TRUE) # Plotting the posterior mean for fitted values, with index on axis.
```

In order to access the estimated coefficients and standard deviations of each parameter, we can use

```{r eval = FALSE}
mod$summary.fixed
mod$summary.random
mod$summary.fitted.values
```

```{r}
# Plot solution from INLA, with prediction bounds
g_inla_slr <- data.frame(dat_simple) %>% ggplot(aes(x, y)) +
  geom_point() +
  geom_ribbon(aes(y = mean, ymin = `0.025quant`, ymax=`0.975quant`),
              data = mod$summary.fitted.values,
              alpha = .1) +
  geom_smooth(aes(y = mean), formula = y~x, method="lm", data = mod$summary.fitted.values) +
  labs(title = "INLA")
  
# Plot solution from lm
g_smooth_slr <- data.frame(dat_simple) %>% ggplot(aes(x, y)) + 
  geom_point() + 
  geom_smooth(formula = y~x, method="lm") +
  labs(title = "LM")
  

g_inla_slr + g_smooth_slr
```


Clearly the INLA solution is very very similar to the one produced by LM as we expected.

### Laplace Approximation

Let's see an example with a closed form solution conjugate families. A good example of one is the Poisson-Gamma.

$$
\begin{aligned}
Y | \lambda &\sim Poisson(\lambda) \\
\lambda &\sim \Gamma(\alpha, \beta)
\end{aligned}
$$
Suppose we observe that $y = 6$, and we set a prior $\alpha=\beta=1$.

The update for poisson-gamma upon observing $y_1, y_2, y_3, \dots, y_n$, (note this is using the rate parameterization, $\beta^\alpha$ in the numerator)

$$
\begin{aligned}
\Gamma(\alpha, \beta) \longrightarrow \Gamma(\alpha + \sum_i y_i \, , \beta + n)
\end{aligned}
$$
Hence, the true posterior is $\Gamma(1+6, 1 +1)$, which has density,

$$
\begin{aligned}
p(\lambda | Y=6) = \frac{2^7}{\Gamma(2)}\exp(-2 \lambda)\lambda^{7 - 1}
\end{aligned}
$$
If we didn't know this, then we would be trying to approximate the integral in the denominator of bayes rule, we do this with Laplace's Approximation,

$$
\begin{aligned}
p(\lambda|y) &= \frac{f(y|\lambda) \xi(\lambda)}{\int f(y|\lambda)\xi(\lambda)\, d\lambda} \\
\end{aligned} 
$$
to approximate the bottom, we have

$$
\begin{aligned}
\int f(y|\lambda)\xi(\lambda)\, d\lambda \approx f(y|\lambda_0)\xi(\lambda_0)\sqrt{\frac{2\pi}{-h''(\lambda_0)}}
\end{aligned}
$$

where $h = \log \left[f(y|\lambda) \xi(\lambda)\right]$, and $h''(\lambda_0)$ is the second derivative evaluated at $\lambda_0$, the maximum of the integrand $f(y|\lambda) \xi(\lambda)$. Finding the maximum can be done with an optimization procedure, `optim` or it can be done analytically. 

Here's a dump of the calculations!

$$
\begin{aligned}
f(y=6|\lambda)\xi(\lambda) &= \frac{e^{-\lambda} \lambda^6}{6!}\exp(-\lambda) \\
h(\lambda) &= -\log 6! -2\lambda + 6 \log \lambda \\
h'(\lambda) &= \frac{6}{\lambda} -2
\end{aligned}
$$

Setting $h'(\lambda) = 0$, we find the maximum of the integrand to be $\lambda_0 = 3$

$$
\begin{aligned}
h''(\lambda) &= -\frac{6}{\lambda^2} \\
-h''(\lambda)^{-1} &= \frac{\lambda^2}{6}
\end{aligned}
$$
Evaluating the maximum, we get $-h''(\lambda_0)^{-1} = 1.5$. Now we're ready to code it up!

```{r}
# Poisson-Gamma Update
# gamma(a, b) -> gamma(a + \sum_i x_i, b + n)

# Let prior be gamma(1, 1)
# Observe y = 6
# True Posterior is gamma(1 + 6, 1 + 1)

lgrid <- seq(0, 10, .01)

plot(lgrid, dgamma(lgrid, 1, 1), type="l", xlab="lambda", ylab="density")
lines(lgrid,dgamma(lgrid, 7, 2), type="l", col=2)
legend("topright",legend =  c("Prior", "Posterior"),  col = c(1, 2),lty=1)
```


```{r}
est_mode <- (6) / 2 # (alpha + x) / beta
est_cov <- 1 / ((1 + 6 - 1) / est_mode^2) # (alpha + x - 1) / mode^2
est_cov

# integral approximation is
# likelihood * prior * constant based on curvurature of function
# The constant in the denominator of Bayes rule, also the integral approximation
Z <- dpois(6, lambda = est_mode) * dgamma(est_mode, 1, 1) * sqrt(2 * pi * est_cov)

posterior <- function(lambda) {
  dpois(6, lambda = lambda) * dgamma(lambda, 1, 1) / Z
}
```


```{r}
plot(lgrid, dgamma(lgrid, 1, 1), type="l", xlab="lambda", ylab="density",
     main = "Poisson-Gamma Example with Laplace Approximation")
lines(lgrid,dgamma(lgrid, 7, 2), type="l", col=2, lwd=3)
lines(lgrid, posterior(lgrid), type = "l", col=3)
lines(lgrid, dnorm(lgrid, mean=est_mode, sd = sqrt(est_cov)), type = "l", col=3, lty=2) # The approximating normal distribution used in laplace approximation, centered on posterior mean
legend("topright", legend = c("Prior", "True Posterior", "Posterior with Laplace approximated Integral", "Laplace Approximation"), 
       lty = c(1, 1, 1, 2),
       col = c(1, 2, 3, 3),
       cex = .8)
```


```{r eval=FALSE}
# A different way of visualizing the above information
plot(dgamma(0:10, 7, 2), posterior(0:10), pch=19, cex=.2, xlab = "True Posterior density, Gamma(7,2)",
     ylab = "posterior density by laplace approximation",
     main = "Comparison of True Posterior and Laplace Approximation")
abline(0,1, col=2)
legend("bottomright", col = c(1,2), pch = c(19,-1), lty=c(0, 1), legend = c("evaluated densities", "line of equality"))
```

Resources used when writing this section
- https://www.sumsar.net/blog/2013/11/easy-laplace-approximation/
- https://bookdown.org/rdpeng/advstatcomp/laplace-approximation.html
- https://en.wikipedia.org/wiki/Laplace%27s_method

### Resources for INLA

* [Worked Examples for Bayesian Analysis with INLA from Faraway](https://julianfaraway.github.io/brinla/examples/) - author of the "Extending the Linear Model" book, this site also includes many mixed effects model examples.



## STAN

STAN is sophisticated because it uses the No U-Turn Sampling (NUTS), and thus has a faster convergence.

- there's a "warm-up" rather than a "burn-in" for the sampling.
- faster for a complex model

### rstanarm

The functions offered in rstanarm, and quick explanation

- `stan_lm` - basic linear model
- `stan_aov` - calls `stan_lm` in the backend
- `stan_lmer` - calls

```{r}
data("weightgain", package = "HSAUR3")
# Standard Frequentist Method
fmod <- aov(weightgain ~ source * type, data = weightgain)


# Bayesian aov
bmod <- stan_aov(weightgain ~ source * type, data = weightgain,
         prior = R2(location = .5),
         adapt_delta = .999,
         seed = 1234)
summary(bmod)

# Bayesian lmer method

bmmod <- stan_lmer(weightgain ~ 1 + (1|source) + (1 | type) + (1 | source:type),
                   data = weightgain,
                   prior_intercept = cauchy(),
                   prior_covariance = decov(shape = 2, scale = 2),
                   adapt_delta = 0.999, seed = 12345)

tibble(freq = coef(fmod),
       bayes = coef(bmod))
```



### Resources for STAN

* [Faraway worked examples in STAN](https://people.bath.ac.uk/jjf23/stan/index.html) - largely examples from his books, and pretty basic analyses.

## JAGS

JAGS stands for "Just another Gibbs Sampler", and operates by MCMC.

### Worked Examples 

#### Linear Regression (simulated)

We'll run the same model that did for INLA, a SLR.

```{r}
# https://www.precision-analytics.ca/articles/a-gentle-inla-tutorial/
N <- 100 # 500, 5000, 25000, 100000
x <- rnorm(N, mean = 5, sd = 1)
nu <- rnorm(N, 0, 0.1)
mu <- exp(1 + 0.5 * x + nu)
y <- rpois(N, mu)
```

```{r}
# https://www.precision-analytics.ca/articles/a-gentle-inla-tutorial/

# Save BUGS style specification to .txt file
cat("model {
  for(i in 1:N) {
    y[i] ~ dnorm(mu[i], tau)
    mu[i] <- alpha + beta * x[i]
  }
  alpha ~ dnorm(0, 0.001)
  beta  ~ dnorm(0, 0.001)
  tau   ~ dgamma(0.01, 0.01)
}", file="jags_slr.txt")

# Initialize the Model 
jags_mod_slr <- jags.model(
  file = "jags_slr.txt",
  data = dat_simple,
  n.chains = 3,
)

# Run Jags and save posterior samples
params <- c("alpha", "beta", "tau")
samps <- coda.samples(jags_mod_slr, params, n.iter=1000)

summary(samps)
```


```{r}
plot(samps)
```

These are the mixing plots and show posterior sample draws, as well as the density of those draws. We can see that the mixing times, and such stabilize quite quickly. We can see in the trace three dotted lines, because we requested three chains.

## Cookbook

### Conditional Logistic Regression Model

[Conditional Logistic Regression Example with INLA](https://inla.r-inla-download.org/r-inla.org/doc/vignettes/conditional_logit.pdf)

### Conditional Sampling

[Conditional Sampling from INLA](https://inla.r-inla-download.org/r-inla.org/doc/vignettes/conditional-sampling.pdf)

## Additional Resources

### INLA Related (beginner)
- [Best introduction blog post of INLA](https://www.precision-analytics.ca/articles/a-gentle-inla-tutorial/) with some comparisons to MCMC methods
- [A whole book about INLA, introduction level](https://becarioprecario.bitbucket.io/inla-gitbook/index.html) with heavy emphasis on applied examples, and organized very intuitively for applied statisticians.
- [Miscellaneous list of INLA tutorials](https://haakonbakkagit.github.io/alltopics.html)

### Advanced (spatial, stochastic pde) Modeling with INLA

- [Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA](https://becarioprecario.bitbucket.io/spde-gitbook/)
- [Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny](https://www.paulamoraga.com/book-geospatial/sec-spatialdataandCRS.html)


Software Comparisons
- [Comparing NIMBLE, JAGS, Stan](https://arxiv.org/pdf/2107.09357.pdf)
  - upshot is to learn Stan for best balance of flexibility and speed.
