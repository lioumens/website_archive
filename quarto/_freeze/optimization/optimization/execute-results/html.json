{
  "hash": "2b16e3353c752767a0e958f03e584b0c",
  "result": {
    "markdown": "---\ntitle: \"Optimization\"\nauthor: \"Michael LIou\"\ndate: \"3/25/2022\"\nexecute:\n  cache: true\n---\n\n::: {.cell hash='optimization_cache/html/setup_8ceee31b88c88c1ae1ed3c0790b531f4'}\n\n```{.r .cell-code  code-summary=\"Libraries\"}\nlibrary(CVXR)\nlibrary(tidyverse)\nlibrary(Matrix)\nlibrary(DEoptim)\n```\n:::\n\n\n## Convex Optimization\n\nA convex optimization problem has the form:\n\n$$\n\\begin{aligned}\n\\min \\quad & f_0(x) \\\\\n\\text{s.t.} \\quad & f_i (x) \\leq 0\\qquad i = 1 \\dots n \\\\\n&a_i^Tx = b_i \\qquad i = 1 \\dots p\n\\end{aligned}\n$$\n\n1. objective function $f_0(x)$ is convex\n2. inequality constraints $f_i(x) \\leq 0$ are convex\n3. equality constraints are affine\n\nSome useful compositions that are preserve convexity\n\n* composition of convex w/ affine function\n  - $g(x) = f(Ax + b)$\n* weighted sums ($w_i \\geq 0$) of convex functions $f_i$\n  - $g(x) = w_1f_1(x) + w_2f_2(x) + \\dots w_nf_n(x)$\n  \nNotable counter examples, if $f_1, f_2$ are convex,\n\n* $f_1 - f_2$ may NOT be convex\n  - $0 - x^2$\n* $f_1 \\times f_2$ may NOT be convex\n  - $x * x^2$\n* $f_1 / f_2$ may NOT be convex\n  - $\\frac{x^{3/2}}{x}$\n\n### CVXR\n\nA program for principled convex optimization\n\n0. Define the variables\n- `Betahat <- Variable(p)` where `p` is the length of the vector\n1. Define objective function (`objective <- Minimize(...)`)\n- `Minimize(...)`\n- `Maximize(...)`\n2. Define the problem\n3. Solve\n\nUseful functions for debugging the program \n\n- `is_dcp(problem)` - check if problem follows the DCP rules\n- `is_dgp(problem)` - check if problem follows geometric programming rules\n\n\n::: {.cell hash='optimization_cache/html/unnamed-chunk-1_e4f02a3d9fefd821e3160faa76422d62'}\n\n```{.r .cell-code}\n# Tutorial\nset.seed(123)\n\nn <- 100\np <- 10\nbeta <- -4:5   # beta is just -4 through 5.\n\nX <- matrix(rnorm(n * p), nrow=n)\ncolnames(X) <- paste0(\"beta_\", beta)\nY <- X %*% beta + rnorm(n)\n```\n:::\n\n::: {.cell hash='optimization_cache/html/unnamed-chunk-2_79dae0b9ab8dbb8970a0afff3cb4561b'}\n\n```{.r .cell-code}\nbetahat <- Variable(p)\nobjective <- Minimize(sum((Y-X %*% betahat)^2))\nproblem <- Problem(objective, \n                   constraints = list(betahat >= 0)) # list constraints, betahat >= 0 \nresult <- solve(problem)\n```\n:::\n\n::: {.cell hash='optimization_cache/html/unnamed-chunk-3_954fefb7a543c31b060bde91e39eca0c'}\n\n```{.r .cell-code}\nresult$getValue(betahat) %>% zapsmall()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]\n [1,] 0.000000\n [2,] 0.000000\n [3,] 0.000000\n [4,] 0.000000\n [5,] 1.237449\n [6,] 0.623466\n [7,] 2.123066\n [8,] 2.803564\n [9,] 4.444802\n[10,] 5.207352\n```\n:::\n:::\n\n\n\n#### Examples\n\n::: {.panel-tabset}\n\n##### Largest Eigenvalue\n\nLet $A \\in \\mathbb{R}^{pxp}$ be a symmetric, nonegative matrix. Then our target value is: \n\n\n::: {.cell hash='optimization_cache/html/unnamed-chunk-4_c5ec9a49b9dc20b4c4a15364c2c4cee9'}\n\n```{.r .cell-code}\nset.seed(1)\n# Symmetric Matrix\np <- 10 # matrix is p x p\nA <- matrix(abs(rnorm(p^2)), ncol = p) # non negative matrix\nsA <- forceSymmetric(A, uplo = \"U\") %>% as.matrix()\n\nsA_eig <- eigen(sA)\nsA_eig$values[1] # largest eigenvalue\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7.649905\n```\n:::\n:::\n\n\nWe show 3 ways to use CVXR to solve for the largest eigenvalue, starting with the simplest\n\n1. objective is pf_eigenvalue\n2. perron frobenius, geometric programming\n3. perron frobenius, convex programming\n\n\n\n::: {.cell hash='optimization_cache/html/unnamed-chunk-5_4ef49c42f3264b5d7f0b2c0e46845e71'}\n\n```{.r .cell-code}\n# w/ CVXR\n# using objective \nX <- Variable(p,p, pos = TRUE)\nobjective <- Minimize(pf_eigenvalue(X))\nconstraints <- list(X == sA)\nproblem <- Problem(objective, constraints)\nresults <- solve(problem, gp = TRUE)\n\nc(results$value, sA_eig$values[1])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7.649905 7.649905\n```\n:::\n:::\n\n::: {.cell hash='optimization_cache/html/unnamed-chunk-6_e88c0ce31eab3da87ce95a89adf81cf1'}\n\n```{.r .cell-code}\n# w/ CVXR\n# spectral radius w/ geometric rules, A is known\n\nlambda <- Variable(1, name = \"lambda\", pos = TRUE)\nu <- Variable(p, pos = TRUE)\nobjective <- Minimize(lambda)\nconstraints <- list()\nfor (i in 1:p) {\n  constraints <- c(constraints, sA[i,, drop = F] %*% u / (lambda * u[i]) <= 1)\n}\nproblem <- Problem(objective,constraints)\nresults <- solve(problem, gp = TRUE)\n\nc(results$getValue(lambda), sA_eig$values[1]) # the same\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7.649905 7.649905\n```\n:::\n:::\n\n\n\nIn order to transform the geometric program of minimizing largest eigenvalue, we note that the inequalities can be written\n\n$$\n\\begin{aligned}\n\\frac{\\sum_j A_{ij}u_j}{\\lambda u_i} &\\leq 1 \\qquad \\text{for } i = 1 \\dots n \\\\\n\\log(\\sum_j \\exp(u_j + \\log(A_{ij}) - \\log(\\lambda) - u_i)) & \\leq 0\n\\end{aligned}\n$$\n\n\n::: {.cell hash='optimization_cache/html/unnamed-chunk-7_0217f2cbdcb679007d1eee32cbe918cd'}\n\n```{.r .cell-code}\n# w/ CVXR\n# manually transforming geometric to convex program\nlambda <- Variable(1, name = \"lambda\", pos = TRUE)\nu <- Variable(p, pos = TRUE)\nobjective <- Minimize(lambda)\nconstraints <- list()\nfor ( i in 1:p) {\n constraints <- c(constraints, \n                  log_sum_exp(u + log(sA[i,]) - log(lambda) - u[i]) <= 0)\n}\nproblem <- Problem(objective, constraints)\nresults <- solve(problem)\n# sum(vec + vec - scalar - scalar)\n# curvature(log_sum_exp(u + log(sA[1,]) - log(lambda) - u[1]))\n\nc(results$getValue(lambda), sA_eig$values[1]) # the same! nice!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7.649905 7.649905\n```\n:::\n:::\n\n\n\n##### Minimize $\\lambda_1$\n\nIn addition to just _finding_ the maximum eigenvalue (spectral radius) of a fixed matrix, we can also let the matrix be an optimization variable and minimize the spectral radius of the matrix! Thus, if entries of the matrix $A(x)$ are a function of $x$, and we have some constraints $f(x) \\leq 1$, we can figure out those entries.\n\n:::\n\n## DEoptim\n\nDifferential Evolution Optimization, this is a derivative free way of optimization, and seems to work quite well, at least for rosenbrock function!\n\n\n### Examples\n\n#### First Use\n\n\n\n::: {.cell hash='optimization_cache/html/unnamed-chunk-8_9af7292a7e0994d079e54c1703b46427'}\n\n```{.r .cell-code}\n# Rosenbrock Function for optimization\n# global minimum f(x) = 0, at x = (1, 1).\nRosenbrock <- function(x){\n    x1 <- x[1]\n    x2 <- x[2]\n    100 * (x2 - x1^2)^2 + (1 - x1)^2\n}\n\n# specify the grid for replicability\nlower <- c(-10,-10)\nupper <- -lower\n\n# Optimize over space\nset.seed(1234)\nDEoptim(Rosenbrock, lower, upper) # basic invocation\n\n# invocation with control aspects\noutDEoptim <- DEoptim(Rosenbrock,\n                      lower,\n                      upper,\n                      DEoptim.control(NP = 80,\n                                      itermax = 400, # number of procedure iterations\n                                      F = 1.2,\n                                      CR = 0.7)) \n```\n:::\n\n::: {.cell hash='optimization_cache/html/unnamed-chunk-9_01f52bf1bc18663e191ae968f5eae531'}\n\n```{.r .cell-code}\noutDEoptim$optim$bestmem # the final parameters\noutDEoptim$optim$bestval # the final value\n\nsprintf(\"The found parameters are %.2f %.2f\\n The minimum objective is: %.2f\\n DEoptim did %.2f iterations.\", \n  outDEoptim$optim$bestmem[[1]], outDEoptim$optim$bestmem[[2]], # parameters\n  outDEoptim$optim$bestval, # final value\n  outDEoptim$optim$iter) # final value\n\n\nplot(outDEoptim)\n```\n\n::: {.cell-output-display}\n![](optimization_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](optimization_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\npar1 par2 \n   1    1 \n[1] 3.78756e-25\n[1] \"The found parameters are 1.00 1.00\\n The minimum objective is: 0.00\\n DEoptim did 400.00 iterations.\"\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}