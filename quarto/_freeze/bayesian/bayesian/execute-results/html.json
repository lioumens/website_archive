{
  "hash": "831ac434a64df36af2b8013895ba00fc",
  "result": {
    "markdown": "---\ntitle: \"Bayesian\"\nauthor: \"Michael Liou\"\ndate: \"12/6/2021\"\nexecute:\n  cache: true\n---\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-1_d5cd9b441e0ec30a9776cf5564235536'}\n\n```{.r .cell-code}\nlibrary(INLA)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: sp\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: parallel\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThis is INLA_19.09.03 built 2022-05-17 20:58:09 UTC.\nSee www.r-inla.org/contact-us for how to get help.\nTo enable PARDISO sparse library; see inla.pardiso()\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n✔ ggplot2 3.3.6     ✔ purrr   0.3.4\n✔ tibble  3.1.8     ✔ dplyr   1.0.9\n✔ tidyr   1.2.0     ✔ stringr 1.4.0\n✔ readr   2.1.2     ✔ forcats 0.5.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::expand() masks Matrix::expand()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tidyr::pack()   masks Matrix::pack()\n✖ tidyr::unpack() masks Matrix::unpack()\n```\n:::\n\n```{.r .cell-code}\nlibrary(patchwork)\nlibrary(rjags)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: coda\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLinked to JAGS 4.3.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded modules: basemod,bugs\n```\n:::\n\n```{.r .cell-code}\nlibrary(rstanarm) # regression modeling with rstan backend\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Rcpp\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThis is rstanarm version 2.21.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n  options(mc.cores = parallel::detectCores())\n```\n:::\n\n```{.r .cell-code}\nlibrary(sjPlot)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nInstall package \"strengejacke\" from GitHub (`devtools::install_github(\"strengejacke/strengejacke\")`) to load all sj-packages at once!\n```\n:::\n\n```{.r .cell-code}\nlibrary(effects)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: carData\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n```\n:::\n\n```{.r .cell-code}\nlibrary(sjPlot)\nlibrary(ggeffects)\nlibrary(rstanarm)\n```\n:::\n\n\n## Introduction to Bayesian Modeling\n\n### Software Ecosystem\n\nThe software mostly is split up by the methodology that each method uses. Seems like results will vary, and syntax also will vary. Trade-offs on speed and use cases.\n\n#### R Software\n\n* **INLA** - integrated nested laplace approximations. Uses R formula syntax\n  - not on CRAN because it uses a C library that is machine dependent\n  - option to use pardiso optimizer, for which you need a license.\n* **BUGS** - MCMC based sampling\n  - has many different versions, seems easier to install on windows\n* **JAGS** \n  - the original MCMC based solutions, has an R implementation with \"rjags\"\n  - Stands for \"Just Another Gibbs Sampler\"\n  - text file specification of the model\n  - reimplementation of \"BUGS\"\n* **rstan** - does some magical Hamiltonion Monte Carlo, which supp\n  - **brms** - uses STAN as backend, but allows linear and nonlinear models with R formula syntax. Offers [more flexibility than rstanarm](https://m-clark.github.io/easy-bayes/comparison-to-rstanarm.html)\n  - **rstanarm** - meant as a drop in replacement using STAN for many base R and lme4 models. The main difference is that these models are precompiled in STAN code, while brms will compile everytime into new STAN code, thus will be slower for basic models. However, what you trade in slowness you get flexibility.\n  - **blavaan** - latent variable modeling, like SEM modeling, latent growth curve models, confirmatory factor analysis.\n\n## INLA\n\nBroadly speaking, INLA is for a broad class of models used for Guassian markov random fields (GMRF). INLA tends to be faster than other methods, because the software is based on Laplace Approximation. Because of that, it is also a deterministic method unlike stochastic MCMC based software like STAN or BUGS/JAGS\n\n### Installation Issues\n\nThe [standard installation as recommended by the website](https://www.r-inla.org/download-install) is:\n\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-2_9345568fdc60bd98cd3444b5bc24b1d5'}\n\n```{.r .cell-code}\ninstall.packages(\"INLA\",repos=c(getOption(\"repos\"),INLA=\"https://inla.r-inla-download.org/R/testing\"), dep=TRUE) # R > 4.1\n```\n:::\n\n\nBut I keep getting an error:\n\n> Error in inla.call.builtin() : INLA installation error; no such file \n\nso i tried installing directly from github with devtools.\n\n> /usr/local/lib/R/4.1/site-library/INLA/bin/mac/64bit/inla.run: line 125: /usr/local/lib/R/4.1/site-library/INLA/bin/mac/64bit/inla: No such file or directory\n\nThe \"devel\" version unfortunately gives the following error, so trying the stable version.\n\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-3_eb46a062e208d823b3e29def22cc76c3'}\n\n```{.r .cell-code}\ndevtools::install_github(repo = \"https://github.com/hrue/r-inla\", ref = \"stable\", subdir = \"rinla\", build = FALSE)\n```\n:::\n\nThe stable version also didn't work, was giving some dynamic library errors, finally tried installing manually and it seemed to work. [INLA Binaries for R 4.1](http://inla.r-inla-download.org/R/testing/bin/macosx/contrib/4.1/). [The thread that gave me idea of installing manually](https://groups.google.com/g/r-inla-discussion-group/c/X7VsO41DnGA/m/TSMD3NQeCAAJ).\n\nIn order to install it manually, \n\n1. Download the precompiled binary here: https://inla.r-inla-download.org/R/testing/bin/macosx/contrib/4.1/\n  - back up directories if you need a different binary\n2. From R, run `install.packages(\"pathtobinary.tgz\", repos = NULL)`\n\n#### Pardiso License\n\nPardiso is parallel computing library, stands for \"Parallel Sparse Direct Solver\", and academics can get a license, just need to set the `$PARDISO_LIC_PATH` to where you download the license \n\n\nThere might be some useful documentation on how R finds its matrix libraries.\n\nSee the Advanced R sections for more on how R finds its libraries. Ultimnately, after some permissioning issues, we have a working example.\n\n### INLA Basics\n\n`inla()` - this is the main function that is used from the package.\n\n* `formula` - formula object\n* `data` - the dataframe\n* `family` - string or vector to\n\n### INLA Examples (simulated)\n\n#### Linear Regression\n\nWe first simulate some data, and plot the points that \n\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-4_0aff2a687a456ab65f867f2860b6eab0'}\n\n```{.r .cell-code}\n# create the simulated data\nN <- 100 # 500, 5000, 25000, 100000\nx <- rnorm(N, mean = 6, sd = 2)\ny <- rnorm(N, mean = x, sd = 1)\ndat_simple <- list(x = x, y = y, N = N)\n```\n:::\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-5_3e30d4591859452c30c300c57d38800a'}\n\n```{.r .cell-code}\n# Basic fit of model, assuming gaussian link\nmod <- inla(y ~ x,\n  family = \"gaussian\",\n  data = dat_simple,\n  control.predictor = list(link = 1)\n)\n```\n:::\n\n\nYou can plot the posterior mean for each of the observations, but this plot is not particularly useful.\n\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-6_1f9ba5d6e9f94c14b93a529cb3909c09'}\n\n```{.r .cell-code}\n# attributes(mod)\n# ?plot.inla\n# methods(\"inla\")\nplot(mod, plot.prior = TRUE, single=TRUE) # Plotting the posterior mean for fitted values, with index on axis.\n```\n:::\n\n\nIn order to access the estimated coefficients and standard deviations of each parameter, we can use\n\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-7_078f813eff79b44d47fab97e68fd0647'}\n\n```{.r .cell-code}\nmod$summary.fixed\nmod$summary.random\nmod$summary.fitted.values\n```\n:::\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-8_e7e89224ee3ff0a93ac911bd748354b9'}\n\n```{.r .cell-code}\n# Plot solution from INLA, with prediction bounds\ng_inla_slr <- data.frame(dat_simple) %>% ggplot(aes(x, y)) +\n  geom_point() +\n  geom_ribbon(aes(y = mean, ymin = `0.025quant`, ymax=`0.975quant`),\n              data = mod$summary.fitted.values,\n              alpha = .1) +\n  geom_smooth(aes(y = mean), formula = y~x, method=\"lm\", data = mod$summary.fitted.values) +\n  labs(title = \"INLA\")\n  \n# Plot solution from lm\ng_smooth_slr <- data.frame(dat_simple) %>% ggplot(aes(x, y)) + \n  geom_point() + \n  geom_smooth(formula = y~x, method=\"lm\") +\n  labs(title = \"LM\")\n  \n\ng_inla_slr + g_smooth_slr\n```\n\n::: {.cell-output-display}\n![](bayesian_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nClearly the INLA solution is very very similar to the one produced by LM as we expected.\n\n### Laplace Approximation\n\nLet's see an example with a closed form solution conjugate families. A good example of one is the Poisson-Gamma.\n\n$$\n\\begin{aligned}\nY | \\lambda &\\sim Poisson(\\lambda) \\\\\n\\lambda &\\sim \\Gamma(\\alpha, \\beta)\n\\end{aligned}\n$$\nSuppose we observe that $y = 6$, and we set a prior $\\alpha=\\beta=1$.\n\nThe update for poisson-gamma upon observing $y_1, y_2, y_3, \\dots, y_n$, (note this is using the rate parameterization, $\\beta^\\alpha$ in the numerator)\n\n$$\n\\begin{aligned}\n\\Gamma(\\alpha, \\beta) \\longrightarrow \\Gamma(\\alpha + \\sum_i y_i \\, , \\beta + n)\n\\end{aligned}\n$$\nHence, the true posterior is $\\Gamma(1+6, 1 +1)$, which has density,\n\n$$\n\\begin{aligned}\np(\\lambda | Y=6) = \\frac{2^7}{\\Gamma(2)}\\exp(-2 \\lambda)\\lambda^{7 - 1}\n\\end{aligned}\n$$\nIf we didn't know this, then we would be trying to approximate the integral in the denominator of bayes rule, we do this with Laplace's Approximation,\n\n$$\n\\begin{aligned}\np(\\lambda|y) &= \\frac{f(y|\\lambda) \\xi(\\lambda)}{\\int f(y|\\lambda)\\xi(\\lambda)\\, d\\lambda} \\\\\n\\end{aligned} \n$$\nto approximate the bottom, we have\n\n$$\n\\begin{aligned}\n\\int f(y|\\lambda)\\xi(\\lambda)\\, d\\lambda \\approx f(y|\\lambda_0)\\xi(\\lambda_0)\\sqrt{\\frac{2\\pi}{-h''(\\lambda_0)}}\n\\end{aligned}\n$$\n\nwhere $h = \\log \\left[f(y|\\lambda) \\xi(\\lambda)\\right]$, and $h''(\\lambda_0)$ is the second derivative evaluated at $\\lambda_0$, the maximum of the integrand $f(y|\\lambda) \\xi(\\lambda)$. Finding the maximum can be done with an optimization procedure, `optim` or it can be done analytically. \n\nHere's a dump of the calculations!\n\n$$\n\\begin{aligned}\nf(y=6|\\lambda)\\xi(\\lambda) &= \\frac{e^{-\\lambda} \\lambda^6}{6!}\\exp(-\\lambda) \\\\\nh(\\lambda) &= -\\log 6! -2\\lambda + 6 \\log \\lambda \\\\\nh'(\\lambda) &= \\frac{6}{\\lambda} -2\n\\end{aligned}\n$$\n\nSetting $h'(\\lambda) = 0$, we find the maximum of the integrand to be $\\lambda_0 = 3$\n\n$$\n\\begin{aligned}\nh''(\\lambda) &= -\\frac{6}{\\lambda^2} \\\\\n-h''(\\lambda)^{-1} &= \\frac{\\lambda^2}{6}\n\\end{aligned}\n$$\nEvaluating the maximum, we get $-h''(\\lambda_0)^{-1} = 1.5$. Now we're ready to code it up!\n\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-9_4bd018b3ce2255d98d8b64dc0eda199d'}\n\n```{.r .cell-code}\n# Poisson-Gamma Update\n# gamma(a, b) -> gamma(a + \\sum_i x_i, b + n)\n\n# Let prior be gamma(1, 1)\n# Observe y = 6\n# True Posterior is gamma(1 + 6, 1 + 1)\n\nlgrid <- seq(0, 10, .01)\n\nplot(lgrid, dgamma(lgrid, 1, 1), type=\"l\", xlab=\"lambda\", ylab=\"density\")\nlines(lgrid,dgamma(lgrid, 7, 2), type=\"l\", col=2)\nlegend(\"topright\",legend =  c(\"Prior\", \"Posterior\"),  col = c(1, 2),lty=1)\n```\n\n::: {.cell-output-display}\n![](bayesian_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-10_493265a12b105df35e936594225f1bf2'}\n\n```{.r .cell-code}\nest_mode <- (6) / 2 # (alpha + x) / beta\nest_cov <- 1 / ((1 + 6 - 1) / est_mode^2) # (alpha + x - 1) / mode^2\nest_cov\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.5\n```\n:::\n\n```{.r .cell-code}\n# integral approximation is\n# likelihood * prior * constant based on curvurature of function\n# The constant in the denominator of Bayes rule, also the integral approximation\nZ <- dpois(6, lambda = est_mode) * dgamma(est_mode, 1, 1) * sqrt(2 * pi * est_cov)\n\nposterior <- function(lambda) {\n  dpois(6, lambda = lambda) * dgamma(lambda, 1, 1) / Z\n}\n```\n:::\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-11_a2e38d08219181193ea4a9b9a33bc390'}\n\n```{.r .cell-code}\nplot(lgrid, dgamma(lgrid, 1, 1), type=\"l\", xlab=\"lambda\", ylab=\"density\",\n     main = \"Poisson-Gamma Example with Laplace Approximation\")\nlines(lgrid,dgamma(lgrid, 7, 2), type=\"l\", col=2, lwd=3)\nlines(lgrid, posterior(lgrid), type = \"l\", col=3)\nlines(lgrid, dnorm(lgrid, mean=est_mode, sd = sqrt(est_cov)), type = \"l\", col=3, lty=2) # The approximating normal distribution used in laplace approximation, centered on posterior mean\nlegend(\"topright\", legend = c(\"Prior\", \"True Posterior\", \"Posterior with Laplace approximated Integral\", \"Laplace Approximation\"), \n       lty = c(1, 1, 1, 2),\n       col = c(1, 2, 3, 3),\n       cex = .8)\n```\n\n::: {.cell-output-display}\n![](bayesian_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-12_63b08ffe14dfabc3ec4437062a054bc8'}\n\n```{.r .cell-code}\n# A different way of visualizing the above information\nplot(dgamma(0:10, 7, 2), posterior(0:10), pch=19, cex=.2, xlab = \"True Posterior density, Gamma(7,2)\",\n     ylab = \"posterior density by laplace approximation\",\n     main = \"Comparison of True Posterior and Laplace Approximation\")\nabline(0,1, col=2)\nlegend(\"bottomright\", col = c(1,2), pch = c(19,-1), lty=c(0, 1), legend = c(\"evaluated densities\", \"line of equality\"))\n```\n:::\n\n\nResources used when writing this section\n- https://www.sumsar.net/blog/2013/11/easy-laplace-approximation/\n- https://bookdown.org/rdpeng/advstatcomp/laplace-approximation.html\n- https://en.wikipedia.org/wiki/Laplace%27s_method\n\n### Resources for INLA\n\n* [Worked Examples for Bayesian Analysis with INLA from Faraway](https://julianfaraway.github.io/brinla/examples/) - author of the \"Extending the Linear Model\" book, this site also includes many mixed effects model examples.\n\n\n\n## STAN\n\nSTAN is sophisticated because it uses the No U-Turn Sampling (NUTS), and thus has a faster convergence.\n\n- there's a \"warm-up\" rather than a \"burn-in\" for the sampling.\n- faster for a complex model\n\n### rstanarm\n\nThe functions offered in rstanarm, and quick explanation\n\n- `stan_lm` - basic linear model\n- `stan_aov` - calls `stan_lm` in the backend\n- `stan_lmer` - calls\n\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-13_7dbeb5af85ea7c7d8f6c0fb2576a8caf'}\n\n```{.r .cell-code}\ndata(\"weightgain\", package = \"HSAUR3\")\n# Standard Frequentist Method\nfmod <- aov(weightgain ~ source * type, data = weightgain)\n\n\n# Bayesian aov\nbmod <- stan_aov(weightgain ~ source * type, data = weightgain,\n         prior = R2(location = .5),\n         adapt_delta = .999,\n         seed = 1234)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'lm' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.55 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.286155 seconds (Warm-up)\nChain 1:                0.22461 seconds (Sampling)\nChain 1:                0.510765 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'lm' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 9e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.276645 seconds (Warm-up)\nChain 2:                0.263396 seconds (Sampling)\nChain 2:                0.540041 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'lm' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.322498 seconds (Warm-up)\nChain 3:                0.182252 seconds (Sampling)\nChain 3:                0.50475 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'lm' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 9e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.30836 seconds (Warm-up)\nChain 4:                0.199297 seconds (Sampling)\nChain 4:                0.507657 seconds (Total)\nChain 4: \n```\n:::\n\n```{.r .cell-code}\nsummary(bmod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nModel Info:\n function:     stan_aov\n family:       gaussian [identity]\n formula:      weightgain ~ source * type\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 40\n\nEstimates:\n                       mean   sd    10%   50%   90%\n(Intercept)           98.7    4.4  93.1  98.7 104.4\nsourceCereal         -12.7    6.2 -20.6 -12.7  -4.8\ntypeLow              -18.7    6.2 -26.4 -18.8 -10.4\nsourceCereal:typeLow  16.9    8.8   5.4  17.1  27.8\nsigma                 14.8    1.8  12.8  14.7  17.2\nlog-fit_ratio          0.0    0.1  -0.1   0.0   0.2\nR2                     0.2    0.1   0.1   0.2   0.3\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 87.3    3.4 83.0  87.2  91.6 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                     mcse Rhat n_eff\n(Intercept)          0.1  1.0  1492 \nsourceCereal         0.1  1.0  1853 \ntypeLow              0.2  1.0  1422 \nsourceCereal:typeLow 0.2  1.0  2093 \nsigma                0.0  1.0  2533 \nlog-fit_ratio        0.0  1.0  2240 \nR2                   0.0  1.0  1733 \nmean_PPD             0.1  1.0  4009 \nlog-posterior        0.1  1.0   850 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n```\n:::\n\n```{.r .cell-code}\n# Bayesian lmer method\n\nbmmod <- stan_lmer(weightgain ~ 1 + (1|source) + (1 | type) + (1 | source:type),\n                   data = weightgain,\n                   prior_intercept = cauchy(),\n                   prior_covariance = decov(shape = 2, scale = 2),\n                   adapt_delta = 0.999, seed = 12345)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 7.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.78 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 6.87361 seconds (Warm-up)\nChain 1:                5.0979 seconds (Sampling)\nChain 1:                11.9715 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 6.09438 seconds (Warm-up)\nChain 2:                5.36233 seconds (Sampling)\nChain 2:                11.4567 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2.8e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 7.68205 seconds (Warm-up)\nChain 3:                7.34715 seconds (Sampling)\nChain 3:                15.0292 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 6.01372 seconds (Warm-up)\nChain 4:                7.2315 seconds (Sampling)\nChain 4:                13.2452 seconds (Total)\nChain 4: \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n```\n:::\n\n```{.r .cell-code}\ntibble(freq = coef(fmod),\n       bayes = coef(bmod))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 2\n   freq bayes\n  <dbl> <dbl>\n1 100    98.7\n2 -14.1 -12.7\n3 -20.8 -18.8\n4  18.8  17.1\n```\n:::\n:::\n\n\n\n\n### Resources for STAN\n\n* [Faraway worked examples in STAN](https://people.bath.ac.uk/jjf23/stan/index.html) - largely examples from his books, and pretty basic analyses.\n\n## JAGS\n\nJAGS stands for \"Just another Gibbs Sampler\", and operates by MCMC.\n\n### Worked Examples \n\n#### Linear Regression (simulated)\n\nWe'll run the same model that did for INLA, a SLR.\n\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-14_464ebc0b00d05d55a9d6c51c5d40dc26'}\n\n```{.r .cell-code}\n# https://www.precision-analytics.ca/articles/a-gentle-inla-tutorial/\nN <- 100 # 500, 5000, 25000, 100000\nx <- rnorm(N, mean = 5, sd = 1)\nnu <- rnorm(N, 0, 0.1)\nmu <- exp(1 + 0.5 * x + nu)\ny <- rpois(N, mu)\n```\n:::\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-15_288726eecf218bc7e91f1b576ab1f103'}\n\n```{.r .cell-code}\n# https://www.precision-analytics.ca/articles/a-gentle-inla-tutorial/\n\n# Save BUGS style specification to .txt file\ncat(\"model {\n  for(i in 1:N) {\n    y[i] ~ dnorm(mu[i], tau)\n    mu[i] <- alpha + beta * x[i]\n  }\n  alpha ~ dnorm(0, 0.001)\n  beta  ~ dnorm(0, 0.001)\n  tau   ~ dgamma(0.01, 0.01)\n}\", file=\"jags_slr.txt\")\n\n# Initialize the Model \njags_mod_slr <- jags.model(\n  file = \"jags_slr.txt\",\n  data = dat_simple,\n  n.chains = 3,\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 100\n   Unobserved stochastic nodes: 3\n   Total graph size: 407\n\nInitializing model\n```\n:::\n\n```{.r .cell-code}\n# Run Jags and save posterior samples\nparams <- c(\"alpha\", \"beta\", \"tau\")\nsamps <- coda.samples(jags_mod_slr, params, n.iter=1000)\n\nsummary(samps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nIterations = 1:1000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 1000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean      SD Naive SE Time-series SE\nalpha 0.2382 0.50615 0.009241       0.039769\nbeta  0.9858 0.08229 0.001502       0.006511\ntau   0.9590 0.14776 0.002698       0.004212\n\n2. Quantiles for each variable:\n\n         2.5%      25%    50%    75%  97.5%\nalpha -0.4555 -0.02924 0.2080 0.4334 0.9664\nbeta   0.8656  0.95217 0.9898 1.0312 1.1020\ntau    0.6885  0.86472 0.9578 1.0522 1.2467\n```\n:::\n:::\n\n::: {.cell hash='bayesian_cache/html/unnamed-chunk-16_ad5c78264058677a77c9edfa0167480d'}\n\n```{.r .cell-code}\nplot(samps)\n```\n\n::: {.cell-output-display}\n![](bayesian_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nThese are the mixing plots and show posterior sample draws, as well as the density of those draws. We can see that the mixing times, and such stabilize quite quickly. We can see in the trace three dotted lines, because we requested three chains.\n\n## Cookbook\n\n### Conditional Logistic Regression Model\n\n[Conditional Logistic Regression Example with INLA](https://inla.r-inla-download.org/r-inla.org/doc/vignettes/conditional_logit.pdf)\n\n### Conditional Sampling\n\n[Conditional Sampling from INLA](https://inla.r-inla-download.org/r-inla.org/doc/vignettes/conditional-sampling.pdf)\n\n## Additional Resources\n\n### INLA Related (beginner)\n- [Best introduction blog post of INLA](https://www.precision-analytics.ca/articles/a-gentle-inla-tutorial/) with some comparisons to MCMC methods\n- [A whole book about INLA, introduction level](https://becarioprecario.bitbucket.io/inla-gitbook/index.html) with heavy emphasis on applied examples, and organized very intuitively for applied statisticians.\n- [Miscellaneous list of INLA tutorials](https://haakonbakkagit.github.io/alltopics.html)\n\n### Advanced (spatial, stochastic pde) Modeling with INLA\n\n- [Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA](https://becarioprecario.bitbucket.io/spde-gitbook/)\n- [Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny](https://www.paulamoraga.com/book-geospatial/sec-spatialdataandCRS.html)\n\n\nSoftware Comparisons\n- [Comparing NIMBLE, JAGS, Stan](https://arxiv.org/pdf/2107.09357.pdf)\n  - upshot is to learn Stan for best balance of flexibility and speed.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}