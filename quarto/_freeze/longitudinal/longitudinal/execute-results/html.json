{
  "hash": "ccdbb4f1a99a86b4aa5e715b87a96308",
  "result": {
    "markdown": "---\ntitle: \"Longitudinal Data Analysis\"\nauthor: \"Michael Liou\"\nbibliography: longitudinal.bib\ndate: \"2/21/2021\"\nexecute:\n  cache: true\n---\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-1_7b3e4fcf70130e30e557f2cf41430e5a'}\n\n```{.r .cell-code  code-summary=\"Libraries\"}\nlibrary(plm)\nlibrary(car)\nlibrary(skimr)\nlibrary(sloop) # For object sloothing\nlibrary(lme4) # mixed\nlibrary(nlme)\nlibrary(emmeans)\nlibrary(faraway)\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(here)\nlibrary(kableExtra)\nlibrary(foreign)\nlibrary(lmeInfo) # for information matrices of lme models\nlibrary(broom.mixed)\nlibrary(ProfileLikelihood)\nlibrary(sandwich)\nlibrary(lmtest)\n```\n:::\n\n\n# Why Longitudinal Data Analysis\n\nMore information about the change, at the trade-off of needing to take repeated measurments.\n\n## Specifics of LDA\n\n-   Replication is a series of observations (each subject) and not individual measurements. Thus the EU is the subject.\n\n## Approaches in LDA\n\n-   Marginal (averages over population. state employee perspective)\n-   (Random effects) Mixed Models (doctor perspective, subject specifics)\n-   Transition Model (Ware et al, 1988)\n\n## Historical Methods\n\n### Split plot in time\n\n-   Induces a compound symmetric structure with observations\n\n### Repeated Measures ANOVA\n\nI think this is the same as split plot in time... The model is\n\n$$\nY_{ij} = X_{ij}'\\beta + b_i + e_{ij}\n$$ The covariance structure is *compound symmetry*, meaning\n\n$$\n\\begin{aligned}\n\\text{Cov}(Y_{i}) = \n\\begin{bmatrix}\n\\sigma_b + \\sigma_e & \\sigma_e & \\dots &\\sigma_e \\\\\n \\sigma_e & \\sigma_b + \\sigma_e & \\dots &\\sigma_e \\\\\n \\vdots & \\vdots & \\ddots & \\vdots \\\\\n \\sigma_e &  \\sigma_e & \\dots & \\sigma_b + \\sigma_e \\\\\n\\end{bmatrix}\n\\end{aligned}\n$$\n\n### MANOVA\n\nSpecial case of the so called *profile analysis*. The main idea for MANOVA is to make some trasnformations and make some derived response varaibles to analyze. The multiple comes from having multiple responses. One is to make the sum of all the responses, to examine average over time. You could also create a variable for linear change across time, or quadratic change within subject.\n\nThere are some disadvantages though,\n\n-   If design is unbalanced across time, MANOVA can't be used.\n-   Also if there are any missing data, the entire case must be thrown out.\n\n### Summary values\n\nReduce the sequence of each individual to a small set of summary values. Then, you can use the classic t-test or ANOVA, univariate tests.\n\n-   Area under curve (AUC) - Can only be used when the people have the same time measurements.\n\nDrawbacks\n\n-   forces data analyst to think about one aspect of the repeated measures.\n-   Might have the same summary measure but different response profile.\n-   Method can't be applied if one of the covariates is time varying. b/c variance will not be constant from summary to summary.\n\n## Inference on parameters\n\nLikelihood test requires an additional fit on the null hypothesis, but better properties. Recommend Likelihood ratio based tests and CI. Note the RMLE is a correction for the data estimating both the mean and covariance.\n\n# The setup\n\nThe short hand notation for the observations of a single individual are:\n\n$$\n\\begin{aligned}\nY_{i} = X_{i}\\beta + \\varepsilon_{i}\n\\end{aligned}\n$$ with $\\varepsilon_i \\sim N(0, \\Sigma)$, which expanded would look like,\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\ny_{i1} \\\\\ny_{i2} \\\\\n\\vdots \\\\\ny_{in_{i}} \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\nX_{i11} & X_{i12} & \\dots  &X_{i1p}  \\\\\nX_{i21} &  X_{i22} & \\dots  &X_{i2p}  \\\\\n\\vdots &  \\vdots & \\ddots  & \\vdots  \\\\\nX_{in_{i}1} &  X_{in_{i}2} & \\dots  & X_{in_{i}p}  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} + \n\\begin{bmatrix}\n\\varepsilon_{i1} \\\\\n\\varepsilon_{i2} \\\\\n\\vdots \\\\\n\\varepsilon_{in_{i}}\n\\end{bmatrix}\n\\end{aligned} \n$$\n\nIn total, we would stack them, so we have\n\n$$\n\\begin{aligned}\n\\begin{bmatrix}\nY_1 \\\\\nY_2 \\\\\n\\vdots \\\\\nY_N \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\n\\vdots \\\\\nX_N \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} + \n\\begin{bmatrix}\n\\mathbf{\\varepsilon_{1}} \\\\\n\\mathbf{\\varepsilon_{2}} \\\\\n\\vdots \\\\\n\\mathbf{\\varepsilon_{N}}\n\\end{bmatrix}\n\\end{aligned}\n$$ Thus, $\\varepsilon$ has a block diagonal structure with each subject having the same covariance matrix.\n\n# Modeling the Mean\n\nThis section follows 5.5 in @fitzmaurice_applied_2011.\n\nThe main questions to ask in this model are\n\n1.  Group x time interaction effect - do the group means change over time. Biggest question of longitudinal studies normally\n2.  Group effect - a main effect, but similar in normal analyses because if the interaction is significant, then, it's relatively meaningless to talk about these effects. (Note, if randomized trial, this should coincide with the interaction, because in randomized trial it was assumed that the two groups were the same.)\n3.  Time effect - are they same across time?\n\nThere are primarily two ways of studying the longitudinal responses\n\n1.  response profiles (perfect fit, treating time as a factor)\n2.  (semi-) parametric form\n3.  summary measure (like area under the curve)\n\n## Example TLC {.tabset}\n\nThis is an exploratory plot summarizing the eventual statements we'd like to say about the model.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-2_38a1dfe48c61fe9dc625d1dba15f25db'}\n\n```{.r .cell-code}\ntlc_raw <- read.csv(\"data/tlc.csv\", header=FALSE)\nnames(tlc_raw) <- c(\"id\", \"trt\",\"w0\", \"w1\", \"w4\",\"w6\")\n\ntlc <- tlc_raw %>% gather(\"week\", \"lead\", 3:6)\n\n# numeric version of week\ntlc$week_int <- tlc$week %>% gsub(\".*([0-9]+).*\", \"\\\\1\", .) %>% as.numeric()\n\n# timepoint (1:4)\ntlc$time <- c(1:4)[as.factor(tlc$week)]\n\n# change from baseline\ntlc_diff <- tlc %>% group_by(id) %>% \n  arrange(trt, id, time) %>% \n  mutate(lead_diff = lead - lead[1],\n         lead_base = lead[1],\n         trt = factor(trt, levels = c(\"P\", \"A\"))) %>% \n  arrange(desc(trt), id, time) %>% \n  filter(week != \"w0\")\n```\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-3_30dc1d1eddffac3eae9fbc7750d47ab1'}\n\n```{.r .cell-code}\ntlc %>% ggplot(aes(x = week, y = lead, group = id)) +\n  geom_line(alpha = .2) +\n  facet_wrap(~trt)\n```\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-4_871afba696f2806fcbb0343d19a19faf'}\n\n```{.r .cell-code}\n# Mean Response Profile by Trt\ntlc_mrp <- tlc %>% \n  group_by(trt, week) %>% \n  summarise(mlead = mean(lead), \n            .groups = \"drop_last\")\n\ntlc_mrp %>% \n  ggplot(aes(x = week, y=mlead, group=trt)) +\n  geom_line(aes(linetype=trt)) +\n  geom_point() +\n  ggtitle(\"Lead over time\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### SAS\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-5_df84558dba335a7b25240588c7212dd7'}\n\n```{.sas .cell-code}\n* TLC data analysis example, from chapter 5;\n* https://content.sph.harvard.edu/fitzmaur/ala2e/;\n\nDATA tlc_long;\n  INFILE \"~/lda/tlc-data.txt\" DLM=\" \";\n  input id group $ lead0 lead1 lead4 lead6;\n  * create 4 observations from each row;\n  y=lead0; time=0; output;\n  y=lead1; time=1; output;\n  y=lead4; time=4; output;\n  y=lead6; time=6; output;\n  drop lead0 lead1 lead4 lead6; * drop original \"wide\" data columns;\nrun;\n\n/* set reference level */\n/* http://support.sas.com/kb/37/108.html */\nproc mixed noclprint=10 data=tlc_long\nclass id group(ref=\"P\") time(ref=\"0\");\nmodel y = group time group*time / s chisq;\nrepeated time / type=un subject=id r;\nlsmeans group / cl diff;\nrun;\n```\n:::\n\n\n<center>\n\n<figure>\n\n<img src=\"img/tlc_sas_summary.png\" style=\"width:50%\"/></img>\n\n<figcaption>\n\n</figcaption>\n\n</figure>\n\n<figure>\n\n<img src=\"img/tlc_sas_type3.png\" style=\"width:50%\"/></img>\n\n<figcaption>\n\n</figcaption>\n\n</figure>\n\n<figure>\n\n<img src=\"img/tlc_sas_cov.png\" style=\"width:50%\"/></img>\n\n<figcaption>\n\n</figcaption>\n\n</figure>\n\n</center>\n\n### R: gls\n\nWe fit an unstructured covariance, with fully crossed time and treatment factors. In order to get the unstructured covariance matrix, we must use `gls`, for generalized least squares. This is basically\n\n\n::: {.cell paged.print='true' hash='longitudinal_cache/html/modeling_415ebc6c0168d7ee2bf7b7bf3d8a75ba'}\n\n```{.r .cell-code}\ntlc$trt <- factor(tlc$trt, levels = c(\"P\", \"A\")) # use P as the reference level\n\n# Generalized Least Squares, defaults to REML\ntlc_gls <- gls(lead ~ trt*week,\n               corr=corSymm(form = ~ time | id),\n               weights = varIdent(form = ~ 1 | week),\n               data=tlc)\n\n# Table 5.5 in Fitzmaurice\nsummary(tlc_gls)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized least squares fit by REML\n  Model: lead ~ trt * week \n  Data: tlc \n       AIC      BIC    logLik\n  2452.076 2523.559 -1208.038\n\nCorrelation Structure: General\n Formula: ~time | id \n Parameter estimate(s):\n Correlation: \n  1     2     3    \n2 0.571            \n3 0.570 0.775      \n4 0.577 0.582 0.581\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | week \n Parameter estimates:\n      w0       w1       w4       w6 \n1.000000 1.325880 1.370442 1.524813 \n\nCoefficients:\n              Value Std.Error   t-value p-value\n(Intercept)  26.272 0.7102929  36.98756  0.0000\ntrtA          0.268 1.0045059   0.26680  0.7898\nweekw1       -1.612 0.7919199  -2.03556  0.0425\nweekw4       -2.202 0.8149021  -2.70217  0.0072\nweekw6       -2.626 0.8885253  -2.95546  0.0033\ntrtA:weekw1 -11.406 1.1199438 -10.18444  0.0000\ntrtA:weekw4  -8.824 1.1524456  -7.65676  0.0000\ntrtA:weekw6  -3.152 1.2565645  -2.50843  0.0125\n\n Correlation: \n            (Intr) trtA   weekw1 weekw4 weekw6 trtA:1 trtA:4\ntrtA        -0.707                                          \nweekw1      -0.218  0.154                                   \nweekw4      -0.191  0.135  0.680                            \nweekw6      -0.096  0.068  0.386  0.385                     \ntrtA:weekw1  0.154 -0.218 -0.707 -0.481 -0.273              \ntrtA:weekw4  0.135 -0.191 -0.481 -0.707 -0.272  0.680       \ntrtA:weekw6  0.068 -0.096 -0.273 -0.272 -0.707  0.386  0.385\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-2.1756456 -0.6849980 -0.1515547  0.5294176  5.6327571 \n\nResidual standard error: 5.02253 \nDegrees of freedom: 400 total; 392 residual\n```\n:::\n:::\n\n\nThe estimated (unstructured), marginal covariance structure can be extracted by `getVarCov`.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-6_dd765b2111bc5aee2faf8c59545b4cf2'}\n\n```{.r .cell-code}\ngetVarCov(tlc_gls) # covariance matrix\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 25.226 19.107 19.700 22.202\n[2,] 19.107 44.346 35.535 29.675\n[3,] 19.700 35.535 47.377 30.620\n[4,] 22.202 29.675 30.620 58.651\n  Standard Deviations: 5.0225 6.6593 6.8831 7.6584 \n```\n:::\n:::\n\n\nGetting the type III fixed effect tests, similar to SAS, is a little more work. There must be an easier way, but this shows how to do it manually. Here P0 denotes the mean of placebo at time 0.\n\n| Conditional Mean ($\\mu$) | Coef                                    |\n|--------------------------|-----------------------------------------|\n| P0                       | $\\beta_1$                               |\n| P1                       | $\\beta_1 + \\beta_3$                     |\n| P4                       | $\\beta_1 + \\beta_4$                     |\n| P6                       | $\\beta_1 + \\beta_5$                     |\n| S0                       | $\\beta_1 + \\beta_2$                     |\n| S1                       | $\\beta_1 + \\beta_2 + \\beta_3 + \\beta_6$ |\n| S4                       | $\\beta_1 + \\beta_2 + \\beta_4 + \\beta_7$ |\n| S7                       | $\\beta_1 + \\beta_2 + \\beta_5 + \\beta_8$ |\n\nFurthermore, the Wald test statistic is\n\n$$\n\\begin{aligned}\nW^2 = (L\\hat\\beta)'\\{L\\mathrm{Cov}(\\hat\\beta)L'\\}^{-1}L\\hat\\beta\n\\end{aligned}\n$$ Compare the following table to the SAS Type 3 tests for Fixed Effects\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-7_5a636d3df51a8f73f2cbd4b27f473a37'}\n\n```{.r .cell-code}\n# variance of estimated coef, beta hat\ncovbeta <- tlc_gls$varBeta\nbeta <- coef(tlc_gls)\n\n# testing interactions together, manually\ntrt_week_coef <- beta[6:8]\ntrt_week_cov <- covbeta[6:8, 6:8]\n# t(trt_week_coef) %*% solve(trt_week_cov) %*% trt_week_coef\n\n# avg treatment - avg control, H0: S = P\ntrt_L <- c(4, 4, 1, 1, 1, 1, 1, 1) - c(4, 0, 1, 1, 1, 0, 0, 0) %>% matrix() %>% t()# c(0,4,0,0,0,1,1,1)\n# manually, its a scalar\n# trt_L[c(2, 6:8)] %*% beta[c(2, 6:8)] * \n#   solve(trt_L[,c(2, 6:8)] %*% covbeta[c(2, 6:8), c(2, 6:8)] %*% cbind(trt_L[,c(2, 6:8)])) * \n#   trt_L[c(2, 6:8)] %*% beta[c(2, 6:8)]\n\n# average by week, H0: w0 = w1 = w4 = w6\nw0_L <- c(2, 1, 0, 0, 0, 0, 0, 0)\nw1_L <- c(2, 1, 2, 0, 0, 1, 0, 0)\nw4_L <- c(2, 1, 0, 2, 0, 0, 1, 0)\nw6_L <- c(2, 1, 0, 0, 2, 0, 0, 1)\nweek_L <- matrix(c(w1_L - w0_L,\n                   w4_L - w0_L,\n                   w6_L - w0_L), nrow = 3, byrow=T)\n\n# Combine the code above\nrbind(anova(tlc_gls, L = trt_L), # Wald F value = 25.43\n      anova(tlc_gls, L = week_L), # Wald F value = 61.49\n      anova(tlc_gls, Terms = 4)) %>%  # Wald F value = 35.9\n  as.data.frame() %>% \n  dplyr::mutate(Chisq = numDF * `F-value`,\n                .after = `F-value`) %>% \n  add_column(source = c(\"Trt\", \"Week\", \"Trt x Week\"), .before = 1)\n```\n:::\n\n\n### R: anova.gls\n\nI'm not entirely sure what's going on in the anova function in nlme. There is a value for intercept and treatment, and I'm not sure how to interpret that.\n\nIt seems the week and interaction are being calculated correctly, not sure why the treatment is different from the manual calculations above. In the presence of an interaction effect however, it doesn't really matter.\n\nAlso notice that Chisq statistic is just the F statistic \\* ndf. Assuming chisq gives more liberal estimates, effectively infinite residual degrees of freedom.\n\nThe following are two anova tables of the same model,\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-8_e4976ea0905a4ba037e4fbdc89038e04'}\n\n```{.r .fold-show .cell-code}\n# these results match\nanova(tlc_gls, type = \"marginal\") # nlme\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDenom. DF: 392 \n            numDF   F-value p-value\n(Intercept)     1 1368.0793  <.0001\ntrt             1    0.0712  0.7898\nweek            3    3.8731  0.0095\ntrt:week        3   35.9293  <.0001\n```\n:::\n\n```{.r .fold-show .cell-code}\nAnova(tlc_gls, \"III\", test.statistic = \"F\", error.df = tlc_gls$dims$N - tlc_gls$dims$p) # car w/ 392 error df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: lead\n             Df         F  Pr(>F)    \n(Intercept)   1 1368.0793 < 2e-16 ***\ntrt           1    0.0712 0.78977    \nweek          3    3.8731 0.00946 ** \ntrt:week      3   35.9293 < 2e-16 ***\nResiduals   392                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-9_b3a502e725308e1c9b92187a355d3810'}\n\n```{.r .fold-show .cell-code}\n# more liberal\nAnova(tlc_gls, \"III\", test.statistic = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: lead\n            Df     Chisq Pr(>Chisq)    \n(Intercept)  1 1368.0793  < 2.2e-16 ***\ntrt          1    0.0712   0.789625    \nweek         3   11.6192   0.008808 ** \ntrt:week     3  107.7880  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-10_663123f614f96eb6eb2e0101f7a95be0'}\n\n```{.r .cell-code}\n# not sure what's going on here\nanova(tlc_gls, type = \"sequential\")\n```\n:::\n\n\n### R: lm\n\nFor comparison, we can see that gls gives the same estimates as the gls model, for profile curves, it basically goes through the mean of each group. lm is NOT the right way to do this analysis, but we want to see what it shows for comparison.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-11_18147f731a3be05b6c8a6004977403e8'}\n\n```{.r .cell-code}\ntlc_lm <- lm(lead ~ trt*week, data = tlc)\ncbind(\"gls\" = coef(tlc_gls), \"lm\" = coef(tlc_lm))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                gls      lm\n(Intercept)  26.272  26.272\ntrtA          0.268   0.268\nweekw1       -1.612  -1.612\nweekw4       -2.202  -2.202\nweekw6       -2.626  -2.626\ntrtA:weekw1 -11.406 -11.406\ntrtA:weekw4  -8.824  -8.824\ntrtA:weekw6  -3.152  -3.152\n```\n:::\n:::\n\n\ncompare the standard errors of the estimates though\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-12_d48d3c287553099c27d07de23adc14b5'}\n\n```{.r .cell-code}\ncbind(\"gls\" = sqrt(diag(as.matrix(tlc_gls$varBeta))),\n      \"lm\" = tlc_lm %>% tidy() %>% pull(std.error))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  gls        lm\n(Intercept) 0.7102929 0.9370175\ntrtA        1.0045059 1.3251428\nweekw1      0.7919199 1.3251428\nweekw4      0.8149021 1.3251428\nweekw6      0.8885253 1.3251428\ntrtA:weekw1 1.1199438 1.8740349\ntrtA:weekw4 1.1524456 1.8740349\ntrtA:weekw6 1.2565645 1.8740349\n```\n:::\n:::\n\n\nnot quite sure this is the fairest comparison, but the heterogeneity in the estimated variance seems to have stabilized the standardized residuals. Need to double check if this is the right stabilization.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-13_b6a752c403b8a24ab6d50f7f96e8ea18'}\n\n```{.r .cell-code}\nplot(tlc_gls, resid(., type = \"pearson\") ~ week_int) # show against time (instead of default fitted)\n```\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(tlc$week_int, rstudent(tlc_lm))\n```\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-13-2.png){width=672}\n:::\n:::\n\n\n\n\n### R: AUC\n\nA third strategy is to do summary statistics of the curves. The section in the book refers to creating 1 df tests, which are more powerful than the overall F test of trt x time interaction. Often times, if you measure many many time points, the overall f test becomes diluted and it becomes more difficult to detect differences in the two curves, so these are designed to have more power than those tests. *should only be specified prior to analysis though, to keep with proper significance control*\n\nWe consider two tests,\n\n1.  average difference minus baseline.\n\n-   the contrast will take the form (where $n$ is the number of occassions)\n\n$$\n\\begin{aligned}\nL &= (L_1, -L_1) \\\\\nL_1 &= \\left(-1, \\frac{1}{n-1}, \\frac{1}{n-1}, \\dots, \\frac{1}{n-1}\\right)\n\\end{aligned}\n$$\n\n2.  Area under curve (AUC) minus baseline\n\n-   approximated by trapezoids\n\nboth of these can be formulated as a contrast, we'll use emmeans for convenience here.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-15_6088a49c365a70c06ea9919c44721bd6'}\n\n```{.r .cell-code}\nemm_tlc_gls <- emmeans(tlc_gls, specs=c(\"week\", \"trt\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nAnalytical Satterthwaite method not available; using appx-satterthwaite\n```\n:::\n\n```{.r .cell-code}\nemm_tlc_gls\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n week trt emmean    SE   df lower.CL upper.CL\n w0   P     26.3 0.710 97.9     24.9     27.7\n w1   P     24.7 0.942 98.1     22.8     26.5\n w4   P     24.1 0.973 98.1     22.1     26.0\n w6   P     23.6 1.083 98.1     21.5     25.8\n w0   A     26.5 0.710 97.9     25.1     27.9\n w1   A     13.5 0.942 98.1     11.7     15.4\n w4   A     15.5 0.973 98.1     13.6     17.4\n w6   A     20.8 1.083 98.1     18.6     22.9\n\nDegrees-of-freedom method: appx-satterthwaite \nConfidence level used: 0.95 \n```\n:::\n\n```{.r .cell-code}\navg_minus_baseline_L <- c(-1, 1/3, 1/3, 1/3, 1, -1/3, -1/3, -1/3)\nauc_minus_baseline_L <- c(5.5, -2, -2.5, -1, -5.5, 2, 2.5, 1)\ncontrast(emm_tlc_gls, list(\"avg minus baseline\" = avg_minus_baseline_L,\n                           \"AUC minus baseline\" = auc_minus_baseline_L))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n contrast           estimate   SE   df t.ratio p.value\n avg minus baseline     7.79 0.95 98.1   8.205  <.0001\n AUC minus baseline   -48.02 5.35 97.9  -8.973  <.0001\n\nDegrees-of-freedom method: appx-satterthwaite \n```\n:::\n\n```{.r .cell-code}\nanova(tlc_gls, L = avg_minus_baseline_L)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDenom. DF: 392 \n F-test for linear combination(s)\n(Intercept)        trtA      weekw1      weekw4      weekw6 trtA:weekw1 \n -1.0000000   0.3333333   0.3333333   0.3333333   1.0000000  -0.3333333 \ntrtA:weekw4 trtA:weekw6 \n -0.3333333  -0.3333333 \n  numDF F-value p-value\n1     1 91.2127  <.0001\n```\n:::\n:::\n\n\n## Example: Ratdrink {.tabset}\n\nThe following example is from [\\@faraway_extending_2016](#References)\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-16_86020ba37d14bc124ab9bb83bb919356'}\n\n```{.r .cell-code}\ndata(ratdrink)\nratdrink %>% ggplot(aes(weeks, wt, group = subject)) +\n  geom_line() +\n  facet_wrap(~treat)\n```\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n### different intercepts\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-17_16300df22155985f764738c08f2bfbbe'}\n\n```{.r .cell-code}\n# full lm\nrat_lm <- lm(wt ~ weeks*treat + subject, data = ratdrink) # fixed \"block\"\nrat_mmer <- lmer(wt ~ weeks*treat + (1|subject), data = ratdrink) # random \"block\"\n\n# plotting the predictions\nrat_preds <- ratdrink %>% add_column(lm_yhat = predict(rat_lm),\n                                     mmer_yhat = predict(rat_mmer))\nrat_preds %>% pivot_longer(c(wt, lm_yhat, mmer_yhat), names_to = \"response\", values_to = \"y\") %>% \n  ggplot() +\n  geom_line(aes(weeks, y, groups = subject)) +\n  facet_grid(response~treat)\n```\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n### different slopes\n\nThe bottom row is the raw data while, the second row is the predictions from a random subject, and finally the top row is the predictions from the fully fixed model.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-18_822445a527ce1f2dd049c07a044d6eed'}\n\n```{.r .cell-code}\nrat_lm_diffslope <- lm(wt~weeks*treat + subject + subject:weeks, data = ratdrink)\n# rat_mmer_interaction <- lmer(wt ~ weeks*treat + (1|weeks:subject), data = ratdrink) # weeks is continuous! doesn't work.... too many random effect intercepts\nrat_mmer_interaction <- lmer(wt ~ weeks*treat + (1|subject) + subject:weeks, data = ratdrink) # weird model....b/c subject:weeks is fixed not sure when \nrat_mmer_random_slope <- lmer(wt ~ weeks*treat + (weeks|subject), data = ratdrink)\nrat_mmer_random_slope_nocor <- lmer(wt ~ weeks*treat + (1 | subject) + (0 + weeks || subject), data = ratdrink) # without correlation between intercept and slope\n\nrat_preds_slope <- ratdrink %>% add_column(lm_diffslope_yhat = predict(rat_lm_diffslope),\n                                     mmer_interaction_yhat = predict(rat_mmer_interaction),\n                                     mmer_random_slope_yhat = predict(rat_mmer_random_slope),\n                                     mmer_random_slope_nocor_yhat = predict(rat_mmer_random_slope_nocor))\n\nrat_preds_slope %>% \n  pivot_longer(c(wt, \n                 lm_diffslope_yhat, \n                 mmer_interaction_yhat, \n                 mmer_random_slope_yhat, \n                 mmer_random_slope_nocor_yhat), names_to = \"response\", values_to = \"y\") %>% \n  ggplot() +\n  geom_line(aes(weeks, y, group = subject)) +\n  facet_grid(response~treat)\n```\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-19_9a512d095fb83b7b1090cda9193fc2ca'}\n\n```{.r .cell-code}\n# profile conf intervals\nconfint(rat_mmer_random_slope)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nComputing profile confidence intervals ...\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                            2.5 %     97.5 %\n.sig01                  3.4506344  7.6654748\n.sig02                 -0.5261030  0.3794203\n.sig03                  2.6064121  4.8687653\n.sigma                  3.7555591  5.1142342\n(Intercept)            48.8692859 56.8907140\nweeks                  24.0547424 28.9052555\ntreatthiouracil        -0.8920062 10.4520061\ntreatthyroxine         -7.0445321  5.4559606\nweeks:treatthiouracil -12.7998322 -5.9401707\nweeks:treatthyroxine   -3.1166339  4.4423449\n```\n:::\n:::\n\n\n\n## Example: Body Fat and Menarche\n\nWe use the MIT growth study menarche example for this section, from [Fitzmaurice website](https://content.sph.harvard.edu/fitzmaur/ala2e/)\n\nCovariates/Response:\n\n- id : girl id\n- age : age at observation\n- agemen : age of menarche for girl\n- time : age - agemen, time relative to menarche.\n- pbf : percentage body fat\n\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-20_5b8ad96f281c8640088909fc16809797'}\n\n```{.r .cell-code}\nfat <- read.dta(\"data/fat.dta\")\n\n# select 20 random girls and show response curve\nfat %>% \n  group_nest(id) %>% \n  slice_sample(n=20) %>% \n  unnest(data) %>% \n  ggplot(aes(time, pbf)) + \n  geom_point() +\n  geom_line() +\n  facet_wrap(~id) +\n  geom_vline(xintercept = 0, color = \"red\")\n```\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n### Modeling\n\n@fitzmaurice_applied_2011 in chapter 8.8 analyzes this as a piecewise random effects model.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-21_f762c93ef2b534474094181648eb9dc1'}\n\n```{.r .cell-code}\n# to fit piecewise function, need variable with after menarche time\nfat_post <- fat %>% mutate(timepost = time * (time > 0)) # create variable for post menarche time\nfat_lme <- lme(pbf ~ time + timepost,\n    random = ~time + timepost | id,\n    data = fat_post)\n```\n:::\n\n\nThe fixed effects of the model:\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-22_dcbb64f4317bda8f3acbf6a26b863897'}\n\n```{.r .cell-code}\n# Fixed effects\nfat_lme %>% tidy() %>% filter(effect == \"fixed\") %>% \n  dplyr::select(term, estimate, std.error, statistic, df, p.value)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 Ã— 6\n  term        estimate std.error statistic    df   p.value\n  <chr>          <dbl>     <dbl>     <dbl> <dbl>     <dbl>\n1 (Intercept)   21.4       0.565     37.8    885 3.95e-187\n2 time           0.417     0.157      2.65   885 8.09e-  3\n3 timepost       2.05      0.228      8.98   885 1.60e- 18\n```\n:::\n:::\n\n\nThe random effects and standard errors calculated from inverse expected fisher information (with package `lmeInfo`)\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-23_b1a86c1c971525427f52956f519a7d59'}\n\n```{.r .cell-code}\n# https://stats.oarc.ucla.edu/r/faq/how-can-i-calculate-standard-errors-for-variance-components-from-mixed-models/\n# Random effects and standard errors\nfat_varcomp <- getVarCov(fat_lme)[upper.tri(getVarCov(fat_lme), diag = TRUE)]\ncbind(estimate = fat_varcomp,\n      `std. err` = sqrt(diag(varcomp_vcov(fat_lme)))[c(1, 2, 3, 4, 5, 6)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                  estimate  std. err\nTau.id.var((Intercept))          45.939153 5.7556122\nTau.id.cov(time,(Intercept))      2.526047 1.1984610\nTau.id.var(time)                  1.630971 0.4023225\nTau.id.cov(timepost,(Intercept)) -6.109078 1.8486864\nTau.id.cov(timepost,time)        -1.750363 0.5746242\nTau.id.var(timepost)              2.749384 0.9273656\n```\n:::\n:::\n\n\n# Testing for heteroskedasticiy and autocorrelation\n\n\n- Goldfeld-Quandt - 2 subsamples with possibly different variances.\n- Breusch-Pagan Test - for conditional heteroskedasticity\n- White Test - more generalized test, including exogenous variables but also polynomial and interaction terms\n- Breush-Godfrey Test - \n- Durbin Watson Test\n\nThere's \n- `strucchange::gefp` - structural change testing.\n\n## Resources\n\n- [Practical Econometrics Book](http://web.vu.lt/mif/a.buteikis/wp-content/uploads/PE_Book/4-7-Multiple-heteroskedastic.html)\n\n# Modeling the Covariance\n\nWe should note that there are econometric approaches, such as sandwich estimators, and mixed model approaches that are all competing in the space of modeling the variance. Broadly there are three approaches for variance modeling:\n\n1. random effects (\"lme4\")\n2. generalized estimating equations (\"gee\")\n3. feasible generalized least squares (\"plm\")\n\nPackages with this approach from the [sandwich vignette](https://cran.r-project.org/web/packages/sandwich/vignettes/sandwich-CL.pdf)\n\n- \"sandwich\"\n- \"multiwayvcov\" - mostly lm or glm like objects\n- \"plm\" -  several sandwich covariances for panel and fixed effect linear models.\n- \"geepack\" - for glm type models, and subsequent geeglm objects\n- \"clubSandwich\" - orginary or weighted least sqaures regression\n- \"clusterSEs\" - ordinary or weighted least squares regression\n- \"lfe\" - standard models\n\n## Examples  {.tabset}\n\n### Example: Spruce Trees\n\n-   y: response (log of product of tree size and diameter squared)\n-   tx: treatment ( 2 levels )\n    -   0: control condition (25 trees per time point)\n    -   1: ozone exposure at 70 ppb (54 trees per time point)\n-   day: time (days since 1988 Jan 1)\n-   chamber: block (ozone controlled chamber)\n\n\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-24_7f5ce801bdbaec4c04f89c37f414708a'}\n\n```{.r .cell-code}\n# str(spruce88)\n# xtabs(~day + tx, data=spruce88)\n# xtabs(~tx,data = spruce88)\n\n# Example of selecting out some groups of elements for longitudinal analysis. Used above for highlighting\nspruce88 %>% filter(id %in% sample(levels(spruce88[,\"id\"])))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] y       day     tx      chamber id     \n<0 rows> (or 0-length row.names)\n```\n:::\n\n```{.r .cell-code}\nsid <-spruce88 %>% group_by(chamber) %>% sample_n(5)\nspruce_highlight <- spruce88 %>% filter(id %in% sid[[\"id\"]])\n\n# Emphasizing lines in the plot with background lighter\ng_base <- ggplot(data = spruce88, mapping = aes(x=day, y=y, color = factor(tx), group=factor(id))) +\n  facet_wrap(~chamber)\n(g_base +\n   geom_point(alpha=.2)+ geom_line(alpha=.2) + geom_line(data=spruce_highlight, aes(x=day, y=y, group=factor(id))))\n```\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# (g_base + geom_point(alpha=.2) + geom_smooth(span=.5, aes(group=\"abc\")))\n# Note: using a constant for the group, will override the previous grouping.\n# Also, the smoothing will come up with \"singularities\" since there are only 5 distinct x data points, and lowess will have trouble.\n```\n:::\n\n\n### Exploring correlation from week to week, and getting correlation structure.\n\nSuggestion if the data is taken at many different points, just round it to the nearest year and we can still get some sense of the correlation over time. In the case everything is discrete, we can just use the data categories as is.\n\nWe should remove the effect of the means from each week. The suggestion in the book is to remove the covariate effect by fitting a regression on the data.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-25_fffeb54e1d663875d47bdb476cb95d23'}\n\n```{.r .cell-code}\n# remove the effect of means from each week\n# residuals(lm(y~day + tx + chamber, data=spruce88))\n\nstr(spruce88)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t395 obs. of  5 variables:\n $ y      : num  4.51 4.98 5.41 5.9 6.14 ...\n $ day    : num  -49 -27 0 26 57 -49 -27 0 26 57 ...\n $ tx     : num  1 1 1 1 1 1 1 1 1 1 ...\n $ chamber: num  1 1 1 1 1 1 1 1 1 1 ...\n $ id     : int  1 1 1 1 1 2 2 2 2 2 ...\n```\n:::\n\n```{.r .cell-code}\nspruce_resid <- spruce88 %>% mutate(resid = residuals(lm(y~day + tx + chamber)))\n```\n:::\n\n\n### Example: TLC\n\nFor exploratory analysis, we should show the unstructured estimated covariance matrix from the model, shown for an individual $i$,\n\n$$\n\\begin{aligned}\nY_i = X_i\\beta + \\varepsilon_i\n\\end{aligned}\n$$\n\nassuming that $\\varepsilon_i \\sim N(0,\\Sigma)$. We show the estimate $\\hat \\Sigma$ as unstructured covariance matrix:\n\nThis is table 5.3 in @fitzmaurice_applied_2011.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-26_1b54d9079c474ae861585d9c8ac2a870'}\n\n```{.r .cell-code}\n# direct group means covariance\ntlc_gls_un_cov <- gls(lead ~ trt*week,\n                      correlation=corSymm(form = ~ time | id), \n                      weights = varIdent(form = ~ 1 | week), \n                      data=tlc)\n\ngetVarCov(tlc_gls_un_cov)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 25.226 19.107 19.700 22.202\n[2,] 19.107 44.346 35.535 29.675\n[3,] 19.700 35.535 47.377 30.620\n[4,] 22.202 29.675 30.620 58.651\n  Standard Deviations: 5.0225 6.6593 6.8831 7.6584 \n```\n:::\n:::\n\n\nWe can also look at the correlation with a scatter plot.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-27_a1c2e28ca7e3850658037650babcd91d'}\n\n```{.r .cell-code}\ntlc[\"week\"] <- as.factor(tlc$week)\n\n# Split into the exposed and placebo groups and make plots from the placebo group\ntlc_exposed <- tlc %>% filter(trt == \"A\") %>% spread(week, lead)\ntlc_placebo <- tlc %>% filter(trt == \"P\") %>% spread(week, lead)\npairs(tlc_placebo[3:6])\n```\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncor(tlc_placebo[3:6])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          week_int      time w0 w1\nweek_int 1.0000000 0.9844952 NA NA\ntime     0.9844952 1.0000000 NA NA\nw0              NA        NA  1 NA\nw1              NA        NA NA  1\n```\n:::\n:::\n\n\n### Example: Body Fat and Menarche\n\nThe variance covariance matrix can be extracted in two ways:\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-28_4afe036f0af592fb855dcae7f258026a'}\n\n```{.r .cell-code}\n# using reStruct from model is scale version of covariance matrix\nlist(as.matrix(fat_lme$modelStruct$reStruct[[1]]) * fat_lme$sigma^2,\n     getVarCov(fat_lme))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n            (Intercept)      time  timepost\n(Intercept)   45.939153  2.526047 -6.109078\ntime           2.526047  1.630971 -1.750363\ntimepost      -6.109078 -1.750363  2.749384\n\n[[2]]\nRandom effects variance covariance matrix\n            (Intercept)    time timepost\n(Intercept)     45.9390  2.5260  -6.1091\ntime             2.5260  1.6310  -1.7504\ntimepost        -6.1091 -1.7504   2.7494\n  Standard Deviations: 6.7778 1.2771 1.6581 \n```\n:::\n:::\n\n\nWe can also model the within correlation differently with `correlation`\n\n`corCAR1` - models the correlation by `phi = .2` (default), has autocorrelation function $h(\\cdot)$, with parameters $s$ distance, and $\\phi$ correlation\n\n$$\n\\begin{aligned}\nh(s, \\phi) = \\phi^s \\quad s \\geq 0, \\phi \\geq 0\n\\end{aligned}\n$$\n\nWe can manually create the correlation matrix using this function and pairwise distances, or use the corStruct class in `nlme`.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-29_9b14e40a765250703752038805e4ecf2'}\n\n```{.r .cell-code}\nlist(manual = .2^dist(fat_post$time[fat_post$id == 1]),\n     lme = corMatrix(Initialize(corCAR1(form = ~time | id), data = fat_post))[[1]]) # phi = .2 default\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$manual\n             1            2            3            4            5\n2 0.1968068917                                                    \n3 0.0454964703 0.2311731563                                       \n4 0.0098617995 0.0501090149 0.2167596607                          \n5 0.0018198587 0.0092469255 0.0399999969 0.1845361667             \n6 0.0003639718 0.0018493853 0.0080000000 0.0369072362 0.2000000156\n\n$lme\n             [,1]        [,2]       [,3]        [,4]        [,5]         [,6]\n[1,] 1.0000000000 0.196806892 0.04549647 0.009861799 0.001819859 0.0003639718\n[2,] 0.1968068917 1.000000000 0.23117316 0.050109015 0.009246926 0.0018493853\n[3,] 0.0454964703 0.231173156 1.00000000 0.216759661 0.039999997 0.0080000000\n[4,] 0.0098617995 0.050109015 0.21675966 1.000000000 0.184536167 0.0369072362\n[5,] 0.0018198587 0.009246926 0.04000000 0.184536167 1.000000000 0.2000000156\n[6,] 0.0003639718 0.001849385 0.00800000 0.036907236 0.200000016 1.0000000000\n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-30_35eefae2b4a4943c00f3bcc6511974e7'}\n\n```{.r .cell-code}\nfat_car1 <- lme(pbf~time + timepost,\n    random = ~ 1 | id,\n    corr=corCAR1(,form= ~ time | id),\n    data = fat_post)\n\nfat_car1 %>% getVarCov(type = \"marginal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nid 1 \nMarginal variance covariance matrix\n       1      2      3      4      5      6\n1 48.672 39.818 35.798 33.626 32.441 31.897\n2 39.818 48.672 40.439 35.991 33.563 32.448\n3 35.798 40.439 48.672 40.185 35.554 33.427\n4 33.626 35.991 40.185 48.672 39.581 35.408\n5 32.441 33.563 35.554 39.581 48.672 39.878\n6 31.897 32.448 33.427 35.408 39.878 48.672\n  Standard Deviations: 6.9765 6.9765 6.9765 6.9765 6.9765 6.9765 \n```\n:::\n\n```{.r .cell-code}\nfat_lme %>% getVarCov(type = \"marginal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nid 1 \nMarginal variance covariance matrix\n       1      2      3      4      5      6\n1 60.288 46.991 43.546 39.949 36.007 32.886\n2 46.991 54.304 42.885 40.853 38.553 35.311\n3 43.546 42.885 51.763 41.668 40.846 37.496\n4 39.949 40.853 41.668 51.991 43.240 39.776\n5 36.007 38.553 40.846 43.240 55.056 42.044\n6 32.886 35.311 37.496 39.776 42.044 48.858\n  Standard Deviations: 7.7645 7.3691 7.1946 7.2105 7.42 6.9898 \n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-31_0a35fc0550c50cf68677e3a924220c0a'}\n\n```{.r .cell-code}\ngetVarCov(fat_car1, type = \"marginal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nid 1 \nMarginal variance covariance matrix\n       1      2      3      4      5      6\n1 48.672 39.818 35.798 33.626 32.441 31.897\n2 39.818 48.672 40.439 35.991 33.563 32.448\n3 35.798 40.439 48.672 40.185 35.554 33.427\n4 33.626 35.991 40.185 48.672 39.581 35.408\n5 32.441 33.563 35.554 39.581 48.672 39.878\n6 31.897 32.448 33.427 35.408 39.878 48.672\n  Standard Deviations: 6.9765 6.9765 6.9765 6.9765 6.9765 6.9765 \n```\n:::\n\n```{.r .cell-code}\nVarCorr(fat_car1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nid = pdLogChol(1) \n            Variance StdDev  \n(Intercept) 31.37028 5.600917\nResidual    17.30157 4.159516\n```\n:::\n:::\n\n\n\n### Example: Dental\n\nThis section mostly uses `gls` with a variety of different covariance matrices.\n\n\nThis dataset shows measuruments of pituitary gland to pteryomaxillary fissure. It has 11 girls and 16 boys at ages 8, 10, 12, 14.\n\n- id : patient id\n- gender: male/female\n- distance: the response variable\n\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-32_7c72fa75651f327f08add9f795752b9e'}\n\n```{.r .cell-code}\ndental_wide <- read.table(\"data/dental.txt\",\n                          header = FALSE,\n                          col.names = c(\"id\", \"gender\", \"y1\", \"y2\", \"y3\", \"y4\"))\n\n# long data format\ndental <- dental_wide %>% \n  pivot_longer(cols = y1:y4, \n               names_to = \"age\",\n               values_to = \"distance\") %>% \n  dplyr::mutate(\n    id = factor(id),\n    age = recode(age,\n                 y1 = 8,\n                 y2 = 10,\n                 y3 = 12,\n                 y4 = 14),\n    agef = factor(age),\n    .after = \"age\")\n\ndental_mean <- dental %>%\n  group_by(age, gender) %>% \n  dplyr::summarize(avg_distance = mean(distance),\n                   .groups = \"drop_last\")\n\ndental %>% ggplot(aes(age, distance, group = id)) +\n  geom_point(alpha = .3) +\n  geom_line(alpha = .3) +\n  geom_line(data = dental_mean, mapping = aes(age, avg_distance, group = NULL), color = \"red\") + \n  facet_wrap(~gender)\n```\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-33_50886dd0a6c5bfd9f85d75a6cf83d24e'}\n\n```{.r .cell-code}\n# all fixed with id\ndental_fixed <- lm(distance ~ age*gender + id, data = dental)\ndental_lm <- lm(distance ~ age*gender, data = dental)\n```\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-34_e48b5f6547161ba80078e7fece357648'}\n\n```{.r .cell-code}\n# modeling covariance\ndental_ident <- gls(distance~age*gender, data = dental) # same as lm\n\n## heterogenous \ndental_het <- gls(distance~age*gender,\n    data = dental,\n    weights = varIdent(form = ~ 1 | age))\n\n## unstructured\ndental_un <- gls(distance ~ age*gender, \n    data = dental,\n    correlation = corSymm(form = ~1 | id))\n\n## heterogenous unstructured\ndental_hun <- gls(distance ~ age*gender, \n    data = dental,\n    correlation = corSymm(form = ~1 | id),\n    weights = varIdent(form = ~1 | age))\n\n## compound symmetry\ndental_cs <- gls(distance ~ age*gender, \n    data = dental,\n    correlation = corCompSymm(form = ~1 | id))\n\n## Heterogenous CS\ndental_csh <- gls(distance ~ age*gender,\n    data = dental,\n    correlation = corCompSymm(form = ~1 | id),\n    weights = varIdent(form = ~1 | age))\n\n## Autoregressive\ndental_ar <- gls(distance ~ age*gender,\n    data = dental,\n    correlation = corAR1(form = ~1 | id),\n    weights = varIdent(form = ~1 | age))\n\n## Autoregressive Heterogeneous\ndental_arh <- gls(distance ~ age*gender,\n    data = dental,\n    correlation = corAR1(form = ~1 | id),\n    weights = varIdent(form = ~1 | age))\n```\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-35_f06bca76bd9884c2dbe6b3bace4077d1'}\n\n```{.r .cell-code}\ngetVarCov(dental_un)\ngetVarCov(dental_hun)\ngetVarCov(dental_cs)\ngetVarCov(dental_csh)\ngetVarCov(dental_ar)\ngetVarCov(dental_arh)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 5.2300 3.0094 3.3345 2.6923\n[2,] 3.0094 5.2300 3.0035 3.9166\n[3,] 3.3345 3.0035 5.2300 3.7687\n[4,] 2.6923 3.9166 3.7687 5.2300\n  Standard Deviations: 2.2869 2.2869 2.2869 2.2869 \nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 5.4252 2.7092 3.8411 2.7152\n[2,] 2.7092 4.1906 2.9745 3.3137\n[3,] 3.8411 2.9745 6.2632 4.1333\n[4,] 2.7152 3.3137 4.1333 4.9862\n  Standard Deviations: 2.3292 2.0471 2.5026 2.233 \nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 5.2207 3.2986 3.2986 3.2986\n[2,] 3.2986 5.2207 3.2986 3.2986\n[3,] 3.2986 3.2986 5.2207 3.2986\n[4,] 3.2986 3.2986 3.2986 5.2207\n  Standard Deviations: 2.2849 2.2849 2.2849 2.2849 \nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 5.6967 3.1209 3.7419 3.3309\n[2,] 3.1209 4.2365 3.2269 2.8724\n[3,] 3.7419 3.2269 6.0901 3.4440\n[4,] 3.3309 2.8724 3.4440 4.8256\n  Standard Deviations: 2.3868 2.0583 2.4678 2.1967 \nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 5.8149 3.2683 2.3721 1.3044\n[2,] 3.2683 4.5807 3.3246 1.8281\n[3,] 2.3721 3.3246 6.0172 3.3087\n[4,] 1.3044 1.8281 3.3087 4.5369\n  Standard Deviations: 2.4114 2.1403 2.453 2.13 \nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 5.8149 3.2683 2.3721 1.3044\n[2,] 3.2683 4.5807 3.3246 1.8281\n[3,] 2.3721 3.3246 6.0172 3.3087\n[4,] 1.3044 1.8281 3.3087 4.5369\n  Standard Deviations: 2.4114 2.1403 2.453 2.13 \n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-36_89b3ca7e7d7720e15141514156833f61'}\n\n```{.r .cell-code}\n# Modelings including random effects\ndental_lme <- lme(distance ~ age*gender, \n    random = ~1 | id,\n    data = dental)\ndental_lmer <- lmer(distance~age*gender + (1 | id), data = dental)\n\n# same model in lmer and lme, G unstructured\ndental_lmer_age <- lmer(distance ~ age*gender + (age | id), data = dental)\ndental_lme_age <- lme(distance ~ age*gender, \n    random = ~ age | id,\n    data = dental)\n\n# uncorrelated age and id in random effect, G diag\ndental_lme_age_diag <- lme(distance ~ age*gender,\n                       data = dental,\n                       random = list(id = pdDiag(form = ~age))) # heterogenous, but uncorrelated age and id (age || id) in lmer\ndental_lmer_age_diag <- lmer(distance~age*gender + (age || id), data = dental)\n```\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-37_82b00c9119dddced770e661d1c6591cc'}\n\n```{.r .cell-code}\ngetVarCov(dental_lme)\ngetVarCov(dental_lme_age)\ngetVarCov(dental_lme_age_diag)\n\ngetVarCov(dental_lme, type = \"marginal\") # same as CS variance\ngetVarCov(dental_lme_age, type = \"marginal\")\ngetVarCov(dental_lme_age_diag, type = \"marginal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom effects variance covariance matrix\n            (Intercept)\n(Intercept)      3.2986\n  Standard Deviations: 1.8162 \nRandom effects variance covariance matrix\n            (Intercept)       age\n(Intercept)     5.78640 -0.289630\nage            -0.28963  0.032524\n  Standard Deviations: 2.4055 0.18035 \nRandom effects variance covariance matrix\n            (Intercept)       age\n(Intercept)      2.4168 0.0000000\nage              0.0000 0.0077469\n  Standard Deviations: 1.5546 0.088017 \nid 1 \nMarginal variance covariance matrix\n       1      2      3      4\n1 5.2207 3.2986 3.2986 3.2986\n2 3.2986 5.2207 3.2986 3.2986\n3 3.2986 3.2986 5.2207 3.2986\n4 3.2986 3.2986 3.2986 5.2207\n  Standard Deviations: 2.2849 2.2849 2.2849 2.2849 \nid 1 \nMarginal variance covariance matrix\n       1      2      3      4\n1 4.9502 3.1751 3.1162 3.0574\n2 3.1751 4.9625 3.3176 3.3888\n3 3.1162 3.3176 5.2351 3.7202\n4 3.0574 3.3888 3.7202 5.7679\n  Standard Deviations: 2.2249 2.2277 2.288 2.4016 \nid 1 \nMarginal variance covariance matrix\n       1      2      3      4\n1 4.7772 3.0366 3.1605 3.2845\n2 3.0366 5.0561 3.3464 3.5014\n3 3.1605 3.3464 5.3970 3.7183\n4 3.2845 3.5014 3.7183 5.7998\n  Standard Deviations: 2.1857 2.2486 2.3231 2.4083 \n```\n:::\n:::\n\n\n\nThe covariance stuff is harder to calculate from lmer, I only know how to get from manual components, there may be something better that I don't know about.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-38_270b4c7e7a82349c3ae20e95165f85d1'}\n\n```{.r .cell-code}\n# G\nVarCorr(dental_lmer_age)[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)        age\n(Intercept)   5.7744874 -0.2886962\nage          -0.2886962  0.0324516\nattr(,\"stddev\")\n(Intercept)         age \n  2.4030163   0.1801433 \nattr(,\"correlation\")\n            (Intercept)        age\n(Intercept)   1.0000000 -0.6669087\nage          -0.6669087  1.0000000\n```\n:::\n\n```{.r .cell-code}\n# Manual calculation of G = Sigma^2 L'L\nL <- getME(dental_lmer_age, \"Lambda\") # Marginal covariance\nsig <- getME(dental_lmer_age, \"sigma\")\n(crossprod(L) * sig^2)[1:2, 1:2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2 x 2 sparse Matrix of class \"dsCMatrix\"\n                           \n[1,]  5.7889208 -0.01612650\n[2,] -0.0161265  0.01801819\n```\n:::\n\n```{.r .cell-code}\n# ZGZ' + R\nZ <- getME(dental_lmer_age, \"Z\")\nG <- Matrix(diag(27)) %x% VarCorr(dental_lmer_age)[[1]] # hadamard\nR <- Matrix(diag(rep(sigma(dental_lmer_age)^2, 108))) # \nvarY <- Z %*% G %*% t(Z) + R # Var(Y)\nvarY[1:4, 1:4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4 x 4 sparse Matrix of class \"dgCMatrix\"\n         1        2        3        4\n1 4.948875 3.174083 3.115916 3.057749\n2 3.174083 4.962347 3.317362 3.389001\n3 3.115916 3.317362 5.235433 3.720254\n4 3.057749 3.389001 3.720254 5.768131\n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-39_31f71726d02391a6178a671592ea01cc'}\n\n```{.r .cell-code}\n# both criteria choose the cs matrix, the covariance is quite similar\nAIC(dental_lm,\n  dental_het,\n    dental_un,\n    dental_hun,\n    dental_cs, \n    dental_csh,\n    dental_ar,\n    dental_arh,\n    dental_lme) %>% \n  add_column(BIC = BIC(\n    dental_lm,\n    dental_het,\n    dental_un,\n    dental_hun,\n    dental_cs,\n    dental_csh,\n    dental_ar,\n    dental_arh,\n    dental_lme)$BIC)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in AIC.default(dental_lm, dental_het, dental_un, dental_hun,\ndental_cs, : models are not all fitted to the same number of observations\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in BIC.default(dental_lm, dental_het, dental_un, dental_hun,\ndental_cs, : models are not all fitted to the same number of observations\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n           df      AIC      BIC\ndental_lm   5 488.2418 501.6524\ndental_het  8 498.4114 519.5666\ndental_un  11 448.1706 477.2589\ndental_hun 14 452.5468 489.5683\ndental_cs   6 445.7572 461.6236\ndental_csh  9 449.9724 473.7719\ndental_ar   9 460.7962 484.5957\ndental_arh  9 460.7962 484.5957\ndental_lme  6 445.7572 461.6236\n```\n:::\n:::\n\n\n\nI was also curious how this compares to just calculating the sample covariance. It seems not all that different from the unstructured covariance matrix. They are different, but it seems the residuals being calculated are different. I thought ML estimator should be pretty close to the unstructured estimate.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-40_56046c434214f748ec4712614bb5f3bc'}\n\n```{.r .cell-code}\n# sample covariance with group centering\ndental %>% group_by(gender, age) %>% \n  mutate(cdist = distance - mean(distance, na.rm = TRUE)) %>%  # group center\n  pivot_wider(id_cols = c(age), names_from = id, values_from = cdist) %>% # matrix format for cov \n  ungroup() %>% select(-age) %>% \n  data.matrix() %>% \n  t() %>% cov()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         [,1]     [,2]     [,3]     [,4]\n[1,] 5.207168 2.612325 3.759834 2.605988\n[2,] 2.612325 4.023820 2.814576 3.189576\n[3,] 3.759834 2.814576 6.207441 3.971864\n[4,] 2.605988 3.189576 3.971864 4.793979\n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-41_0e3f1c0d3342556d077d52c5a4ecc0a5'}\n\n```{.r .cell-code}\n# predictions from each of the models\ndental_yhat <- dental %>% add_column(un = predict(dental_un),\n                                     hun = predict(dental_hun),\n                                     cs = predict(dental_cs),\n                                     csh = predict(dental_csh),\n                                     ar = predict(dental_ar),\n                                     arh = predict(dental_arh),\n                                     het = predict(dental_het),\n                                     lme = predict(dental_lme),\n                                     fixed = predict(dental_fixed),\n                                     lm = predict(dental_lm),\n                                     lme_age_diag = predict(dental_lme_age_diag),\n                                     lme_age = predict(dental_lme_age))\n\ndental_yhat %>% filter(id == 2) %>% \n  pivot_longer(cols = distance:lme_age, names_to = \"type\",\n               values_to = \"response\") %>% \n  ggplot(aes(age, response, color = type)) +\n  geom_point() +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\nHere we examine what all these models actually predicted. For the marginal model, most of them are being predicted together, which makes sense. The \"distance\" are the raw values displayed for one individual. We can see that the fixed model (separate fixed effect for id) and total mean model (lm, also called pooled) runs with the other coefficients. As far as estimates go,  there are 3 separate groups, and probably some are due to rounding error. I think theoretically, the gls beta estimates should all be the same?\n\n\nNow we should also be concered about the effects of `vcov` of the different models\n\nThe sandwich estimators here are also worth comparing. All estimators have the form $X'\\hat\\Sigma X$ with different \"meat\" for $\\hat \\Sigma$.\n\n- HC0: Original white estimator\n  - $\\hat\\Sigma = X'\\{\\hat\\varepsilon_i^2\\}_dX$\n- HC1: adjust degrees of freedom of HC0\n  - $\\hat\\Sigma = \\frac{N}{N-(k+1)}X'\\{\\hat\\varepsilon_i^2\\}_dX$\n- HC2: incorporate leverage\n  - $\\hat\\Sigma = X'\\left\\{\\frac{\\hat\\varepsilon_i^2}{1- h_{ii}}\\right\\}_dX$\n- HC3: incorporate leverage with differnt weights\n  - $\\hat\\Sigma = X'\\left\\{\\frac{\\hat\\varepsilon_i^2}{(1- h_{ii})^2}\\right\\}_dX$\n- HC4: incorporate leverage with differnt weights\n  - $\\hat\\Sigma = X'\\left\\{\\frac{\\hat\\varepsilon_i^2}{(1- h_{ii})^{\\delta_{i}}}\\right\\}_dX$\n  - $\\delta_i = \\min \\{4, \\frac{Nh_{ii}}{k + 1}\\}$\n  \n  \nFor Heteroscedastic errors, there are HAC and Feasible Generalized Least Squares:\n\n- \n\n\n\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-43_ff5e41487b7b613832ed2f485d66ffe8'}\n\n```{.r .cell-code}\n# autocorrelated\nNeweyWest(dental_lm) # Bartlett kernel weights\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)         age    genderM age:genderM\n(Intercept)   1.5805513 -0.11935718 -1.5695856  0.11933808\nage          -0.1193572  0.01170302  0.1289546 -0.01245172\ngenderM      -1.5695856  0.12895463  3.9660625 -0.33731829\nage:genderM   0.1193381 -0.01245172 -0.3373183  0.03234847\n```\n:::\n\n```{.r .cell-code}\nNeweyWest(dental_lm, lag = 0) #??\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)         age    genderM age:genderM\n(Intercept)   2.3956703 -0.20743588 -2.4248178  0.20948506\nage          -0.2074359  0.02111911  0.2205834 -0.02204344\ngenderM      -2.4248178  0.22058338  5.1489658 -0.46060717\nage:genderM   0.2094851 -0.02204344 -0.4606072  0.04503617\n```\n:::\n\n```{.r .cell-code}\nNeweyWest(dental_lm, lag = 1) # Bartlett kernel weights\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)         age    genderM age:genderM\n(Intercept)   2.1562577 -0.17466452 -2.1221197  0.17190552\nage          -0.1746645  0.01688017  0.1804904 -0.01725257\ngenderM      -2.1221197  0.18049043  4.8322769 -0.41689414\nage:genderM   0.1719055 -0.01725257 -0.4168941  0.03939289\n```\n:::\n\n```{.r .cell-code}\nvcovHAC(dental_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)          age     genderM  age:genderM\n(Intercept)  0.98793825 -0.076828444 -0.85989312  0.061591713\nage         -0.07682844  0.008502217  0.07210153 -0.007497458\ngenderM     -0.85989312  0.072101528  2.07233695 -0.160873344\nage:genderM  0.06159171 -0.007497458 -0.16087334  0.015550636\n```\n:::\n\n```{.r .cell-code}\nvcovHAC(dental_lm, weights = weightsAndrews)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)          age     genderM  age:genderM\n(Intercept)  0.98793825 -0.076828444 -0.85989312  0.061591713\nage         -0.07682844  0.008502217  0.07210153 -0.007497458\ngenderM     -0.85989312  0.072101528  2.07233695 -0.160873344\nage:genderM  0.06159171 -0.007497458 -0.16087334  0.015550636\n```\n:::\n\n```{.r .cell-code}\nvcovHAC(dental_lm, weights = weightsLumley)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)         age    genderM age:genderM\n(Intercept)   2.3180590 -0.19769599 -2.6024990  0.21980484\nage          -0.1976960  0.01957897  0.2314731 -0.02220841\ngenderM      -2.6024990  0.23147308  5.2298357 -0.44622075\nage:genderM   0.2198048 -0.02220841 -0.4462207  0.04203649\n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-44_eebc0a355b4b2f9c7c265cff96d37d30'}\n\n```{.r .cell-code}\n# vcovPL is most appropriate I think, bu\n\nvcovPL(dental_lm, cluster = ~age , kernel = \"Bartlett\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)         age     genderM age:genderM\n(Intercept)  0.36436517 -0.03429299 -0.37025009  0.03404624\nage         -0.03429299  0.00491551  0.03440182 -0.00480140\ngenderM     -0.37025009  0.03440182  1.50047722 -0.12229004\nage:genderM  0.03404624 -0.00480140 -0.12229004  0.01217204\n```\n:::\n\n```{.r .cell-code}\nvcovPL(dental_lm, cluster = dental$agef) # on the order\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)         age     genderM age:genderM\n(Intercept)  0.36436517 -0.03429299 -0.37025009  0.03404624\nage         -0.03429299  0.00491551  0.03440182 -0.00480140\ngenderM     -0.37025009  0.03440182  1.50047722 -0.12229004\nage:genderM  0.03404624 -0.00480140 -0.12229004  0.01217204\n```\n:::\n\n```{.r .cell-code}\nvcovCL(dental_lm, cluster = ~id)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)          age     genderM  age:genderM\n(Intercept)  0.56190635 -0.031179559 -0.56190635  0.031179559\nage         -0.03117956  0.004258417  0.03117956 -0.004258417\ngenderM     -0.56190635  0.031179559  2.02816740 -0.145146882\nage:genderM  0.03117956 -0.004258417 -0.14514688  0.014592405\n```\n:::\n\n```{.r .cell-code}\nvcov(dental_cs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)          age    genderM  age:genderM\n(Intercept)   1.4006890 -0.096102802 -1.4006890  0.096102802\nage          -0.0961028  0.008736618  0.0961028 -0.008736618\ngenderM      -1.4006890  0.096102802  2.3636627 -0.162173479\nage:genderM   0.0961028 -0.008736618 -0.1621735  0.014743044\n```\n:::\n\n```{.r .cell-code}\nvcov(dental_lme_age) # is cluster supposed to be age?\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)        age    genderM age:genderM\n(Intercept)   1.5089562 -0.1121399 -1.5089562  0.11213994\nage          -0.1121399  0.0107577  0.1121399 -0.01075770\ngenderM      -1.5089562  0.1121399  2.5463635 -0.18923615\nage:genderM   0.1121399 -0.0107577 -0.1892361  0.01815361\n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-45_6a7bb7ec4ddb3383e4d4d1862af85d88'}\n\n```{.r .cell-code}\n# standard errors are wildly different, why and how? Need to test reliability.\ntidy(dental_csh)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 Ã— 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   17.4      1.18      14.7   3.08e-27\n2 age            0.479    0.0929     5.15  1.22e- 6\n3 genderM       -1.27     1.53      -0.831 4.08e- 1\n4 age:genderM    0.316    0.121      2.62  1.02e- 2\n```\n:::\n\n```{.r .cell-code}\ncoeftest(dental_lm, vcov = vcovCL, cluster = ~id)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(>|t|)    \n(Intercept) 17.372727   0.749604 23.1759 < 2.2e-16 ***\nage          0.479545   0.065257  7.3486 4.712e-11 ***\ngenderM     -1.032102   1.424137 -0.7247   0.47025    \nage:genderM  0.304830   0.120799  2.5234   0.01313 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\ncoeftest(dental_lm, vcov = vcovPL, cluster = ~id) # almost 10x lower standard error?\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nt test of coefficients:\n\n             Estimate Std. Error  t value  Pr(>|t|)    \n(Intercept) 17.372727   0.056518 307.3812 < 2.2e-16 ***\nage          0.479546   0.004701 102.0090 < 2.2e-16 ***\ngenderM     -1.032102   0.527300  -1.9573   0.05299 .  \nage:genderM  0.304829   0.043791   6.9610 3.128e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\ncoeftest(dental_lm, vcov = vcovHAC)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(>|t|)    \n(Intercept) 17.372727   0.993951 17.4785 < 2.2e-16 ***\nage          0.479545   0.092207  5.2007 9.998e-07 ***\ngenderM     -1.032102   1.439561 -0.7170   0.47501    \nage:genderM  0.304830   0.124702  2.4445   0.01619 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nIntroducing this many different ways for estimating \"robust\" standard errors is quite confusing.\n\n\n# Random Coefficient Models\n\nThis is a method of adjusting for differing slopes and effects.\n\n## Example: Autism\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-46_c8caade76d9598f825c34aaa94367b79'}\n\n```{.r .cell-code}\nautism <- read.csv(\"data/autism.csv\") %>% \n  mutate(sicdegp = factor(sicdegp),\n         childid = factor(childid)) \n\nautism_copy <- autism\n\nautism %>% ggplot(aes(age, vsae, group = childid, color = sicdegp)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~sicdegp)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row(s) containing missing values (geom_path).\n```\n:::\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-46-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-47_2d96e65d0025538b083eadefad3f1221'}\n\n```{.r .cell-code}\n# how many distinct in each\nautism %>% count(sicdegp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  sicdegp   n\n1       1 192\n2       2 255\n3       3 165\n```\n:::\n\n```{.r .cell-code}\nautism %>% count(sicdegp, childid) %>%\n  pivot_wider(names_from = sicdegp, values_from = n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 158 Ã— 4\n   childid   `1`   `2`   `3`\n   <fct>   <int> <int> <int>\n 1 2           5    NA    NA\n 2 6           4    NA    NA\n 3 8           3    NA    NA\n 4 10          4    NA    NA\n 5 13          3    NA    NA\n 6 22          3    NA    NA\n 7 31          4    NA    NA\n 8 32          3    NA    NA\n 9 38          5    NA    NA\n10 41          3    NA    NA\n# â€¦ with 148 more rows\n# â„¹ Use `print(n = ...)` to see more rows\n```\n:::\n\n```{.r .cell-code}\n# 2 missing vsae values\nautism %>% lapply(rlang::as_function(~which(is.na(.x)))) # columnwise which missing\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$age\ninteger(0)\n\n$vsae\n[1] 507 553\n\n$sicdegp\ninteger(0)\n\n$childid\ninteger(0)\n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-48_2b64934894e0f0852d177d3e31904a33'}\n\n```{.r .cell-code}\n# without child id\nautism_lm <- lm(vsae ~ age*sicdegp, data = autism) # autism\n\n# with child id\nautism_lm_id <- lm(vsae ~ age*sicdegp + childid, data = autism)\n\n# with id varying slopes\nautism_lm_id_slope <- lm(vsae ~ age*sicdegp + childid + childid:age + childid:sicdegp, data = autism)\n\n\n# predictions, plotted raw\nautism_predict <- autism %>% filter(complete.cases(.)) %>% \n  add_column(\n  yhat_lm = predict(autism_lm),\n  yhat_lm_id = predict(autism_lm_id),\n  yhat_lm_id_slope = predict(autism_lm_id_slope))\n\nautism_predict %>% pivot_longer(cols = c(vsae, starts_with(\"yhat\")),\n                                names_to = \"type\",\n                                values_to = \"y\") %>% \n  ggplot(aes(age, y, group = childid, color = type)) +\n  geom_line(alpha = .2) +\n  facet_grid(type~sicdegp)\n```\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-48-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-49_3334f85ac037383cdec06f9a12273092'}\n\n```{.r .cell-code}\n# fully fixed\nc(slope1 = coef(autism_lm)[2],\n  slope2 = sum(coef(autism_lm)[c(2,5)]),\n  slope3 = sum(coef(autism_lm)[c(2,6)]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nslope1.age     slope2     slope3 \n  2.615931   4.233238   6.980310 \n```\n:::\n\n```{.r .cell-code}\n# with id\nc(slope1 = coef(autism_lm_id)[\"age\"],\n  slope2 = sum(coef(autism_lm_id)[c(\"age\", \"age:sicdegp2\")]),\n  slope3 = sum(coef(autism_lm_id)[c(\"age\", \"age:sicdegp3\")]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nslope1.age     slope2     slope3 \n  2.598931   4.072511   7.075103 \n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-50_467b64cebedd9851c93ab544e33acc6f'}\n\n```{.r .cell-code}\n# TODO: with id_slope\n# average coefficients for each fitted child? seems okay way to summarize this fixed effects model.\nrg_lm_id_slope <- ref_grid(autism_lm_id_slope, at = list(\"age\" = c(2, 3), \"sicdegp\")) # nesting\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nNOTE: A nesting structure was detected in the fitted model:\n    childid %in% sicdegp\n```\n:::\n\n```{.r .cell-code}\nemmp_lm_id_slope <- pairs(emmeans(rg_lm_id_slope, specs = c(\"age\", \"sicdegp\"), by = \"childid\")) %>% as.data.frame()\nemmp_lm_id_slope %>% group_by(sicdegp) %>% \n  summarize(mean_slope = -mean(estimate, na.rm = TRUE)) %>% pull(mean_slope)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.004215 3.631826 7.258098\n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-51_e079e6b8a28284681549d4a38cd9be0a'}\n\n```{.r .cell-code}\n# random models\nautism_lme_id <- lme(vsae~age*sicdegp,\n                  random = ~ 1 | childid,\n                  data = autism[complete.cases(autism),])\n\nautism_lmer_id <- lmer(vsae~ age * sicdegp + (1| childid),\n                       data = autism)\n\nautism_lmer_id_slope <- lmer(vsae~ age * sicdegp + ( age | childid),\n                             data = autism)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n```{.r .cell-code}\nautism_lmer_id_slope_split <- lmer(vsae~ age * sicdegp + (1 | childid) + ( 0 + age | childid),\n                             data = autism)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n```{.r .cell-code}\n# uncorrelated intercept and age\nautism_lmer_id_slope_uncor <- lmer(vsae ~ age * sicdegp + (age || childid),\n                             data = autism)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n```{.r .cell-code}\n# unstructured covariance matrix\n# gls(vsae~age*sicdegp,\n#     random = ~1 | childid,\n#     correlation = corSymm(form = ~1|sicdegp),\n#     weights = varIdent(form = ~ age)) \n\n\nautism_lme_id_slope <- lme(vsae~age*sicdegp,\n                           random = ~ age | childid,\n                           data = autism[complete.cases(autism),],\n                           control = lmeControl(opt = \"optim\"))\n\nsummary(autism_lme_id_slope)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed-effects model fit by REML\n  Data: autism[complete.cases(autism), ] \n      AIC      BIC    logLik\n  4716.13 4760.166 -2348.065\n\nRandom effects:\n Formula: ~age | childid\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev   Corr  \n(Intercept) 8.602952 (Intr)\nage         4.019888 -0.998\nResidual    7.769026       \n\nFixed effects:  vsae ~ age * sicdegp \n                 Value Std.Error  DF   t-value p-value\n(Intercept)   1.843174 1.6899783 449  1.090650  0.2760\nage           2.971975 0.6268443 449  4.741170  0.0000\nsicdegp2     -0.324782 2.2318720 155 -0.145520  0.8845\nsicdegp3     -3.858107 2.4775418 155 -1.557232  0.1215\nage:sicdegp2  0.715160 0.8290366 449  0.862640  0.3888\nage:sicdegp3  4.334815 0.9181896 449  4.721046  0.0000\n Correlation: \n             (Intr) age    scdgp2 scdgp3 ag:sc2\nage          -0.891                            \nsicdegp2     -0.757  0.675                     \nsicdegp3     -0.682  0.608  0.517              \nage:sicdegp2  0.674 -0.756 -0.891 -0.460       \nage:sicdegp3  0.609 -0.683 -0.461 -0.888  0.516\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.87638128 -0.34680674 -0.02510791  0.31120771  5.28293064 \n\nNumber of Observations: 610\nNumber of Groups: 158 \n```\n:::\n\n```{.r .cell-code}\n# lmer_id\nc(fixef(autism_lmer_id)[\"age\"],\n  sum(fixef(autism_lmer_id)[c(\"age\", \"age:sicdegp2\")]),\n  sum(fixef(autism_lmer_id)[c(\"age\", \"age:sicdegp3\")]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     age                   \n2.618120 4.147079 7.041573 \n```\n:::\n\n```{.r .cell-code}\n# lme_id_slope\nc(fixef(autism_lme_id_slope)[\"age\"],\n  sum(fixef(autism_lme_id_slope)[c(\"age\", \"age:sicdegp2\")]),\n  sum(fixef(autism_lme_id_slope)[c(\"age\", \"age:sicdegp3\")]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     age                   \n2.971975 3.687136 7.306791 \n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-52_aa220141626690824a44cd1273d3a657'}\n\n```{.r .cell-code}\n# random models\n```\n:::\n\n\n\n\n# Baseline adjustment\n\nThere are 4 ways that we can discuss for baseline adjustment from @fitzmaurice_applied_2011 chapter 5.7\n\n1.  Keep in outcome vector, no assumptions about group differences in mean response at baseline. \"baseline group different\" (basically what we did prior)\n2.  Keep in outcome vecto, make assumption that group means are equal at baseline (like in randomized trial) \"no baseline\n3.  Subtract baseline response from post-baseline. Analyze the diffrences from baseline\n4.  Use baseline value as a covariates in the analysis of the post-baseline responses.\n\nIn summary, Fitzmaurice shows and recommends:\n\n-   1 = 3.\n-   2 is just as efficient as 4\n-   2 is more efficient than 1.\n-   assumption 2 is most accurate in a randomized control trial, in which you expect baseline to be similar. introducing backdoor bias (conditioning on collider) may happen in observational studies. In which case use 1\n\nDepending on the strategy that you choose, the interpretation of the coefficients will differ, so beware, although some cases reduce to linear combinations of the coefficients.\n\nWe examine the differences of these 4 approaches on the TLC dataset.\n\n## Method Implementations {.tabset}\n\n### 1. raw\n\n<figure>\n\n<center>\n\n<img src=\"img/tlc_baseline_raw.png\" style=\"width:50%\"/></img>\n\n<figcaption>\n\n</figcaption>\n\n</center>\n\n</figure>\n\nThis method is just the standard response profile analysis, which we've shown above.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-53_b007fa7944744c9001a423ae702aedaf'}\n\n```{.r .cell-code}\ntlc_baseline_1 <- tlc_gls\n```\n:::\n\n\n### 2. constrain baseline\n\n<figure>\n\n<center>\n\n<img src=\"img/tlc_constrain_baseline.png\" style=\"width:80%\"/></img>\n\n<figcaption>\n\n</figcaption>\n\n</center>\n\n</figure>\n\nIn order to force baseline to be modeled the same, we need to remove the main effect for treament, and the interaction term involving the baseline. Unfortunately we can't remove a single level of the interaction if we specify `week*trt`, nor does `gls` accept a manually made model matrix, so we must construct the variables through the formula interface.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-54_e6428e6f348b7f3e79077fb4a8b2763b'}\n\n```{.r .cell-code}\n# keep in outcome, assume similar intercept \n# remove main effect, and week0 interactions.\n# gls doesn't take model.matrix, so manual specification through formula is the way to do it\n# update also acts kinda weird\n# table 5.7  in Fitzmaurice\n# https://content.sph.harvard.edu/fitzmaur/ala2e/\n\ntlc_baseline_2 <- gls(lead ~ week + \n                        I(week == \"w1\" & trt == \"A\") +\n                        I(week == \"w4\" & trt == \"A\") +\n                        I(week == \"w6\" & trt == \"A\"),\n                      data = tlc,\n                      correlation = corSymm(form = ~ time | id),\n                      weights = varIdent(form = ~ 1 | week))\n\n# type III fixed effect test for interaction hypothesis\nanova(tlc_baseline_2, Terms = 3:5) # Chisq = F * ndf = 111.94\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDenom. DF: 393 \n F-test for: I(week == \"w1\" & trt == \"A\"), I(week == \"w4\" & trt == \"A\"), I(week == \"w6\" & trt == \"A\") \n  numDF  F-value p-value\n1     3 37.31422  <.0001\n```\n:::\n:::\n\n\n### 3. change from baseline\n\n<figure>\n\n<center>\n\n<img src=\"img/tlc_baseline_change.png\" style=\"width:60%\"/></img>\n\n<figcaption>\n\n</figcaption>\n\n</center>\n\n</figure>\n\nThis analysis is just a reframed version of the first analysis, at least in terms of estimates.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-55_0b1e37877b09bd1ece6cedda157142a7'}\n\n```{.r .cell-code}\n# modeling the difference\n# time - 1, because gls throws error. corSymm needs consecutive integers starting at 1.\ntlc_baseline_3 <- gls(lead_diff ~ trt*week,\n                      correlation = corSymm(form = ~ time-1 | id), # unstructured\n                      weights = varIdent(form = ~ 1 | week),  # heterogenous\n                      data = tlc_diff %>% mutate(trt = factor(trt, levels = c(\"P\", \"A\"))))\n\nanova(tlc_baseline_3, Terms = c(2, 4)) # trt x week interaction test is now test on combined c(trt, trt:week) of diff\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDenom. DF: 294 \n F-test for: trt, trt:week \n  numDF  F-value p-value\n1     3 35.92872  <.0001\n```\n:::\n:::\n\n\n::: {style=\"display:flex;justify-content:center;align-items:flex-start\"}\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-56_027789d6c9f7a8c1e282ad54c486a26d'}\n::: {.cell-output-display}\n`````{=html}\n<table style=\"width:30%; float: left; margin-right: 10px;\" class=\"table\">\n<caption>Baseline Change Coefficients</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> change </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -1.612 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trtA </td>\n   <td style=\"text-align:right;\"> -11.406 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> weekw4 </td>\n   <td style=\"text-align:right;\"> -0.590 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> weekw6 </td>\n   <td style=\"text-align:right;\"> -1.014 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trtA:weekw4 </td>\n   <td style=\"text-align:right;\"> 2.582 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trtA:weekw6 </td>\n   <td style=\"text-align:right;\"> 8.254 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"width:30%; \" class=\"table\">\n<caption>Raw Coefficients</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> raw </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> 26.272 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trtA </td>\n   <td style=\"text-align:right;\"> 0.268 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> weekw1 </td>\n   <td style=\"text-align:right;\"> -1.612 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> weekw4 </td>\n   <td style=\"text-align:right;\"> -2.202 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> weekw6 </td>\n   <td style=\"text-align:right;\"> -2.626 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trtA:weekw1 </td>\n   <td style=\"text-align:right;\"> -11.406 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trtA:weekw4 </td>\n   <td style=\"text-align:right;\"> -8.824 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trtA:weekw6 </td>\n   <td style=\"text-align:right;\"> -3.152 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n:::\n\nTo see that they are the same, notice that the baseline change model term estimates are simply linear combinations of the raw model.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-57_1d6b2239c2760038aaf856318f6e7685'}\n\n```{.r .cell-code}\ntribble(~`change model`, ~`raw model terms`, ~`value` , ~`interpretation`,\n        \"(Intercept)\", \"weekw1\", \"-1.612\",  \"in placebo, diff between w1 and baseline\",\n        \"trtA\", \"trtA:weekw1\", \"-11.406\",  \"difference in trt slopes from baseline to w1\",\n        \"weekw4\", \"weekw4 - weekw1\",\"-0.59\", \"in placebo, diff between w4 and w1\",\n        \"trtA:weekw4\", \"trtA:weekw4 - trtA:weekw1\",\"2.582\", \"diff in trt slope from w4 to w1\") %>% kbl() %>% kable_styling(full_width = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> change model </th>\n   <th style=\"text-align:left;\"> raw model terms </th>\n   <th style=\"text-align:left;\"> value </th>\n   <th style=\"text-align:left;\"> interpretation </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:left;\"> weekw1 </td>\n   <td style=\"text-align:left;\"> -1.612 </td>\n   <td style=\"text-align:left;\"> in placebo, diff between w1 and baseline </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trtA </td>\n   <td style=\"text-align:left;\"> trtA:weekw1 </td>\n   <td style=\"text-align:left;\"> -11.406 </td>\n   <td style=\"text-align:left;\"> difference in trt slopes from baseline to w1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> weekw4 </td>\n   <td style=\"text-align:left;\"> weekw4 - weekw1 </td>\n   <td style=\"text-align:left;\"> -0.59 </td>\n   <td style=\"text-align:left;\"> in placebo, diff between w4 and w1 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> trtA:weekw4 </td>\n   <td style=\"text-align:left;\"> trtA:weekw4 - trtA:weekw1 </td>\n   <td style=\"text-align:left;\"> 2.582 </td>\n   <td style=\"text-align:left;\"> diff in trt slope from w4 to w1 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nGiven that they are the same (or linear combinations), what about the standard errors of the estimates? Let's check for \"weekw4\"\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-58_93733c2a8ca69f0be74f465cea410c77'}\n\n```{.r .cell-code}\ncbind(c(-1, 1) %*% tlc_gls$varBeta[3:4, 3:4] %*% c(-1, 1), # old model std err\n      tlc_baseline_3$varBeta[\"weekw4\", \"weekw4\"]) # baseline change model std err\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]     [,2]\n[1,] 0.4130701 0.413059\n```\n:::\n:::\n\n\nThey are the same!\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-59_ced0cead5df221d44502de6a24e1a427'}\n\n```{.r .cell-code}\nlist(raw = tlc_gls$varBeta,\n     base3=tlc_baseline_3$varBeta)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$raw\n            (Intercept)        trtA     weekw1     weekw4      weekw6\n(Intercept)  0.50451606 -0.50451606 -0.1223674 -0.1105218 -0.06048317\ntrtA        -0.50451606  1.00903212  0.1223674  0.1105218  0.06048317\nweekw1      -0.12236737  0.12236737  0.6271371  0.4390662  0.27183933\nweekw4      -0.11052175  0.11052175  0.4390662  0.6640654  0.27889731\nweekw6      -0.06048317  0.06048317  0.2718393  0.2788973  0.78947714\ntrtA:weekw1  0.12236737 -0.24473475 -0.6271371 -0.4390662 -0.27183933\ntrtA:weekw4  0.11052175 -0.22104350 -0.4390662 -0.6640654 -0.27889731\ntrtA:weekw6  0.06048317 -0.12096634 -0.2718393 -0.2788973 -0.78947714\n            trtA:weekw1 trtA:weekw4 trtA:weekw6\n(Intercept)   0.1223674   0.1105218  0.06048317\ntrtA         -0.2447347  -0.2210435 -0.12096634\nweekw1       -0.6271371  -0.4390662 -0.27183933\nweekw4       -0.4390662  -0.6640654 -0.27889731\nweekw6       -0.2718393  -0.2788973 -0.78947714\ntrtA:weekw1   1.2542741   0.8781324  0.54367866\ntrtA:weekw4   0.8781324   1.3281308  0.55779461\ntrtA:weekw6   0.5436787   0.5577946  1.57895428\n\n$base3\n            (Intercept)       trtA     weekw4     weekw6 trtA:weekw4\n(Intercept)   0.6271416 -0.6271416 -0.1880546 -0.3553027   0.1880546\ntrtA         -0.6271416  1.2542831  0.1880546  0.3553027  -0.3761093\nweekw4       -0.1880546  0.1880546  0.4130590  0.1951241  -0.4130590\nweekw6       -0.3553027  0.3553027  0.1951241  0.8729404  -0.1951241\ntrtA:weekw4   0.1880546 -0.3761093 -0.4130590 -0.1951241   0.8261179\ntrtA:weekw6   0.3553027 -0.7106053 -0.1951241 -0.8729404   0.3902482\n            trtA:weekw6\n(Intercept)   0.3553027\ntrtA         -0.7106053\nweekw4       -0.1951241\nweekw6       -0.8729404\ntrtA:weekw4   0.3902482\ntrtA:weekw6   1.7458807\n```\n:::\n:::\n\n\nThe marginal covariances are also the same *if we change the estimation method to ML*.\n\nThe idea here was that we could calculate the change from baseline covariance estimate from the raw model estimates of covariance.\n\n$$\n\\begin{aligned}\n\\underbrace{\\widehat{\\mathrm{Cov}}(Y_{i2} -Y_{i1}, Y_{i2} -Y_{i1})}_{\\Delta \\text{ baseline model estimates}} = \\underbrace{\\widehat{\\mathrm{Var}}(Y_{{i2}}) - 2\\widehat{\\mathrm{Cov}}(Y_{i2}, Y_{i1}) + \\widehat{\\mathrm{Var}}(Y_{i1})}_{\\text{Raw model estimates}}\n\\end{aligned}\n$$\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-60_20f76c78a153f179dab81064389dea72'}\n\n```{.r .cell-code}\n# We note this is NOT true when we use the REML estimates\n# linear \nL <- matrix(c(-1, 1, 0, 0,\n              -1, 0, 1, 0,\n              -1, 0, 0, 1), byrow = TRUE, ncol = 4)\n\n# not the same!\nlist(`from_baseline_change_model` = getVarCov(tlc_baseline_3),\n     `from_raw_model` =  L %*% getVarCov(tlc_baseline_1) %*% t(L))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$from_baseline_change_model\nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]\n[1,] 31.357 21.954 13.592\n[2,] 21.954 33.205 13.945\n[3,] 13.592 13.945 39.474\n  Standard Deviations: 5.5997 5.7623 6.2828 \n\n$from_raw_model\n         [,1]     [,2]     [,3]\n[1,] 31.35685 21.95331 13.59197\n[2,] 21.95331 33.20327 13.94487\n[3,] 13.59197 13.94487 39.47386\n```\n:::\n:::\n\n\nWith REML estimates, they are NOT equal. But if we refit the models with ML,\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-61_b0131af57b6004f31621bcd0b404e5c4'}\n\n```{.r .cell-code}\n# what if we try ML?\ntlc_baseline_3_ml <- tlc_baseline_3 %>% update(method = \"ML\")\ntlc_baseline_1_ml <- tlc_gls %>% update(method = \"ML\")\n```\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-62_7a2288125ebbdcfde059a8dd62bfbf60'}\n\n```{.r .cell-code}\nlist(`from_baseline_change_model` = getVarCov(tlc_baseline_3_ml),\n     `from_raw_model` =  L %*% getVarCov(tlc_baseline_1_ml) %*% t(L))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$from_baseline_change_model\nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]\n[1,] 30.729 21.515 13.319\n[2,] 21.515 32.541 13.666\n[3,] 13.319 13.666 38.684\n  Standard Deviations: 5.5434 5.7044 6.2196 \n\n$from_raw_model\n         [,1]     [,2]     [,3]\n[1,] 30.72886 21.51453 13.31984\n[2,] 21.51453 32.54042 13.66637\n[3,] 13.31984 13.66637 38.68419\n```\n:::\n:::\n\n\n### 4. ancova change\n\nThis method results in standard error estimates that are more efficient, but generally only recommended for randomized studies. In observational studies, there's danger in introducing bias.\n\nWe can do ancova on the raw values, or change from baseline values. That is, we could consider either:\n\n\n```{=tex}\n\\begin{enumerate}\n\\item\n$$\n\\begin{aligned}\nY_{ij} = X_{ij}\\beta^{(ancova)} + Y_{i1}\\gamma + \\varepsilon_{ij} \n\\end{aligned}\n$$\n\\item\n$$\n\\begin{aligned}\nY_{ij} - Y_{i1} = X_{ij}\\beta^{(ancova)} + Y_{i1}\\gamma' + \\varepsilon_{ij} \n\\end{aligned}\n$$\n\\end{enumerate}\n```\n\nBoth of these will give the same estimates (maybe obviously when written out this way), but $\\gamma' = \\gamma + 1$.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-63_0deef1167666fbdc83e610262df444ab'}\n\n```{.r .cell-code}\n# ancova\ntlc_baseline_4 <- gls(lead ~ trt*week + lead_base,\n                      correlation = corSymm(form = ~ time-1 | id), # unstructured\n                      weights = varIdent(form = ~ 1 | week),  # heterogeneous\n                      data = tlc_diff)\n\ntlc_baseline_4b <- gls(lead_diff ~ trt*week + lead_base,\n                       correlation = corSymm(form = ~ time-1 | id), # unstructured\n                       weights = varIdent(form = ~ 1 | week),  # heterogeneous\n                       data = tlc_diff)\n\ncbind(coef(tlc_baseline_4),\n               coef(tlc_baseline_4b)) %>% \n  `colnames<-`(c(\"Ancova\", \"Ancova Diff\")) %>% \n  as.data.frame() %>% \n  rownames_to_column(\"term\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         term      Ancova Ancova Diff\n1 (Intercept)   3.5236964   3.5236964\n2        trtA -11.3536109 -11.3536109\n3      weekw4  -0.5900000  -0.5900000\n4      weekw6  -1.0140000  -1.0140000\n5   lead_base   0.8045183  -0.1954817\n6 trtA:weekw4   2.5820000   2.5820000\n7 trtA:weekw6   8.2540000   8.2540000\n```\n:::\n:::\n\n\n## Comparison\n\nIt's hard to compare the estimates of the coefficients directly, so we compare them for the hypothesis test of the treatment by time interaction.\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-64_31d792cfc1d967f424176920b31b7853'}\n\n```{.r .cell-code}\n# contrast matrix for tlc_baseline_3\n# trtA (interaction w1), trtA + trtA:w4, trtA + trtA:w6\nL <- matrix(c(0, 1, 0, 0, 0, 0,\n              0, 1, 0, 0, 1, 0,\n              0, 1, 0, 0, 0, 1), byrow = T, nrow = 3)\nrbind(\n  `raw` = anova(tlc_baseline_1, Terms = 4),\n  `constrain baseline` = anova(tlc_baseline_2, Terms = 3:5),\n  `baseline change` = anova(tlc_baseline_3, L = L), # same as tlc_baseline_1 as expected\n  `ancova` = anova(tlc_baseline_4, Terms = c(2,5))) %>%  # similarly efficient as baseline 2 model\n  as.data.frame() %>% \n  mutate(`Chisq` = numDF * `F-value`,\n         .after= `F-value`)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   numDF  F-value    Chisq      p-value\nraw                    3 35.92934 107.7880 1.557061e-20\nconstrain baseline     3 37.31422 111.9427 3.074120e-21\nbaseline change        3 35.92872 107.7862 8.227085e-20\nancova                 3 37.04297 111.1289 2.517144e-20\n```\n:::\n:::\n\n\nWe see that 2 and 4 have larger F-statistics, implying that there is slightly more power to detect the group differences (assuming the assumptions are reasonable).\n\n@fitzmaurice_applied_2011 concludes that 2 should be recommended, because we get similar efficiency gains, and there is an implicit assumption about the covariance matrix of the ancova model. In the ANCOVA model, we only have a 3x3 covariance matrix, where as in baseline change model, we estimated a 4x4 covariance matrix. Hence, we are implicitly assuming that $\\mathrm{Cov}(Y_{i1}, Y_{i2}) = \\mathrm{Cov}(Y_{i1}, Y_{i3}) = \\mathrm{Cov}(Y_{i1}, Y_{i4})$\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-65_d9177e698d94e1df26ef0bd4175d80d4'}\n\n```{.r .cell-code}\ntlc_baseline_4 %>% getVarCov()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]\n[1,] 30.151 20.864 12.991\n[2,] 20.864 32.230 13.460\n[3,] 12.991 13.460 39.478\n  Standard Deviations: 5.491 5.6771 6.2831 \n```\n:::\n:::\n\n\n## References {#references}\n\n-   [European Agency for evaluation of medicinal products](https://www.ema.europa.eu/en/documents/scientific-guideline/points-consider-adjustment-baseline-covariates_en.pdf) from a quick google search\n\n# Residuals\n\nResiduals from a longitudinal model will be correlated. @fitzmaurice_applied_2011 recommends that we \"decorrelate\" them with a cholesky decomposition of the estimated covariance of the errors. It can get confusing with the estimators and terminology here...\n\n$$\n\\begin{aligned}\n\\varepsilon &= Y_i - X_i\\beta \\\\\n\\hat\\varepsilon &= Y_i - X_i \\hat\\beta \\\\\n\\mathrm{Cov}(\\varepsilon) &= \\Sigma \\\\\n\\widehat{\\mathrm{Cov}}(\\varepsilon) &= \\hat\\Sigma \\\\\n\\mathrm{Cov}(\\hat\\varepsilon) &= ??\n\\end{aligned}\n$$\n\nWe assume that $\\mathrm{Cov}(\\hat \\varepsilon) \\approx \\mathrm{Cov}(\\varepsilon)$.\n\n## Decorrelation\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-66_52bff609b030cb20b15752e4228a5e64'}\n\n```{.r .cell-code}\n# can extract residuals with type = \"normalized\"\nfat_post %>% count(id)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     id  n\n1     1  6\n2     2  9\n3     3  8\n4     4  6\n5     5  7\n6     6  8\n7     7  9\n8     8  3\n9     9  7\n10   10  7\n11   11  7\n12   12  6\n13   13  5\n14   14  8\n15   15  9\n16   16 10\n17   17  8\n18   18  9\n19   19  9\n20   20  8\n21   21  9\n22   22  6\n23   23  5\n24   24  8\n25   25  7\n26   26  7\n27   27  5\n28   28  7\n29   29  6\n30   30  8\n31   31  6\n32   32  5\n33   33  5\n34   34  5\n35   35  6\n36   36  7\n37   37  6\n38   38  8\n39   39  6\n40   40  6\n41   41  5\n42   42  9\n43   43  5\n44   44  3\n45   45  7\n46   46  7\n47   47  6\n48   48  5\n49   49  7\n50   50  8\n51   51  8\n52   52  7\n53   53  8\n54   54  7\n55   55  9\n56   56  8\n57   57  6\n58   58  5\n59   59  5\n60   60  5\n61   61  8\n62   62  8\n63   63  7\n64   64  6\n65   65  7\n66   66  8\n67   67  8\n68   68  7\n69   69  7\n70   70  3\n71   71  8\n72   72  6\n73   73  8\n74   74  8\n75   75  7\n76   76  6\n77   77  3\n78   78  8\n79   79  7\n80   80  8\n81   81  6\n82   82  6\n83   83  6\n84   84  6\n85   85  6\n86   86  8\n87   87  8\n88   88  5\n89   89  3\n90   90  3\n91   91  5\n92   92  8\n93   93  7\n94   94  6\n95   95  7\n96   96  6\n97   97  5\n98   98  8\n99   99  8\n100 100  8\n101 101  8\n102 102  8\n103 103  6\n104 104  5\n105 105  5\n106 106  5\n107 107  6\n108 108  4\n109 109  7\n110 110  7\n111 111  8\n112 112  7\n113 113  7\n114 114  7\n115 115  7\n116 116  7\n117 117  6\n118 118  5\n119 119  5\n120 120  7\n121 121  7\n122 122  7\n123 123  7\n124 124  5\n125 125  6\n126 126  4\n127 127  5\n128 128  7\n129 129  7\n130 130  6\n131 131  6\n132 132  7\n133 133  7\n134 134  4\n135 135  7\n136 136  6\n137 137  7\n138 138  7\n139 139  7\n140 140  7\n141 141  7\n142 142  7\n143 143  5\n144 144  6\n145 145  7\n146 146  7\n147 147  5\n148 148  5\n149 149  5\n150 150  6\n151 151  6\n152 152  6\n153 153  6\n154 154  6\n155 155  6\n156 156  6\n157 157  6\n158 158  6\n159 159  5\n160 160  3\n161 161  5\n162 162  5\n```\n:::\n\n```{.r .cell-code}\nS <- getVarCov(fat_lme, type = \"marginal\", individuals = 1)\nS\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nid 1 \nMarginal variance covariance matrix\n       1      2      3      4      5      6\n1 60.288 46.991 43.546 39.949 36.007 32.886\n2 46.991 54.304 42.885 40.853 38.553 35.311\n3 43.546 42.885 51.763 41.668 40.846 37.496\n4 39.949 40.853 41.668 51.991 43.240 39.776\n5 36.007 38.553 40.846 43.240 55.056 42.044\n6 32.886 35.311 37.496 39.776 42.044 48.858\n  Standard Deviations: 7.7645 7.3691 7.1946 7.2105 7.42 6.9898 \n```\n:::\n\n```{.r .cell-code}\nR <- S[[1]] %>% chol()\nR\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         1        2        3        4        5        6\n1 7.764524 6.051983 5.608287 5.145088 4.637412 4.235458\n2 0.000000 4.204493 2.127138 2.310672 2.494246 2.301845\n3 0.000000 0.000000 3.973052 1.987756 2.399240 2.226406\n4 0.000000 0.000000 0.000000 4.028538 2.196216 2.045436\n5 0.000000 0.000000 0.000000 0.000000 4.092661 1.668148\n6 0.000000 0.000000 0.000000 0.000000 0.000000 3.700942\n```\n:::\n\n```{.r .cell-code}\ncrossprod(R) # R'R\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         1        2        3        4        5        6\n1 60.28784 46.99077 43.54568 39.94916 36.00730 32.88632\n2 46.99077 54.30426 42.88479 40.85319 38.55258 35.31101\n3 43.54568 42.88479 51.76274 41.66771 40.84585 37.49563\n4 39.94916 40.85319 41.66771 51.99143 43.23992 39.77628\n5 36.00730 38.55258 40.84585 43.23992 55.05645 42.04400\n6 32.88632 35.31101 37.49563 39.77628 42.04400 48.85798\n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-67_5c6dfb42619311ccc695d37a59c87eac'}\n\n```{.r .cell-code}\neps_hat <- residuals(fat_lme, type = \"pearson\")\neps_hat[fat_post$id == 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         1          1          1          1          1          1 \n-1.4696365  0.6821217 -0.3313941  2.4943915 -2.0320743  0.2480700 \n```\n:::\n\n```{.r .cell-code}\nRinv <- solve(R)\neps_hat[fat_post$id == 1] %*% Rinv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              1         2           3         4          5          6\n[1,] -0.1892758 0.4346816 -0.04895698 0.6357493 -0.8594188 0.07874242\n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-68_402e140c7f7f4c45c57d5553d4a9dce7'}\n\n```{.r .cell-code}\nresiduals(fat_lme, type = \"normalized\")[fat_post$id == 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         1          1          1          1          1          1 \n-1.4696365  0.6821217 -0.3313941  2.4943915 -2.0320743  0.2480700 \n```\n:::\n\n```{.r .cell-code}\nfat_lme$modelStruct\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nreStruct  parameters:\n       id1        id2        id3        id4        id5        id6 \n 0.7894148 -0.9241612 -1.3828910  0.1210868 -0.2928406 -0.3762166 \n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-69_d977aa715638d384c5fc894f07bd07ac'}\n\n```{.r .cell-code}\n?nlme:::recalc\nnlme:::residuals.lme\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfunction (object, level = Q, type = c(\"response\", \"pearson\", \n    \"normalized\"), asList = FALSE, ...) \n{\n    type <- match.arg(type)\n    Q <- object$dims$Q\n    val <- object[[\"residuals\"]]\n    if (is.character(level)) {\n        nlevel <- match(level, names(val))\n        if (any(aux <- is.na(nlevel))) {\n            stop(sprintf(ngettext(sum(aux), \"nonexistent level %s\", \n                \"nonexistent levels %s\"), level[aux]), domain = NA)\n        }\n        level <- nlevel\n    }\n    else {\n        level <- 1 + level\n    }\n    if (type != \"response\") {\n        val <- val[, level]/attr(val, \"std\")\n    }\n    else {\n        val <- val[, level]\n    }\n    if (type == \"normalized\") {\n        if (!is.null(cSt <- object$modelStruct$corStruct)) {\n            val <- recalc(cSt, list(Xy = as.matrix(val)))$Xy[, \n                seq_along(level)]\n        }\n        else {\n            type <- \"pearson\"\n        }\n    }\n    if (length(level) == 1) {\n        grps <- as.character(object[[\"groups\"]][, max(c(1, level - \n            1))])\n        if (asList) {\n            val <- as.list(split(val, ordered(grps, levels = unique(grps))))\n        }\n        else {\n            grp.nm <- row.names(object[[\"groups\"]])\n            val <- naresid(object$na.action, val)\n            names(val) <- grps[match(names(val), grp.nm)]\n        }\n        attr(val, \"label\") <- switch(type, response = {\n            if (!is.null(aux <- attr(object, \"units\")$y)) paste(\"Residuals\", \n                aux) else \"Residuals\"\n        }, pearson = \"Standardized residuals\", normalized = \"Normalized residuals\")\n        val\n    }\n    else naresid(object$na.action, val)\n}\n<bytecode: 0x7f7ee5f19a40>\n<environment: namespace:nlme>\n```\n:::\n:::\n\n\n\n## Aggregation\n\nThe next method deals with aggregation\n\n\n\n# Econometrics Approaches\n\nThe Econometrics methods that deal with longitudinal data is a little different than statistics. We'll explore the terminology and methods here.\n\n-   Pooled Model\n\n# Software Comparisons\n\n-   plm (unequal observations between groups, needs adjustment from nlme)\n-   lme4, nlme (maximum likelihood approach, not intuitive)\n\n# PLM Vignette\n\nProvides functions for panel data from \"econometricians\" point of view.\n\nProvides a few functions\n\n## `plm`\n\n-   the fixed effects model (\"within\"),\n-   the pooling model (\"pooling\"),\n-   the first-difference model (\"fd\"),\n-   the between model (\"between\"),\n-   the error components model (\"random\").\n\n### EmplUK\n\n-   firm - firm index\n-   year - year\n-   sector - the sector of activity\n-   emp - employment\n-   wage - wages\n-   capital - capital\n-   output - output\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-70_69311d5da64bba3b91460b60e3044b52'}\n\n```{.r .cell-code}\ndata(\"EmplUK\")\n```\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-71_948e32491af018d6dd297c94899a27d1'}\n\n```{.r .cell-code}\n# pdata.frame creats a dataframe for plm to work with\nE <- pdata.frame(EmplUK,                 # The orig dataframe\n            index=c(\"firm\",\"year\"),      # The \"individual\" and \"time\" variable names\n            drop.index = TRUE,           # Don't show index columns\n            row.names = TRUE)            # Show rownames that combine individual-time\n\nclass(E) # pdata.frame\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"pdata.frame\" \"data.frame\" \n```\n:::\n\n```{.r .cell-code}\nsummary(E$emp) # Selecting the column from a pdata.frame give a \"pseries\" object\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntotal sum of squares: 261539.4 \n         id        time \n0.980765381 0.009108488 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.104   1.181   2.287   7.892   7.020 108.562 \n```\n:::\n\n```{.r .cell-code}\nmethods(class=\"pseries\") # Looking at the methods with a pseries object\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] as.matrix         between           Between           Complex          \n [5] D                 diff              F                 G                \n [9] index             is.pbalanced      is.pconsecutive   L                \n[13] lag               lead              make.pbalanced    make.pconsecutive\n[17] Math              Ops               pcdtest           pdim             \n[21] plot              print             pvar              Sum              \n[25] summary           Within           \nsee '?methods' for accessing help and source code\n```\n:::\n\n```{.r .cell-code}\n# plm:::print.summary.pseries\n# plm:::summary.pseries # The function for summarizing the series\n```\n:::\n\n\npseries datatype comes with various functions to operate on them to compute correctly time lag within each individual\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-72_fb808961760364be5c15fe317ba4c555'}\n\n```{.r .cell-code}\nbetween(E$emp) # means across the time periods\nSum(E$emp) # sum across the time periods\nWithin(E$emp) # deviation from mean\nhead(plm:::lag.pseries(E$emp, 0:2)) # Lag the sequence within subject by however many\n```\n:::\n\n\n### Grunfield example\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-73_274d74309d681692c3d67fac40e5e28e'}\n\n```{.r .cell-code}\ndata(Grunfeld)\nGrunfeld$firm <- factor(Grunfeld$firm)\nGrunfeld$year <- factor(Grunfeld$year)\n\nskim(Grunfeld)\n```\n\n::: {.cell-output-display}\n<table style='width: auto;'\n      class='table table-condensed'>\n<caption>Data summary</caption>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Name </td>\n   <td style=\"text-align:left;\"> Grunfeld </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Number of rows </td>\n   <td style=\"text-align:left;\"> 200 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Number of columns </td>\n   <td style=\"text-align:left;\"> 5 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> _______________________ </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Column type frequency: </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> factor </td>\n   <td style=\"text-align:left;\"> 2 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> numeric </td>\n   <td style=\"text-align:left;\"> 3 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ________________________ </td>\n   <td style=\"text-align:left;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Group variables </td>\n   <td style=\"text-align:left;\"> None </td>\n  </tr>\n</tbody>\n</table>\n\n\n**Variable type: factor**\n\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> skim_variable </th>\n   <th style=\"text-align:right;\"> n_missing </th>\n   <th style=\"text-align:right;\"> complete_rate </th>\n   <th style=\"text-align:left;\"> ordered </th>\n   <th style=\"text-align:right;\"> n_unique </th>\n   <th style=\"text-align:left;\"> top_counts </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> firm </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:left;\"> 1: 20, 2: 20, 3: 20, 4: 20 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> year </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 20 </td>\n   <td style=\"text-align:left;\"> 193: 10, 193: 10, 193: 10, 193: 10 </td>\n  </tr>\n</tbody>\n</table>\n\n\n**Variable type: numeric**\n\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> skim_variable </th>\n   <th style=\"text-align:right;\"> n_missing </th>\n   <th style=\"text-align:right;\"> complete_rate </th>\n   <th style=\"text-align:right;\"> mean </th>\n   <th style=\"text-align:right;\"> sd </th>\n   <th style=\"text-align:right;\"> p0 </th>\n   <th style=\"text-align:right;\"> p25 </th>\n   <th style=\"text-align:right;\"> p50 </th>\n   <th style=\"text-align:right;\"> p75 </th>\n   <th style=\"text-align:right;\"> p100 </th>\n   <th style=\"text-align:left;\"> hist </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> inv </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 145.96 </td>\n   <td style=\"text-align:right;\"> 216.88 </td>\n   <td style=\"text-align:right;\"> 0.93 </td>\n   <td style=\"text-align:right;\"> 33.56 </td>\n   <td style=\"text-align:right;\"> 57.48 </td>\n   <td style=\"text-align:right;\"> 138.04 </td>\n   <td style=\"text-align:right;\"> 1486.7 </td>\n   <td style=\"text-align:left;\"> â–‡â–â–â–â– </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> value </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1081.68 </td>\n   <td style=\"text-align:right;\"> 1314.47 </td>\n   <td style=\"text-align:right;\"> 58.12 </td>\n   <td style=\"text-align:right;\"> 199.98 </td>\n   <td style=\"text-align:right;\"> 517.95 </td>\n   <td style=\"text-align:right;\"> 1679.85 </td>\n   <td style=\"text-align:right;\"> 6241.7 </td>\n   <td style=\"text-align:left;\"> â–‡â–‚â–â–â– </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> capital </td>\n   <td style=\"text-align:right;\"> 0 </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 276.02 </td>\n   <td style=\"text-align:right;\"> 301.10 </td>\n   <td style=\"text-align:right;\"> 0.80 </td>\n   <td style=\"text-align:right;\"> 79.17 </td>\n   <td style=\"text-align:right;\"> 205.60 </td>\n   <td style=\"text-align:right;\"> 358.10 </td>\n   <td style=\"text-align:right;\"> 2226.3 </td>\n   <td style=\"text-align:left;\"> â–‡â–â–â–â– </td>\n  </tr>\n</tbody>\n</table>\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-74_ef8d8eb3b27e9eaafb6f2e32077594a5'}\n\n```{.r .cell-code}\nGrunfeld %>% ggplot(aes(year, inv, color = factor(firm))) + \n  geom_line()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ngeom_path: Each group consists of only one observation. Do you need to adjust\nthe group aesthetic?\n```\n:::\n\n::: {.cell-output-display}\n![](longitudinal_files/figure-html/unnamed-chunk-74-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-75_e7c7630b3ebd42a1380053f998f25317'}\n\n```{.r .cell-code}\ngrun.po <- plm(inv~value + capital, data = Grunfeld, model = \"pooling\") # pooled\ngrun.fe <- plm(inv~value + capital, data = Grunfeld, model = \"within\") # fixed effects\ngrun.re <- plm(inv~value + capital, data = Grunfeld, model = \"random\") # random effects\ngrun.fd <- plm(inv~value + capital, data = Grunfeld, model = \"fd\") # first difference\n\nsummary(grun.po)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPooling Model\n\nCall:\nplm(formula = inv ~ value + capital, data = Grunfeld, model = \"pooling\")\n\nBalanced Panel: n = 10, T = 20, N = 200\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-291.6757  -30.0137    5.3033   34.8293  369.4464 \n\nCoefficients:\n               Estimate  Std. Error t-value  Pr(>|t|)    \n(Intercept) -42.7143694   9.5116760 -4.4907 1.207e-05 ***\nvalue         0.1155622   0.0058357 19.8026 < 2.2e-16 ***\ncapital       0.2306785   0.0254758  9.0548 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    9359900\nResidual Sum of Squares: 1755900\nR-Squared:      0.81241\nAdj. R-Squared: 0.8105\nF-statistic: 426.576 on 2 and 197 DF, p-value: < 2.22e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(grun.fe)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = inv ~ value + capital, data = Grunfeld, model = \"within\")\n\nBalanced Panel: n = 10, T = 20, N = 200\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-184.00857  -17.64316    0.56337   19.19222  250.70974 \n\nCoefficients:\n        Estimate Std. Error t-value  Pr(>|t|)    \nvalue   0.110124   0.011857  9.2879 < 2.2e-16 ***\ncapital 0.310065   0.017355 17.8666 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    2244400\nResidual Sum of Squares: 523480\nR-Squared:      0.76676\nAdj. R-Squared: 0.75311\nF-statistic: 309.014 on 2 and 188 DF, p-value: < 2.22e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(grun.re)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOneway (individual) effect Random Effect Model \n   (Swamy-Arora's transformation)\n\nCall:\nplm(formula = inv ~ value + capital, data = Grunfeld, model = \"random\")\n\nBalanced Panel: n = 10, T = 20, N = 200\n\nEffects:\n                  var std.dev share\nidiosyncratic 2784.46   52.77 0.282\nindividual    7089.80   84.20 0.718\ntheta: 0.8612\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-177.6063  -19.7350    4.6851   19.5105  252.8743 \n\nCoefficients:\n              Estimate Std. Error z-value Pr(>|z|)    \n(Intercept) -57.834415  28.898935 -2.0013  0.04536 *  \nvalue         0.109781   0.010493 10.4627  < 2e-16 ***\ncapital       0.308113   0.017180 17.9339  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    2381400\nResidual Sum of Squares: 548900\nR-Squared:      0.7695\nAdj. R-Squared: 0.76716\nChisq: 657.674 on 2 DF, p-value: < 2.22e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(grun.fd)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOneway (individual) effect First-Difference Model\n\nCall:\nplm(formula = inv ~ value + capital, data = Grunfeld, model = \"fd\")\n\nBalanced Panel: n = 10, T = 20, N = 200\nObservations used in estimation: 190\n\nResiduals:\n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-200.889558  -13.889063    0.016677    9.504223  195.634938 \n\nCoefficients:\n              Estimate Std. Error t-value  Pr(>|t|)    \n(Intercept) -1.8188902  3.5655931 -0.5101    0.6106    \nvalue        0.0897625  0.0083636 10.7325 < 2.2e-16 ***\ncapital      0.2917667  0.0537516  5.4281 1.752e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    584410\nResidual Sum of Squares: 345460\nR-Squared:      0.40888\nAdj. R-Squared: 0.40256\nF-statistic: 64.6736 on 2 and 187 DF, p-value: < 2.22e-16\n```\n:::\n:::\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-76_30edc51ed731b151dfb01ae96e7d62f1'}\n\n```{.r .cell-code}\n# Statistical equivalent\nsummary(lm(inv~value + capital, data = Grunfeld)) # pooled\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = inv ~ value + capital, data = Grunfeld)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-291.68  -30.01    5.30   34.83  369.45 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -42.714369   9.511676  -4.491 1.21e-05 ***\nvalue         0.115562   0.005836  19.803  < 2e-16 ***\ncapital       0.230678   0.025476   9.055  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 94.41 on 197 degrees of freedom\nMultiple R-squared:  0.8124,\tAdjusted R-squared:  0.8105 \nF-statistic: 426.6 on 2 and 197 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(lm(inv~firm + capital + value, data = Grunfeld)) # fixed effects\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = inv ~ firm + capital + value, data = Grunfeld)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-184.009  -17.643    0.563   19.192  250.710 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -70.29672   49.70796  -1.414    0.159    \nfirm2        172.20253   31.16126   5.526 1.08e-07 ***\nfirm3       -165.27512   31.77556  -5.201 5.14e-07 ***\nfirm4         42.48742   43.90988   0.968    0.334    \nfirm5        -44.32010   50.49226  -0.878    0.381    \nfirm6         47.13542   46.81068   1.007    0.315    \nfirm7          3.74324   50.56493   0.074    0.941    \nfirm8         12.75106   44.05263   0.289    0.773    \nfirm9        -16.92555   48.45327  -0.349    0.727    \nfirm10        63.72887   50.33023   1.266    0.207    \ncapital        0.31007    0.01735  17.867  < 2e-16 ***\nvalue          0.11012    0.01186   9.288  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52.77 on 188 degrees of freedom\nMultiple R-squared:  0.9441,\tAdjusted R-squared:  0.9408 \nF-statistic: 288.5 on 11 and 188 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nsummary(lmer(inv~ (1|firm) + capital + value, data = Grunfeld)) # random model, but different random effect estimators\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerMod']\nFormula: inv ~ (1 | firm) + capital + value\n   Data: Grunfeld\n\nREML criterion at convergence: 2195.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4319 -0.3498  0.0210  0.3592  4.8145 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n firm     (Intercept) 7367     85.83   \n Residual             2781     52.74   \nNumber of obs: 200, groups:  firm, 10\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept) -57.86442   29.37776   -1.97\ncapital       0.30819    0.01717   17.95\nvalue         0.10979    0.01053   10.43\n\nCorrelation of Fixed Effects:\n        (Intr) capitl\ncapital -0.019       \nvalue   -0.328 -0.368\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n```\n:::\n:::\n\n\n# GLS\n\nSee [Wikipedia](https://en.wikipedia.org/wiki/Generalized_least_squares), the article is fairly comprehensive.\n\n\n\n\n\nCommon forms of the `gls` command are as follows (from @pinheiro_mixed-effects_2000):\n\n-   `gls( model, data, correlation )` - correlated errors\n-   `gls( model, data, weights )` - heteroscedastic errors\n-   `gls( model, data, correlation, weights )` - both\n\nWe can play around with the structure of covariance matrices with:\n\n\n::: {.cell hash='longitudinal_cache/html/unnamed-chunk-78_62f02c5ded49dc2966c0474b4ec48d30'}\n\n```{.r .cell-code}\n# In order to specify the correlation structure to be unstructured, we allow each group to be different for id. But for each time point to be related.\n# ?corSymm\n# corSymm(form=tlc$time | tlc$id)\n# In order to see all the correlation matrices for just the first individual\ncorMatrix(Initialize(corSymm(form= ~ 1 | id), data=tlc))$`1`\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              [,1]          [,2]          [,3]          [,4]\n[1,]  1.000000e+00 6.925961e-310 6.925961e-310 6.925961e-310\n[2,] 6.925961e-310  1.000000e+00 6.925961e-310 6.925961e-310\n[3,] 6.925961e-310 6.925961e-310  1.000000e+00 6.925961e-310\n[4,] 6.925961e-310 6.925961e-310 6.925961e-310  1.000000e+00\n```\n:::\n:::\n\n\n# References {#References}\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}