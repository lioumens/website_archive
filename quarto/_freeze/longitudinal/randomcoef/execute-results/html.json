{
  "hash": "e30ed61ad6fe0037d92ad1394055f780",
  "result": {
    "markdown": "---\ntitle: \"Random Coefficient Models (CALS)\"\nauthor: \"Michael Liou\"\ndate: \"2022-12-19\"\nexecute:\n  cache: true\n---\n\n\n\n\n# Modeling Heterogeneity: Fixed Effect models vs Random Coefficient Models\n\nIn continuation of looking at heterogenous variance modeling, random coefficient models are a specific type of mixed effect model that allow richness in modeling the variance through random effects. This is adding to the toolbox that we've built upon from last week in we looked at modeling the covariance matrix directly. Since we are introducing two \"angles\" of attack and giving structure to the variance $Var(Y)$, SAS commonly refers to this method of covariance modeling through random effects as \"G\"-side. Modeling the covariance directly as we did last week is referred to as \"R\"-side for \"residuals\". The terminology becomes more clear with the framework:\n\n$$\n\\begin{aligned}\nY = X\\beta + Zu + e\n\\end{aligned}\n$$\n\nwhere:\n\n- $u\\sim N(0,G)$\n- $\\varepsilon \\sim N(0, R)$\n\n$$\n\\begin{aligned}\nVar(Y) &= Z\\underbrace{\\operatorname{Var}(u)}_GZ' + \\underbrace{\\operatorname{Var}(e)}_{R} \\\\\n&= \\Sigma\n\\end{aligned}\n$$\nNote we can also define the following variances when additional random effects are present, such that:\n\n- $\\operatorname{Var}(Y | u) = R$ \"conditional variance\"\n- $\\operatorname{Var}(Y) = ZGZ' + R$ \"marginal (individual) variance\"\n- $\\operatorname{Var}(u) = G$ \"random effect variance\"\n\nThese are directly extractable through `lme` with `getVarCov(obj, type = c(\"random.effects\", \"conditional\", \"marginal\"))`.\n\n\nIn the fixed effects models, we have\n\n$$\n\\begin{aligned}\nY = X\\beta + \\varepsilon\n\\end{aligned}\n$$\n\nwhere:\n\n- $\\varepsilon \\sim N(0,\\Sigma)$\n\nAgain, we have that $\\operatorname{Var}(Y) = \\Sigma$. This is exclusively \"R\"-side modeling when the $ZGZ'$ matrix is 0, which is the case when we don't have any random effects.\n\n# Introducing Autism Data (part b)\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-1_67e03147ab7b1e1c3d1d662ae31ed83e'}\n\n```{.r .cell-code}\nautism <- read.csv(\"data/autism.csv\") %>% \n  mutate(sicdegp = factor(sicdegp),\n         childid = factor(childid),\n         agef = factor(age)) # if we choose to model age as a factor \n\nautism_complete <- autism[complete.cases(autism),]\n\nautism %>% ggplot(aes(age, vsae, group = childid, color = sicdegp)) +\n  geom_line(alpha = as.numeric(autism$childid)^2 / 30000) + # hack to emphasize just a few response curves\n  facet_wrap(~sicdegp)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row(s) containing missing values (geom_path).\n```\n:::\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-1-1.png){width=1248}\n:::\n:::\n\n\n\nIn context of this dataset, we note some research questions in particular that we are interested in modeling:\n\n* The age trend seems different among `sicdegp` levels.\n  - I'd like to quantify the differences in these trends, and pick models that make this comparison easier to interpret.\n* The age trend itself seems mostly increasing linear, but each child seems to have their own linear trajectory. \n  - There is perhaps an sharper inflection upwards around age 9. It would be worthwhile testing models that inflect upward as age increases.\n* There seem to be a greater variability in the _slope_ for `sicdegp = 2,3` than in `sicdegp = 1`.\n  - There is fanning present from variability in slopes for each child, but the diversity among those slopes seem to differ as well.\n  - We should make another plot that more directly shows/quantifies the extent of this. This also helps inform whether or not trying heterogenous models would be empirically necessary.\n* There are a non-trivial number of individuals that show \"drop out\" before the end of the study.\n  - since the data doesn't show the incompleteness, we should make the missingness \"explicit\" and try to quantify the extent that this missingness may bias our results.\n\n\n# Fixed Effect Modeling (part c)\n\nIn this section, we try to \"push\" fixed effects as far as we can by modeling the mean and covariance (R-side) and see how good of a model we can get. We'll first focus on modeling the mean.\n\n## Modeling the Mean\n\nFrom the exploratory plots, we loosely fit linear/quadratic/stick models to the data.\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-2_f0446679ce94fa12ae55e9a00a94f6fa'}\n\n```{.r .cell-code}\n# without child id\nautism_lm <- lm(vsae ~ age*sicdegp, data = autism_complete) \n\n# with child id, varying intercept\nautism_lm_id <- lm(vsae ~ age*sicdegp + childid, data = autism_complete)\n\n# with child id varying slope and intercept\n# Note: these estimates are the same as if we subset dataset to just the single child and ran lm. \n# sum(coef(autism_lm_id_slope)[c(\"(Intercept)\", \"childid10\")])\n# sum(coef(autism_lm_id_slope)[c(\"age\", \"age:childid10\")])\n# coef(lm(vsae~age, data = subset(autism, childid == 10)))\nautism_lm_id_slope <- lm(vsae ~ age + childid + childid:age, data = autism_complete)\n\n\n# stick models\nautism_stick <- autism_complete %>% \n  mutate(age_gt_9 = as.numeric(age > 9), # create indicator for stick models\n         age_relu_9 = age_gt_9 * (age - 9)) # ReLu function for adding slope above 9\n\nautism_lm_stick <- lm(vsae ~ age*sicdegp + age_relu_9:sicdegp, data = autism_stick) # without child id, \"pooled\"\nautism_lm_stick_id <- lm(vsae ~ age*sicdegp + age_relu_9:sicdegp + childid, data = autism_stick) # without child id, \"pooled\"\nautism_lm_stick_id_slope <- lm(vsae ~ age + childid + age:childid + age_relu_9:childid, data = autism_stick) # individual stick model\n\n# quadratic trend\nautism_lm_quad <- lm(vsae ~ age*sicdegp + I(age^2):sicdegp, data = autism_complete)\nautism_lm_quad_id <- lm(vsae ~ age*sicdegp + I(age^2):sicdegp + childid, data = autism_complete) # id specific intercept\nautism_lm_quad_id_slope1 <- lm(vsae ~ age*childid + I(age^2):sicdegp, data = autism_complete) # id specific intercept and linear terms\nautism_lm_quad_id_slope2 <- lm(vsae ~ age*childid + I(age^2)*childid, data = autism_complete) # id specific intercept, linear and quadratic\n\n\n# using poly macro\n# autism_lm_poly2 <- lm(vsae ~ poly(age, degree = 2) * sicdegp, data = autism)\n# autism_lm_poly2 <- lm(vsae ~ poly(age, degree = 2) * child_id, data = autism)\n```\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-3_ba37255e4d15615abe7313af3e23d467'}\n\n```{.r .cell-code}\n# predictions, plotted raw\nautism_predict <- autism %>% filter(complete.cases(.)) %>% \n  add_column(\n  yhat_lm = predict(autism_lm),\n  yhat_lm_id = predict(autism_lm_id),\n  yhat_lm_id_slope = predict(autism_lm_id_slope),\n  yhat_lm_stick = predict(autism_lm_stick),\n  yhat_lm_stick_id_slope = predict(autism_lm_stick_id_slope),\n  yhat_lm_quad = predict(autism_lm_quad),\n  yhat_lm_quad_id = predict(autism_lm_quad_id),\n  yhat_lm_quad_id_slope1 = predict(autism_lm_quad_id_slope1),\n  yhat_lm_quad_id_slope2 = predict(autism_lm_quad_id_slope2),\n  )\n\nautism_predict %>% \n  pivot_longer(cols = c(vsae, starts_with(\"yhat\")),\n                                names_to = \"type\",\n                                values_to = \"y\") %>% \n  arrange(childid, age) %>% \n  ggplot(aes(age, y, group = childid, color = type)) +\n  geom_line(alpha = .4) +\n  facet_grid(type~sicdegp)\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-3-1.png){width=1152}\n:::\n:::\n\n\n- The stick model estimates look a little funky because there are some individuals with only observations at age = 2, 13, thus a stick model would be degenerate in those individuals and the plotting is simply showing the direct line instead of the stick estimates. I think the same thing is happening in the id quad estimates.\n\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-4_a816c22e51f5084645384731a0dcc23c'}\n\n```{.r .fold-hide .cell-code}\nautism_fixed_ic <- AIC(autism_lm,\n    autism_lm_id,\n    autism_lm_id_slope,\n    autism_lm_stick,\n    autism_lm_stick_id_slope,\n    autism_lm_quad,\n    autism_lm_quad_id,\n    autism_lm_quad_id_slope1,\n    autism_lm_quad_id_slope2) %>% \n  add_column(\n    BIC = BIC(autism_lm,\n              autism_lm_id,\n              autism_lm_id_slope,\n              autism_lm_stick,\n              autism_lm_stick_id_slope,\n              autism_lm_quad,\n              autism_lm_quad_id,\n              autism_lm_quad_id_slope1,\n              autism_lm_quad_id_slope2)$BIC)\n\nautism_fixed_ic %>% \n  kbl(format = \"html\",\n      caption = \"Fixed Model Mean Information Criteria\",\n      table.attr = \"style='width:50%;'\") %>% \n  kable_classic(full_width = TRUE) %>% \n  column_spec(3, color = c(\"black\", \"red\")[as.numeric(autism_fixed_ic$AIC == min(autism_fixed_ic$AIC)) + 1]) %>% \n  column_spec(4, color = c(\"black\", \"red\")[as.numeric(autism_fixed_ic$BIC == min(autism_fixed_ic$BIC)) + 1])\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style='width:50%; font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; margin-left: auto; margin-right: auto;' class=\" lightable-classic\">\n<caption>Fixed Model Mean Information Criteria</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> df </th>\n   <th style=\"text-align:right;\"> AIC </th>\n   <th style=\"text-align:right;\"> BIC </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lm </td>\n   <td style=\"text-align:right;\"> 7 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5538.140 </td>\n   <td style=\"text-align:right;color: red !important;\"> 5569.035 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lm_id </td>\n   <td style=\"text-align:right;\"> 162 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5440.346 </td>\n   <td style=\"text-align:right;color: black !important;\"> 6155.327 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lm_id_slope </td>\n   <td style=\"text-align:right;\"> 315 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4553.386 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5943.626 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lm_stick </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5538.678 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5582.813 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lm_stick_id_slope </td>\n   <td style=\"text-align:right;\"> 408 </td>\n   <td style=\"text-align:right;color: red !important;\"> 4153.635 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5954.326 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lm_quad </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5539.403 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5583.538 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lm_quad_id </td>\n   <td style=\"text-align:right;\"> 165 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5440.750 </td>\n   <td style=\"text-align:right;color: black !important;\"> 6168.970 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lm_quad_id_slope1 </td>\n   <td style=\"text-align:right;\"> 318 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4542.668 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5946.148 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lm_quad_id_slope2 </td>\n   <td style=\"text-align:right;\"> 457 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4237.499 </td>\n   <td style=\"text-align:right;color: black !important;\"> 6254.450 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nThe information criteria here show something very interesting! AIC chooses one of the most complex models, with 408 parameters, while BIC chooses the model with 7 parameters! How you go about model selection is very much a philosophical decision, and the information criteria tend to reflect those camps of thinking. I'll be moving forward with `autism_lm`.\n\n### Diagnostics {.tabset}\n\nWe can use the diagnostic plots to examine missing trends and start to get a sense of the variance modeling we will need.\n\n#### Simple Linear Model\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-5_302e6f9bc911642845ddcc7efd9b7c9b'}\n\n```{.r .fold-hide .cell-code}\npar(mfrow = c(2,2))\nplot(autism_lm)\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n#### Individual Stick Model\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-6_23db4956b09312da9f8aa89b6dbb8af0'}\n\n```{.r .fold-hide .cell-code}\npar(mfrow = c(2,2))\nplot(autism_lm_stick_id_slope) # danger with this model is that there are a number of points with leverage one.\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n### {- .unlisted .unnumbered}\n\nBased on these plots, we see that when we choose the super parameterized models, we are risking overfitting with a number of values with leverage 1.\n\nI would ultimately base modeling decisions on specific research questions the scientist had. If there was greater interest in ages 10 - 13, I may be more hesitant to choose the stick model because it's quite overfit in this domain. If they wanted to summarize \"rules of thumb\" for presentation or big picture of this trait, I'd likely choose the simple linear model. If they were hoping for prediction applications based on younger children and valued accuracy of predictions, I may opt for the over-parameterized models like quadratic or stick model.\n\n## Modeling the variance\n\nFor simplicity, we'll chose the linear functional form, as it is quite simple, and performs reasonably well for the number of parameters that it uses. Note, obviously the variance modeling will depend on your chosen mean model.\n\nSimilar to how we modeled the mean, we start with trying to visualize the covariance matrix with as few restrictions as possible, so we fit an unstructured covariance matrix with `gls`.\n\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-7_b45230132395f817286ec325feb1632b'}\n\n```{.r .cell-code}\n# start with unstructured covariance matrix, and let the optimizer tell us the best estimate with no restrictions.\nautism_gls_un <- gls(vsae~age*sicdegp,\n                     correlation = corSymm(form = ~1 | childid), # unstructured correlation\n                     weights = varIdent(form = ~1 | agef), # parameter for each entry along diagonal\n                     data = autism_complete)\ngetVarCov(autism_gls_un)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMarginal variance covariance matrix\n        [,1]     [,2]     [,3]    [,4]     [,5]\n[1,] 10.8500   8.9204   9.4352   25.49   48.503\n[2,]  8.9204  55.6110  45.2460  108.23  203.970\n[3,]  9.4352  45.2460 135.6600  223.11  404.000\n[4,] 25.4900 108.2300 223.1100  765.99 1206.800\n[5,] 48.5030 203.9700 404.0000 1206.80 2319.400\n  Standard Deviations: 3.2939 7.4573 11.647 27.677 48.16 \n```\n:::\n:::\n\n\nThere are some visual guides that we can use to help us model the covariance:\n\n1. line graph of matrix entries, grouped by row\n2. Variogram\n3. Autocorrelation Function\n\n### 1. Line Graph of matrix entries, grouped by row\n\nI picked up this visualization from Generalized Linear Mixed Models by Walter Stroup, one of the authors of the SAS for Mixed Models book. I like this visualization better than a heat map because instead of using a color channel for the variance, it uses y-position which is much more clear.\n\n<center>\n<figure>\n<img src=\"img/cov_line_plot.png\" style=\"width:50%\"></img>\n<figcaption></figcaption>\n</figure>\n</center>\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-8_eaaea149123fe3bc71ffb6c55371c981'}\n\n```{.r .fold-hide .cell-code}\nsigmahat <- getVarCov(autism_gls_un)\n\nsigmahat_df <- data.frame(row = rep(1:5, each = 5),\n           col = rep(1:5, 5),\n           age = rep(c(2, 3, 5, 9, 13), 5), # un\n           cov = c(sigmahat)) %>% \n  filter(row <= col) # pull upper triangle w/ diagonal entries\n\nsigmahat_df %>% \n  ggplot(aes(col, cov, color = factor(row), group = factor(row))) +\n  geom_point() +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nThis graphic is confusing at first, but I think it's one of the more intuitive visualizations once you're used to it. each point in the graph is the estimated covariance/variance. The lines show the trend that is happening in each row, as you move away from the main diagonal. For example, the red line is the first row of $\\hat \\Sigma$. There are 5 dots because the first row starts in column 1. The main diagonal (all the variances) are the left most point in each of the trend lines.\n\nWe see a similar pattern of the estimated covariances increasing as the age increases. column 5 represents age 13, and we can see the estimated variance is ~2500.\n\nSince age has a meaning on a continuous scale, it would be slightly more helpful to visualize the appropriate distances in the x axis when looking at the estimated covariances.\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-9_b0934430846e669c6c688522787e53f3'}\n\n```{.r .cell-code}\nsigmahat_age_plot <- sigmahat_df %>%\n  mutate(diag = col - row) %>% \n  ggplot(aes(age, cov, color = factor(row), group = factor(row))) +\n  geom_point() +\n  geom_line()\nsigmahat_age_plot\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nWe can see some resemblance of a power relationship for the main diagonal, and there is a pretty clean pattern in the covariance here, so it's likely we can capture most of the trends with just a few extra parameters. I would like to graphically test what i'm thinking, so i'll add a smoothed parametric fit to the left most points of each colored trend (main diagonal).\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-10_a01d082a131af695c907ad24ce9bbf28'}\n\n```{.r .cell-code}\n# hackish way to checking how a power relationship on age might fit for the heterogenous variance function\nsigmahat_age_plot + \n  geom_line(stat = \"smooth\",\n            method = \"lm\",\n              aes(group = diag),\n              formula = (y~ I(x^3)), # can play with this form to visualize how the variance estimates might look. Try exponential here, doesn't fit well! (so AR covariance models probably inappropriate)\n              se = FALSE,\n              size = .2,\n              linetype = 2,\n              alpha = .4,\n              color = \"black\") \n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe main diagonal band here seems like a pretty decent fit, which is primarily what I'm interested in here. The fact that the off-diagonal bands also fit this power relationship pretty well implies that I can probably get away with a fairly simple covariance matrix structure (few parameters for off diagonals, like compound symmetry) and still describe this covariance matrix quite well.\n\n### 2. Semi-(Variogram)\n\nThis is another method of visualizing covariances that is popular in spatial statistics. This is more useful when one (or more) of your datapoints have a continuous/spatial interpretation. In this case, we are interested in how the correlation of observations between ages as the distance in age increases.\n\nThe semi-variogram is defined as: (for isotropic and stationary processes)\n\n$$\n\\begin{aligned}\n\\gamma(h) &= \\frac{1}{2}Var(\\epsilon_{age} - \\epsilon_{age+h}) \\\\\n&= \\frac{1}{2}E(\\epsilon_{age} - \\epsilon_{age+h})^2\n\\end{aligned}\n$$\n\nEmpirically to estimate this, we take pairs of observations certain distances apart, and average the squared distances of the estimated residuals. \n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-11_fae4f66ba7a9faa3bbcae84ddc3a9913'}\n\n```{.r .cell-code}\nvario_autism <- Variogram(autism_gls_un, form = ~ age | childid, resType = \"response\")\nvario_autism\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     variog dist n.pairs\n1  22.25863    1     147\n2  41.44491    2      86\n3  53.94080    3      91\n4 303.74329    4     153\n5 275.86605    6     115\n6 382.44231    7     118\n7 707.25001    8      49\n8 841.57324   10      91\n9 927.39613   11      93\n```\n:::\n:::\n\n\nWe have oddly spaced age observations, `age = c(2, 3, 5, 9, 13)`, so in this case, it's pretty distinctive that our estimate of $dist = 1$ come from $age = 2,3$, and $dist = 8$ comes from $age = 5, 13$.\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-12_84fce463da8b11f56b595d445e2e60eb'}\n\n```{.r .cell-code}\nplot(vario_autism)\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n<center>\n<figure>\n<img src=\"img/variogram_formulas.png\" style=\"width:47%\"></img>\n<img src=\"img/variogram_graphs.png\" style=\"width:47%\"></img>\n<figcaption>Figures from Mixed Effects Models </figcaption>\n</figure>\n</center>\n\n\n### 3. Auto Correlation Functions\n\n(sample) auto correlation functions show the correlation with lagged versions of itself.\n\n$$\n\\begin{aligned}\nACF(k) = \\frac{\\operatorname{Cov}(\\varepsilon, \\varepsilon_{lag(k)})}{\\operatorname{Var}(\\varepsilon)}\n\\end{aligned}\n$$\n\nThe ACF is most useful when we have data that is more time series like, with many measured timepoints and somewhat equally spaced measurements because we're calculating correlation with itself.\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-13_e514852b5a254bc9c47b4f00a146e4f2'}\n\n```{.r .cell-code}\nACF(autism_gls_un, form = ~ age | childid, resType = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  lag        ACF\n1   0 1.00000000\n2   1 0.46705931\n3   2 0.19978821\n4   3 0.11193700\n5   4 0.09748354\n```\n:::\n\n```{.r .cell-code}\nplot(ACF(autism_gls_un, form = ~ age | childid, resType = \"response\"), alpha = .01) # observed - fitted, not accounting for covariance estimates\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\nWe can ignore the first bar, but we're looking at the trend made by the top of the bars. An AR1 model would have the tops of the bars decrease exponentially quickly. The fact that the bars poke out from the dotted alpha = .01 curve means that there's likely some sequential correlation happening in our data that we haven't accounted for.\n\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-14_462997eeae7583ae0716e56f557ef128'}\n\n```{.r .fold-hide .cell-code}\nplot(ACF(autism_gls_un, form = ~ age | childid, resType = \"normalized\"), alpha = .01) # accounting for our estimated covariance matrix\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nIf we plot the ACF of residuals that account for our unstructured covariance matrix, we can see the sequential correlations drop out of significance.\n\n\n### Fitting variance models\n\nThere are many covariance shapes we can try here, and SAS has even more! See [SAS Repeated Statement](https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.3/statug/statug_mixed_syntax14.htm).\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-15_c9373cd10cb1edc843e9f276fa8304b2'}\n\n```{.r .cell-code}\n# heterogeneous, diagonal\nautism_gls_het <- gls(vsae~age*sicdegp,\n                      weights = varIdent(form = ~ 1 | agef),\n                      data = autism_complete)\n\n# should be a bad fit since we already know there is strong heterogeneity\nautism_gls_cs <- gls(vsae~age*sicdegp,\n                     correlation = corCompSymm(form = ~1 | childid),\n                     data = autism_complete)\n\n# heterogeneous, compound symmetry\nautism_gls_csh <- gls(vsae~age*sicdegp,\n                     correlation = corCompSymm(form = ~1 | childid),\n                     weights = varIdent(form = ~ 1 | agef),\n                     data = autism_complete)\n\n# continuous autoregressive\n# correlation = \\phi^distance\nautism_gls_carh <- gls(vsae~age*sicdegp,\n                      correlation = corCAR1(form = ~ age | childid), # i.e. exponential decay with distance\n                      weights = varIdent(form = ~1 | agef),\n                      data = autism_complete)\n\n# based on the visualizations, should be decent\n# variance = age^\\theta\nautism_gls_pow <- gls(vsae~age*sicdegp,\n                      correlation = corCompSymm(form= ~1 | childid),\n                      weights = varPower(form = ~age), # fit heterogeneous variance function\n                      data = autism_complete)\n\nautism_gls_cpow <- gls(vsae~age*sicdegp,\n                      correlation = corCompSymm(form= ~1 | childid),\n                      weights = varConstPower(form = ~age), # additional constant variable to power relationship\n                      data = autism_complete)\n\n# with linear paramterization of corstruct\nautism_gls_powlin <- gls(vsae~age*sicdegp,\n                      correlation = corLin(form= ~age | childid),\n                      weights = varPower(form = ~age), # fit heterogeneous variance function\n                      data = autism_complete)\n```\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-16_1db97b4ef9face2d304b8af0f0e2f574'}\n\n```{.r .fold-hide .cell-code}\nautism_gls_ic <- AIC(\n    autism_gls_het,\n    autism_gls_cs,\n    autism_gls_csh,\n    autism_gls_carh,\n    autism_gls_un,\n    autism_gls_pow,\n    autism_gls_cpow,\n    autism_gls_powlin) %>% \n  add_column(\n    BIC = BIC(autism_gls_het,\n              autism_gls_cs,\n              autism_gls_csh,\n              autism_gls_carh,\n              autism_gls_un,\n              autism_gls_pow,\n              autism_gls_cpow,\n              autism_gls_powlin)$BIC)\n\nautism_gls_ic %>% kbl(caption = \"Information Criteria for \") %>% \n  kable_classic(full_width = F) %>%\n  column_spec(3, color = c(\"black\", \"red\")[as.numeric(autism_gls_ic$AIC == min(autism_gls_ic$AIC)) + 1]) %>% \n  column_spec(4, color = c(\"black\", \"red\")[as.numeric(autism_gls_ic$BIC == min(autism_gls_ic$BIC)) + 1])\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n<caption>Information Criteria for </caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> df </th>\n   <th style=\"text-align:right;\"> AIC </th>\n   <th style=\"text-align:right;\"> BIC </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> autism_gls_het </td>\n   <td style=\"text-align:right;\"> 11 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4661.843 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4710.282 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_gls_cs </td>\n   <td style=\"text-align:right;\"> 8 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5461.844 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5497.072 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_gls_csh </td>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4529.853 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4582.696 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_gls_carh </td>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4604.352 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4657.195 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_gls_un </td>\n   <td style=\"text-align:right;\"> 21 </td>\n   <td style=\"text-align:right;color: red !important;\"> 4482.384 </td>\n   <td style=\"text-align:right;color: red !important;\"> 4574.859 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_gls_pow </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4536.168 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4575.800 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_gls_cpow </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4538.168 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4582.203 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_gls_powlin </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4654.410 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4694.043 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nAt this stage, it seems the unstructured covariance matrix would fit the best if we follow either AIC or BIC, though notably the power variance structure has a very comparable BIC with only 9 parameters vs 21 parameters total. Since I've already abided by principles of parsimony in choosing to use the very simple linear model, I'd probably elect to use the `autism_gls_pow` as my final model for fixed effects modeling for the same reasons. The model is also very appealing for explaining variance as simply a power function of age (respecting the continuous scale) and similar performance in information criteria.\n\n\n\n## Final Model\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-17_3caf7f504cab8a08f88d52f1bf529d7d'}\n\n```{.r .cell-code}\nplot(autism_gls_pow)\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\nspread among the residuals looks much better against the fitted values.\n\nSome basic inference from the fixed effect model...\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-18_6e232c63453ba5d0ee84a24c92604e7f'}\n\n```{.r .cell-code}\nintervals(autism_gls_pow) # asymptotic normal approximation, based on inverse hessian\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nApproximate 95% confidence intervals\n\n Coefficients:\n                  lower       est.      upper\n(Intercept)  -0.5248735  1.2011843  2.9272421\nage           2.2896001  3.1507950  4.0119900\nsicdegp2     -1.9420685  0.3411552  2.6243789\nsicdegp3     -5.7898786 -3.2664553 -0.7430321\nage:sicdegp2 -0.5218454  0.6171416  1.7561285\nage:sicdegp3  3.0205486  4.2837888  5.5470290\n\n Correlation structure:\n        lower      est.     upper\nRho 0.3682194 0.4549824 0.5387205\n\n Variance function:\n         lower     est.    upper\npower 1.217197 1.291734 1.366271\n\n Residual standard error:\n   lower     est.    upper \n1.372947 1.575796 1.808615 \n```\n:::\n:::\n\n\n\n## Weighted Least Squares\n\nI mentioned weighted least squares as a very simple quick fix that give similar qualities of inference as you'd get with more sophisticated fitting procedures (increased variance for larger ages). You can see here that it works decently, but the fact that we're not accounting for measurements from individuals (correlation) means that we can do much better.\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-19_b528b7cef9a509b255fd1052ce4d9aea'}\n\n```{.r .cell-code}\ngls_un_var <- getVarCov(autism_gls_un, individual = 2) %>% diag()\n\nautism_wls <- lm(vsae~age*sicdegp, weights = 1/(gls_un_var[autism$agef]), data = autism)\n\nAIC(autism_wls)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4655.256\n```\n:::\n\n```{.r .cell-code}\nBIC(autism_wls)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4686.151\n```\n:::\n:::\n\n\nI believe comparing the information criteria in WLS to GLS is legitimate? but I'm not certain... the former situation we're assuming variance parameters are known which makes this comparison weird.\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-20_32215382f0a26daff7219466527c9ad1'}\n\n```{.r .cell-code}\nautism_new <- expand.grid(age = 2:13, sicdegp = factor(1:3))\n\n# from direct lm\nautism_new %>% bind_cols(predict(autism_lm, newdata = autism_new, interval = \"confidence\")) %>% pivot_longer(cols = fit:upr, names_to = \"type\") %>% \n  ggplot(aes(age, value, linetype = type, group = type)) +\n  geom_line() + \n  facet_wrap(~sicdegp)\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# from weighted least squares\nautism_new %>% bind_cols(predict(autism_wls, newdata = autism_new, interval = \"confidence\")) %>% pivot_longer(cols = fit:upr, names_to = \"type\") %>%\n  ggplot(aes(age, value, linetype = type, group = type)) +\n  geom_line() + \n  facet_wrap(~sicdegp)\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-20-2.png){width=672}\n:::\n:::\n\n\n# Mixed Effect Modeling (part d)\n\nWe're working with a different toolbox now, as far as covariance modeling goes. We can now control the covariance matrix directly (R-side) as we did above or implicitly through the use of random effects/coefficients (G-side). \n\nThe advantages:\n\n1. aligns parameter interpretation with the random design matrix $Z$, which is more intuitive.\n2. Allows for rather complex covariance structures with very few parameters\n3. implicit averaging of individual models, and we can pull out the individually fit \n\nThe disadvantages:\n\n1. Harder to visualize what's happening with G-matrix, and effect on final covariance matrix of Y.\n2. More computationally intensive, will probably be slower for larger datasets.\n\nThe reason I recommend `lme` over `lmer` is that you have more control over the structure of \"G\" and \"R\" matrices in `lme`. `lmer` is only capable of fitting diagonal and unstructured covariances for G, and homogenous diagonal matrix for \"R\". (And i recommend `SAS` over `lme` because the syntax is much easier!)\n\n## Fitting Models\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-21_223a73c3ae334d27f7efc88e28ab0b04'}\n\n```{.r .cell-code}\n# random intercept\nautism_lme_id <- lme(fixed = vsae~sicdegp*age,\n    random = ~ 1 | childid,\n    data = autism_complete)\n\n# random slopes model\n# doesn't converge!!\nautism_lme_id_slope <- lme(fixed = vsae~sicdegp*age,\n    random = ~ age | childid,\n    data = autism_complete)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in lme.formula(fixed = vsae ~ sicdegp * age, random = ~age | childid, : nlminb problem, convergence error code = 1\n  message = iteration limit reached without convergence (10)\n```\n:::\n:::\n\n\nWe hit some optimizer problems trying to fit the random slopes model. By default, `lme` uses the outdated `nlminb` optimizer, which is similar to \"BFGS\", a quasi-newton optimization routine. ^[[Stack Overflow](https://stackoverflow.com/questions/49375840/algorithm-name-in-nlminbs-port-routines) for more details and link to the original paper for nlminb]. It's mostly used for compatibility reasons, and `optim` is the general optimizer that is now preferred. `lmeControl` has the option `opt = \"optim\"`, which switches the optimizer, and now looks for `optimMethod = \"BFGS\"` which says to run BFGS algorithm in `optim`.\n\nWe can also switch the function call to lmer, because this is a model that can be handled in that library as well. The default callback is `lmer` -> `nloptwrap` (wrapper function) -> `nloptr` (R interface into NLopt) -> `NLopt` ([Free/Open Source library for Nonlinear optimization](https://nlopt.readthedocs.io/en/latest/)) -> `NLOPT_LN_BOBYQA` (BOBYQA routine written in C). BOBYQA is a derivative free optimization program.\n\nI try to avoid diving down the optimizer rabbit hole as much as possible... Fix 2 is normally the route I take, if you're curious, in which I fit successively simpler models until boundary issues don't exist.\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-22_a03c79f605ed3d3fef73f6bfa8165ea0'}\n\n```{.r .cell-code}\n# Fix 1: change the optimizer to \"optim\" (BFGW) in lme\nautism_lme_id_slope <- lme(fixed = vsae~sicdegp*age,\n    random = ~ age | childid,\n    data = autism_complete,\n    control = lmeControl(opt = \"optim\"))\n# summary(autism_lme_id_slope) # note correlation of random effects is _very_ close to boundary -1 (even though fits with no complaints)\n\n# Fix 2: change optimizer to use ------------------\n# lmer fits, but warns about boundary...\nautism_lmer_id_slope <- lmer(vsae~sicdegp*age + (age | childid), data = autism_complete)\n## boundary (singular) fit: see help('isSingular')\n# looking at summary, we see that correlation of random effects is -1 (boundary)\n# summary(autism_lmer_id_slope)\n\n# A useful function is \"allFit\", which tries to fit the model with \"all\" appropriate optimizers.\n# they all have boundary warnings.\n# allFit(autism_lmer_id_slope)\n\n# try the uncorrelated model, still boundary w/ intercept variance estimated as 0.\nautism_lmer_id_slope_nocor <- lmer(vsae~sicdegp*age + (age || childid), data = autism_complete)\n## boundary (singular) fit: see help('isSingular')\n# summary(autism_lmer_id_slope_nocor)\n\n# take out random intercept, finally no boundary estimates, and no warnings!\nautism_lmer_id_slope_noint <- lmer(vsae~sicdegp*age + (0 + age | childid), data = autism_complete)\n```\n:::\n\n\n\nBased on these issues, I'm skeptical the optimizer will be able to handle more complicated random coefficient models reliably. But we'll try! We'll try to fit the same gamut of linear/quadratic/stick models that we had fit in the fixed case. Since `lmer` is likely more familiar, I show these fits in `lmer` first, and the `lme` equivalent underneath.\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-23_8a801aa899ba515335a20bb343ad028d'}\n\n```{.r .cell-code}\n### Fitting in lmer ---------------------------------------------------------------------\n## Quadratic Models\nautism_lmer_quad_id <- lmer(vsae ~ age*sicdegp + I(age^2):sicdegp + (1 | childid), # random intercept\n                            data = autism_complete) # no warnings\n\n# quadratic, with random linear term.\nautism_lmer_quad_id_slope1 <- lmer(vsae~age + I(age^2):sicdegp + (1 + age | childid), # random intercept, linear\n                                   data = autism_complete) # boundary warning\n## boundary (singular) fit: see help('isSingular')\nautism_lmer_quad_id_slope1_nocor <- lmer(vsae~age + I(age^2):sicdegp + (1 + age || childid), # random intercept, linear, uncorrelated\n                                   data = autism_complete) # boundary warning\n## boundary (singular) fit: see help('isSingular')\nautism_lmer_quad_id_slope1_noint <- lmer(vsae~age + I(age^2):sicdegp + (0  + age | childid), # random linear, no intercept\n                                   data = autism_complete) # no warnings\n\n# Quadratic: unstructured, heterogenous G matrix, diagonal, homogenous R\nautism_lmer_quad_id_slope2 <- lmer(vsae ~ age*sicdegp + I(age^2):sicdegp + (1 + age + I(age^2) | childid), data = autism_complete) # singular, corr = -1\n## boundary (singular) fit: see help('isSingular')\n\n# Quadratic: diagonal, heterogenous G matrix, diagonal, homogenous R\nautism_lmer_quad_id_slope2_nocor <- lmer(vsae ~ age*sicdegp + I(age^2):sicdegp + (1 + age + I(age^2) || childid), data = autism_complete) # singular, intercept â‰ˆ 0\n## boundary (singular) fit: see help('isSingular')\n\n\n# Quadratic, no intercept: unstructured, heterogenous G matrix, diagonal, homogenous R\nautism_lmer_quad_id_slope2_noint <- lmer(vsae ~ age*sicdegp + I(age^2):sicdegp + (0 + age + I(age^2) | childid), data = autism_complete, \n                                         control = lmerControl(optimizer = \"Nelder_Mead\")) # no warnings\n\n# Quadratic, no intercept: diagonal, heterogenous G matrix, diagonal, homogenous R\n# autism_lmer_quad_id_slope2_noint_nocor <- lmer(vsae ~ age*sicdegp + I(age^2):sicdegp + (0 + age + I(age^2) || childid), data = autism_complete) # fail to converge\nautism_lmer_quad_id_slope2_noint_nocor <- lmer(vsae ~ age*sicdegp + I(age^2):sicdegp + (0 + age + I(age^2) || childid), data = autism_complete, \n                                         control = lmerControl(optimizer = \"Nelder_Mead\")) # no warnings\n\n## Stick Models\nautism_lmer_stick_id <- lmer(vsae ~ age*sicdegp + age_relu_9:sicdegp + (1 | childid), data = autism_stick) \nautism_lmer_stick_id_slope <- lmer(vsae ~ age*sicdegp + age_relu_9:sicdegp + (0 + age + age_relu_9 | childid), data = autism_stick)\n```\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-24_6560594b33a58f4d622c1dc9db481ec9'}\n\n```{.r .cell-code}\n# Fitting in lme ---------------------------------------------------------------------\n# Quadratic, random intercept: diagonal, homogenous R\nautism_lme_quad_id <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp, # quadratic \n                       random = ~ 1 | childid, # random intercept\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\n\n# Quadratic, random linear: unstructured, heterogenous G matrix, diagonal, homogenous R\nautism_lme_quad_id_slope1 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = ~ age | childid, # random intercept and linear term\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\n# Quadratic, random linear: diagonal, heterogenous G matrix:  diagonal, homogenous R\nautism_lme_quad_id_slope1_nocor <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid = pdDiag(form = ~ age)), # random intercept and linear term, no correlation\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\nautism_lme_quad_id_slope1_noint <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid = pdDiag(form = ~ 0 + age)), # random intercept and linear term, no intercept\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\n\n# Quadratic, random quadratic: unstructured, heterogenous G matrix:  diagonal, homogenous R\nautism_lme_quad_id_slope2 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = ~ age + I(age^2) | childid, # random intercept, linear and quadratic term\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\n# Quadratic, random quadratic: diagonal, heterogenous G matrix:  diagonal, homogenous R\nautism_lme_quad_id_slope2_nocor <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid=pdDiag(form = ~ age + I(age^2))), # random intercept, linear and quadratic term, no correlation\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\n# Quadratic, random quadratic (no int): unstructured, heterogenous G matrix:  diagonal, homogenous R\nautism_lme_quad_id_slope2_noint <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid=pdSymm(form = ~ 0 + age + I(age^2))), # Unstructured G\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\n# Quadratic, random quadratic (no int): diagonal, heterogenous G matrix:  diagonal, homogenous R\nautism_lme_quad_id_slope2_noint_nocor <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid=pdDiag(form = ~ 0 + age + I(age^2))),\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\n## Stick models\nautism_lme_stick_id <- lme(vsae ~ age*sicdegp + age_relu_9:sicdegp,\n                       random = ~ 1 | childid, # random intercept\n                       data = autism_stick,\n                       control = lmeControl(opt = \"optim\"))\nautism_lme_stick_id_slope <- lme(vsae ~ age*sicdegp + age_relu_9:sicdegp,\n                       random = ~ 0 + age + age_relu_9 | childid, # random linear and quadratic term, no intercept\n                       data = autism_stick,\n                       control = lmeControl(opt = \"optim\"))\n```\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-25_9c5374725d65698823fbd3ae52807b1e'}\n\n```{.r .cell-code}\n# see predicted values from some random effect models\nautism_random_predict <- autism_complete %>% add_column(yhat_lme_id = predict(autism_lme_id),\n                                 yhat_lme_id_slope = predict(autism_lme_id_slope),\n                                 yhat_lme_quad_id = predict(autism_lme_quad_id),\n                                 yhat_lme_quad_id_slope1 = predict(autism_lme_quad_id_slope1),\n                                 yhat_lme_quad_id_slope2 = predict(autism_lme_quad_id_slope2),\n                                 yhat_lme_stick_id = predict(autism_lme_stick_id),\n                                 yhat_lme_stick_id_slope = predict(autism_lme_stick_id_slope),\n                                 yhat_lmer_quad_id_slope2_noint = predict(autism_lmer_quad_id_slope2_noint),\n                                 yhat_lmer_stick_id = predict(autism_lmer_stick_id),\n                                 yhat_lmer_stick_id_slope = predict(autism_lmer_stick_id_slope))\n\nautism_random_predict %>% pivot_longer(cols = c(vsae, starts_with(\"yhat\")),\n                                names_to = \"type\",\n                                values_to = \"y\") %>% \n  arrange(childid, age) %>% \n  ggplot(aes(age, y, group = childid, color = type)) +\n  geom_line(alpha = .4) +\n  facet_grid(type~sicdegp)\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-25-1.png){width=1152}\n:::\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-26_e96045f0006d261a696090e8ee65c380'}\n\n```{.r .fold-hide .cell-code}\nautism_lme_ic <- AIC(\n  autism_lme_id,\n  autism_lme_id_slope, # warning here too, probably because includes individuals that \n  autism_lme_quad_id,\n  autism_lme_quad_id_slope1,\n  autism_lme_quad_id_slope2,\n  autism_lmer_quad_id_slope2, # lmer version, get warning when included, not quite sure why, maybe refit as ML? but very close to lme counterpart\n  autism_lme_quad_id_slope2_nocor,\n  autism_lmer_quad_id_slope2_nocor, # lmer version\n  autism_lme_quad_id_slope2_noint,\n  autism_lmer_quad_id_slope2_noint, # lmer version\n  autism_lme_quad_id_slope2_noint_nocor,\n  autism_lme_stick_id,\n  autism_lme_stick_id_slope\n    ) %>% \n  add_column(\n    BIC = BIC(\n      autism_lme_id,\n    autism_lme_id_slope,\n    autism_lme_quad_id,\n    autism_lme_quad_id_slope1,\n    autism_lme_quad_id_slope2,\n    autism_lmer_quad_id_slope2, # lmer version\n    autism_lme_quad_id_slope2_nocor,\n    autism_lmer_quad_id_slope2_nocor, # lmer version\n    autism_lme_quad_id_slope2_noint,\n    autism_lmer_quad_id_slope2_noint, # lmer version\n    autism_lme_quad_id_slope2_noint_nocor,\n    autism_lme_stick_id,\n    autism_lme_stick_id_slope)$BIC)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in AIC.default(autism_lme_id, autism_lme_id_slope, autism_lme_quad_id, :\nmodels are not all fitted to the same number of observations\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in BIC.default(autism_lme_id, autism_lme_id_slope, autism_lme_quad_id, :\nmodels are not all fitted to the same number of observations\n```\n:::\n\n```{.r .fold-hide .cell-code}\nautism_lme_ic %>% kbl() %>% \n  kable_classic(full_width = F) %>% \n  column_spec(3, color = c(\"black\", \"red\")[as.numeric(autism_lme_ic$AIC == min(autism_lme_ic$AIC)) + 1]) %>% \n  column_spec(4, color = c(\"black\", \"red\")[as.numeric(autism_lme_ic$BIC == min(autism_lme_ic$BIC)) + 1])\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> df </th>\n   <th style=\"text-align:right;\"> AIC </th>\n   <th style=\"text-align:right;\"> BIC </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_id </td>\n   <td style=\"text-align:right;\"> 8 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5461.844 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5497.072 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_id_slope </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4716.130 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4760.166 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id </td>\n   <td style=\"text-align:right;\"> 11 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5470.017 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5518.402 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope1 </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4724.230 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4781.412 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope2 </td>\n   <td style=\"text-align:right;\"> 16 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4640.705 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4711.083 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lmer_quad_id_slope2 </td>\n   <td style=\"text-align:right;\"> 16 </td>\n   <td style=\"text-align:right;color: red !important;\"> 4639.983 </td>\n   <td style=\"text-align:right;color: red !important;\"> 4710.598 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope2_nocor </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4680.454 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4737.636 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lmer_quad_id_slope2_nocor </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4680.391 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4737.766 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope2_noint </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4680.009 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4737.191 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lmer_quad_id_slope2_noint </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4680.009 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4737.384 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope2_noint_nocor </td>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4678.391 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4731.174 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_stick_id </td>\n   <td style=\"text-align:right;\"> 11 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5454.106 </td>\n   <td style=\"text-align:right;color: black !important;\"> 5502.490 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_stick_id_slope </td>\n   <td style=\"text-align:right;\"> 13 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4748.713 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4805.895 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nWe've fit all these \"intuitive\" models, and we can see most of them give pretty intuitive predicted values for our dataset. They capture the main trends we're after pretty well. Given that we're using random coefficient models, we'll probably rule out the models that only vary the intercept. It seems from these predictive plots that we at the very least need to be modeling complexity at the linear or quadratic level (or splines). We'll look further into these models in the diagnostics, as well as the variance modeling in the diagnostics.\n\nThe information criteria seems to pick out the \"autism_lme_quad_id_slope2\", which is the random effect model with random coefficients up to quadratic order, and unstructured correlation matrix in G. thought we remember there were some boundary warnings with that model, so removing the intercept for a model that is not near the boundary may be desired.\n\n## Diagnostics {.tabset}\n\nLet's look at the mean structure with standard residual plots first. We'll just pick out a few plots to look at.\n\n### Quad\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-27_a1a82ff9d3ae770ed36d1e3e2f735597'}\n\n```{.r .cell-code}\nplot(autism_lme_quad_id_slope2, form = resid(.,type = \"normalized\") ~ age | sicdegp) # normalized\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\nIn order to study the variance covariance pattern of this, I'll examine how closely it matches up with the unstructured estimate of the covariance. we can also look at the sample covariance for some direction.\n\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-28_cb73d43f03d63e2aa77a98b4107e9c76'}\n\n```{.r .cell-code}\nautism_gls_quad_un <- gls(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       correlation = corSymm(form = ~1 | childid),\n                       weights = varIdent(form = ~ 1 | age),\n                       data = autism_complete)\n\ngetVarCov(autism_gls_quad_un)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMarginal variance covariance matrix\n        [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 10.8700   9.0082   9.3212   25.916   48.667\n[2,]  9.0082  55.7270  45.7570  110.010  201.300\n[3,]  9.3212  45.7570 135.7700  222.120  397.840\n[4,] 25.9160 110.0100 222.1200  771.040 1197.400\n[5,] 48.6670 201.3000 397.8400 1197.400 2287.700\n  Standard Deviations: 3.297 7.4651 11.652 27.768 47.83 \n```\n:::\n\n```{.r .cell-code}\ngetVarCov(autism_lme_quad_id_slope2, type = \"marginal\") # ZGZ + R\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nchildid 1 \nMarginal variance covariance matrix\n        1        2        3        4        5\n1 39.0350   2.0468   5.4054   21.354   49.612\n2  2.0468  53.4340  43.3380   96.860  148.630\n3  5.4054  43.3380 156.7100  270.900  422.580\n4 21.3540  96.8600 270.9000  748.630 1274.200\n5 49.6120 148.6300 422.5800 1274.200 2568.100\n  Standard Deviations: 6.2478 7.3098 12.518 27.361 50.677 \n```\n:::\n\n```{.r .cell-code}\n# we can also look at the sample covariance to get a rough sense for how the variances and covariances should be behaving\n```\n:::\n\n\nIt seems the variance pattern in the random effect model is severely overestimating the variance at age 2. There is also an interesting pattern in which it seems to underestimate the covariance between age 2 and 3. Other than that, the covariance looks to be within reason of capturing the overall trends. there are likely some optimizations in parameterization that can be made, but it's difficult to know exactly how...\n\n\n### Quad simplified, no intercept\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-29_cf4a239f129368a63f25b27f539ba0e1'}\n\n```{.r .cell-code}\nplot(autism_lme_quad_id_slope2_noint_nocor, form = resid(.,type = \"normalized\") ~ age | sicdegp) # normalized\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-30_41f52cd6c669d88fbab639881752a173'}\n\n```{.r .cell-code}\ngetVarCov(autism_gls_quad_un)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMarginal variance covariance matrix\n        [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 10.8700   9.0082   9.3212   25.916   48.667\n[2,]  9.0082  55.7270  45.7570  110.010  201.300\n[3,]  9.3212  45.7570 135.7700  222.120  397.840\n[4,] 25.9160 110.0100 222.1200  771.040 1197.400\n[5,] 48.6670 201.3000 397.8400 1197.400 2287.700\n  Standard Deviations: 3.297 7.4651 11.652 27.768 47.83 \n```\n:::\n\n```{.r .cell-code}\ngetVarCov(autism_lme_quad_id_slope2_noint_nocor, type = \"marginal\") # ZGZ + R\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nchildid 1 \nMarginal variance covariance matrix\n       1       2       3        4        5\n1 55.373  11.014  21.709   51.146   91.311\n2 11.014  67.485  38.850   97.086  179.460\n3 21.709  38.850 134.400  229.700  440.750\n4 51.146  97.086 229.700  706.560 1303.300\n5 91.311 179.460 440.750 1303.300 2667.700\n  Standard Deviations: 7.4413 8.2149 11.593 26.581 51.65 \n```\n:::\n:::\n\n\nThe model with the simplified G structure is also lacking in the earlier ages, and also the covariances with age2 seems to be increasing too quickly. The second diagonal band is also both over and underestimated sometimes.\n\n\n## Further covariance adjustments\n\nThe mixed effects model already has some complexity in the variance that is modeled, but it's missing some parts of the covariance that we can try to adjust for on the R side of things. I find that there aren't many guardrails when modeling things in this manner, so likelihood is generally my guide. You can try to create plots to clue you in on certain patterns but often it's just faster to fit a bunch of parameterizations and check the results.\n\nFor the diagonal of the R matrix, it's useful to know how some of the variance classes can be combined and to know what your options are, you can do this by exploring `?varClasses`. \n\n- `varIdent` allows for a different level for each variance on the diagonal.\n- `varExp` is a (fitted) exponential relationship to covariate\n- `varPower` is an (fitted) power relationship\n- `varFixed` allows for a constant (fixed) covariate value\n- `varComb` allows combinations of any of the above\n\nFor the structure of the R matrix, you can see `?corStructs`\n\n- `corAR1` allows for exponential decay in rows, as measured from distance from diagonal\n- `corCAR1` allows for exponential decay in rows, as measured from distance in continuous covariate\n- `corARMA` allows for exponential decay in rows, as distance from diagonal, AND first q diagonal bands \n- `corCompSymm` constant off diagonals\n\nFor the structure of the G matrix, there are also a number of spatial related matrices, which have a functional form of how correlation drops off in relation to distance.\n\n- `pdDiag` is useful for specifying that you only want a _diagonal_ matrix for G. This is the `(1 + age || childid)` double bar option in `lmer`\n- `pdSymm` specifies that you want to esetimate an _unstructured_ matrix for G. This is `(1 + age | childid)` default option in `lmer`.\n- `pdBlocked` is useful for composing matrix structures for nested effects in the G matrix.\n- `pdCompSymm` _compound symmetry_ in G matrix. Only possible with `flexLambda` branch in `lmer`.\n\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-31_ef3a256152f9ad23dcf2c848e1687882'}\n\n```{.r .cell-code}\n# This first model tries to add a parameter to adjust the variance of age=2, since the diagnostics above were close except for this term.\nautism_lme_quad_id_slope2_het2 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                                     random = list(childid = pdSymm(form = ~ 1 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | I(age == 2)),\n                       control = lmeControl(opt = \"optim\", maxIter = 1000, msMaxIter = 1000, msVerbose = TRUE)) # unfortunately it's a fight with the optimizer, so we need to simplify.\n## initial  value 3387.370642\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 1\n## Error in logLik.reStruct(object, conLin): NA/NaN/Inf in foreign function call (arg 3)\n\n# try removing the intercept, and fit simple model\nautism_lme_quad_id_slope2_noint_het2 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                                     random = list(childid = pdSymm(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | I(age == 2)),\n                       control = lmeControl(opt = \"optim\", maxIter = 1000, msMaxIter = 1000, msVerbose = TRUE)) # converged w/ warnings\n## initial  value 3397.116545\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 1\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 47\n## iter  10 value 3324.372416\n## final  value 3324.349202 \n## converged\n\n# the more flexible diagonal values\nautism_lme_quad_id_slope2_noint_het <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                                     random = list(childid = pdSymm(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age),\n                       control = lmeControl(opt = \"optim\")) # converged w/ warnings\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 1\n\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 47\n\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 47\n\n# we'll try more heterogenous matrices, but with the simplified G matrix and without intercept to avoid optimizer issues\nautism_lme_quad_id_slope2_noint_nocor_het <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                                     random = list(childid = pdDiag(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age),\n                       control = lmeControl(opt = \"optim\"))\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 1\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 4\n\nautism_lme_quad_id_slope2_noint_nocor_het2 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                                     random = list(childid = pdDiag(form = ~0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | I(age == 2)),\n                       control = lmeControl(opt = \"optim\"))\n\n# sincethe model onl\nautism_lme_quad_id_slope2_noint_nocor_het2 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                                     random = list(childid = pdDiag(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age),\n                       control = lmeControl(opt = \"optim\"))\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 1\n\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 4\n\nautism_lme_quad_id_slope2_noint_nocor_ar1 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                                    random = list(childid = pdDiag(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       correlation = corAR1(form = ~ 1 | childid),\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age),\n                       control = lmeControl(opt = \"optim\"))\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 1\n\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 4\n\nautism_lme_quad_id_slope2_noint_nocor_ma1 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid = pdDiag(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       correlation = corARMA(p = 0, q = 1),\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age))\n\nautism_lme_quad_id_slope2_noint_nocor_ma2 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid = pdDiag(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       correlation = corARMA(p = 0, q = 2),\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age))\n\nautism_lme_quad_id_slope2_noint_nocor_ar1ma2 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid = pdDiag(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       correlation = corARMA(p = 1, q = 2),\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age))\n\nautism_lme_quad_id_slope2_noint_nocor_car1 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid = pdDiag(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       correlation = corCAR1(form = ~ 1 | childid),\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age))\n```\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-32_0ebfaf3d05294545db42020f0a2c643b'}\n\n```{.r .fold-hide .cell-code}\nautism_lme_cor_ic <- AIC(\n  autism_lmer_quad_id_slope2,\n  autism_lme_quad_id_slope2_noint_het,\n  autism_lme_quad_id_slope2_noint_het2,\n  autism_lme_quad_id_slope2_noint_nocor,\n  autism_lme_quad_id_slope2_noint_nocor_het,\n  autism_lme_quad_id_slope2_noint_nocor_het2,\n     autism_lme_quad_id_slope2_noint_nocor_ma1,\n     autism_lme_quad_id_slope2_noint_nocor_ma2,\n     autism_lme_quad_id_slope2_noint_nocor_ar1,\n  autism_lme_quad_id_slope2_noint_nocor_ar1ma2,\n     autism_lme_quad_id_slope2_noint_nocor_car1) %>% \n  add_column(BIC = BIC(\n  autism_lmer_quad_id_slope2,\n  autism_lme_quad_id_slope2_noint_het,\n  autism_lme_quad_id_slope2_noint_het2,\n    autism_lme_quad_id_slope2_noint_nocor,\n    autism_lme_quad_id_slope2_noint_nocor_het,\n  autism_lme_quad_id_slope2_noint_nocor_het2,\n    autism_lme_quad_id_slope2_noint_nocor_ma1,\n    autism_lme_quad_id_slope2_noint_nocor_ma2,\n    autism_lme_quad_id_slope2_noint_nocor_ar1,\n    autism_lme_quad_id_slope2_noint_nocor_ar1ma2,\n    autism_lme_quad_id_slope2_noint_nocor_car1)$BIC)\n## Warning in AIC.default(autism_lmer_quad_id_slope2,\n## autism_lme_quad_id_slope2_noint_het, : models are not all fitted to the same\n## number of observations\n## Warning in BIC.default(autism_lmer_quad_id_slope2,\n## autism_lme_quad_id_slope2_noint_het, : models are not all fitted to the same\n## number of observations\n\nautism_lme_cor_ic %>% kbl() %>% \n  kable_classic(full_width=F) %>% \n  column_spec(3, color = c(\"black\", \"red\")[as.numeric(autism_lme_cor_ic$AIC == min(autism_lme_cor_ic$AIC)) + 1]) %>%\n  column_spec(4, color = c(\"black\", \"red\")[as.numeric(autism_lme_cor_ic$BIC == min(autism_lme_cor_ic$BIC)) + 1])\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-classic\" style='font-family: \"Arial Narrow\", \"Source Sans Pro\", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> df </th>\n   <th style=\"text-align:right;\"> AIC </th>\n   <th style=\"text-align:right;\"> BIC </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lmer_quad_id_slope2 </td>\n   <td style=\"text-align:right;\"> 16 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4639.983 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4710.598 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope2_noint_het </td>\n   <td style=\"text-align:right;\"> 17 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4484.389 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4559.165 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope2_noint_het2 </td>\n   <td style=\"text-align:right;\"> 14 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4536.707 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4598.287 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope2_noint_nocor </td>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4678.391 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4731.174 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope2_noint_nocor_het </td>\n   <td style=\"text-align:right;\"> 16 </td>\n   <td style=\"text-align:right;color: red !important;\"> 4482.832 </td>\n   <td style=\"text-align:right;color: red !important;\"> 4553.209 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope2_noint_nocor_het2 </td>\n   <td style=\"text-align:right;\"> 16 </td>\n   <td style=\"text-align:right;color: red !important;\"> 4482.832 </td>\n   <td style=\"text-align:right;color: red !important;\"> 4553.209 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope2_noint_nocor_ma1 </td>\n   <td style=\"text-align:right;\"> 17 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4483.408 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4558.184 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope2_noint_nocor_ma2 </td>\n   <td style=\"text-align:right;\"> 18 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4484.452 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4563.627 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope2_noint_nocor_ar1 </td>\n   <td style=\"text-align:right;\"> 17 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4483.537 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4558.313 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope2_noint_nocor_ar1ma2 </td>\n   <td style=\"text-align:right;\"> 19 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4486.409 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4569.982 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> autism_lme_quad_id_slope2_noint_nocor_car1 </td>\n   <td style=\"text-align:right;\"> 17 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4483.531 </td>\n   <td style=\"text-align:right;color: black !important;\"> 4558.307 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nIt seems like our efforts paid off by improving the model, but some of the parameterizations did not improve the fit as much as we may have been hoping. Many are quite close, so I honestly believe we've done most of the corrections appropriate for this model and we're fighting for scraps now, at least with these modeling techniques. The best fit by both AIC and BIC (among what we tried) is the simplified G model with heterogeneous structure in R.\n\nIf you want more formal quantification of model comparisons, use the likelihood ratio test.\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-33_90f62e81319c0f17cb65236ae3160ce7'}\n\n```{.r .cell-code}\nanova(autism_lme_quad_id_slope2_noint_nocor, autism_lme_quad_id_slope2_noint_nocor_het)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                          Model df      AIC      BIC    logLik\nautism_lme_quad_id_slope2_noint_nocor         1 12 4678.391 4731.174 -2327.196\nautism_lme_quad_id_slope2_noint_nocor_het     2 16 4482.832 4553.209 -2225.416\n                                            Test  L.Ratio p-value\nautism_lme_quad_id_slope2_noint_nocor                            \nautism_lme_quad_id_slope2_noint_nocor_het 1 vs 2 203.5595  <.0001\n```\n:::\n:::\n\n\n\n## Final Model\n\nWe have probably done enough modeling to come up with a final model, we'll use the best fit from the last iteration of improvements, though our final model that was selected from AIC/BIC still has some singularity issues and troubles with the optimizer. We'll have to spend some extra effort to track down why that's happening.\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-34_77fb292805fa5bf01fa3bb5ac7b530e7'}\n\n```{.r .cell-code}\ngetVarCov(autism_gls_quad_un)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMarginal variance covariance matrix\n        [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 10.8700   9.0082   9.3212   25.916   48.667\n[2,]  9.0082  55.7270  45.7570  110.010  201.300\n[3,]  9.3212  45.7570 135.7700  222.120  397.840\n[4,] 25.9160 110.0100 222.1200  771.040 1197.400\n[5,] 48.6670 201.3000 397.8400 1197.400 2287.700\n  Standard Deviations: 3.297 7.4651 11.652 27.768 47.83 \n```\n:::\n\n```{.r .cell-code}\ngetVarCov(autism_lme_quad_id_slope2_noint_nocor_het, type = \"marginal\") # seems to reflect the pattern in covariance quite well.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nchildid 1 \nMarginal variance covariance matrix\n        1        2       3        4        5\n1 12.1870   6.9707  14.602   37.024   68.995\n2  6.9707  47.3600  27.497   73.663  141.310\n3 14.6020  27.4970 128.690  183.190  361.580\n4 37.0240  73.6630 183.190  752.910 1104.700\n5 68.9950 141.3100 361.580 1104.700 2252.800\n  Standard Deviations: 3.4909 6.8819 11.344 27.439 47.464 \n```\n:::\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-35_d83e28c36f9bd9ef8e5055aff2a9ab7e'}\n\n```{.r .cell-code}\nplot(autism_lme_quad_id_slope2_noint_nocor_het, form = resid(., type = \"normalized\")~fitted(.) | sicdegp, ylim = c(-8, 8))\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-36_b2bb5edce28595f5741c134eae6a0791'}\n\n```{.r .cell-code}\nintervals(autism_lme_quad_id_slope2_noint_nocor_het)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nApproximate 95% confidence intervals\n\n Fixed effects:\n                        lower        est.     upper\n(Intercept)       -2.25946345  0.77647542 3.8124143\nage                1.68325878  3.26560911 4.8479594\nsicdegp2          -2.31980385  1.70808682 5.7359775\nsicdegp3          -7.07004509 -2.63431483 1.8014154\nage:sicdegp2      -2.17606966 -0.09458308 1.9869035\nage:sicdegp3       1.65369494  3.93648717 6.2192794\nsicdegp1:I(age^2) -0.15892339 -0.01247629 0.1339708\nsicdegp2:I(age^2) -0.08121223  0.04156100 0.1643342\nsicdegp3:I(age^2) -0.13239419  0.01868912 0.1697724\n\n Random Effects:\n  Level: childid \n                 lower      est.     upper\nsd(age)      0.5372926 0.8451180 1.3293025\nsd(I(age^2)) 0.2364546 0.2731189 0.3154684\n\n Variance function:\n         lower      est.       upper\n3  1.713870157 2.0708134    2.502096\n5  2.243430178 2.8094052    3.518165\n9  4.001851151 5.0275041    6.316026\n13 0.000146793 0.4443621 1345.144133\n\n Within-group standard error:\n   lower     est.    upper \n2.460852 2.852393 3.306230 \n```\n:::\n:::\n\n\nit seems like the variance function confidence interval for age is quite wide, which hints to where the singularity the optimizer was warning about.\n\n\n\n\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-39_31482202cc4c6d05944db45804642111'}\n\n```{.r .cell-code}\n# try this model instead\nalt_mod <- lmer(vsae ~ age*sicdegp + I(age^2):sicdegp + (1 + age + I(age^2) | sicdegp/childid), data = autism_complete) # singular, corr = -1\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n```{.r .cell-code}\nplot_redres(alt_mod, type = \"raw_mar\", xvar = \"age\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required namespace: testthat\n```\n:::\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n\n```{.r .cell-code}\nplot_redres(alt_mod, type = \"raw_mar\")\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-39-2.png){width=672}\n:::\n\n```{.r .cell-code}\nhigh_idx <- fitted(alt_mod) > 100\n\nautism_complete %>% filter(high_idx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   age vsae sicdegp childid agef\n1    9  114       3       4    9\n2   13  107       3      19   13\n3   13  124       3      42   13\n4   13  153       3      49   13\n5   13  104       3      95   13\n6    9  135       3     100    9\n7    9  171       3     105    9\n8   13  107       3     106   13\n9   13  135       3     115   13\n10   9  116       3     136    9\n11  13  192       3     136   13\n12  13  165       3     151   13\n13  13  165       3     180   13\n14  13  130       3     190   13\n15   9  110       2      91    9\n16  13  198       2      91   13\n17  13  124       2      96   13\n18  13  107       2     101   13\n19  13  144       2     107   13\n20  13  171       2     139   13\n21  13  104       2     150   13\n22  13  165       2     193   13\n23  13  147       2     212   13\n24  13  120       1      80   13\n25  13  110       1     124   13\n26  13  126       1     187   13\n27   9  130       1     210    9\n```\n:::\n\n```{.r .cell-code}\nalt_df <- autism_complete %>% \n  add_column(xb = predict(alt_mod, re.form = NA),\n             xb_zb = predict(alt_mod))\n\nalt_df %>% filter(high_idx) %>% \n  ggplot() +\n  geom_point(aes(age, xb)) +\n  geom_point(aes(age, vsae), color = \"blue\") + \n  geom_point(aes(age, xb_zb), color = \"red\") +\n  facet_wrap(~sicdegp)\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-39-3.png){width=672}\n:::\n\n```{.r .cell-code}\ncompute_redres(alt_mod, type = \"raw_mar\")[high_idx]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  51.014254   6.922364  23.922364  52.922364   3.922364  72.014254\n [7] 108.014254   6.922364  34.922364  53.014254  91.922364  64.922364\n[13]  64.922364  29.922364  76.881986 148.440308  74.440308  57.440308\n[19]  94.440308 121.440308  54.440308 115.440308  97.440308  78.275818\n[25]  68.275818  84.275818 102.239244\n```\n:::\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-40_7791562ac0520a1253d848d31b65c575'}\n\n```{.r .cell-code}\nautism_complete\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    age vsae sicdegp childid agef\n1     2    6       3       1    2\n2     3    7       3       1    3\n3     5   18       3       1    5\n4     9   25       3       1    9\n5    13   27       3       1   13\n6     2   17       3       3    2\n7     3   18       3       3    3\n8     5   12       3       3    5\n9     9   18       3       3    9\n10   13   24       3       3   13\n11    2   12       3       4    2\n12    3   14       3       4    3\n13    5   38       3       4    5\n14    9  114       3       4    9\n15    2   17       3      19    2\n16    3   27       3      19    3\n17    9   75       3      19    9\n18   13  107       3      19   13\n19    2   11       3      21    2\n20    3   21       3      21    3\n21    9   50       3      21    9\n22   13   69       3      21   13\n23    2    8       3      27    2\n24    3   17       3      27    3\n25    5   24       3      27    5\n26    9   31       3      27    9\n27   13   29       3      27   13\n28    2   11       3      36    2\n29    3   17       3      36    3\n30    9   15       3      36    9\n31    2   10       3      42    2\n32    3   27       3      42    3\n33    9   77       3      42    9\n34   13  124       3      42   13\n35    2   13       3      46    2\n36    3   26       3      46    3\n37    5   71       3      46    5\n38    9   31       3      46    9\n39   13   77       3      46   13\n40    2   19       3      48    2\n41    3   22       3      48    3\n42    5   26       3      48    5\n43    9   52       3      48    9\n44    2    9       3      49    2\n45    3   26       3      49    3\n46    5   77       3      49    5\n47    9   81       3      49    9\n48   13  153       3      49   13\n49    2   10       3      51    2\n50    3   20       3      51    3\n51    5   21       3      51    5\n52    3   16       3      60    3\n53    9   34       3      60    9\n54   13   66       3      60   13\n55    2   16       3      61    2\n56    5   33       3      61    5\n57    2   10       3      62    2\n58    3   19       3      62    3\n59    5   25       3      62    5\n60    9   67       3      62    9\n61   13   88       3      62   13\n62    2   11       3      78    2\n63    3   12       3      78    3\n64    5   15       3      78    5\n65    2   12       3      95    2\n66    3   18       3      95    3\n67    5   38       3      95    5\n68    9   73       3      95    9\n69   13  104       3      95   13\n70    2   14       3      97    2\n71    3   11       3      97    3\n72    5   25       3      97    5\n73    9   67       3      97    9\n74   13   50       3      97   13\n75    2   15       3     100    2\n76    3   24       3     100    3\n77    5   37       3     100    5\n78    9  135       3     100    9\n79    2   17       3     105    2\n80    5   65       3     105    5\n81    9  171       3     105    9\n82    2   10       3     106    2\n83    3   39       3     106    3\n84    9   63       3     106    9\n85   13  107       3     106   13\n86    2   15       3     115    2\n87    3   24       3     115    3\n88    9   71       3     115    9\n89   13  135       3     115   13\n90    2   20       3     122    2\n91    3   21       3     122    3\n92    5   34       3     122    5\n93    9   65       3     122    9\n94   13   81       3     122   13\n95    2    9       3     123    2\n96    3   13       3     123    3\n97    9   27       3     123    9\n98   13   66       3     123   13\n99    2   13       3     131    2\n100   3   31       3     131    3\n101   5   22       3     131    5\n102   9   50       3     131    9\n103  13   71       3     131   13\n104   2   12       3     133    2\n105   3   18       3     133    3\n106   5   22       3     133    5\n107   2    9       3     136    2\n108   3   33       3     136    3\n109   9  116       3     136    9\n110  13  192       3     136   13\n111   2   12       3     141    2\n112   3   15       3     141    3\n113   5   26       3     141    5\n114   9   53       3     141    9\n115  13   72       3     141   13\n116   2   18       3     151    2\n117   3   20       3     151    3\n118   5   33       3     151    5\n119   9  101       3     151    9\n120  13  165       3     151   13\n121   2   15       3     154    2\n122   3   20       3     154    3\n123   5   36       3     154    5\n124   2   13       3     158    2\n125   3   24       3     158    3\n126   5   37       3     158    5\n127   9   57       3     158    9\n128   2   12       3     174    2\n129   3   20       3     174    3\n130   5   39       3     174    5\n131   9   90       3     174    9\n132   2   15       3     180    2\n133   3   63       3     180    3\n134   9   68       3     180    9\n135  13  165       3     180   13\n136   2   11       3     182    2\n137   3   19       3     182    3\n138   9   26       3     182    9\n139  13   39       3     182   13\n140   2   15       3     183    2\n141   3   22       3     183    3\n142   5   35       3     183    5\n143   9   65       3     183    9\n144  13   62       3     183   13\n145   2    8       3     190    2\n146   3   22       3     190    3\n147   5   35       3     190    5\n148   9   69       3     190    9\n149  13  130       3     190   13\n150   2    8       3     195    2\n151   3   19       3     195    3\n152   9   59       3     195    9\n153   2    8       3     198    2\n154   3   14       3     198    3\n155   9   34       3     198    9\n156  13   55       3     198   13\n157   2   12       3     201    2\n158   3   14       3     201    3\n159   5   38       3     201    5\n160   9   93       3     201    9\n161   2    8       3     207    2\n162   3   14       3     207    3\n163   9   22       3     207    9\n164  13   48       3     207   13\n165   2   15       3     211    2\n166   2   12       2       9    2\n167   3   21       2       9    3\n168   9   66       2       9    9\n169  13   68       2       9   13\n170   2    7       2      12    2\n171   3   10       2      12    3\n172   5    8       2      12    5\n173   2   12       2      14    2\n174   3   19       2      14    3\n175   5   14       2      14    5\n176   9   28       2      14    9\n177  13   68       2      14   13\n178   2   13       2      15    2\n179   3    8       2      15    3\n180   5   29       2      15    5\n181   9   24       2      15    9\n182  13   44       2      15   13\n183   2    7       2      16    2\n184   3    6       2      16    3\n185   9   39       2      16    9\n186  13   24       2      16   13\n187   2    5       2      17    2\n188   3   10       2      17    3\n189   5   29       2      17    5\n190   9   32       2      17    9\n191  13   67       2      17   13\n192   2   10       2      18    2\n193   3    6       2      18    3\n194   2   11       2      24    2\n195   3   13       2      24    3\n196   5   20       2      24    5\n197   9   33       2      24    9\n198   2    3       2      28    2\n199   3   10       2      28    3\n200   5   17       2      28    5\n201  13   42       2      28   13\n202   2   10       2      30    2\n203   3   15       2      30    3\n204   9   40       2      30    9\n205  13  101       2      30   13\n206   2    7       2      33    2\n207   3   10       2      33    3\n208   5   12       2      33    5\n209   9   15       2      33    9\n210  13   10       2      33   13\n211   2    7       2      35    2\n212   3   10       2      35    3\n213   2    6       2      37    2\n214  13   15       2      37   13\n215   2    7       2      39    2\n216   3   12       2      39    3\n217   5    9       2      39    5\n218   9    6       2      39    9\n219   2    8       2      40    2\n220   3   22       2      40    3\n221   5   24       2      40    5\n222   9   54       2      40    9\n223  13   54       2      40   13\n224   2   17       2      44    2\n225   3   18       2      44    3\n226   9   21       2      44    9\n227   2    8       2      47    2\n228   3    9       2      47    3\n229   5   19       2      47    5\n230   2    7       2      55    2\n231   3    9       2      55    3\n232   5   13       2      55    5\n233   9   29       2      55    9\n234  13   36       2      55   13\n235   2   10       2      58    2\n236   3   19       2      58    3\n237   9   53       2      58    9\n238  13   95       2      58   13\n239   2    6       2      63    2\n240   3    7       2      63    3\n241   5   14       2      63    5\n242   9   32       2      63    9\n243   2    7       2      64    2\n244   3   14       2      64    3\n245   5    9       2      64    5\n246  13   40       2      64   13\n247   2    6       2      65    2\n248   3   10       2      65    3\n249   5   11       2      65    5\n250   9   15       2      65    9\n251   2    3       2      67    2\n252   3   11       2      67    3\n253   9   15       2      67    9\n254  13   17       2      67   13\n255   2   14       2      76    2\n256   3   21       2      76    3\n257   5   22       2      76    5\n258   2    9       2      81    2\n259   3   13       2      81    3\n260   9   22       2      81    9\n261  13   44       2      81   13\n262   2   10       2      85    2\n263   5   22       2      85    5\n264   2   12       2      87    2\n265   3   27       2      87    3\n266   5   18       2      87    5\n267   9    8       2      87    9\n268  13    9       2      87   13\n269   2   10       2      91    2\n270   3   25       2      91    3\n271   9  110       2      91    9\n272  13  198       2      91   13\n273   2    6       2      92    2\n274   5    4       2      92    5\n275   9   15       2      92    9\n276  13   18       2      92   13\n277   2   17       2      96    2\n278   3   18       2      96    3\n279   5   27       2      96    5\n280   9   66       2      96    9\n281  13  124       2      96   13\n282   2    8       2     101    2\n283   3   24       2     101    3\n284   9   75       2     101    9\n285  13  107       2     101   13\n286   2   12       2     107    2\n287   3   15       2     107    3\n288   5   31       2     107    5\n289  13  144       2     107   13\n290   2   10       2     108    2\n291   3   20       2     108    3\n292   9   40       2     108    9\n293  13   73       2     108   13\n294   2   12       2     110    2\n295   3   22       2     110    3\n296   9   63       2     110    9\n297  13   44       2     110   13\n298   2   11       2     113    2\n299   3   13       2     113    3\n300   5   10       2     113    5\n301   9   18       2     113    9\n302  13   24       2     113   13\n303   2    1       2     116    2\n304   3    9       2     116    3\n305   9   15       2     116    9\n306  13   14       2     116   13\n307   2    4       2     119    2\n308   3    6       2     119    3\n309   5   12       2     119    5\n310   2   12       2     120    2\n311   3    9       2     120    3\n312   5    8       2     120    5\n313   2    9       2     126    2\n314   3    8       2     126    3\n315   5   18       2     126    5\n316   9   12       2     126    9\n317   2    9       2     128    2\n318   3   14       2     128    3\n319   9   14       2     128    9\n320   2    4       2     129    2\n321   3   14       2     129    3\n322   2   13       2     139    2\n323   3   18       2     139    3\n324   5   30       2     139    5\n325   9   73       2     139    9\n326  13  171       2     139   13\n327   2    3       2     142    2\n328   3   13       2     142    3\n329   9   12       2     142    9\n330  13   17       2     142   13\n331   2    7       2     145    2\n332   3    8       2     145    3\n333   5   12       2     145    5\n334   9   21       2     145    9\n335  13   28       2     145   13\n336   2   11       2     146    2\n337   3   18       2     146    3\n338   9   21       2     146    9\n339  13   32       2     146   13\n340   2    9       2     147    2\n341   3   12       2     147    3\n342   5   11       2     147    5\n343   9   13       2     147    9\n344   2    6       2     148    2\n345   3    9       2     148    3\n346   5    9       2     148    5\n347   9   21       2     148    9\n348  13   15       2     148   13\n349   2   18       2     150    2\n350   3   21       2     150    3\n351   5   35       2     150    5\n352   9   66       2     150    9\n353  13  104       2     150   13\n354   2    7       2     152    2\n355   3   18       2     152    3\n356   9   10       2     152    9\n357  13   24       2     152   13\n358   2   10       2     155    2\n359   3   25       2     155    3\n360   9   59       2     155    9\n361  13   40       2     155   13\n362   2   10       2     159    2\n363   3   11       2     159    3\n364   5   16       2     159    5\n365   2    8       2     165    2\n366   3   13       2     165    3\n367   9   18       2     165    9\n368   2    6       2     169    2\n369   3    9       2     169    3\n370   5   17       2     169    5\n371   9   13       2     169    9\n372  13   14       2     169   13\n373   2    7       2     170    2\n374   3   14       2     170    3\n375   2    9       2     173    2\n376   3   11       2     173    3\n377   2    4       2     175    2\n378   3   10       2     175    3\n379   5   24       2     175    5\n380   9   33       2     175    9\n381  13   54       2     175   13\n382   2   15       2     178    2\n383   3   15       2     178    3\n384   9   54       2     178    9\n385   2   11       2     181    2\n386   3   11       2     181    3\n387   5   26       2     181    5\n388   9   13       2     181    9\n389   2    8       2     191    2\n390   3   11       2     191    3\n391   5   11       2     191    5\n392   9   12       2     191    9\n393  13   14       2     191   13\n394   3   39       2     193    3\n395  13  165       2     193   13\n396   2    5       2     194    2\n397   3   13       2     194    3\n398   2    7       2     196    2\n399   3   12       2     196    3\n400   9   15       2     196    9\n401  13    9       2     196   13\n402   2    9       2     204    2\n403   3   10       2     204    3\n404   5   17       2     204    5\n405   9   26       2     204    9\n406   2   11       2     205    2\n407   3   15       2     205    3\n408   9   18       2     205    9\n409  13   66       2     205   13\n410   2   10       2     208    2\n411   3   13       2     208    3\n412   2    2       2     209    2\n413   3    4       2     209    3\n414   9   12       2     209    9\n415  13   32       2     209   13\n416   2    7       2     212    2\n417   3   21       2     212    3\n418   5   29       2     212    5\n419   9   72       2     212    9\n420  13  147       2     212   13\n421   2    6       1       2    2\n422   3    7       1       2    3\n423   5    7       1       2    5\n424   9    8       1       2    9\n425  13   14       1       2   13\n426   2    6       1       6    2\n427   3   12       1       6    3\n428   9   12       1       6    9\n429  13   45       1       6   13\n430   2    9       1       8    2\n431   3   12       1       8    3\n432   5   14       1       8    5\n433   2    9       1      10    2\n434   3   11       1      10    3\n435   9   18       1      10    9\n436  13   39       1      10   13\n437   2    6       1      13    2\n438   3   10       1      13    3\n439   5   12       1      13    5\n440   2   11       1      22    2\n441   3   14       1      22    3\n442   5   22       1      22    5\n443   2    6       1      31    2\n444   3   13       1      31    3\n445   5   15       1      31    5\n446   9   54       1      31    9\n447   2    5       1      32    2\n448   3   11       1      32    3\n449   5   12       1      32    5\n450   2   10       1      38    2\n451   3    6       1      38    3\n452   5   11       1      38    5\n453   9    3       1      38    9\n454  13    8       1      38   13\n455   2    8       1      41    2\n456   3   23       1      41    3\n457   5   28       1      41    5\n458   2    6       1      43    2\n459   3    9       1      43    3\n460   5   10       1      43    5\n461   9   10       1      43    9\n462  13   15       1      43   13\n463   2    5       1      45    2\n464   3   10       1      45    3\n465   9   13       1      45    9\n466  13   16       1      45   13\n467   2    9       1      50    2\n468   3   14       1      50    3\n469   9   15       1      50    9\n470  13   54       1      50   13\n471   2    7       1      57    2\n472   3   11       1      57    3\n473   5   15       1      57    5\n474   9   26       1      57    9\n475   2    7       1      59    2\n476   3    8       1      59    3\n477   5   26       1      59    5\n478   9   11       1      59    9\n479  13   12       1      59   13\n480   2    6       1      66    2\n481   3    9       1      66    3\n482   5   12       1      66    5\n483   9   11       1      66    9\n484  13   13       1      66   13\n485   2    7       1      70    2\n486   3   11       1      70    3\n487   9    7       1      70    9\n488   2    5       1      71    2\n489   3   11       1      71    3\n490   9   42       1      71    9\n491   2   10       1      77    2\n492   3   12       1      77    3\n493   5   13       1      77    5\n494   9   16       1      77    9\n495  13   83       1      77   13\n496   2   13       1      80    2\n497   3   45       1      80    3\n498   9   73       1      80    9\n499  13  120       1      80   13\n500   2   10       1      82    2\n501   3    8       1      82    3\n502   5   12       1      82    5\n503   9   14       1      82    9\n504   2    5       1      86    2\n505   3   10       1      86    3\n506   5   13       1      86    5\n508   2    4       1      88    2\n509   3   12       1      88    3\n510   2    9       1      99    2\n511   3   13       1      99    3\n512   5   10       1      99    5\n513   9   12       1      99    9\n514   2   12       1     104    2\n515   3   14       1     104    3\n516   5   13       1     104    5\n517   9   21       1     104    9\n518   2    6       1     109    2\n519   3    5       1     109    3\n520   5    6       1     109    5\n521   9    7       1     109    9\n522  13    8       1     109   13\n523   2   10       1     111    2\n524   3   10       1     111    3\n525   5   12       1     111    5\n526   9   13       1     111    9\n527  13   12       1     111   13\n528   2    7       1     112    2\n529   3    7       1     112    3\n530   5   11       1     112    5\n531   9   16       1     112    9\n532   2    6       1     114    2\n533   2    9       1     117    2\n534   3   12       1     117    3\n535   9   12       1     117    9\n536  13   12       1     117   13\n537   2    5       1     124    2\n538   9  114       1     124    9\n539  13  110       1     124   13\n540   2    4       1     134    2\n541   3   12       1     134    3\n542   9   23       1     134    9\n543  13   21       1     134   13\n544   2    4       1     135    2\n545   3   10       1     135    3\n546   9   13       1     135    9\n547  13   35       1     135   13\n548   2    5       1     143    2\n549   3   11       1     143    3\n550   9   20       1     143    9\n551  13   45       1     143   13\n552   2    8       1     153    2\n554   5   17       1     153    5\n555   9   22       1     153    9\n556  13   29       1     153   13\n557   2    8       1     156    2\n558   3   16       1     156    3\n559   5   42       1     156    5\n560   2   13       1     161    2\n561   3   15       1     161    3\n562   5   30       1     161    5\n563  13   79       1     161   13\n564   2    4       1     167    2\n565   3    8       1     167    3\n566   2    1       1     168    2\n567   3    6       1     168    3\n568   5   12       1     168    5\n569   9   11       1     168    9\n570  13   10       1     168   13\n571   2    4       1     171    2\n572   3    8       1     171    3\n573   5   16       1     171    5\n574   9   33       1     171    9\n575  13   49       1     171   13\n576   2    7       1     172    2\n577   3   11       1     172    3\n578   9   10       1     172    9\n579  13   15       1     172   13\n580   2    1       1     179    2\n581   3    6       1     179    3\n582   5    7       1     179    5\n583  13    7       1     179   13\n584   2    4       1     184    2\n585   3   12       1     184    3\n586   5   13       1     184    5\n587   9   15       1     184    9\n588   2    6       1     185    2\n589   3   15       1     185    3\n590   9   18       1     185    9\n591  13   42       1     185   13\n592   2    9       1     186    2\n593   3   18       1     186    3\n594   5   19       1     186    5\n595   9   20       1     186    9\n596   2    6       1     187    2\n597   3   13       1     187    3\n598   9   65       1     187    9\n599  13  126       1     187   13\n600   2    9       1     197    2\n601   3   12       1     197    3\n602   9   12       1     197    9\n603  13    8       1     197   13\n604   2   11       1     200    2\n605   3    8       1     200    3\n606   2    8       1     202    2\n607   3    9       1     202    3\n608   5    6       1     202    5\n609  13   12       1     202   13\n610   2    4       1     210    2\n611   3   25       1     210    3\n612   9  130       1     210    9\n```\n:::\n\n```{.r .cell-code}\nfitted(autism_lmer_quad_id_slope2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         1          2          3          4          5          6          7 \n 12.313226  14.430375  18.241976  24.174403  27.852459  12.395592  13.304193 \n         8          9         10         11         12         13         14 \n 15.222047  19.460365  24.235497  16.170573  24.152411  45.161463 107.361071 \n        15         16         17         18         19         20         21 \n 13.629456  21.967135  73.091758 108.221079  12.989889  18.398589  49.778019 \n        22         23         24         25         26         27         28 \n 69.675951  12.069754  16.235883  23.062853  30.695639  30.300220  12.094076 \n        29         30         31         32         33         34         35 \n 13.545404  17.829659  14.166637  21.833236  77.436628 123.652032  12.602822 \n        36         37         38         39         40         41         42 \n 20.702286  35.199620  57.387903  70.501009  13.542575  17.920102  27.875873 \n        43         44         45         46         47         48         49 \n 52.590288  14.241925  25.942024  49.626245  98.130782 148.150113  13.262949 \n        50         51         52         53         54         55         56 \n 16.697904  24.529874  15.235062  38.083033  64.518021  13.878495  32.947150 \n        57         58         59         60         61         62         63 \n 13.466873  19.571263  32.311947  59.920928  90.366726  12.991207  14.447106 \n        64         65         66         67         68         69         70 \n 18.413099  13.640252  21.354394  37.150001  70.210506 105.230066  12.242385 \n        71         72         73         74         75         76         77 \n 20.135835  33.605942  51.278979  56.595781  17.139047  26.143664  50.708238 \n        78         79         80         81         82         83         84 \n126.058748  17.628913  69.290723 164.001448  13.789259  20.580422  67.830205 \n        85         86         87         88         89         90         91 \n105.523210  14.782266  20.111561  74.784843 132.850376  13.068419  20.505418 \n        92         93         94         95         96         97         98 \n 34.755578  60.760541  83.438364  13.600581  13.822077  33.188316  63.277493 \n        99        100        101        102        103        104        105 \n 13.136834  17.959221  27.905883  49.006763  71.717714  13.339514  16.786846 \n       106        107        108        109        110        111        112 \n 24.783822  15.485703  27.020928 114.949508 191.394498  13.165329  18.099080 \n       113        114        115        116        117        118        119 \n 28.288123  49.952377  73.331519  15.382148  22.903610  41.649184  93.950938 \n       120        121        122        123        124        125        126 \n166.000169  13.956704  20.546750  35.032095  13.207279  19.921226  33.206897 \n       127        128        129        130        131        132        133 \n 59.209347  14.826611  22.645858  40.851234  87.529510  15.479535  20.898888 \n       134        135        136        137        138        139        140 \n 85.101530 158.080933  12.584575  14.908647  29.025613  38.601241  12.372833 \n       141        142        143        144        145        146        147 \n 21.172602  36.424444  57.537339  66.129185  14.619951  20.064712  33.856215 \n       148        149        150        151        152        153        154 \n 73.047142 127.715298  13.561293  19.199636  58.990763  12.995255  15.557264 \n       155        156        157        158        159        160        161 \n 36.001681  54.462111  15.119631  22.468128  40.426692  89.390102  13.070192 \n       162        163        164        165        166        167        168 \n 13.597671  27.163064  46.111915  13.832097   9.426625  18.542731  58.677894 \n       169        170        171        172        173        174        175 \n 71.566596   9.245111   9.627182  10.632153  10.530568  11.251127  15.247897 \n       176        177        178        179        180        181        182 \n 33.464048  65.310348   9.534244  12.550592  18.564961  30.520388  42.378067 \n       183        184        185        186        187        188        189 \n  8.736631  14.254922  31.395095  27.612759  10.153066  13.160093  20.322694 \n       190        191        192        193        194        195        196 \n 39.242083  64.287056   9.493371  10.689731   9.712336  12.791451  19.247219 \n       197        198        199        200        201        202        203 \n 33.348907   9.614452  12.003108  17.104312  41.827684  11.398791  12.298342 \n       204        205        206        207        208        209        210 \n 46.635458  97.088592   8.863234  10.452103  12.784462  14.067662  10.842172 \n       211        212        213        214        215        216        217 \n  9.546941  11.762262   8.973412  15.439253   8.788553   9.241661   9.508242 \n       218        219        220        221        222        223        224 \n  7.482875   9.363647  16.219079  28.420411  46.784940  57.098625   9.309624 \n       225        226        227        228        229        230        231 \n 11.380442  22.737046   9.658775  12.327023  17.985555   9.510663  11.673792 \n       232        233        234        235        236        237        238 \n 16.197459  26.034428  36.924243  10.777448  15.131933  54.644818  93.735290 \n       239        240        241        242        243        244        245 \n  9.946879  11.738965  16.454198  30.408907   9.784800  10.634290  13.446794 \n       246        247        248        249        250        251        252 \n 39.543782   9.173339  10.138338  11.996459  15.425196   9.066062  10.446956 \n       253        254        255        256        257        258        259 \n 15.929162  16.914295  10.088128  14.269057  23.299460   9.848249  10.850226 \n       260        261        262        263        264        265        266 \n 25.166817  42.620479   9.879266  21.267206   8.679327  10.938629  14.061540 \n       267        268        269        270        271        272        273 \n 14.724598   7.943970  12.874612  22.403533 109.535512 196.155360   9.319229 \n       274        275        276        277        278        279        280 \n  9.846023  12.850566  18.486140  11.557316  16.120018  28.561203  66.706694 \n       281        282        283        284        285        286        287 \n122.536347  10.542735  19.081239  71.679521 108.047192  11.987365  17.383216 \n       288        289        290        291        292        293        294 \n 32.045666 142.305417  10.324534  13.566102  42.812639  71.640924   8.627624 \n       295        296        297        298        299        300        301 \n 18.879554  52.885235  49.359591   9.323848  10.358239  12.617105  17.895176 \n       302        303        304        305        306        307        308 \n 24.187033   8.974839  10.452840  15.109307  14.202632   9.249247  10.233207 \n       309        310        311        312        313        314        315 \n 12.266257   9.391514   9.589576  10.558014   8.801750  10.643243  13.288186 \n       316        317        318        319        320        321        322 \n 14.425899   9.024984  10.423305  15.432000   9.600511  12.834794  12.976064 \n       323        324        325        326        327        328        329 \n 16.228751  29.117984  80.431878 165.793013   9.124003  10.018363  14.382501 \n       330        331        332        333        334        335        336 \n 16.337619   9.398701  10.691067  13.528045  20.210982  28.239226   9.387462 \n       337        338        339        340        341        342        343 \n 11.359653  23.359869  31.519122   9.082187  10.024664  11.673147  14.024218 \n       344        345        346        347        348        349        350 \n  9.008839  10.724108  13.544624  16.745577  16.693090  10.670133  17.637212 \n       351        352        353        354        355        356        357 \n 32.485366  65.837659 104.064600   9.385313   9.666184  15.099252  22.290668 \n       358        359        360        361        362        363        364 \n  8.549260  18.490214  50.261778  44.896004   9.636706  11.776664  16.498147 \n       365        366        367        368        369        370        371 \n  9.175527  10.869210  19.002063   8.927391  10.628135  13.270822  15.520996 \n       372        373        374        375        376        377        378 \n 13.724235   9.707307  12.955068   9.658230  12.140647   9.799907  13.052686 \n       379        380        381        382        383        384        385 \n 19.967567  35.434621  53.084734  10.526255  15.286603  53.061066   8.490054 \n       386        387        388        389        390        391        392 \n 11.855571  16.497665  17.426083   9.066235   9.890038  11.307342  13.220742 \n       393        394        395        396        397        398        399 \n 13.905866  22.580534 164.298198   9.596018  12.576684   8.777818  10.734808 \n       400        401        402        403        404        405        406 \n 14.648869   9.803123   9.511384  11.735515  16.363946  26.341487  10.736915 \n       407        408        409        410        411        412        413 \n  9.456753  26.651682  61.806303   9.774011  12.777141   9.772482   8.967104 \n       414        415        416        417        418        419        420 \n 15.298580  30.151702  12.150547  16.758810  30.393123  75.332905 143.834228 \n       421        422        423        424        425        426        427 \n  7.950103   7.743019   7.753710   9.474538  13.461292   8.878659   7.835014 \n       428        429        430        431        432        433        434 \n 18.558896  41.885056   8.319251  10.380780  14.931520   8.519414   9.044830 \n       435        436        437        438        439        440        441 \n 21.104604  37.627575   8.131068   9.655584  12.942021   8.690439  12.612237 \n       442        443        444        445        446        447        448 \n 21.014656   9.631713  12.873242  21.893077  50.079863   8.110661   9.737559 \n       449        450        451        452        453        454        455 \n 13.159163   7.756369   7.626380   7.403338   7.104996   7.003642   8.910275 \n       456        457        458        459        460        461        462 \n 14.933917  27.312822   7.867558   8.398595   9.505249  11.896877  14.526264 \n       463        464        465        466        467        468        469 \n  7.822006   8.907238  13.948313  15.908728   9.108414   8.160644  22.280460 \n       470        471        472        473        474        475        476 \n 50.556945   8.278063  10.513856  15.282527  26.008208   7.402012  10.358043 \n       477        478        479        480        481        482        483 \n 14.699736  17.101654  11.128277   7.737870   8.802123  10.568715  12.654241 \n       484        485        486        487        488        489        490 \n 12.809556   7.583098   8.221749   8.390500   8.796311  12.593755  40.949588 \n       491        492        493        494        495        496        497 \n 10.018678   7.920314   8.616073  29.577545  76.632290   9.549987  18.360377 \n       498        499        500        501        502        503        504 \n 76.020802 119.030685   7.897566   8.973225  11.059548  14.972210   8.135320 \n       505        506        508        509        510        511        512 \n  9.897384  13.594502   8.248076  10.935928   7.784617   8.816855  10.617110 \n       513        514        515        516        517        518        519 \n 13.160734   8.133265   9.919524  13.653791  21.769312   7.800100   7.565070 \n       520        521        522        523        524        525        526 \n  7.246226   7.213400   7.987058   7.672886   9.091652  11.336963  13.458702 \n       527        528        529        530        531        532        533 \n 12.421929   8.014459   9.007845  11.174583  16.227927   8.263066   7.693886 \n       534        535        536        537        538        539        540 \n  8.934786  12.890696  12.204648   8.279086  97.049843 117.339002   7.701847 \n       541        542        543        544        545        546        547 \n 10.702548  21.566891  22.009918   8.506604   8.265251  17.191919  33.023782 \n       548        549        550        551        552        554        555 \n  8.668426   9.253363  23.505340  43.237478   7.987193  14.988224  22.869013 \n       556        557        558        559        560        561        562 \n 29.088181   9.333139  17.822941  35.177812   8.896548  14.622649  26.471353 \n       563        564        565        566        567        568        569 \n 79.152863   8.087710   9.743122   7.647100   8.791028  10.521940  11.755987 \n       570        571        572        573        574        575        576 \n 10.019664   8.524305  11.157592  17.063273  31.431060  49.207413   7.862572 \n       577        578        579        580        581        582        583 \n  8.428136  11.994262  14.536197   7.704075   7.924851   8.198044   7.046037 \n       584        585        586        587        588        589        590 \n  7.753380   9.475242  12.385761  16.073983   8.569591   9.220894  22.428053 \n       591        592        593        594        595        596        597 \n 40.089343   7.719017  10.848544  16.080643  22.437024  10.349085  14.826560 \n       598        599        600        601        602        603        604 \n 66.257393 123.940789   7.549049   9.072484  12.213593   8.593856   8.336900 \n       605        606        607        608        609        610        611 \n 10.023762   7.862503   7.979278   8.374183  12.105195  11.566102  24.507304 \n       612 \n124.684849 \n```\n:::\n\n```{.r .cell-code}\npredict(autism_lmer_quad_id_slope2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         1          2          3          4          5          6          7 \n 12.313226  14.430375  18.241976  24.174403  27.852459  12.395592  13.304193 \n         8          9         10         11         12         13         14 \n 15.222047  19.460365  24.235497  16.170573  24.152411  45.161463 107.361071 \n        15         16         17         18         19         20         21 \n 13.629456  21.967135  73.091758 108.221079  12.989889  18.398589  49.778019 \n        22         23         24         25         26         27         28 \n 69.675951  12.069754  16.235883  23.062853  30.695639  30.300220  12.094076 \n        29         30         31         32         33         34         35 \n 13.545404  17.829659  14.166637  21.833236  77.436628 123.652032  12.602822 \n        36         37         38         39         40         41         42 \n 20.702286  35.199620  57.387903  70.501009  13.542575  17.920102  27.875873 \n        43         44         45         46         47         48         49 \n 52.590288  14.241925  25.942024  49.626245  98.130782 148.150113  13.262949 \n        50         51         52         53         54         55         56 \n 16.697904  24.529874  15.235062  38.083033  64.518021  13.878495  32.947150 \n        57         58         59         60         61         62         63 \n 13.466873  19.571263  32.311947  59.920928  90.366726  12.991207  14.447106 \n        64         65         66         67         68         69         70 \n 18.413099  13.640252  21.354394  37.150001  70.210506 105.230066  12.242385 \n        71         72         73         74         75         76         77 \n 20.135835  33.605942  51.278979  56.595781  17.139047  26.143664  50.708238 \n        78         79         80         81         82         83         84 \n126.058748  17.628913  69.290723 164.001448  13.789259  20.580422  67.830205 \n        85         86         87         88         89         90         91 \n105.523210  14.782266  20.111561  74.784843 132.850376  13.068419  20.505418 \n        92         93         94         95         96         97         98 \n 34.755578  60.760541  83.438364  13.600581  13.822077  33.188316  63.277493 \n        99        100        101        102        103        104        105 \n 13.136834  17.959221  27.905883  49.006763  71.717714  13.339514  16.786846 \n       106        107        108        109        110        111        112 \n 24.783822  15.485703  27.020928 114.949508 191.394498  13.165329  18.099080 \n       113        114        115        116        117        118        119 \n 28.288123  49.952377  73.331519  15.382148  22.903610  41.649184  93.950938 \n       120        121        122        123        124        125        126 \n166.000169  13.956704  20.546750  35.032095  13.207279  19.921226  33.206897 \n       127        128        129        130        131        132        133 \n 59.209347  14.826611  22.645858  40.851234  87.529510  15.479535  20.898888 \n       134        135        136        137        138        139        140 \n 85.101530 158.080933  12.584575  14.908647  29.025613  38.601241  12.372833 \n       141        142        143        144        145        146        147 \n 21.172602  36.424444  57.537339  66.129185  14.619951  20.064712  33.856215 \n       148        149        150        151        152        153        154 \n 73.047142 127.715298  13.561293  19.199636  58.990763  12.995255  15.557264 \n       155        156        157        158        159        160        161 \n 36.001681  54.462111  15.119631  22.468128  40.426692  89.390102  13.070192 \n       162        163        164        165        166        167        168 \n 13.597671  27.163064  46.111915  13.832097   9.426625  18.542731  58.677894 \n       169        170        171        172        173        174        175 \n 71.566596   9.245111   9.627182  10.632153  10.530568  11.251127  15.247897 \n       176        177        178        179        180        181        182 \n 33.464048  65.310348   9.534244  12.550592  18.564961  30.520388  42.378067 \n       183        184        185        186        187        188        189 \n  8.736631  14.254922  31.395095  27.612759  10.153066  13.160093  20.322694 \n       190        191        192        193        194        195        196 \n 39.242083  64.287056   9.493371  10.689731   9.712336  12.791451  19.247219 \n       197        198        199        200        201        202        203 \n 33.348907   9.614452  12.003108  17.104312  41.827684  11.398791  12.298342 \n       204        205        206        207        208        209        210 \n 46.635458  97.088592   8.863234  10.452103  12.784462  14.067662  10.842172 \n       211        212        213        214        215        216        217 \n  9.546941  11.762262   8.973412  15.439253   8.788553   9.241661   9.508242 \n       218        219        220        221        222        223        224 \n  7.482875   9.363647  16.219079  28.420411  46.784940  57.098625   9.309624 \n       225        226        227        228        229        230        231 \n 11.380442  22.737046   9.658775  12.327023  17.985555   9.510663  11.673792 \n       232        233        234        235        236        237        238 \n 16.197459  26.034428  36.924243  10.777448  15.131933  54.644818  93.735290 \n       239        240        241        242        243        244        245 \n  9.946879  11.738965  16.454198  30.408907   9.784800  10.634290  13.446794 \n       246        247        248        249        250        251        252 \n 39.543782   9.173339  10.138338  11.996459  15.425196   9.066062  10.446956 \n       253        254        255        256        257        258        259 \n 15.929162  16.914295  10.088128  14.269057  23.299460   9.848249  10.850226 \n       260        261        262        263        264        265        266 \n 25.166817  42.620479   9.879266  21.267206   8.679327  10.938629  14.061540 \n       267        268        269        270        271        272        273 \n 14.724598   7.943970  12.874612  22.403533 109.535512 196.155360   9.319229 \n       274        275        276        277        278        279        280 \n  9.846023  12.850566  18.486140  11.557316  16.120018  28.561203  66.706694 \n       281        282        283        284        285        286        287 \n122.536347  10.542735  19.081239  71.679521 108.047192  11.987365  17.383216 \n       288        289        290        291        292        293        294 \n 32.045666 142.305417  10.324534  13.566102  42.812639  71.640924   8.627624 \n       295        296        297        298        299        300        301 \n 18.879554  52.885235  49.359591   9.323848  10.358239  12.617105  17.895176 \n       302        303        304        305        306        307        308 \n 24.187033   8.974839  10.452840  15.109307  14.202632   9.249247  10.233207 \n       309        310        311        312        313        314        315 \n 12.266257   9.391514   9.589576  10.558014   8.801750  10.643243  13.288186 \n       316        317        318        319        320        321        322 \n 14.425899   9.024984  10.423305  15.432000   9.600511  12.834794  12.976064 \n       323        324        325        326        327        328        329 \n 16.228751  29.117984  80.431878 165.793013   9.124003  10.018363  14.382501 \n       330        331        332        333        334        335        336 \n 16.337619   9.398701  10.691067  13.528045  20.210982  28.239226   9.387462 \n       337        338        339        340        341        342        343 \n 11.359653  23.359869  31.519122   9.082187  10.024664  11.673147  14.024218 \n       344        345        346        347        348        349        350 \n  9.008839  10.724108  13.544624  16.745577  16.693090  10.670133  17.637212 \n       351        352        353        354        355        356        357 \n 32.485366  65.837659 104.064600   9.385313   9.666184  15.099252  22.290668 \n       358        359        360        361        362        363        364 \n  8.549260  18.490214  50.261778  44.896004   9.636706  11.776664  16.498147 \n       365        366        367        368        369        370        371 \n  9.175527  10.869210  19.002063   8.927391  10.628135  13.270822  15.520996 \n       372        373        374        375        376        377        378 \n 13.724235   9.707307  12.955068   9.658230  12.140647   9.799907  13.052686 \n       379        380        381        382        383        384        385 \n 19.967567  35.434621  53.084734  10.526255  15.286603  53.061066   8.490054 \n       386        387        388        389        390        391        392 \n 11.855571  16.497665  17.426083   9.066235   9.890038  11.307342  13.220742 \n       393        394        395        396        397        398        399 \n 13.905866  22.580534 164.298198   9.596018  12.576684   8.777818  10.734808 \n       400        401        402        403        404        405        406 \n 14.648869   9.803123   9.511384  11.735515  16.363946  26.341487  10.736915 \n       407        408        409        410        411        412        413 \n  9.456753  26.651682  61.806303   9.774011  12.777141   9.772482   8.967104 \n       414        415        416        417        418        419        420 \n 15.298580  30.151702  12.150547  16.758810  30.393123  75.332905 143.834228 \n       421        422        423        424        425        426        427 \n  7.950103   7.743019   7.753710   9.474538  13.461292   8.878659   7.835014 \n       428        429        430        431        432        433        434 \n 18.558896  41.885056   8.319251  10.380780  14.931520   8.519414   9.044830 \n       435        436        437        438        439        440        441 \n 21.104604  37.627575   8.131068   9.655584  12.942021   8.690439  12.612237 \n       442        443        444        445        446        447        448 \n 21.014656   9.631713  12.873242  21.893077  50.079863   8.110661   9.737559 \n       449        450        451        452        453        454        455 \n 13.159163   7.756369   7.626380   7.403338   7.104996   7.003642   8.910275 \n       456        457        458        459        460        461        462 \n 14.933917  27.312822   7.867558   8.398595   9.505249  11.896877  14.526264 \n       463        464        465        466        467        468        469 \n  7.822006   8.907238  13.948313  15.908728   9.108414   8.160644  22.280460 \n       470        471        472        473        474        475        476 \n 50.556945   8.278063  10.513856  15.282527  26.008208   7.402012  10.358043 \n       477        478        479        480        481        482        483 \n 14.699736  17.101654  11.128277   7.737870   8.802123  10.568715  12.654241 \n       484        485        486        487        488        489        490 \n 12.809556   7.583098   8.221749   8.390500   8.796311  12.593755  40.949588 \n       491        492        493        494        495        496        497 \n 10.018678   7.920314   8.616073  29.577545  76.632290   9.549987  18.360377 \n       498        499        500        501        502        503        504 \n 76.020802 119.030685   7.897566   8.973225  11.059548  14.972210   8.135320 \n       505        506        508        509        510        511        512 \n  9.897384  13.594502   8.248076  10.935928   7.784617   8.816855  10.617110 \n       513        514        515        516        517        518        519 \n 13.160734   8.133265   9.919524  13.653791  21.769312   7.800100   7.565070 \n       520        521        522        523        524        525        526 \n  7.246226   7.213400   7.987058   7.672886   9.091652  11.336963  13.458702 \n       527        528        529        530        531        532        533 \n 12.421929   8.014459   9.007845  11.174583  16.227927   8.263066   7.693886 \n       534        535        536        537        538        539        540 \n  8.934786  12.890696  12.204648   8.279086  97.049843 117.339002   7.701847 \n       541        542        543        544        545        546        547 \n 10.702548  21.566891  22.009918   8.506604   8.265251  17.191919  33.023782 \n       548        549        550        551        552        554        555 \n  8.668426   9.253363  23.505340  43.237478   7.987193  14.988224  22.869013 \n       556        557        558        559        560        561        562 \n 29.088181   9.333139  17.822941  35.177812   8.896548  14.622649  26.471353 \n       563        564        565        566        567        568        569 \n 79.152863   8.087710   9.743122   7.647100   8.791028  10.521940  11.755987 \n       570        571        572        573        574        575        576 \n 10.019664   8.524305  11.157592  17.063273  31.431060  49.207413   7.862572 \n       577        578        579        580        581        582        583 \n  8.428136  11.994262  14.536197   7.704075   7.924851   8.198044   7.046037 \n       584        585        586        587        588        589        590 \n  7.753380   9.475242  12.385761  16.073983   8.569591   9.220894  22.428053 \n       591        592        593        594        595        596        597 \n 40.089343   7.719017  10.848544  16.080643  22.437024  10.349085  14.826560 \n       598        599        600        601        602        603        604 \n 66.257393 123.940789   7.549049   9.072484  12.213593   8.593856   8.336900 \n       605        606        607        608        609        610        611 \n 10.023762   7.862503   7.979278   8.374183  12.105195  11.566102  24.507304 \n       612 \n124.684849 \n```\n:::\n\n```{.r .cell-code}\nranef(autism_lmer_quad_id_slope2)$childid %>% colMeans()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)           age      I(age^2) \n 3.224847e-13 -1.754406e-13  1.440551e-14 \n```\n:::\n:::\n\n\n\n# SAS Modeling\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-41_dae02b57dcc2d9f2c6561bceb09465ae'}\n\n```{.sas .cell-code}\ndata autism;\nINFILE \"~/cals/training/autism.csv\" DLM=\",\" FIRSTOBS=2;\nINPUT age vsae sicdegp childid;\nagef = age;\n\n/* Basic plotting */\nproc sgpanel data=autism;\npanelby sicdegp;\nSERIES x=age y=vsae / group=childid;\n\n/* R Side Modeling */\nproc mixed data=autism IC plots=all method=REML;\n\tclass sicdegp childid;\n\tmodel vsae=age|sicdegp / ddfm=kr2 solution OutPM=RanPAFit OutP=RanSSFit;\n\trepeated / type=un subject=childid rcorr R;\n\tods output covparms = cov;\n\t\n/* Visualize UN Covariance Matrix */\ndata times;\n\tdo time1=1 to 5;\n\t\tdo time2=1 to time1;\n\t\t\toutput;\n\t\tend;\n\tend;\n\ndata covplot;\n\tmerge times cov;\nrun;\n\nproc sgplot data=covplot;\n\ttitle \"Unstructured Covariance Estimates Line Plot\";\n\tseries x=time1 y=estimate / group=time2;\n\n\t\n/* helper for modeling many R matrices */\n%MACRO IC_cov (Rtype=UN);\n\t%let tablename=IC_&Rtype;\n\t%let modelname=Model_&Rtype;\n\t\n\t* convert types to usable string in repeated statement;\n\t%if &Rtype=ANTE %then %let Rtypename=%str(ANTE(1));\n\t%else %if &Rtype=AR %then %let Rtypename=%str(AR(1));\n\t%else %if &Rtype=ARH %then %let Rtypename=%str(ARH(1));\n\t%else %if &Rtype=SPPOW %then %let Rtypename=%str(SP(POW)(age));\n\t%else %let Rtypename=&Rtype;\t\n\t/* %put &tablename; */\n\t/* %put &modelname; */\n\t\n\tproc mixed data=autism IC plots=all method=REML;\n\tclass sicdegp childid;\n\tmodel vsae=age|sicdegp / ddfm=kr2;\n\trepeated / type=&Rtypename subject=childid rcorr R;\n\tods output InfoCrit=&tablename;\n\t\n\tdata &modelname;\n\t\tlength Name $ 10;\n\t\tset &tablename;\n\t\tName=\"&Rtype\";\n\t\tkeep Name Parms Neg2LogLike AIC AICC BIC CAIC HQIC;\n%MEND IC_cov;\n\n/* test different covariance structures */\n%IC_cov(Rtype=UN)\n%IC_cov(Rtype=CS)\n%IC_cov(Rtype=ANTE);\n%IC_cov(Rtype=AR);\n%IC_cov(Rtype=ARH);\n%IC_cov(Rtype=TOEP);\n%IC_cov(Rtype=TOEPH);\n%IC_cov(Rtype=SPPOW); /* not sure how to combine power + diagonal */\n\n/* combine IC results and show table */\ndata model_combine;\n\tset Model_UN Model_CS Model_ANTE Model_AR Model_ARH Model_TOEP Model_TOEPH Model_SPPOW;\n\tkeep Name Parms Neg2LogLike AIC AICC BIC CAIC HQIC;\n\t\nproc print data=model_combine;\n\nproc mixed data=autism;\nclass sicdegp childid;\nmodel vsae=age|sicdegp age*age|sicdegp / ddfm=kr2;\nrepeated / type=UN subject=childid R;\n\n/* G Side Modeling */\nproc mixed data=autism;\nclass sicdegp childid;\nmodel vsae=age|sicdegp / ddfm=kr2;\nrandom age age*age / type=UN subject=childid G;\nrepeated / subject=childid R;\n\n/* helper for modeling many G and R matrices */\n%MACRO IC_G_cov (Rtype=VC, Gtype=VC, id=VCVC);\n\t%let tablename=IC_&id;\n\t%let modelname=Model_&id;\n\t\n\t* convert types to usable string in repeated statement;\n\t%if &Rtype=ANTE %then %let Rtypename=%str(ANTE(1));\n\t%else %if &Rtype=AR %then %let Rtypename=%str(AR(1));\n\t%else %if &Rtype=ARH %then %let Rtypename=%str(ARH(1));\n\t%else %if &Rtype=SPPOW %then %let Rtypename=%str(SP(POW)(age));\n\t%else %let Rtypename=&Rtype;\n\n\tproc mixed data=autism IC plots=all method=REML;\n\tclass sicdegp childid;\n\tmodel vsae=age|sicdegp age*age|sicdegp / ddfm=kr2;\n\trandom int age age*age / type=&Gtype G;\n\trepeated / type=&Rtypename subject=childid rcorr R;\n\tods output InfoCrit=&tablename;\n\t\n\tdata &modelname;\n\t\tlength Name $ 10;\n\t\tset &tablename;\n\t\tName=\"&id\";\n\t\tkeep Name Parms Neg2LogLike AIC AICC BIC CAIC HQIC;\n%MEND IC_G_cov;\n\n%IC_G_cov(Rtype=VC, Gtype=UN, id=VC_UN);\n%IC_G_cov(Rtype=VC, Gtype=VC, id=VC_VC);\n/* %IC_G_cov(Rtype=CSH, Gtype=UN, id=CSH_UN); * optimizer issues; */\n\ndata model_G_combine;\n\tset Model_VC_UN Model_VC_VC;\n\tkeep Name Parms Neg2LogLike AIC AICC BIC CAIC HQIC;\n\t\n/* final model from R, no int, no correlation, SAS has trouble fitting */\n/* proc mixed data=autism IC plots=all method=REML; */\n/* \tclass sicdegp childid; */\n/* \tmodel vsae=age|sicdegp age*age|sicdegp / ddfm=kr2; */\n/* \trandom age age*age / type=UN(1) G; */\n/* \trepeated / type=UN(1) subject=childid R; */\n```\n:::\n\n\n\n[HTML SAS Output](sas_results.html)\n\n\n# Other thoughts: Mixed modeling is smart averaging to the pooled model.\n\nHere we'll compare the \n\n1. individual fixed model (lm fit for each child separately)\n2. pooled fixed model\n3. mixed model (with various assumptions on random effects)\n\nThis idea has come up a number of times, and I've mentioned this plot a few times so I thought I'd actually just show the effect that's happening with this dataset.\n\nThis will also offer some extra intuition as to how the structure of the G matrix is affecting the effects of the individuals\n\n$$\n\\begin{aligned}\nE[u | y] &= GZ'V^{-1}(y - X\\beta) \\\\\neBLUP(u) &= \\hat GZ'\\hat V^{-1}(y - X\\hat\\beta)\n\\end{aligned}\n$$\n\n\n\n## G effect on BLUPs\n\nLet's just look at panel 1 for simplicity of visualization.\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-42_ebda5011c20a1c43b0c5c79a48e45d63'}\n\n```{.r .cell-code}\n# the pooled model\nautism_lm_1 <- lm(vsae ~ age, data = autism_complete %>% filter(sicdegp == 1)) # the pooled model\n\npooled_data <- data.frame(as.list(coef(autism_lm_1)[c(1, 2 )])) %>% \n  rename(\"intercept\" = 1,\n         \"slope\" = \"age\") %>% \n  add_column(model = \"pooled\")\n```\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-43_61000cebd775aeabc59a100ef47bbf37'}\n\n```{.r .cell-code}\n# the individual model\nautism_lm_id_slope_1 <- lmList(vsae~age | childid, data = autism_complete %>% filter(sicdegp == 1)) # individual slopes model, just easier to extract coefs with this function\n\n# sig1_children <- autism_complete %>% filter(sicdegp == 1) %>% distinct(childid) %>% pull(childid) \nindiv_data <- coef(autism_lm_id_slope_1) %>% \n  rownames_to_column(var = \"child_id\") %>% \n  rename(\"intercept\" = 2,\n         \"slope\" = 3) %>% \n  add_column(model = \"indiv\", .after = \"child_id\")\n```\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-44_466b8d524265fd24b9341b137d147dd0'}\n\n```{.r .cell-code}\n# the mixed models\n# G matrix affect shape of pull for \"BLUPs\"\n# G symm\nautism_lme_id_slope_1_un <-  lme(vsae ~ age,\n                              random = list(childid = pdSymm(form = ~age)),\n                              data =  autism_complete %>% filter(sicdegp == 1),\n                              control = lmeControl(opt = \"optim\"))\n\n# G diag, het\nautism_lme_id_slope_1_diag <-  lme(vsae ~ age,\n                              random = list(childid = pdDiag(form = ~age)),\n                              data =  autism_complete %>% filter(sicdegp == 1),\n                              control = lmeControl(opt = \"optim\"))\n\n# G diag, homo\nautism_lme_id_slope_1_ident <- lme(vsae ~ age,\n                                   random = list(childid = pdIdent(form = ~age)),\n                                   data =  autism_complete %>% filter(sicdegp == 1),\n                                   control = lmeControl(opt = \"optim\"))\n\n# G CS, homo\nautism_lme_id_slope_1_cs <- lme(vsae ~ age,\n                                   random = list(childid = pdCompSymm(form = ~age)),\n                                   data =  autism_complete %>% filter(sicdegp == 1),\n                                   control = lmeControl(opt = \"optim\"))\n\n\n# list of lme models\nlme_models <-  list(mixed_un = autism_lme_id_slope_1_un, \n                   mixed_diag = autism_lme_id_slope_1_diag,\n                   mixed_ident = autism_lme_id_slope_1_ident,\n                   mixed_cs = autism_lme_id_slope_1_cs)\n\n# mixed, fixed effects\nmixed_data <- lme_models %>%  \n  map_dfr(~data.frame(as.list(fixed.effects(.x))), # extract fixed effects as data frame\n          .id = \"model\") %>% \n  rename(\"intercept\" = 2,\n         \"slope\" = 3)\n\n# helper function for extracting individual lines\nget_indiv_lines <- function(lme_object) {\n  coef(lme_object) %>% as.matrix() %>% \n    as.data.frame() %>% \n    rownames_to_column(\"child_id\") %>% \n    select(\"child_id\", `(Intercept)`, `age`) %>% \n    rename(\"intercept\" = 2,\n           \"slope\" = 3)\n}\n\n# mixed model individuals\nmixed_indiv_data <- lme_models %>% map_dfr(~get_indiv_lines(.x),\n                       .id = \"model\")\n```\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-45_5650b5c0d4f76bbef726b7aab14d5566'}\n\n```{.r .cell-code}\ncombined_indiv_data <- bind_rows(indiv_data, mixed_indiv_data)\ncombined_avg_data <- combined_indiv_data %>%\n  group_by(model) %>%\n  summarize(across(intercept:slope, mean, na.rm = T))\nindiv_avg_data <- indiv_data %>% group_by(model) %>%\n  summarize(across(intercept:slope, mean, na.rm = T))\n```\n:::\n\n\nWe've done all the fitting, and data extraction so now we're ready to plot and create dataframes for plotting\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-46_1493475cd9d08462734d8f4505cc96e8'}\n\n```{.r .cell-code}\n# hack for each specifying what data goes to which \"facet\" of ggplot\n# needs to duplicate data that will appear in multiple facets.\nmixed_data_facet <- mixed_data %>% \n  mutate(facet_id = c(\"mixed_un\" = 1, \"mixed_ident\" = 2, \"mixed_diag\" = 3, \"mixed_cs\" = 4)[model])\nmixed_indiv_data_facet <- mixed_indiv_data %>%\n  mutate(facet_id = c(\"mixed_un\" = 1, \"mixed_ident\" = 2, \"mixed_diag\" = 3, \"mixed_cs\" = 4)[model])\nindiv_data_facet <-  1:4 %>% map_dfr(~add_column(indiv_data, facet_id = .)) # 4 copies of individual data, one for each facet\ncombined_indiv_data_facet <- bind_rows(mixed_indiv_data_facet, indiv_data_facet) %>% arrange(model)\n\n# ellipse level curves of G\nmixed_ellipses <- lme_models %>% \n  map_dfr(\n    function(lme_obj) {\n      G_hat <- getVarCov(lme_obj)\n      mu <- fixed.effects(lme_obj) \n      ellipse(G_hat, centre = mu) %>% \n        as_tibble(.name_repair = ~c(\"intercept\", \"slope\"))\n    },\n    .id = \"model\") %>% \n  mutate(facet_id = c(\"mixed_un\" = 1, \"mixed_ident\" = 2, \"mixed_diag\" = 3, \"mixed_cs\" = 4)[model]) # specify which facet for each ellipse \n```\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-47_9e9903bbe53ed8a4676fcf4ce5e65887'}\n\n```{.r .fold-hide .cell-code}\nggplot(data = combined_indiv_data_facet) +\n  geom_path(mapping = aes(intercept, slope, group = child_id), # fixed -> mixed\n            arrow = arrow(ends = \"last\", length = unit(.1, \"cm\")),\n            alpha =.7) +\n  geom_point(mapping = aes(intercept, slope, color = model, alpha = model), # mixed estimates\n             pch = 16) + \n  geom_point(mapping = aes(intercept, slope, color = model, alpha = model), # mixed center\n             data = mixed_data_facet, size = 8, shape = 4) +\n  geom_path(mapping = aes(intercept, slope, color = model), # G level ellipses\n            data = mixed_ellipses,\n            linetype = 2,\n            alpha = .4) + \n  facet_wrap(~facet_id) +\n  scale_color_manual(values = c(\"#000000\", \"#E69F00\", \"#009E73\", \"#0072B2\", \"#CC79A7\")) +\n  scale_alpha_manual(values = c(.4, rep(.6, 4))) +\n  labs(title = \"G structure effect on mixed effect estimation\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 4 row(s) containing missing values (geom_path).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 4 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-47-1.png){width=768}\n:::\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-48_2a1ed76884c3ae0b1871e2e00d3fb73a'}\n\n```{.r .cell-code}\n# show G hats for each of the models\nlme_models %>% map(getVarCov)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$mixed_un\nRandom effects variance covariance matrix\n            (Intercept)     age\n(Intercept)      47.664 -24.470\nage             -24.470  12.565\n  Standard Deviations: 6.9039 3.5447 \n\n$mixed_diag\nRandom effects variance covariance matrix\n            (Intercept)    age\n(Intercept)    0.013694 0.0000\nage            0.000000 6.4369\n  Standard Deviations: 0.11702 2.5371 \n\n$mixed_ident\nRandom effects variance covariance matrix\n            (Intercept)    age\n(Intercept)      6.4617 0.0000\nage              0.0000 6.4617\n  Standard Deviations: 2.542 2.542 \n\n$mixed_cs\nRandom effects variance covariance matrix\n            (Intercept)     age\n(Intercept)      8.9804 -8.9780\nage             -8.9780  8.9804\n  Standard Deviations: 2.9967 2.9967 \n```\n:::\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-49_2cc465d23603eeb5c972eac6323db936'}\n\n```{.r .cell-code}\ng_mu <- ggplot(mapping = aes(intercept, slope, color = model, alpha = model)) +\n  geom_point(data = indiv_data, pch = 16) + \n  geom_point(data = mixed_data_facet, shape = 4, size = 4) +\n  geom_point(data = pooled_data, shape = 4, size = 4) + \n  scale_color_manual(values = c(\"#000000\", \"#E69F00\", \"#009E73\", \"#0072B2\", \"#CC79A7\", \"347827\")) +\n  scale_alpha_manual(values = c(.4, rep(.6, 5)))\ng_mu_zoom <- g_mu + coord_cartesian(xlim = c(1,3), ylim = c(2,3.5)) +\n  theme(legend.position = \"none\")\n\nvp_zoom <- viewport(width = .3, height = .3, x = .67, y = .85)\n\nprint(g_mu)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 rows containing missing values (geom_point).\n```\n:::\n\n```{.r .cell-code}\nprint(g_mu_zoom, vp = vp_zoom)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n:::\n\n\n\n## R effect on BLUPs\n\nThis section we'll play with the estimates through \"R\" a little, and see how that affects the BLUPs. This should affect the BLUPs by changing the estimate of \"V\" in the equation:\n\n$$\n\\begin{aligned}\nE[u | y] &= GZ'V^{-1}(y - X\\beta) \\\\\neBLUP(u) &= \\hat GZ'\\hat V^{-1}(y - X\\hat\\beta)\n\\end{aligned}\n$$\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-50_3c05ba8e41775bfef2c8fae48a42df18'}\n\n```{.r .cell-code}\n# heterogenous\nautism_lme_id_slope_1_ident_het <- lme(vsae ~ age,\n                                   random = list(childid = pdIdent(form = ~age)),\n                                   data =  autism_complete %>% filter(sicdegp == 1),\n                                   weights = varIdent(form = ~ age),\n                                   control = lmeControl(opt = \"optim\"))\n\nautism_lme_id_slope_1_ident_cs <- lme(vsae ~ age,\n                                   random = list(childid = pdIdent(form = ~age)),\n                                   data =  autism_complete %>% filter(sicdegp == 1),\n                                   correlation = corCompSymm(form = ~ 1 | childid),\n                                   control = lmeControl(opt = \"optim\"))\n\nautism_lme_id_slope_1_ident_car <- lme(vsae ~ age,\n                                   random = list(childid = pdIdent(form = ~age)),\n                                   data =  autism_complete %>% filter(sicdegp == 1),\n                                   correlation = corCAR1(form = ~ age | childid),\n                                   weights = varIdent(form = ~ age),\n                                   control = lmeControl(opt = \"optim\"))\n\nautism_lme_id_slope_1_ident_un <- lme(vsae ~ age,\n                                   random = list(childid = pdIdent(form = ~age)),\n                                   data =  autism_complete %>% filter(sicdegp == 1),\n                                   correlation = corSymm(form = ~ 1 | childid),\n                                   weights = varIdent(form = ~ age),\n                                   control = lmeControl(opt = \"optim\"))\n```\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-51_2a67c6c0b9411db54f6d81583c82ae61'}\n\n```{.r .cell-code}\nlme_r_models <- list(mixed_r_ident = autism_lme_id_slope_1_ident,\n                     mixed_r_un = autism_lme_id_slope_1_ident_un,\n                     mixed_r_cs = autism_lme_id_slope_1_ident_cs,\n                     mixed_r_car = autism_lme_id_slope_1_ident_car)\n\nmixed_r_indiv_data <- lme_r_models %>% map_dfr(~get_indiv_lines(.x),\n                       .id = \"model\")\n```\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-52_66b1f561bda4efe83634a6fbdee0ee90'}\n\n```{.r .cell-code}\n# plotting dfs\n# for faceting\n# can use same one\n# indiv_data_facet <-  1:4 %>% map_dfr(~add_column(indiv_data, facet_id = .)) # 4 copies of individual data, one for each facet\nmixed_r_indiv_data_facet <- mixed_r_indiv_data %>% mutate(facet_id = c(\"mixed_r_ident\" = 1, \"mixed_r_un\" = 2, \"mixed_r_cs\" = 3, \"mixed_r_car\" = 4)[model])\ncombined_r_indiv_data_facet <- bind_rows(mixed_r_indiv_data_facet, indiv_data_facet) %>% arrange(model)\n```\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-53_94a4353113be291e0ac2145775c04383'}\n\n```{.r .cell-code}\ncombined_r_indiv_data_facet %>% \n  ggplot(aes(intercept, slope, color = model, alpha = model)) +\n  geom_path(aes(intercept, slope, group = child_id),\n            arrow = arrow(ends = \"last\", length = unit(.1, \"cm\")),\n            color = \"black\",\n            alpha =.7) +\n  geom_point(pch = 16) +\n  scale_color_manual(values = c(\"#000000\", \"#E69F00\", \"#009E73\", \"#0072B2\", \"#CC79A7\")) +\n  scale_alpha_manual(values = c(.4, rep(.7, 4))) +\n  facet_wrap(~facet_id)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 4 row(s) containing missing values (geom_path).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 4 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-53-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-54_2016d929c5d7a2e23b0fdabed8a414ea'}\n\n```{.r .cell-code}\nlme_r_models %>% map(getVarCov, type = \"marginal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$mixed_r_ident\nchildid 2 \nMarginal variance covariance matrix\n        1       2       3      4       5\n1  85.996  45.232  71.079 122.77  174.47\n2  45.232 118.300 103.390 180.93  258.47\n3  71.079 103.390 221.690 297.24  426.47\n4 122.770 180.930 297.240 583.55  762.48\n5 174.470 258.470 426.470 762.48 1152.20\n  Standard Deviations: 9.2734 10.877 14.889 24.157 33.944 \n\n$mixed_r_un\nchildid 2 \nMarginal variance covariance matrix\n        1      2      3      4       5\n1  87.144  75.38 103.39 124.70  202.00\n2  75.380 124.58 116.16 202.06  296.72\n3 103.390 116.16 244.38 329.34  465.61\n4 124.700 202.06 329.34 663.67  856.77\n5 202.000 296.72 465.61 856.77 1322.50\n  Standard Deviations: 9.3351 11.162 15.633 25.762 36.367 \n\n$mixed_r_cs\nchildid 2 \nMarginal variance covariance matrix\n        1       2       3      4       5\n1  76.107  28.296  51.154  96.87  142.59\n2  28.296 104.680  79.726 148.30  216.88\n3  51.154  79.726 196.110 251.16  365.45\n4  96.870 148.300 251.160 516.13  662.61\n5 142.590 216.880 365.450 662.61 1019.00\n  Standard Deviations: 8.724 10.231 14.004 22.718 31.922 \n\n$mixed_r_car\nchildid 2 \nMarginal variance covariance matrix\n        1       2       3      4       5\n1  99.642  92.726  96.156 134.07  184.84\n2  92.726 133.680 139.880 197.49  273.81\n3  96.156 139.880 242.580 327.73  452.51\n4 134.070 197.490 327.730 623.76  817.82\n5 184.840 273.810 452.510 817.82 1222.80\n  Standard Deviations: 9.9821 11.562 15.575 24.975 34.968 \n```\n:::\n:::\n\n\n\n\n\n# Other thoughts: Fixed vs Random\n\nRandom effects represent this \"middle ground\" between fixed effects and error, so there are more decisions into which level effects fall under for modeling. Can you explain the differences between the following models? All of them loosely fall under the category of \"fitting line to each child\".\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-55_d359b9b0c417c8257954948bd174460e'}\n\n```{.r .cell-code}\nautism_complete_3 <- autism_complete %>% filter(sicdegp == 3) # just worry about one panel of sicdegp for now\n\nmod0 <- lm(vsae ~ age*childid, data = autism_complete_3)\nmod1 <- lmer(vsae ~ (age | childid), data = autism_complete_3)\n## boundary (singular) fit: see help('isSingular')\nmod2 <- lmer(vsae ~ age + age:childid + (1 | childid), data = autism_complete_3)\nmod3 <- lmer(vsae ~ childid + age + age:childid + (1 | childid), data = autism_complete_3)\n## fixed-effect model matrix is rank deficient so dropping 1 column / coefficient\nmod4 <- lmer(vsae ~ age + (age | childid), data = autism_complete_3) # the one I would choose\n## boundary (singular) fit: see help('isSingular')\nmod5 <- lmer(vsae ~ childid + age +  (age | childid), data = autism_complete_3)\n## Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n## unable to evaluate scaled gradient\n## Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n## Model failed to converge: degenerate Hessian with 1 negative eigenvalues\nmod6 <- lmer(vsae ~ age + age:childid + (age | childid), data = autism_complete_3)\n## Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n## unable to evaluate scaled gradient\n\n## Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\n## Model failed to converge: degenerate Hessian with 1 negative eigenvalues\nmod7 <- lmer(vsae ~ childid*age +  (age | childid), data = autism_complete_3)\n## fixed-effect model matrix is rank deficient so dropping 1 column / coefficient\n```\n:::\n\n\nAll give slightly different predictions, but similar enough in terms of functional form and information that is modeled.\n\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-56_c2918764670dd412df4168ffd915ff72'}\n\n```{.r .cell-code}\nmod_yhat <- autism_complete_3 %>% bind_cols(\n  yhat_mod0 = predict(mod0),\n  yhat_mod1 = predict(mod1),\n  yhat_mod2 = predict(mod2),\n  yhat_mod3 = predict(mod3),\n  yhat_mod4 = predict(mod4),\n  yhat_mod5 = predict(mod5),\n  yhat_mod6 = predict(mod6),\n  yhat_mod7 = predict(mod7), # mostly same as 3\n)\n\nmod_yhat %>% select(starts_with(\"yhat\")) %>% \n  head()\n##   yhat_mod0 yhat_mod1 yhat_mod2 yhat_mod3 yhat_mod4 yhat_mod5 yhat_mod6\n## 1  7.725962  15.58255  4.839585  7.725962  13.63232  6.220948  4.839452\n## 2  9.742788  16.67122  7.268751  9.742788  15.08613  8.579824  7.268638\n## 3 13.776442  18.84857 12.127084 13.776442  17.99374 13.297574 12.127008\n## 4 21.843750  23.20327 21.843750 21.843750  23.80898 22.733076 21.843750\n## 5 29.911058  27.55797 31.560416 29.911058  29.62421 32.168578 31.560492\n## 6 15.028846  15.79317  6.937472 15.028846  13.72317 13.134900  6.937088\n##   yhat_mod7\n## 1  7.725962\n## 2  9.742788\n## 3 13.776442\n## 4 21.843750\n## 5 29.911058\n## 6 15.028846\n```\n:::\n\n\nNote that the predictions are similar between some models, but enough differences that it's probably not \"rounding\" error. The closest ones are 2,6 and 3,7.\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-57_397ee0de08544831965051601ca9be6e'}\n\n```{.r .cell-code}\nmod_yhat %>% pivot_longer(cols = c(vsae, starts_with(\"yhat\"))) %>% \n  ggplot(aes(age, value, group_id = childid, color = name)) +\n  geom_line() +\n  facet_grid(name~.)\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-57-1.png){width=672}\n:::\n:::\n\n\nWhen looking at the predictions at a whole glance though, they are mostly the same, capturing all of the main trends.\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-58_47be63990486cef0259091a87cbd9a83'}\n\n```{.r .cell-code}\nanova(mod1,\n      mod2,\n      mod3,\n      mod4,\n      mod5,\n      mod6,\n      mod7) %>% as.data.frame() %>% \n  rownames_to_column() %>% \n  arrange(rowname) %>% \n  select(-last_col(), -Chisq) # get rid of test columns\n## refitting model(s) with ML (instead of REML)\n##   rowname npar      AIC      BIC    logLik deviance Df\n## 1    mod1    5 1399.561 1415.091 -694.7805 1389.561 NA\n## 2    mod2   44 1312.159 1448.821 -612.0796 1224.159 38\n## 3    mod3   83 1309.771 1567.564 -571.8854 1143.771 37\n## 4    mod4    6 1355.623 1374.258 -671.8114 1343.623  1\n## 5    mod5   46 1436.483 1579.357 -672.2417 1344.483  2\n## 6    mod6   46 1311.218 1454.092 -609.6090 1219.218  0\n## 7    mod7   85 1313.771 1577.776 -571.8854 1143.771  2\n```\n:::\n\n\nThe number of parameters for each of these models is wildly different, and thus the information criteria. What's going on?\n\nI think the differences in modeling are quite a little nuanced, but largely I think of this as a reframing of the fixed vs random debate. The random coefficients just make this even more confusing to me in terms of separation of the two effects... Definitionally, treating something as fixed implies that you're interested in making inferences on those topics, whereas treating something as random is accounting for population randomness _in the population you want to make inferences about_. In experimental designs, the split is somewhat easier, because you have \"experimental\" components of your design like treatments you wish to distinguish, which really should be fixed effects, and everything else about your experiment, like blocks and the units you randomize your treatments to. Walter Stroup advocates an ANOVA-esque approach to this, and apparently IMS bulletins discuss this approach as \"What would Fisher Do?\", Stroup calls the blocks and units the \"topographical features\" and the treatments \"treatment features\". I think Miliken and Johnson describe a similar split with \"experiemental\" and \"design\" components of your study. \n\nAt the end of the day, I consider these things: \n\n1. \"inference spaces\" - a matter of what population you are trying to make inferences about. What parts of the design do I truly wish to assign a probability distribution to?\n2. \"borrow strength\" - due to the shrinkage effect, if you have blocks with complete information, and blocks with very little information, you can recover a lot by making the assumption that blocks with little information are similar to blocks with complete information. I.e, missing data situations are something you'd naturally want to attach a probability distribution to in order to make better inferences.\n3. \"variance implications\" - modeling random effects carry forward implications about the covariance matrix of observations. Am I willing to accept that, do random effect assumptions capture population well? ^[https://stats.stackexchange.com/questions/366483/why-are-random-effects-assumed-to-follow-a-normal-distribution-in-glmms]\n4. computation...\n\nThe models in which `childid` appears as a fixed effect AND a random effect grouping is a little unreasonable, as those fits will be fighting for the same information. Using `age` as both a fixed and random covariate seems appropriate because there will be populational growth rate I'm interested in studying, as well as variation in that among the children I've measured. That leaves model 4.\n\nThe \"randomness I'm assuming in the population\" can be more clearly visualized by just showing random effect components of the estimates. We can also plot the fitted values alongside them to show the split of fixed and random among all these models!\n\n\n::: {.cell hash='randomcoef_cache/html/unnamed-chunk-59_610bca3f96974366a1024b1ab75e7a44'}\n\n```{.r .cell-code}\nmod_random <- autism_complete_3 %>% bind_cols(\n  # yhat_mod0 = predict(mod0), # not a random effect model\n  yhat_mod1 = predict(mod1, random.only = T),\n  yhat_mod2 = predict(mod2, random.only = T),\n  yhat_mod3 = predict(mod3, random.only = T),\n  yhat_mod4 = predict(mod4, random.only = T),\n  yhat_mod5 = predict(mod5, random.only = T),\n  yhat_mod6 = predict(mod6, random.only = T),\n  yhat_mod7 = predict(mod7, random.only = T), # mostly same as 3\n)\n\nmod_fixed <- autism_complete_3 %>% bind_cols(\n  # yhat_mod0 = predict(mod0), # not a random effect model\n  yhat_mod1 = predict(mod1, re.form = NA),\n  yhat_mod2 = predict(mod2, re.form = NA),\n  yhat_mod3 = predict(mod3, re.form = NA),\n  yhat_mod4 = predict(mod4, re.form = NA),\n  yhat_mod5 = predict(mod5, re.form = NA),\n  yhat_mod6 = predict(mod6, re.form = NA),\n  yhat_mod7 = predict(mod7, re.form = NA), # mostly same as 3\n)\n\n\nmod_fixed_plot <- mod_fixed %>% pivot_longer(cols = c(starts_with(\"yhat\"))) %>% \n  ggplot(aes(age, value, group_id = childid, color = name)) +\n  geom_line() +\n  facet_grid(name~.) + \n  labs(title = \"Fixed\")\n\nmod_random_plot <- mod_random %>% pivot_longer(cols = c(starts_with(\"yhat\"))) %>% \n  ggplot(aes(age, value, group_id = childid, color = name)) +\n  geom_line() +\n  facet_grid(name~.) +\n  labs(title = \"Random\")\n\nggarrange(mod_fixed_plot, mod_random_plot, ncol = 2, common.legend = TRUE, legend = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](randomcoef_files/figure-html/unnamed-chunk-59-1.png){width=960}\n:::\n:::\n\n\n\nModels in which we overparameterize the fixed effects, we see no random variability. Since the whole point of using a random model is to capture the variability of the population, 2, 3, 6, 7 simply don't reflect that distribution. The difference between 1 and 4 is simply whether or not the population randomness is centered around a population trend. In 4, they fan out around 0, because the fixed `age` accounts for the population level trend, whereas model 1 fans out above 0.\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}