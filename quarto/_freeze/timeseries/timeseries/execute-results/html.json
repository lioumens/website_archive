{
  "hash": "8c9e63a600fd62a255cc229efe9847e1",
  "result": {
    "markdown": "---\ntitle: \"Time Series\"\nauthor: \"Michael Liou\"\ndate: \"2023-02-04\"\nexecute:\n  cache: true\n---\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-1_9f9cd05643ffaefe4da690725ed23b05'}\n\n```{.r .cell-code  code-summary=\"Libraries and Setup\"}\nlibrary(tidyverse)  # general data science\nlibrary(glue)\nlibrary(reshape2)   # for melt function\nlibrary(astsa)\nlibrary(kableExtra) # kableExtra\nlibrary(tseries)\nlibrary(forecast)   # time series forecasting, common package\nlibrary(vars)       # vector autoregression\nlibrary(broom)      # tidy results\nlibrary(sparsevar)  # sparse vector autoregression\nlibrary(marima)     # multivariate arima\nlibrary(dlm)        # dynamic linear models\nlibrary(ggfortify)  # adds some autoplot models, overwrites many functions from forecast\nlibrary(KFAS)       # kalman filter, sequential univariate (fast), exponential family\nlibrary(fGarch)    # garch models\nlibrary(kableExtra) # Kable extra\nlibrary(TSrepr)   # lower dimensional representation of time series\nlibrary(here) # for relative file finding\n\n\n# detach(\"package:forecast\", unload = T)\n\nsource(here(\"scripts/knit_latex.R\"))\n```\n:::\n\n\n## Introduction\n\n### Types of Models\n\n- Autoregression (AR)\n- Moving Average (AM)\n- Vector Autoregression (VAR)\n\n- HMM\n  - related to bayesian inference, special\n- State Space Models\n  - very broad space of models that includes arima models, and dynamic linear models. Can also account for linear/nonlinear gaussian/non-gaussian errors\n- Multivariate autoregression (MAR)\n\n### Software\n\n- `stats`\n  - `KalmanLike`, `KalmanRun`, `KalmanSmooth`, `KalmanForecast`\n- `dlm`\n- `KFAS`\n\nSee also [State Space Models in R](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjng_vXx4P6AhWdk4kEHZ4KBUAQFnoECBoQAQ&url=https%3A%2F%2Fwww.jstatsoft.org%2Farticle%2Fdownload%2Fv041i04%2F488&usg=AOvVaw0bbJoxE4fWu74zhZs0qggz).\n\n### Theory\n\nCourses \n\n- Kevin Kotzé's [Time Series Analysis Course](https://www.economodel.com/time-series-analysis)\n  - has a great overview and description, and packaged R commands for univariate and multivariate from an economics perspective.\n\n## ARIMAX\n\nThe general pattern for univariate time series analysis is as follows\n\n1. figure out how to make stationary\n  - by trend modeling (exogenous), or differencing\n  - [\"Breusch-Godfrey\"](https://stats.stackexchange.com/questions/518737/durbin-vs-breusch-godfrey-test-for-autocorrelation-which-is-better) : up to p order ar\n  - \"Durbin-Watson\" : first order ar testing\n2. \n\n### base tools\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-2_afbf375d0dfdb3a05859f0917563e291'}\n\n```{.r .cell-code}\n# acf, ccf, pacf, arima, arima.sim\n```\n:::\n\n\n### AR(1) and MA(1)\n\nAutoregressive of order 1 is the simplest time series you can have.\n\n$$\n\\begin{aligned}\nY_t = \\alpha Y_{t-1} + \\varepsilon_t\n\\end{aligned}\n$$\n\n\n### Simulated Examples\n\nthis section we'll simulate a number of AR and MA models so that you get a better sense of what they look like.\n\n\n\n\n::: {.cell .caption-margin hash='timeseries_cache/html/unnamed-chunk-3_16049c08522e4ea7482dbfa065412672'}\n\n```{.r .cell-code}\n# AR models\nsim_ar <- tibble(p = 1:3,\n                 ar = list(.7,\n                           c(.7,-.3),\n                           c(.7, -.3, .5)),\n                 n = 200) %>% \n  rowwise() %>% \n  mutate(y = list(arima.sim(list(ar = ar), n = 200)),\n         x = list(time(y)))\n\nsim_ar %>% unnest(c(x, y)) %>% \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  facet_grid(p~.)\n```\n\n::: {.cell-output-display}\n![AR Model Simulations](timeseries_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-4_43e4e76543ad21b46f62dc5639d4aae8'}\n\n```{.r .cell-code}\n# MA processes\nsim_ma <- tibble(q = 1:3,\n                 ma = list(.7,\n                           c(.7,-.3),\n                           c(.7, -.3, .5)),\n                 n = 200) %>% \n  rowwise() %>% \n  mutate(y = list(arima.sim(list(ma = ma), n = 200)),\n         x = list(time(y))) %>% \n  ungroup()\n\nsim_ma %>% unnest(c(x, y)) %>% \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  facet_grid(q~.)\n```\n\n::: {.cell-output-display}\n![MA Model Simulations](timeseries_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### Analyzing simulations\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-5_327d5ed1feedbbed1b36f904ed2549cf'}\n\n```{.r .cell-code}\n# ar1\nset.seed(1)\nar1 <- arima.sim(list(order = c(1, 0, 0), ar = .2), 100)\n\ninvisible(capture.output(\n  mod_sar1 <- sarima(ar1, p = 1, d = 0, q = 0) # asta\n  ))\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmod_Ar1 <- Arima(ar1, order = c(1, 0, 0)) # forecast\nmod_ar1 <- arima(ar1, order = c(1, 0, 0)) # base\n\nbind_rows(\n  tidy(mod_Ar1) |> add_column(model = \"forecast::Arima\", .before = 1),\n  tidy(mod_ar1) |> add_column(model = \"stats::arima\", .before = 1),\n  mod_sar1$ttable[, 1:2] |> \n  as_tibble(rownames = \"term\", .name_repair = ~c(\"estimate\", \"std.error\")) |>\n  add_column(model = \"astsa::sarima\", .before = 1)\n) |> arrange(desc(term), model) |> \n  kable() |> kable_minimal()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-minimal\" style='font-family: \"Trebuchet MS\", verdana, sans-serif; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> model </th>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> astsa::sarima </td>\n   <td style=\"text-align:left;\"> xmean </td>\n   <td style=\"text-align:right;\"> 0.1021000 </td>\n   <td style=\"text-align:right;\"> 0.1125000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> forecast::Arima </td>\n   <td style=\"text-align:left;\"> intercept </td>\n   <td style=\"text-align:right;\"> 0.1020681 </td>\n   <td style=\"text-align:right;\"> 0.1125445 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> stats::arima </td>\n   <td style=\"text-align:left;\"> intercept </td>\n   <td style=\"text-align:right;\"> 0.1020681 </td>\n   <td style=\"text-align:right;\"> 0.1125445 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> astsa::sarima </td>\n   <td style=\"text-align:left;\"> ar1 </td>\n   <td style=\"text-align:right;\"> 0.2171000 </td>\n   <td style=\"text-align:right;\"> 0.0978000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> forecast::Arima </td>\n   <td style=\"text-align:left;\"> ar1 </td>\n   <td style=\"text-align:right;\"> 0.2170632 </td>\n   <td style=\"text-align:right;\"> 0.0977980 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> stats::arima </td>\n   <td style=\"text-align:left;\"> ar1 </td>\n   <td style=\"text-align:right;\"> 0.2170632 </td>\n   <td style=\"text-align:right;\"> 0.0977980 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-6_739096b7ac30f6d84d251efaab3bc6b2'}\n\n```{.r .cell-code}\n# arx1\nset.seed(1)\nx <- sort(runif(400))\nbeta <- 2\narx1 <- arima.sim(list(ar=.3), 400) + x * beta\n\nmod_Arx1 <- Arima(arx1, order = c(1, 0, 0), xreg = x)\nmod_arx1 <- arima(arx1, order = c(1, 0, 0), xreg = x)\ninvisible(capture.output(\n  mod_sarx1 <- sarima(arx1, p = 1, d = 0, q = 0, xreg = x)\n))\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\nbind_rows(\n  mod_Arx1 |> tidy()|> add_column(model = \"forecast::Arima\", .before = 1),\n  mod_arx1 |> tidy() |> add_column(model = \"stats::arima\", .before = 1),\n  mod_sarx1$ttable[,1:2] |>\n    as_tibble(rownames = \"term\", .name_repair = ~c(\"estimate\", \"std.error\")) |>\n    add_column(model = \"astsa::sarima\", .before = 1)\n) |> arrange(desc(term)) |> \n  kbl() |> kable_minimal()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-minimal\" style='font-family: \"Trebuchet MS\", verdana, sans-serif; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> model </th>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> forecast::Arima </td>\n   <td style=\"text-align:left;\"> xreg </td>\n   <td style=\"text-align:right;\"> 1.9302542 </td>\n   <td style=\"text-align:right;\"> 0.2520409 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> astsa::sarima </td>\n   <td style=\"text-align:left;\"> xreg </td>\n   <td style=\"text-align:right;\"> 1.9303000 </td>\n   <td style=\"text-align:right;\"> 0.2520000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> stats::arima </td>\n   <td style=\"text-align:left;\"> x </td>\n   <td style=\"text-align:right;\"> 1.9302542 </td>\n   <td style=\"text-align:right;\"> 0.2520409 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> forecast::Arima </td>\n   <td style=\"text-align:left;\"> intercept </td>\n   <td style=\"text-align:right;\"> 0.0275105 </td>\n   <td style=\"text-align:right;\"> 0.1427769 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> stats::arima </td>\n   <td style=\"text-align:left;\"> intercept </td>\n   <td style=\"text-align:right;\"> 0.0275105 </td>\n   <td style=\"text-align:right;\"> 0.1427769 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> astsa::sarima </td>\n   <td style=\"text-align:left;\"> intercept </td>\n   <td style=\"text-align:right;\"> 0.0275000 </td>\n   <td style=\"text-align:right;\"> 0.1428000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> forecast::Arima </td>\n   <td style=\"text-align:left;\"> ar1 </td>\n   <td style=\"text-align:right;\"> 0.2709738 </td>\n   <td style=\"text-align:right;\"> 0.0481873 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> stats::arima </td>\n   <td style=\"text-align:left;\"> ar1 </td>\n   <td style=\"text-align:right;\"> 0.2709738 </td>\n   <td style=\"text-align:right;\"> 0.0481873 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> astsa::sarima </td>\n   <td style=\"text-align:left;\"> ar1 </td>\n   <td style=\"text-align:right;\"> 0.2710000 </td>\n   <td style=\"text-align:right;\"> 0.0482000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nIt's pretty clear from both of these tests, of the AR1 model and the AR1X model that they estimate very similarly,\n\n### Diagnostics\n\nThere are hypothesis tests, and graphical checks for stationarity\n\nunit root tests are for stability/stationarity:\n\n- Augmented Dickey Fulley Test\n  - want low p value\n- KPSS Test\n  - want high p value for stationarity\n\n\n### Examples: LA County\n\nWe just examine the cardiovascular mortality from the LA Pollution study. These are average weekly cv mortality rates from `astsa` package.\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-7_c1facb0a089cdd5d4f298eb344bb1482'}\n\n```{.r .cell-code}\n# ?cmort\nx <- cmort %>% as.numeric()\nx1 <- x %>% lag()\n# yt = u + beta * yt-1\ncmort_lm <- lm(x~x1)\n\ncbind(coef(cmort_lm)[1] + coef(cmort_lm)[2] * x1,\n      c(NA, fitted(cmort_lm)),\n      c(NA, predict(cmort_lm))) %>% \n  head() # fitted/predicted values match up\n```\n\n::: {.cell-output-display}\n$$\\begin{bmatrix} NA &NA &NA \\\\95.7381108162687 &95.7381108162684 &95.7381108162687 \\\\100.978136273105 &100.978136273105 &100.978136273105 \\\\93.0447840350672 &93.0447840350672 &93.0447840350672 \\\\95.8924561905496 &95.8924561905496 &95.8924561905496 \\\\94.1946570734599 &94.1946570734598 &94.1946570734599 \\\\ \\end{bmatrix}$$\n:::\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-8_937d0f3534fb3a6d0314b48aa1a16c38'}\n\n```{.r .cell-code}\nqplot(time(cmort), x, geom = \"line\", color = \"black\") +\n  geom_line(aes(y =  c(NA, fitted(cmort_lm)), color = \"red\")) +\n  scale_color_manual(values = c(\"black\", \"red\"),\n                     label = c(\"raw\", \"fitted\"))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nDon't know how to automatically pick scale for object of type ts. Defaulting to continuous.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 1 row(s) containing missing values (geom_path).\n```\n:::\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n### Example: AirPassengers\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-9_d3d12122cd402d72302bfe7015a0fa81'}\n\n```{.r .cell-code}\nAirPassengers %>% plot()\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-10_f51c48c1f8ad224a81091d436042b3c9'}\n\n```{.r .cell-code}\nsarima(AirPassengers, d = 1, p = 2, q = 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninitial  value 3.522169 \niter   2 value 3.456359\niter   3 value 3.447683\niter   4 value 3.447001\niter   5 value 3.447000\niter   6 value 3.447000\niter   6 value 3.447000\niter   6 value 3.447000\nfinal  value 3.447000 \nconverged\ninitial  value 3.441124 \niter   2 value 3.441115\niter   3 value 3.441115\niter   4 value 3.441114\niter   4 value 3.441114\niter   4 value 3.441114\nfinal  value 3.441114 \nconverged\n```\n:::\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2  constant\n      0.3792  -0.2314    2.4075\ns.e.  0.0823   0.0834    3.0636\n\nsigma^2 estimated as 973.4:  log likelihood = -694.99,  aic = 1397.98\n\n$degrees_of_freedom\n[1] 140\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.3792 0.0823  4.6065  0.0000\nar2       -0.2314 0.0834 -2.7767  0.0062\nconstant   2.4075 3.0636  0.7858  0.4333\n\n$AIC\n[1] 9.77605\n\n$AICc\n[1] 9.777257\n\n$BIC\n[1] 9.858927\n```\n:::\n:::\n\n\n## Exponential smoothing\n\n- `ets()` is probably the function you want\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-11_62a04487333068198d1767abedeca62e'}\n\n```{.r .cell-code}\ndata(oil)\nfc <- ses(oil, h = 5, alpha = .1) # can leave out the alpha to estimate alpha instead\n\n# oil |> autoplot() +\n#   geom_line(data = data.frame(time = index(fitted(fc)),\n#                        fitted = fitted(fc)),\n#             mapping = aes(time, fitted),\n#             color = \"red\")\n\nautoplot(fc) + \n  autolayer(fitted(fc)) +\n  theme(legend.position =\"none\")\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n## TSrepr\n\nThis R package is designed to give smaller dimension representations of time series. If there are $n$ observations of the time series, this package would like to represent that as $p$. parameters. This roughly equates to nonparametric regression of the signals. We'll investigate how these work by following along in the [tutorial given](https://petolau.github.io/TSrepr-time-series-representations/):\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-12_8a08f48fd5c5610d3f7340818e545655'}\n\n```{.r .cell-code}\ndata(\"elec_load\")\n\nelec <- as.numeric(elec_load[1,])\n \nggplot(data.frame(Time = 1:length(elec), Value = elec), aes(Time, Value)) +\n  geom_line() +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-13_6e9a5a21e565be2e45173c15de1d56ee'}\n\n```{.r .cell-code}\n# DWT with level of 2^3\ndata_dwt <- repr_dwt(elec, level = 3) # discrete wavelet transform\n# first 84 DFT coefficients are extracted and then inverted\ndata_dft <- repr_dft(elec, coef = 84)\n# first 84 DCT coefficients are extracted and then inverted\ndata_dct <- repr_dct(elec, coef = 84)\n# Classical PAA\ndata_paa <- repr_paa(elec, q = 8, func = mean)\n```\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-14_17c1c6100a4c35c789deae467e7e9e45'}\n\n```{.r .cell-code}\ndata_plot <- data.frame(Value = c(data_dwt, data_dft, data_dct, data_paa),\n                        Time = rep(1:length(data_dwt), 4),\n                        Method = factor(rep(c(\"DWT\", \"DFT\", \"DCT\", \"PAA\"),\n                                            each = length(data_dwt))))\n \nggplot(data_plot, aes(Time, Value)) +\n  geom_line(aes(color = Method), alpha = 0.80, size = 0.8) +\n  geom_line(mapping = aes(Time, Value), data = data.frame(Time = 1:length(elec), Value = elec)) + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n## ARCH\n\nArch models have variance that depend on the previous timepoint as well. That is, an ARCH(1) model has dependence on the previous error term.\n\n$$\n\\begin{aligned}\ny_t &= b'x_t + \\varepsilon_t \\\\\n\\varepsilon_t &\\overset{i.i.d.}{\\sim} N(0, \\sigma^2_t) \\\\\n\\text{Var}(\\varepsilon_t) = \\sigma^2_t &= \\omega + \\alpha_1\\varepsilon_{t-1}^2\n\\end{aligned}\n$$\n\nWe can write the zero centered process a little easier, \n\n$$\n\\begin{aligned}\ny_t &= \\sigma_t\\epsilon_t \\\\\n\\sigma_t^2 &= \\omega + \\alpha_1y^2_{t-1} \\\\\n\\varepsilon_t &\\overset{i.i.d}{\\sim} N(0, 1)\n\\end{aligned}\n$$\n\nLet's simulate the ARCH model and see the basic characteristics, we're simulating a zero mean \n\n$$\n\\begin{aligned}\n\\sigma^2_t = 5 + .5 y^2_{t-1}\n\\end{aligned}\n$$\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-15_82457f973b177a65d2fff6e96f5c3336'}\n\n```{.r .cell-code}\n# helper functions to simulate arch/garch models\nsim_arch <- function(y0 = 2, omega = 5, alpha1 = .5, n = 100) {\n  y <- vector(\"numeric\", n)\n  y[1] <- y0\n  inn <- rnorm(n)\n  for (i in 2:n) {\n    y[i] <- inn[i] * sqrt(omega + alpha1 * y[i-1]^2) # arch\n  }\n  y\n}\n```\n:::\n\n::: {.cell layout=\"[[100],[50,50],[50,50]]\" hash='timeseries_cache/html/unnamed-chunk-16_f5d49385becdc32ee3dbb0cea7203b35'}\n\n```{.r .cell-code}\nset.seed(1)\ny <- sim_arch(n=1000)\nplot(y, type = \"l\", main = \"Time Series Plot\")\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\nAcf(y, main = \"ACF y\")\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-16-2.png){width=672}\n:::\n\n```{.r .cell-code}\nPacf(y, main = \"PACF y\")\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-16-3.png){width=672}\n:::\n\n```{.r .cell-code}\nAcf(y^2, main = \"ACF y^2\")\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-16-4.png){width=672}\n:::\n\n```{.r .cell-code}\nPacf(y^2, main = \"PACF y^2\")\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-16-5.png){width=672}\n:::\n:::\n\n\nIn the time series plot, we can see why this is stochastically volatile plot, since there are regions of the time series that go way more wild when the values start to get high. The characteristics of an ARCH(1) model, are that the series $y$ look like white noise, and $y^2$ look like an AR(1) process. There's a spike in the PACF model for lag 1, and a geometric decay in the acf plot of $y^2$ indicating AR(1).\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-17_fba7015e87048c53b3f1e0290811d98d'}\n\n```{.r .cell-code}\ninvisible(capture.output(\n  mod_arch <- garch(y, order = c(0, 1))\n))\nsummary(mod_arch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\ngarch(x = y, order = c(0, 1))\n\nModel:\nGARCH(0,1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.88500 -0.66301 -0.03251  0.65554  3.55454 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(>|t|)    \na0   5.77142     0.38941   14.821  < 2e-16 ***\na1   0.44892     0.05722    7.846 4.22e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n\tJarque Bera Test\n\ndata:  Residuals\nX-squared = 0.0090504, df = 2, p-value = 0.9955\n\n\n\tBox-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.076017, df = 1, p-value = 0.7828\n```\n:::\n:::\n\n\nThe above is the output for fitting an ARCH(1) model, and we can see that the estimates are quite close to one another for the parameters that are controlling the variance of the function. Also notice that we have almost 1000 values that we had to feed in too though, it seems that estimating the variance coefficients is quite inefficient.\n\n## GARCH\n\nA garch model will use the last error value, as well as the last variance for the error model.\n\n$$\n\\begin{aligned}\ny_t &= b'x_t + \\varepsilon_t \\\\\n\\varepsilon_t &\\overset{i.i.d.}{\\sim} N(0, \\sigma^2_t) \\\\\n\\text{Var}(\\varepsilon_t) = \\sigma^2_t &= \\omega + \\alpha_1\\varepsilon_{t-1}^2 + \\beta_1\\sigma_{t-1}^2\n\\end{aligned}\n$$\n\nThe following we are simulating\n\n$$\n\\begin{aligned}\nVar(\\varepsilon_t) = \\sigma^2 = .1 + .1\\varepsilon_{t-1}^2 + .8 \\sigma^2_{t-1}\n\\end{aligned}\n$$\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-18_4d3b31ad6c5a578fceeb30994b7a6608'}\n\n```{.r .cell-code}\n# see also fGarch::garchSim\nsim_garch <- function(y0 = 2, omega = .1, alpha1 = .1, beta1 = .8, n = 100, xt = 0, b = .4) {\n  if (xt != 0 & length(xt) != n) rlang::abort(\"length of xt should be length of simulation\")\n  y <- vector(\"numeric\", n)\n  err <- vector(\"numeric\", n)\n  sds <- vector(\"numeric\", n)\n  y[1] <- y0\n  inn <- rnorm(n)\n  for (i in 2:n) {\n    sds[i] <- sqrt(omega + alpha1 * err[i-1]^2 + beta1 * sds[i-1]^2)\n    err[i] <- rnorm(1, mean = 0, sd = sds[i])\n    y[i] <- xt * b + err[i]\n    # y[i] <- inn[i] * (sqrt(a + b * y[i-1]^2 + c * inn[i-1]^2)) # garch\n  }\n  y\n}\n# spec <- garchSpec(model = list())\n# y <- garchSim(spec, n = 100)\ninvisible(capture.output(\n  mod_garch <- fGarch::garchFit(formula = ~garch(1, 1), y, include.mean = FALSE)\n))\n```\n:::\n\n::: {.cell layout=\"[[100],[50,50],[50,50]]\" hash='timeseries_cache/html/unnamed-chunk-19_fdbe1d31d4de21ff9973985e7832e691'}\n\n```{.r .cell-code}\n# garch\nset.seed(1)\ny <- sim_garch(n=500)\nplot(y, type = \"l\")\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\nAcf(y) # supposed to be white noise\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-19-2.png){width=672}\n:::\n\n```{.r .cell-code}\nPacf(y)\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-19-3.png){width=672}\n:::\n\n```{.r .cell-code}\nAcf(y^2) # supposed to be arma pattern\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-19-4.png){width=672}\n:::\n\n```{.r .cell-code}\nPacf(y^2) # MA(2) maybe\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-19-5.png){width=672}\n:::\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-20_b5a7073535ecacb0bdba0ae4a3bee2b9'}\n\n```{.r .cell-code}\n# fitting the garch model\ninvisible(capture.output(\n  mod_garch <- fGarch::garchFit(~garch(1, 1), y,  include.mean = FALSE)\n))\nsummary(mod_garch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTitle:\n GARCH Modelling \n\nCall:\n fGarch::garchFit(formula = ~garch(1, 1), data = y, include.mean = FALSE) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x7ff80af103d0>\n [data = y]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n  omega   alpha1    beta1  \n0.14254  0.11326  0.77157  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nomega    0.14254     0.07242    1.968  0.04904 *  \nalpha1   0.11326     0.04074    2.780  0.00544 ** \nbeta1    0.77157     0.08115    9.508  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -751.9354    normalized:  -1.503871 \n\nDescription:\n Tue Jan 31 12:37:01 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  0.9215242 0.6308027 \n Shapiro-Wilk Test  R    W      0.9976149 0.7017248 \n Ljung-Box Test     R    Q(10)  16.02394  0.09894901\n Ljung-Box Test     R    Q(15)  27.95652  0.0218428 \n Ljung-Box Test     R    Q(20)  38.01825  0.00881023\n Ljung-Box Test     R^2  Q(10)  8.714543  0.5593918 \n Ljung-Box Test     R^2  Q(15)  22.62514  0.09242972\n Ljung-Box Test     R^2  Q(20)  29.21913  0.08353343\n LM Arch Test       R    TR^2   13.13471  0.3593258 \n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n3.019742 3.045029 3.019670 3.029665 \n```\n:::\n:::\n\n\n+ Jarque-Bera Test - null is normally distributed residuals\n+ Ljung-Box - is portmanteau test for autocorrelation, testing for a set of lags.\n+ LM arch test - fit the model, and examine the square of the residuals for length of ARCH lags. Null is that there is no AR trend in the squared residuals. See [ARCH wiki](https://en.wikipedia.org/wiki/Autoregressive_conditional_heteroskedasticity)\n\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-21_fb355797f690b3c9f857119c5386505b'}\n\n```{.r .cell-code}\ncoef(mod_garch) |> kbl() |> kable_minimal(full_width = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-minimal\" style='font-family: \"Trebuchet MS\", verdana, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> x </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> omega </td>\n   <td style=\"text-align:right;\"> 0.1425396 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> alpha1 </td>\n   <td style=\"text-align:right;\"> 0.1132581 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> beta1 </td>\n   <td style=\"text-align:right;\"> 0.7715676 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n\n## Vector Autoregression\n\nThe simplest vector autoregressive model we can make is VAR(1), which takes the form \n\nWe assume that the process is stable, which implies that the function is stationary.\n\n\n### Simulation\n\nhttps://www.r-econometrics.com/timeseries/svarintro/\n\nThis is an example of a \"B\"-model structural, VAR(1) model in which the coefficients are given. Let's see what we can do with given coefficients.\n\n$$\ny_t = A_1y_{t-1} + B\\epsilon_t\n$$\n$$\nA_1 = \\begin{bmatrix} 0.3 & 0.12 & 0.69 \\\\ 0 & 0.3 & 0.48 \\\\ 0.24 & 0.24 & 0.3 \\end{bmatrix} \\text{, }\nB = \\begin{bmatrix} 1 & 0 & 0 \\\\ -0.14 & 1 & 0 \\\\ -0.06 & 0.39 & 1 \\end{bmatrix} \\text{ and } \\epsilon_t \\sim N(0, I_3)\n$$\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-22_ae99508433b625f46250b973099b3f20'}\n\n```{.r .cell-code}\n# Reset random number generator for reproducibility\nset.seed(24579)\n\ntt <- 500 # Number of time series observations\n\n# Coefficient matrix\nA_1 <- matrix(c(0.3, 0, 0.24,\n                0.12, 0.3, 0.24,\n                0.69, 0.48, 0.3), 3)\n\n# Structural coefficients\nB <- diag(1, 3)\nB[lower.tri(B)] <- c(-0.14, -0.06, 0.39)\n\n# Generate series\nseries <- matrix(rnorm(3, 0, 1), 3, tt + 1) # Raw series with zeros\nfor (i in 2:(tt + 1)){\n  series[, i] <- A_1 %*% series[, i - 1] +  B %*% rnorm(3, 0, 1)\n}\n\nseries <- ts(t(series)) # Convert to time series object\ndimnames(series)[[2]] <- c(\"S1\", \"S2\", \"S3\") # Rename variables\n\n# Plot the series\nplot.ts(series, main = \"Artificial time series\")\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nI this case, we've been handed the correct model, and we can quickly note some properties of the model. We can check for stability (which implies stationarity) by looking at the eigenvalues and checking that they're magnitude is less than 1. It's also clear that the innovations are correlated with one another,\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-23_d6ee8037177e0701625e45c755cc3980'}\n\n```{.r .cell-code}\neigen(A_1)$values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  0.8529906  0.2503329 -0.2033235\n```\n:::\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-24_70d39cfec959e5c3c5b7599cbdca2a0a'}\n\n```{.r .cell-code}\nvar_est <- VAR(series, p = 1, type = \"none\")\n\n# estimated A_1 matrix\nas_latex(\"\\\\hat A_1 = \") + as_latex(round(Bcoef(var_est), digits = 3))\n\n# estimate structural equation for A Model\na <- diag(1, 3)\na[lower.tri(a)] <- NA\n\nsvar_est_a <- SVAR(var_est, Amat = a, max.iter = 1000)\nas_latex(\"\\\\hat A^{-1} = \") + as_latex(round(solve(svar_est_a$A), digits = 33)) # pretty close to B\n\n# estimate structural equation for B Model\nb <- diag(1, 3)\nb[lower.tri(b)] <- NA # specify restriction with NA\n\nsvar_est_b <- SVAR(var_est, Bmat = b)\nas_latex(\"\\\\hat B = \") + as_latex(round(svar_est_b$B, digits = 3))\n```\n\n::: {.cell-output-display}\n$$\\hat A_1 = \\begin{bmatrix} 0.329 &0.112 &0.681 \\\\-0.002 &0.29 &0.513 \\\\0.24 &0.252 &0.319 \\\\ \\end{bmatrix}$$$$\\hat A^{-1} = \\begin{bmatrix} 1 &0 &0 \\\\-0.181774697458286 &1 &0 \\\\-0.107710324655258 &0.313198839004628 &1 \\\\ \\end{bmatrix}$$$$\\hat B = \\begin{bmatrix} 1 &0 &0 \\\\-0.182 &1 &0 \\\\-0.108 &0.313 &1 \\\\ \\end{bmatrix}$$\n:::\n:::\n\n\nThe estimated $A_1$ is given directly by the function `Bcoef` on the fitted VAR model.\nWe invert the matrix A because we need to translate this into a \"B\" model from the estimated \"A\" model, we estimated and find that it's quite correct.\n\n### Moving Average Representations\n\nWe can get the basic moving average representation\n\n$$\n\\begin{aligned}\ny_t = \\Phi_0 u_t + \\Phi_1 u_{t-1} + ...\n\\end{aligned}\n$$\n\nPhi is normally calculated recursively from the last observations, starting with \n\n1. $\\Phi_0 = I_K$\n2. $\\Phi_i = \\sum_{j=1}^{i}\\Phi_{i-j}A_j$\n  \n  + for the VAR(1) case, the sequence is simply $\\Phi = I, A_1, A_1^2, A_1^3, \\dots$.\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-25_b39aa679a6bde9cc49a9146cba543d73'}\n\n```{.r .cell-code}\n# Bcoef(var_est) # A_1\n# Bcoef(var_est) %^% 2 #A_1^2\nPhi(var_est, nstep = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n, , 2\n\n             [,1]      [,2]      [,3]\n[1,]  0.328807868 0.1124078 0.6809865\n[2,] -0.001530688 0.2895784 0.5127013\n[3,]  0.240186093 0.2519319 0.3186145\n\n, , 3\n\n          [,1]      [,2]      [,3]\n[1,] 0.2715060 0.2410736 0.4985175\n[2,] 0.1221972 0.2128494 0.3107789\n[3,] 0.1551162 0.1802220 0.3942445\n```\n:::\n:::\n\n\nFor the other MA representation with orthogonal coefficients,\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-26_b582aec5aabad35bceeae74487b3fea0'}\n\n```{.r .cell-code}\n# Sigmahat <- crossprod(resid(var_est))/497 # estimated covariance, 500 total obs\n# summary(var_est)$covres # i don't know how to get them to match exactly\n# P <- t(chol(Sigmahat))\n# apply(Phi(var_est, nstep = 2), 3, {\\(x) x %*% P}, simplify = FALSE) # multiply all Phi by P\nPsi(var_est, nstep = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n, , 1\n\n           [,1]     [,2]      [,3]\n[1,]  0.9972189 0.000000 0.0000000\n[2,] -0.1812692 1.021112 0.0000000\n[3,] -0.1074108 0.319811 0.9953141\n\n, , 2\n\n           [,1]      [,2]      [,3]\n[1,]  0.2343721 0.3325679 0.6777954\n[2,] -0.1090877 0.4596596 0.5102988\n[3,]  0.1596280 0.3591471 0.3171215\n\n, , 3\n\n           [,1]      [,2]      [,3]\n[1,] 0.17350558 0.4055946 0.4961815\n[2,] 0.04989328 0.3167336 0.3093226\n[3,] 0.07967004 0.3101106 0.3923971\n```\n:::\n:::\n\n\n### Impulse Response Analysis\n\nThere are two types of basic impulse responses:\n\n1. (forecast error impulse responses) These are on the scale of the original error, `Sigma.u` and let `c(0, 0, 1)` reverberate in the system \n\n  + these have the disadvantage that\n2. (orthogonal error impulse responses) These are on the scale of `Sigma.eps` and let `P^{-1}c(0, 0, 1)` reverberate in the system where $Sigma.u = PP'$ by cholesky decomposition\n\n$$\n\\begin{aligned}\ny_t &= \\Phi_0 u_t + \\Phi_1 u_{t-1} + \\dots \\\\\n&= \\underbrace{\\Phi_0 P}_{\\Psi_0}\\underbrace{P^{-1}u_t}_{\\epsilon_{t}} +  \\dots\n\\end{aligned}\n$$\n\n3. (accumulated error impulse response)\n\n\n\n::: {.cell layout=\"[[33,33,33],[33,33,33],[33,33,33]]\" hash='timeseries_cache/html/unnamed-chunk-27_a3a78cb245cbdc91a7731b843d712888'}\n\n```{.r .cell-code}\n# First three rows of irf1\n# Phi(var_est, nstep = 2) # First columns of this\n## even more manually, with A matrix and unit impulse\n# Ahat <- Bcoef(var_est)\n# Ahat %*% cbind(c(1, 0, 0)) # A e_1\n# (Ahat %^% 2) %*% cbind(c(1, 0, 0)) #A_1^2 e_1\nirf1 <- irf(var_est, orth = FALSE, boot = FALSE)\n\n# First three rows of irf2\n# Psi(var_est, nstep= 2 )\n## more manually for VAR(1)\n# Sigmahat <- crossprod(resid(var_est))/497 # estimated covariance, 500 total obs\n# P <- t(chol(Sigmahat))\n# P  %*% cbind(c(1, 0, 0)) # P e_1\n# Ahat %*% P %*%cbind(c(1, 0, 0)) # A_1 P e_1\n# (Ahat %^% 2) %*% P %*% cbind(c(1, 0, 0)) # A_1 P e_1\nirf2 <- irf(var_est, orth = TRUE, boot = FALSE)\n# row 2 \n# P  %*% cbind(c(1, 0, 0)) + Ahat %*% P %*%cbind(c(1, 0, 0)) \nirf3 <- irf(var_est, cumulative = TRUE, boot = FALSE) # accumulates rowwise of regular irf\nplot(irf1)\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-27-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-27-3.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(irf2)\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-27-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-27-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-27-6.png){width=672}\n:::\n\n```{.r .cell-code}\nplot(irf3) \n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-27-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-27-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-27-9.png){width=672}\n:::\n:::\n\n\n\nThe impulse response functions are simply ways of answering the question, \"what would happen if I send a shock through the system\". For non orthogonal \n\nWe assume the process is mean 0, and $irf_1(0) = c(1, 0, 0)$\n$irf_1(1) = A \\begin{bmatrix}1, 0, 0\\end{bmatrix}$\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-28_81b1d76d1935075f3944b9c596957d7e'}\n\n```{.r .cell-code}\n# varsim_orthirf <- irf(var_est, orth = TRUE, boot = FALSE)\n# varsim_orthirf$irf$S1\n# Bcoef(var_est) # gives as large matrix\n```\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-29_f547aad452bca937f33cc66dd69ded34'}\n\n```{.r .cell-code}\n# \n# # ?Psi # MA orthogonal representation\n# Psi(var_est) # W = PD^{-1} =\n# \n# D <- diag(P)\n# W <- P %*% diag((1 / D)) # matches with Psi(1)\n# Psi(var_est)[,,1] # I don't know why these are slightly off... but using d\n# \n# Psi(var_est)\n# \n# P%*% t(P) # Sigmahat\n```\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-30_39a0928a26525a27fd57a8a29cabe4f9'}\n\n```{.r .cell-code}\n# P %*% diag(1/D)\n# svar_est_b$B\n# \n# solve(svar_est_a$A) # This is estimate of B matrix\n```\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-31_597a448cc09ebbbadf59500f5e1f3b3d'}\n\n```{.r .cell-code}\n# svar_est_a$Sigma.U / 100\n# tcrossprod(svar_est_b$B) # is Sigma.U\n# tcrossprod(W) # Sigma U = PD^{-1}\n# Sigmahat\n# solve(W) %*% (svar_est_b$Sigma.U / 100) %*% t(solve(W)) # Identity, Sigma_epsilon\n# Psi(var_est)[,,2]\n# Acoef(var_est) # gives as list\n```\n:::\n\n\n\n### Example: LA County (VAR)\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-32_cb4b01202c440152d5f4a62da44417b7'}\n\n```{.r .cell-code}\n# cardiovascular mortality, temperature and particulates in LA county, weekly.\nla <- cbind(cmort, tempr, part)\n\n# visualization\n# ts.plot(cmort, tempr, part, col = 1:3) # base r of autoplot \nautoplot(la, color = \"black\") + facet_grid(series~., scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-33_3b75c14613d031ed9ae13a0137fb59aa'}\n\n```{.r .cell-code}\nla_var1 <- VAR(la, p = 1, type = \"both\") # w/ trend\nla_var2 <- VAR(la, p = 2, type = \"both\") # w/ trend\n\n# coefficient matrix\nsapply(coef(la_var1),rlang::as_function(~.x[,\"Estimate\"])) |> as.data.frame()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               cmort        tempr         part\ncmort.l1  0.46482370 -0.244046444 -0.124774858\ntempr.l1 -0.36088790  0.486595619 -0.476526201\npart.l1   0.09941503 -0.127660994  0.581308364\nconst    73.22729188 67.585597743 67.463501275\ntrend    -0.01445884 -0.006912455 -0.004650001\n```\n:::\n\n```{.r .cell-code}\n# vcov of coef estimates\nsapply(coef(la_var1), rlang::as_function(~round(.x[,\"Std. Error\"], 3))) |> as.data.frame()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         cmort tempr   part\ncmort.l1 0.037 0.042  0.079\ntempr.l1 0.032 0.037  0.069\npart.l1  0.019 0.022  0.041\nconst    4.834 5.542 10.399\ntrend    0.002 0.002  0.004\n```\n:::\n\n```{.r .cell-code}\n# estimated covariance of errors\nsummary(la_var1)$covres |> as.data.frame()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          cmort     tempr      part\ncmort 31.171944  5.974621  16.65448\ntempr  5.974621 40.964945  42.32335\npart  16.654476 42.323345 144.26025\n```\n:::\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-34_35812632f08463e5e39c86f07eb66aa4'}\n\n```{.r .cell-code}\nla_var1_const <- VAR(la, p = 1, type = \"const\")\n# matching manaully with ar.ols\n# la_ols <- ar.ols(la, order.max = 1, demean = FALSE, intercept = TRUE)\n# \n# # matches constant VAR coefficients\n# rbind(t(la_ols$ar[1,,]),\n#       const = la_ols$x.intercept)\n\nsapply(coef(la_var1_const),rlang::as_function(~.x[,\"Estimate\"])) |> as.data.frame()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               cmort      tempr        part\ncmort.l1  0.60149346 -0.1787076 -0.08082151\ntempr.l1 -0.30946101  0.5111817 -0.45998718\npart.l1   0.07096225 -0.1412636  0.57215788\nconst    54.94579126 58.8456142 61.58412402\n```\n:::\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-35_6b45448ffd231090a4be940cb42a89aa'}\n\n```{.r .cell-code}\nla %>% melt(c(\"time\", \"series\")) %>% \n  ggplot(aes(time, value)) + \n  geom_line(color = \"black\") + \n  geom_line(data = fitted(la_var1) %>% melt(c(\"time\", \"series\")),\n            mapping = aes(time, value, color = \"red\"), alpha= .6) +  # var1\n  geom_line(data = fitted(la_var1_const) %>% melt(c(\"time\", \"series\")),\n            mapping = aes(time, value, color = \"blue\"), alpha = .6) + # var1 const\n  geom_line(data = fitted(la_var2) %>% melt(c(\"time\", \"series\")),\n            mapping = aes(time, value, color = \"green\"), alpha = .6) + # var2\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\"),\n                     labels = c(\"var1\", \"var1_const\", \"var2\")) + \n  facet_grid(series~.)\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\nFrom looking at the models, they're quite difficult to tell which model is fitting better than the others. \n\n\n\n\n\n\n\nWe can try to do some automatic VAR order selection with the function `VARselect()`, which uses a number of other criteria.\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-38_be836240fbb1cb9c170e27ad98274b38'}\n\n```{.r .cell-code}\n# order selection\nVARselect(la) # selects 2 by BIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     9      5      2      9 \n\n$criteria\n                  1           2           3           4           5           6\nAIC(n)     11.84541    11.35895    11.32363    11.28376    11.23095    11.20946\nHQ(n)      11.88523    11.42864    11.42318    11.41318    11.39023    11.39861\nSC(n)      11.94687    11.53651    11.57728    11.61351    11.63679    11.69140\nFPE(n) 139442.85952 85730.12941 82755.88760 79522.85813 75434.43083 73833.93231\n                 7           8           9          10\nAIC(n)    11.21508    11.19717    11.17261    11.17843\nHQ(n)     11.43409    11.44604    11.45134    11.48703\nSC(n)     11.77311    11.83130    11.88283    11.96475\nFPE(n) 74254.62493 72942.17171 71179.59211 71604.50745\n```\n:::\n:::\n\n\nThe CCF plots should all be non significant. The second part of \"x & y\" are the ones that lead.\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-39_dadaca500ffcd81a9e1dab4634a7d57b'}\n\n```{.r .cell-code}\n# serial test, BG test for \nserial.test(la_var2, lags.pt = 12, type = \"PT.adjusted\")\n```\n\n```{.r .cell-code}\nserial.test(la_var2, lags.pt = 12, type = \"BG\")\n```\n\n```{.r .cell-code}\nserial.test(la_var2, lags.pt = 12, type = \"ES\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPortmanteau Test (adjusted)\n\ndata:  Residuals of VAR object la_var2\nChi-squared = 162.35, df = 90, p-value = 4.602e-06\n\n\n\tBreusch-Godfrey LM test\n\ndata:  Residuals of VAR object la_var2\nChi-squared = 151.85, df = 45, p-value = 1.656e-13\n\n\n\tEdgerton-Shukur F test\n\ndata:  Residuals of VAR object la_var2\nF statistic = 3.6803, df1 = 45, df2 = 1429, p-value = 1.221e-14\n```\n:::\n:::\n\n\n`serial.test` will check for portmanteau, and BG\n\n### sparsevar\n\nSince VAR models can grow $O(np^2)$, this is a high dimensional problem when we're tracking many states. We can use sparsity constraints during fitting.\n\nThe theoretical properties of this estimator are studied by VAR\n\nThe [SCAD penalty](https://statisticaloddsandends.wordpress.com/2018/07/31/the-scad-penalty/), and [MCP penalty](https://statisticaloddsandends.wordpress.com/2019/12/09/the-minimax-concave-penalty-mcp/) are used in this package\n\n$$\n\\begin{aligned}\n\\min_\\beta \\frac{1}{2} \\|y - X\\beta\\|^2_2 + p(\\beta)\n\\end{aligned}\n$$\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-40_40d06433ccc25b7b12cd96baebdcbe52'}\n\n```{.r .cell-code}\nset.seed(1)\n\n# 5% of non-zero entries and a Toeplitz variance-covariance matrix with rho = 0.5.\nsim <- simulateVAR(N = 20, p = 2)\n\nfit <- fitVAR(sim$series, p = 2, threshold = TRUE)\nplotVAR(sim, fit) # recovered quite well\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\nThe top image is the true generating values of $A$ in the vector autoregression model.\n\n\n\n\n\n## State Space Model\n\nThese models have a hierarchical form, in which there is some underlying time series process, but then we also observe data on top of that. Thus, the general form of the equations look like this:\n\n$$\n\\begin{aligned}\n\\textbf{State Equation:}& \\\\\nx_t &= \\Phi x_{t-1} + \\Upsilon u_t +   w_t \\\\\n\\textbf{Observation Equation:}& \\\\\ny_t &= A_t x_t + \\Gamma u_t  + v_t\n\\end{aligned}\n$$\nwhere:\n\n- $\\Upsilon u_t$ - is time varying exogenous variables\n- $\\Upsilon u_t$ - is time varying exogenous variables\n- $x_t$ is state at time $t$\n- $\\Phi x_t$ is state at time $t$ describes how $x$ evolves\n- $w_t$ is state noise with $w_t \\sim N(0, Q)$\n- $v_t$ is measurement noise with $v_t \\sim N(0, R)$\n\nThere are more general forms of the state space model: with correlated errors,\nsee Durbin Koopman for more state space methods.\n\nIt seems now that state space models are now also being superseded by recurrent\nneural networks, which can model dynamical properties.\n\n### Example: AR(1) with observational noise\n\nThe state equation:\n\n$$\n\\begin{aligned}\n\\textbf{State Equation:}& \\\\\nx_t &= \\phi x_{t-1} + w_t \\\\\n\\textbf{Observation Equation:}& \\\\\ny_t &= x_t + v_t\n\\end{aligned}\n$$\n\nWhat is interesting about this case with a hierarchical data structure, is that we can show it has the same error structure as an ARMA model. Shumway Stoffer notes that even though it has the same parameterization as an ARMA model, it is often easier to think about the state model form. See example 6.3 for more details.\n\n## Kalman Filter\n\nKalman filter is a recursive, markovian updating algorithm for estimating a hidden state variable given noisy and partial observations. The common example is that we are tracking a truck by gps observations. Since the gps observations are imprecise, they will jump back and forth.\n\nAn excellent resource explanation with pictures is found [from bzarg](https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/)\n\n### Example: Local level Model\n\nConsider the equations:\n\n$$\n\\begin{aligned}\n\\textbf{State Equation:}& \\\\\nx_t &= x_{t-1} + w_t \\\\\n\\textbf{Observation Equation:}& \\\\\ny_t &= x_t + v_t\n\\end{aligned}\n$$\nwhere both $w_t, v_t \\sim N(0, 1)$. We can make the Kalman filter here based on our observation equation. We assume that $A = 1$ (in this case) and $Phi = 1$ are known here. In reality, we can use maximum likelihood of the _innnovations_ (prediction errors) in order to estimate the \n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-41_481cf80002d3fed9cc2b3327231fa1b5'}\n\n```{.r .cell-code}\n# generate the data\nset.seed(1)\nn <-  50\nx0 <- rnorm(1) # random initial state\nw <- rnorm(n, 0, 1) # state level noise\nv = rnorm(n, 0, 1) # observation level noise\nx <- cumsum(c(x0, w)) # true state variables\ny <- x[-1] + v # remove initial state\nks <- Ksmooth0(num = 50, \n               y = y, \n               A = 1, \n               mu0 = 10, # set initial values\n               Sigma0 = 20, # set initial values\n               Phi = 1,\n               cQ = 1,\n               cR = 1)\n```\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-42_198cba5fd25d6423983d221c48f3b137'}\n\n```{.r .cell-code}\n# plot all the predictions\npar(mfrow = c(3, 1),\n    mar = c(2, 4, 2, 4))\n# predictions\nplot(x[-1], main = \"Prediction\", ylim = c(-5, 10))\nlines(ks$xp[1,,])\nlines(ks$xp + 2*sqrt(ks$Pp[1,,]), lty = 2, col = 4)\nlines(ks$xp - 2*sqrt(ks$Pp[1,,]), lty = 2, col = 4) # variance matrices\n\n# filters\nplot(x[-1], main = \"Filters\", ylim = c(-5, 10), xlab = \"\")\nlines(ks$xf[1,,])\nlines(ks$xf + 2*sqrt(ks$Pf[1,,]), lty = 2, col = 4)\nlines(ks$xf - 2*sqrt(ks$Pf[1,,]), lty = 2, col = 4) # variance matrices\n\n\n# ks$xs[1,,] # smooth values\nplot(x[-1], main = \"Smooth\", ylim = c(-5, 10), xlab = \"\")\nlines(ks$xs[1,,])\nlines(ks$xs + 2*sqrt(ks$Ps[1,,]), lty = 2, col = 4)\nlines(ks$xs - 2*sqrt(ks$Ps[1,,]), lty = 2, col = 4) # variance matrices\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-43_8bca2f3ec5c6237afde8848ba658532e'}\n\n```{.r .cell-code}\n# creating table for comparison\ntibble(predict = ks$xp[1,,],\n       filter = ks$xf[1,,],\n       smooth = ks$xs[1,,],\n       predict_sd = sqrt(ks$Pp[1,,]),\n       filter_sd = sqrt(ks$Pf[1,,]),\n       smooth_sd = sqrt(ks$Ps[1,,])) %>%\n  mutate(across(predict:smooth, ~scales::number(.x, accuracy = .001)),\n         across(predict_sd:smooth_sd, ~scales::number(.x,accuracy = .01))) %>% \n  transmute(predict = glue(\"{predict} ({predict_sd})\"),\n         filter = glue(\"{filter} ({filter_sd})\"),\n         smooth = glue(\"{smooth} ({smooth_sd})\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 50 × 3\n   predict       filter        smooth       \n   <glue>        <glue>        <glue>       \n 1 10.000 (4.58) -0.552 (0.98) -0.538 (0.77)\n 2 -0.552 (1.40) -0.807 (0.81) -0.524 (0.69)\n 3 -0.807 (1.29) -0.810 (0.79) -0.096 (0.67)\n 4 -0.810 (1.27) 0.978 (0.79)  1.048 (0.67) \n 5 0.978 (1.27)  1.490 (0.79)  1.161 (0.67) \n 6 1.490 (1.27)  0.536 (0.79)  0.628 (0.67) \n 7 0.536 (1.27)  0.209 (0.79)  0.778 (0.67) \n 8 0.209 (1.27)  1.438 (0.79)  1.699 (0.67) \n 9 1.438 (1.27)  1.283 (0.79)  2.123 (0.67) \n10 1.283 (1.27)  3.726 (0.79)  3.481 (0.67) \n# … with 40 more rows\n# ℹ Use `print(n = ...)` to see more rows\n```\n:::\n:::\n\n\n\n### Estimation of State Parameters\n\nThere are bayesian methods for estimating the state parameters but also maximum likelihood w/ Newton Raphson or EM algorithmss\n\n### Example: Nile {.tabset}\n\nAnnual flow of the river Nile from 1871 - 1970. There's an apparent changepoint near 1898.\n\nI found this example going through all the state space model libraries. The orginal paper is called \"JSS Journal of Statistical Software, State Space Models in R\".Specifically, we walk through the Nile example with 3 functions: \n\n1. `stats::StructTS`\n2. `dlm:dlmMLE`\n3. `KFAS:kf`\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-44_a5654c38820660a60193b9c44df970ea'}\n\n```{.r .cell-code}\n# change point analyses\nts.plot(Nile)\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n:::\n\n\n#### stats::StructTS\n\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-45_70e41dc30534ff3647ce4a5af6a1ef80'}\n\n```{.r .cell-code}\nnile_sts <- StructTS(Nile, \"level\")\n# nile_sts %>% tsdiag() # diagnostics of structural model\n\n# values from model.\ntibble(\n  times = time(Nile),\n  filtered = fitted(nile_sts)[,\"level\"], # filtered values\n  smoothed = tsSmooth(nile_sts)[,\"level\"]) # smoothed valuees\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 100 × 3\n   times filtered smoothed\n   <dbl>    <dbl>    <dbl>\n 1  1871    1120     1112.\n 2  1872    1141.    1111.\n 3  1873    1073.    1105.\n 4  1874    1117.    1114.\n 5  1875    1130.    1112.\n 6  1876    1138.    1107.\n 7  1877    1049.    1096.\n 8  1878    1098.    1112.\n 9  1879    1171.    1117.\n10  1880    1163.    1098.\n# … with 90 more rows\n# ℹ Use `print(n = ...)` to see more rows\n```\n:::\n\n```{.r .cell-code}\n# forecast values\npredict(nile_sts,n.ahead = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$pred\nTime Series:\nStart = 1971 \nEnd = 1975 \nFrequency = 1 \n[1] 798.3682 798.3682 798.3682 798.3682 798.3682\n\n$se\nTime Series:\nStart = 1971 \nEnd = 1975 \nFrequency = 1 \n[1] 143.5266 148.5564 153.4215 158.1370 162.7159\n```\n:::\n\n```{.r .cell-code}\n# plotting forecast values, with package forecast\nplot(forecast::forecast(nile_sts, # StructST object\n                   level = c(50, 90), # CI levels\n                   h = 10), # periods of forecasting\n     xlim = c(1950, 1980))\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n\n#### dlm::dlm\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-46_29987bcfcf009f9295b57608262bdd3c'}\n\n```{.r .cell-code}\n# set up the dlm object\nnile_dlm_ll <- function(theta){\n  dlmModPoly(order = 1, dV = theta[1], dW = theta[2]) # fits local level model\n}\n\n# calls optim internally to optimize model (default BFGS)\n# could use numDeriv::hessian for numerically accurate evaluation of Hessians\nnile_dlm_mle <- dlmMLE(Nile, # data\n                   parm = c(100, 2), # initial parameters\n                   nile_dlm_ll, # model\n                   lower = rep(1e-4, 2)) \n\nnile_dlm_mle$par # similar variance parameters of local linear model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15099.787  1468.438\n```\n:::\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-47_b93411e88613f5d4947dce54f20797d2'}\n\n```{.r .cell-code}\nnile_dlm_ll_best <- nile_dlm_ll(nile_dlm_mle$par) # build model with best fit by optim\nW(nile_dlm_ll_best) # state randomnesss\n```\n\n::: {.cell-output-display}\n$$\\begin{bmatrix} 1468.43775833941 \\\\ \\end{bmatrix}$$\n:::\n\n```{.r .cell-code}\nV(nile_dlm_ll_best) # observation error variance\n```\n\n::: {.cell-output-display}\n$$\\begin{bmatrix} 15099.7867214723 \\\\ \\end{bmatrix}$$\n:::\n\n```{.r .cell-code}\n# can use fitted model to create smooth estimates now\n# $s has time series of smooth estimates\n# $U.S and $D.S have SVD of smoothing varriances (for std err)\nnile_dlm_ll_smooth <- dlmSmooth(Nile , nile_dlm_ll_best)\n# conf ints can be calculated by\n```\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-48_979c6af85dfcfe99e440a9241d30ce8c'}\n\n```{.r .cell-code}\n# calculate standard errors\nhwidth <- sqrt(unlist(dlmSvd2var(nile_dlm_ll_smooth$U.S, nile_dlm_ll_smooth$D.S))) * qnorm(0.025, lower = FALSE)\nnile_dlm_ll_smooth_ci <- cbind(nile_dlm_ll_smooth$s, as.vector(nile_dlm_ll_smooth$s) + hwidth %o% c(-1, 1))\nautoplot(nile_dlm_ll_smooth_ci) + theme(legend.position = \"\") +\n  labs(title = \"smoothed kalman with CI\") +\n  geom_point(data = tibble(time = time(Nile),\n                           flow = Nile,\n                           series = \"real\"),\n             mapping = aes(time, flow))\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-48-1.png){width=672}\n:::\n:::\n\n\n#### KFAS::KFS\n\nKalman filtering/smoothing/simulation for linear state space models in the exponential family. KFAS uses the sequential processing method. This package uses a slightly different parameterization of their state space models.\n\n$$\n\\begin{aligned}\ny_t &= Z_t\\alpha_t + \\varepsilon_t &\\text{observation}\\\\\n\\alpha_{t+1} &= T_t \\alpha_t + R_t\\eta_t &\\text{transition}\n\\end{aligned}\n$$\n\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-49_900232b2f571dd12fed14a95c45f2123'}\n\n```{.r .cell-code}\n# build the local linear model\n# can initialize a specific model with values\n# nile_kfas_ll_model <- SSModel(Nile ~ SSMtrend(1, Q = 15000), # Q = Transition error\n#                               H = 30) # Observation error\n\n# add NA for values you want to optimize\nnile_kfas_ll_model <- SSModel(Nile ~ SSMtrend(1, Q = list(matrix(NA))), # Q = Transition error\n                              H = matrix(NA)) # Observation error\n\n\n# fit the model with wrapper to `optim`\n# -logLik(nile_kfas_ll_model) # is the objective that is optimized\nnile_kfas_ll_fit <- fitSSM(nile_kfas_ll_model,\n       c(log(var(Nile)), log(var(Nile))), # initial parameters for optim\n       method = \"BFGS\")\n\n# extract just the optimal model\nnile_kfas_ll <- nile_kfas_ll_fit$model\n\n# Filter/smooth\nnile_kfas_ll_smooth <- KFS(nile_kfas_ll,\n    filtering = \"state\",\n    smoothing = \"state\")\n\n# autoplot calls fortify, grabbing some components of the model\n# ggfortify:::autoplot.KFS\n# fortify(nile_kfas_ll_smooth)  # create df with: time, raw y, alphahat (smoothed values), raw residual = raw y - alphahat\n# nile_kfas_ll_smooth$alphahat # contains smoothed state variable estimates. for some reason \"fitted\" doesn't return these.\nautoplot(nile_kfas_ll_smooth)\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-50_ddaf5bdc5746c8fa8966d1139d85b7b4'}\n\n```{.r .cell-code}\n# these give same values\n# cbind(predict(nile_kfas_ll),\n      # nile_kfas_ll_smooth$alphahat)\n\nnile_kfas_ll_ci <- predict(nile_kfas_ll, interval = \"confidence\", level = .9)\nnile_kfas_ll_pi <- predict(nile_kfas_ll, interval = \"prediction\", level = .9)\n\n                                                                              \nlegend_labels <- c(\"lower_ci\", \"upper_ci\", \"fit\", \"lower_pi\", \"upper_pi\")\n# forecast:::autoplot.mts\nautoplot(cbind(nile_kfas_ll_ci[,-1], nile_kfas_ll_pi),\n         mapping = aes(x, y, group = series, linetype = series)) +\n  scale_linetype_manual(values = c(2, 2, 1, 3, 3), labels = legend_labels) +\n  scale_color_manual(values = c(2, 2, 1, 3, 3), labels = legend_labels) +\n  labs(y = \"Predicted Annual flow\", main = \"River Nile\")\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-50-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='timeseries_cache/html/unnamed-chunk-51_a49870058075f5b74579ad451492eed9'}\n\n```{.r .cell-code}\n# forecasting\nnile_kfs_mean <- KFS(nile_kfas_ll,\n    filtering = c('state')) # specifying \"mean\" only estimates the smooths (alphahat)\n\ncbind(predict = nile_kfs_mean$a, # one step ahead prediction\n      filter = nile_kfs_mean$att, # filter estimates\n      smooth = nile_kfs_mean$alphahat) %>%  # smoothed\n  autoplot() + \n  coord_cartesian(ylim = c(700, 1250)) +\n  labs(title = \"predict/filter/smooth estiamtes of Nile with KFS\")\n```\n\n::: {.cell-output-display}\n![](timeseries_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}