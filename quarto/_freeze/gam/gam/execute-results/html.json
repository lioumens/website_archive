{
  "hash": "2eaa9d0ee0d42de7a56dbcd199d86308",
  "result": {
    "markdown": "---\ntitle: \"Generalized Additive Models\"\nauthor: \"Michael Liou\"\ndate: \"2022-05-08\"\nexecute:\n  cache: true\n---\n\n\n\n\n## R Packages\n\n- \"mgcv\" - Mixed GAM Computational Vehicle\n  - `gamm` - for mixed models, uses `nlme` to fit\n  - `gam` - can be used more general than exponential\n  - `jagam` - interface to JAGS for bayesian estiamtion\n- \"gamm4\" - like gamm, but with `lme4` in backend instead\n\nSmooth functions in \"mgcv\":\n\n- `s()`- for univariate smooths, isotropic smooths, and random effects\n- `te()` - tensor product smooths constructed from singly pernalized marginal smooths\n- `ti()` - tensor product interactions with marginal smooth and lower order... for smooth anova models\n- `t2()` alternative tensor product smooth construction,useful with gamm4 package.    `\n\nSmooth terms in GAMS, `bs = \"\"`\n- `tp` - low rank, thin plate splines, isotropic smoothers. Isotropic means that rotation of the covariate will not change the result of smoothing. thin plates don't have \"knots\", a truncated eigen-decomposition is used for rank reduction. low rank means fewer coefficients than data to smooth.\n- `ds` - duchon splines\n- `cr` - cubic regression spline\n- `cs` - shrinkage version of cubic splines\n- `cc` - cyclic cubic regression splines\n- `sos` - splines on the sphere\n- `ps` - psplines, b-spline basis and perform well generally\n- `cp` - cyclic version of p-spline\n\n## Univariate Smoothing\n\n4.2 of Wood.\n\nExample on Engine data. Hypothesis is that larger the engine copacity, the faster the engine will wear out.\n\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-1_b7972736b131dd6f936dbbd3e96757ec'}\n\n```{.r .cell-code}\n# read engine data. Hypothesis is that\ndata(engine)\nwith(engine, plot(size, wear, xlab = \"Engine capacity\", ylab=\"Wear Index\"))\n```\n\n::: {.cell-output-display}\n![](gam_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n- `xj` are the knots\n- `x` is the data.\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-2_591e7f5b7b32b3dcf7e38e7620fcd454'}\n\n```{.r .cell-code}\n# function for generating tent functions, the basis function\ntf <- function(x, xj, j) {\n  dj <- xj*0\n  dj[j] <- 1\n  approx(xj, dj, x)$y\n}\ntf.X <- function(x,xj) {\n  # tent function basis matrix given data x, and knot sequence xj\n  nk <- length(xj); n <- length(x)\n  X <- matrix(NA, n, nk)\n  for (j in 1:nk) {\n    X[,j] <- tf(x,xj,j)\n  }\n  X\n}\n```\n:::\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-3_36d5526f388d98a198d00a86fc1ba70d'}\n\n```{.r .cell-code}\nsj <- seq(min(engine$size), max(engine$size), length=6)\nX <- tf.X(engine$size,sj)\nb <- lm(wear~X-1, data=engine)\ns <- seq(min(engine$size), max(engine$size), length = 200) # prediction data\nXp <- tf.X(s,sj) # prediction matrix\nplot(engine$size, engine$wear, main = \"piecewise linear estimate without smoothness\")\nlines(s, Xp %*% coef(b))\n```\n\n::: {.cell-output-display}\n![](gam_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\ncool, looks reasonable, but should introduce a penalty for the smoothness of the function\n\nWe wish to penalize the parameters with some function \n\n$$\n\\begin{aligned}\n|| y-X\\beta ||^2 + \\lambda \\beta'D'D\\beta  \\\\\n= \\bigg\\| \\begin{bmatrix} y \\\\ 0 \\end{bmatrix} - \\begin{bmatrix}X \\\\ \\sqrt{\\lambda}D\\end{bmatrix}\\bigg\\|^2\n\\end{aligned}\n$$\nFor example, the penalty can be expressed in the matrix form\n\n$$\n\\begin{aligned}\n\\lambda \\sum_{j=2}^{k-1}f(x^*_{j-1} - 2f(x^*_j) + f(x^*_{j+1}))^2\n\\end{aligned}\n$$\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-4_443a5bf0df876c56e6ce2fe671a17dcf'}\n\n```{.r .cell-code}\ndiff(diag(5), differences=2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1   -2    1    0    0\n[2,]    0    1   -2    1    0\n[3,]    0    0    1   -2    1\n```\n:::\n\n```{.r .cell-code}\n?diff\n```\n:::\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-5_955c9dbb5ffa56821829273ae982d849'}\n\n```{.r .cell-code}\n#' penalized regression fit\n#'\n#' @param y response variable\n#' @param x covariate\n#' @param xj knot locations\n#' @param sp smoothing parameter\n#'\n#' @return\n#' @export\n#'\n#' @examples\nprs.fit <- function(y,x, xj, sp) {\n  X <- tf.X(x, xj) ## model matrix\n  D <- diff(diag(length(xj)), differences=2) ## sqrt penalty, diff applied columnwise\n  X <- rbind(X, sqrt(sp)*D) # augmented model matrix\n  y <- c(y, rep(0, nrow(D))) # augmented data\n  lm(y~X-1)\n}\n\n# fit the model\nsj <- seq(min(engine$size), max(engine$size), length = 20) ## knots\nb <- prs.fit(engine$wear, engine$size, sj, 2) # smoothing parameter = 2\n\n# prediction data, with same knot locations\ns <- seq(min(engine$size), max(engine$size), length = 200) # prediction data\nXp <- tf.X(s,sj)\n\nwith(engine, plot(size, wear, main = \"smoothed estimate with lambda = 2\"))\nlines(s, Xp %*% coef(b))\n```\n\n::: {.cell-output-display}\n![](gam_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### Examples\n\n#### Brain Imaging\n\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-6_c4789937cee0ccbcdc40fc3df4eb8dc4'}\n\n```{.r .cell-code}\n# specifying your own smoother\nlibrary(mgcv); library(MASS) ## load for mcycle data\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'MASS'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n:::\n\n```{.r .cell-code}\nsm <- smoothCon(s(times, k=10), data = mcycle, knots=NULL)[[1]]\nbeta <- coef(lm(mcycle$accel~sm$X-1))\n# create predictions\ntimes <- seq(0, 60, length = 200)\nXp <- PredictMat(sm, data.frame(times = times)) # get matrix mapping beta to spline predictions at times\nwith(mcycle, plot(times, accel))\nlines(times, Xp %*% beta)\n```\n\n::: {.cell-output-display}\n![](gam_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-7_5888a57adaada47f36d7ac431fcdd1ed'}\n\n```{.r .cell-code}\ndata(brain)\nbrain <- brain[brain$medFPQ > 5e-3,] # exclude 2 outliers\nm0 <- gam(medFPQ ~ s(Y,X,k=100), data=brain)\ngam.check(m0) # general diagnostics\n```\n\n::: {.cell-output-display}\n![](gam_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nMethod: GCV   Optimizer: magic\nSmoothing parameter selection converged after 6 iterations.\nThe RMS GCV score gradient at convergence was 6.236018e-05 .\nThe Hessian was positive definite.\nModel rank =  100 / 100 \n\nBasis dimension (k) checking results. Low p-value (k-index<1) may\nindicate that k is too low, especially if edf is close to k'.\n\n         k'  edf k-index p-value    \ns(Y,X) 99.0 86.8    0.86  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\n - `k'` gives the maximum possible EDF for smooth df.\n \n Residual plots look terrible, variance obviously increasing with mean. Assuming that there's a power relationship with mean-variance,\n \n$$\n\\begin{aligned}\nvar(y_i) \\propto \\mu_i^\\beta\n\\end{aligned}\n$$\n\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-8_e65d8edffd2b8fd61f1b842de5d5b551'}\n\n```{.r .cell-code}\ne <- residuals(m0); fv <- fitted(m0)\nlm(log(e^2) ~ log(fv)) # log \\sigma^2 ~ log mean\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(e^2) ~ log(fv))\n\nCoefficients:\n(Intercept)      log(fv)  \n     -1.961        1.912  \n```\n:::\n:::\n\n\nmeans that $\\beta \\approx 2$, implying that variance increases with square of the mean, so this is the gamma distribution. We could use the log link for the generalized linear family\n\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-9_47824ab38be751953a6dce2ee9a9f88a'}\n\n```{.r .cell-code}\nqplot(brain$X, brain$Y, color = brain$medFPQ) + theme_test()\n```\n\n::: {.cell-output-display}\n![](gam_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-10_02e40d2208d715658ce2e6b2584e28a5'}\n\n```{.r .cell-code}\n# 4th root transformation for stabilization\nm1 <- gam(medFPQ^.25 ~ s(Y,X, k=100), data=brain)\nm2 <- gam(medFPQ ~ s(Y,X, k=100), family = Gamma(link=log), data=brain)\ngam.check(m1)\n```\n\n::: {.cell-output-display}\n![](gam_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nMethod: GCV   Optimizer: magic\nSmoothing parameter selection converged after 4 iterations.\nThe RMS GCV score gradient at convergence was 4.811308e-06 .\nThe Hessian was positive definite.\nModel rank =  100 / 100 \n\nBasis dimension (k) checking results. Low p-value (k-index<1) may\nindicate that k is too low, especially if edf is close to k'.\n\n         k'  edf k-index p-value    \ns(Y,X) 99.0 64.5    0.92  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\n# gam.check(m2) # similar, residuals look better\n```\n:::\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-11_7712a2c1669e0bf290ddafcb3992919b'}\n\n```{.r .cell-code}\nmean(fitted(m1)^4); mean(fitted(m2)); mean(brain$medFPQ)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9855539\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.211483\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.250302\n```\n:::\n\n```{.r .cell-code}\nmean(fitted(m1)); mean(brain$medFPQ^.25)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9832189\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9832189\n```\n:::\n:::\n\n\nThe last value is the actual mean, so it makes sense that the link mean is the most accurate\n\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-12_9cee3c557a89633fe91b2b5129662b11'}\n\n```{.r .cell-code}\n# m1 unbiased on 4th root scale vs unbiased on response scale in m2... \nvis.gam(m2, plot.type=\"contour\", too.far=.03, color = \"gray\", n.grid=60, zlim=c(-1, 2))\n```\n\n::: {.cell-output-display}\n![](gam_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\nm2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: Gamma \nLink function: log \n\nFormula:\nmedFPQ ~ s(Y, X, k = 100)\n\nEstimated degrees of freedom:\n60.6  total = 61.61 \n\nGCV score: 0.6216871     \n```\n:::\n:::\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-13_e1f4149bb46a72f43b04b2a305117b21'}\n\n```{.r .cell-code}\n# try the additive model\nm3 <- gam(medFPQ ~ s(Y, k=30) + s(X, k=30), data = brain, family = Gamma(link=log))\nm3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: Gamma \nLink function: log \n\nFormula:\nmedFPQ ~ s(Y, k = 30) + s(X, k = 30)\n\nEstimated degrees of freedom:\n 9.58 20.20  total = 30.77 \n\nGCV score: 0.6453502     \n```\n:::\n:::\n\n\nGCV score is higher, so it's probably not better...and the aic comparison confirms this\n\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-14_a90cdcb8c6ce3aaf484154d2d9e9f342'}\n\n```{.r .cell-code}\nAIC(m2, m3) # m2 is lower\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         df      AIC\nm2 62.61062 3322.826\nm3 31.77467 3409.491\n```\n:::\n:::\n\n\nIt makes sense that the additive model is not selected, because there's no evidence of a longitudinal/latitude effect in strips. What about isotropic or tensor product smooths?\n\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-15_2faf45924431bbbb45b103cee18b97b0'}\n\n```{.r .cell-code}\ntm <- gam(medFPQ ~ te(Y,X,k=10), data = brain, family = Gamma(link=log))\ntm1 <- gam(medFPQ ~ s(Y, k=10, bs=\"cr\") + s(X, bs=\"cr\", k=10) + ti(X,Y, k=10), data=brain, family = Gamma(link=log))\nAIC(m2, tm, tm1) # selects the isotropic spline. \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          df      AIC\nm2  62.61062 3322.826\ntm  60.34768 3336.805\ntm1 57.08366 3333.671\n```\n:::\n:::\n\nIn summary, the models we consider are \n\n- m1\n\n$$\n\\begin{aligned}\n\\log(\\mu_i) &= f_1(Y_i, X_i), \\qquad medFPQ \\sim Gamma\n\\end{aligned}\n$$\n\n\n\n\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-16_4917d74c15bd5419013d7614e93a719e'}\n\n```{.r .cell-code}\n# is there an interaction now?\nanova(tm1) # the p-value for the interaction term is significant.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: Gamma \nLink function: log \n\nFormula:\nmedFPQ ~ s(Y, k = 10, bs = \"cr\") + s(X, bs = \"cr\", k = 10) + \n    ti(X, Y, k = 10)\n\nApproximate significance of smooth terms:\n           edf Ref.df      F  p-value\ns(Y)     8.258  8.750 14.526  < 2e-16\ns(X)     7.494  8.314  6.959  < 2e-16\nti(X,Y) 39.332 49.891  2.291 1.34e-06\n```\n:::\n:::\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-17_a35455d08cb15f4079700b6b905b2d87'}\n\n```{.r .cell-code}\n# Symmetry with \"by\"\nbrain$Xc <- abs(brain$X - 64.5)\nbrain$right <- as.numeric(brain$X < 64.5)\nm.sy <- gam(medFPQ ~ s(Y, Xc, k=100), family = Gamma(link = log), data=brain) # symmetric\nm.as <- gam(medFPQ ~ s(Y, Xc, k=100) + s(Y, Xc, k=100, by = right), family = Gamma(link = log), data=brain) # asymmetry model\n```\n:::\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-18_5c763f568380b5b52ea9668dc64de747'}\n\n```{.r .cell-code}\nm.sy\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: Gamma \nLink function: log \n\nFormula:\nmedFPQ ~ s(Y, Xc, k = 100)\n\nEstimated degrees of freedom:\n51.4  total = 52.44 \n\nGCV score: 0.6489799     \n```\n:::\n\n```{.r .cell-code}\nm.as # has a better GCV scroe, indicating that this model is better and by AIC it says the same thing.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: Gamma \nLink function: log \n\nFormula:\nmedFPQ ~ s(Y, Xc, k = 100) + s(Y, Xc, k = 100, by = right)\n\nEstimated degrees of freedom:\n50.5 44.7  total = 96.2 \n\nGCV score: 0.6176281     \n```\n:::\n\n```{.r .cell-code}\nanova(m.as) # p-value for the smooth \"right\" term seems to be far from 0.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: Gamma \nLink function: log \n\nFormula:\nmedFPQ ~ s(Y, Xc, k = 100) + s(Y, Xc, k = 100, by = right)\n\nApproximate significance of smooth terms:\n                edf Ref.df     F p-value\ns(Y,Xc)       50.48  65.99 4.344  <2e-16\ns(Y,Xc):right 44.72  59.21 2.457  <2e-16\n```\n:::\n:::\n\n\n## Air Pollution in Chicago\n\n7.4 in Simon Wood book\n\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-19_390feec0e1228ca9d226cff515a63224'}\n\n```{.r .cell-code}\ndata(chicago)\nap0 <- gam(death~s(time, bs=\"cr\", k=200) + pm10median + so2median + o3median + tmpd, data = chicago, family = poisson)\ngam.check(ap0)\n```\n\n::: {.cell-output-display}\n![](gam_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nMethod: UBRE   Optimizer: outer newton\nfull convergence after 3 iterations.\nGradient range [3.514596e-08,3.514596e-08]\n(score 0.2546689 & scale 1).\nHessian positive definite, eigenvalue range [0.004247567,0.004247567].\nModel rank =  204 / 204 \n\nBasis dimension (k) checking results. Low p-value (k-index<1) may\nindicate that k is too low, especially if edf is close to k'.\n\n         k' edf k-index p-value    \ns(time) 199 169    0.92  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nqqplot is obviously quite proble\n\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-20_2a2352d09034a976f94939f6e444950b'}\n\n```{.r .cell-code}\npar(mfrow = c(2,1))\nplot(ap0, n = 1000) # plot the smooth functions that make it up\n# n - needs to be several times larger than the effective degrees of freedom\nplot(ap0, residuals = TRUE, n = 1000)\n```\n\n::: {.cell-output-display}\n![](gam_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-21_996ccf62d207bb00beaaa2ade5032a70'}\n\n```{.r .cell-code}\nap1 <- gam(death ~ s(time, bs = \"cr\", k=200) + s(pm10median, bs = \"cr\") + s(so2median, bs = \"cr\") + s(o3median, bs = \"cr\") + s(tmpd, bs = \"cr\"), data = chicago, family = poisson)\ngam.check(ap1) # honestly not that much better\n```\n\n::: {.cell-output-display}\n![](gam_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nMethod: UBRE   Optimizer: outer newton\nfull convergence after 8 iterations.\nGradient range [-6.661707e-07,4.670663e-08]\n(score 0.2410737 & scale 1).\nHessian positive definite, eigenvalue range [5.217842e-05,0.004273638].\nModel rank =  236 / 236 \n\nBasis dimension (k) checking results. Low p-value (k-index<1) may\nindicate that k is too low, especially if edf is close to k'.\n\n                  k'    edf k-index p-value    \ns(time)       199.00 167.93    0.94  <2e-16 ***\ns(pm10median)   9.00   6.86    1.02    0.90    \ns(so2median)    9.00   7.38    0.99    0.17    \ns(o3median)     9.00   1.58    1.00    0.38    \ns(tmpd)         9.00   8.27    1.02    0.96    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-22_62e07fe5a2d71762f7786ff16fb08162'}\n\n```{.r .cell-code}\n# confirming that I know where poisson residuals are coming from\nresiduals(ap1, type = \"pearson\")[1:10]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  0.7543849 -1.8266695  1.1255760  0.7299294  0.6915362 -1.1494885\n [7]  0.3019724  0.3816246 -0.8354141 -1.6209453\n```\n:::\n\n```{.r .cell-code}\n# chicago$death - predict(ap1)\n\npredict(ap1) %>% length()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4841\n```\n:::\n\n```{.r .cell-code}\ny <- chicago %>% filter(!is.na(pm10median), !is.na(death), !is.na(so2median), !is.na(tmpd)) %>% \n  pull(death)\n\nyhat <- predict(ap1, type = \"response\")\n\ncbind(((y - yhat) / sqrt(yhat))[1:10],\n      residuals(ap1, type = \"pearson\")[1:10])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         [,1]       [,2]\n1   0.7543849  0.7543849\n3  -1.8266695 -1.8266695\n4   1.1255760  1.1255760\n6   0.7299294  0.7299294\n7   0.6915362  0.6915362\n8  -1.1494885 -1.1494885\n9   0.3019724  0.3019724\n11  0.3816246  0.3816246\n12 -0.8354141 -0.8354141\n13 -1.6209453 -1.6209453\n```\n:::\n:::\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-23_394e7e9b38dbee7a2d9aefbaf3157bc3'}\n\n```{.r .cell-code}\n# Consider the single index model\n# ?single.index\n# because we're studying air pollution, there's probably some lag involved, or weights thereof\nlagard <- function(x, n.lag = 6) {\n  n <- length(x); X <- matrix(NA, n, n.lag)\n  for (i in 1:n.lag) X[i:n, i] <- x[i:n-i+1] # set the next column, starting at i.\n  X\n}\n\nlagard(1:5, n.lag = 3) # probably most clear just to look at the model matrix here.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1   NA   NA\n[2,]    2    1   NA\n[3,]    3    2    1\n[4,]    4    3    2\n[5,]    5    4    3\n```\n:::\n:::\n\n::: {.cell hash='gam_cache/html/unnamed-chunk-24_8a90f4d5a2471d2f3d777e5514f2ff8c'}\n\n```{.r .cell-code}\ndat <- list(lag = matrix(0:5, nrow(chicago), 6, byrow = TRUE))\ndat$death <- chicago$death\ndat$time <- chicago$time\ndat$pm10 <- lagard(chicago$pm10median)\ndat$o3 <- lagard(chicago$o3)\ndat$tmp <- lagard(chicago$tmp)\n\n\nsi <- function(theta, dat, opt = TRUE) {\n  alpha <- c(1, theta) ## alpha defined via unconstrained theta\n  kk <- sqrt(sum(alpha^2)); alpha <- alpha/kk ## ||alpha || = 1\n  o3 <- dat$o3 %*% alpha;\n  tmp <- dat$tmp %*% alpha\n  pm10 <- dat$pm10 %*% alpha ## re-weight laggard covariates\n  b <- bam(dat$death ~ s(dat$time, k = 200, bs=\"cr\") + s(pm10, bs=\"cr\") + te(o3, tmp, k=8), family=poisson) ## fit the model\n  cat(\".\") # give user something to watch\n  if (opt) return(b$gcv.ubre) else {\n    b$alpha <- alpha ## add alpha to model object\n    b$J <- outer(alpha, -theta/kk^2) ## get dalpha_i/dtheta_j\n    for (j in 1:length(theta)) b$J[j+1,j] <- b$J[j+1, j] + 1 /kk\n    return(b)\n  }\n}\n\n# f1 <- optim(rep(1, 5), si, method = \"BFGS\", hessian=TRUE, dat = dat)\n# save(f1, file = \"f1.RData\") # The results from f1 took forever to run, almost an hour, so save the results and just load it.\nload(\"f1.RData\")\napsi <- si(f1$par, dat, opt = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n.\n```\n:::\n\n```{.r .cell-code}\napsi$alpha\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  0.03419156  0.67333186  0.63917561  0.25879701  0.24601994 -0.09699472\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}