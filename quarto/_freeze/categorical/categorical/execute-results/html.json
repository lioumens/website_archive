{
  "hash": "ff01389daad52c0dbba527dfbbcc4aad",
  "result": {
    "markdown": "---\ntitle: \"Categorical Data Analysis\"\nauthor: \"Michael Liou\"\ndate: \"2023-02-22\"\nexecute:\n  cache: true\n---\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-1_8c8480924f00adb988d914afbb531d1d'}\n\n```{.r .cell-code  code-summary=\"Libraries\"}\nlibrary(tidyverse)\nlibrary(AMR)\nlibrary(DescTools)\nlibrary(kableExtra)\nlibrary(VGAM)\nlibrary(nnet)\nlibrary(vcd) # visualizing categorical data\nlibrary(vcdExtra) # additional companion code\n```\n:::\n\n\n## Overview\n\nThis is a very broad area, but I feel like it's also super confusing because it's all very confouded in terms of all the chisq tests that are scattered throughout the place. I'm trying to organize everything for myself here.\n\n- CMH test\n- score test for table\n- Pearson\n- Yates (continuity correction of Pearson)\n- Barnard test (based on fixing 1 margin)\n- fisher exact (based on fixing all margins)\n- McNemar Test\n- Generalized CMH test\n\n\n\n## 2x2 sampling mechanisms\n\nThe many different sampling mechanisms arise from different study designs, and are critical to the assumptions of the population you are studying.\n\nIn an epidemiological context, the data is given as\n\n|             | disease | no disease |    |\n|-------------+:---------+:------------+----|\n| exposure    | a       | b          | n1 |\n| no exposure | c       | d          | n0 |\n|             | m1      | m0         | N  |\n\n\n* Poisson (nothing fixed)\n* Multinomial (N) total is fixed\n  - \"Cross Sectional\" studies\n* Two-sample Binomial (1 margin fixed)\n  - \"Cohort Study\" = n1, n0 fixed\n  - \"Case Control Study\" = m1, m0 fixed\n    - a type of \"retrospective\" study.\n* Hypergeometric (2 margins fixed)\n  - very rarely the case in real experiments, but unfortunately many methods are based on this assumption for the 2x2 table.\n\n::: {.panel-tabset}\n\n### Poisson\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-2_6464c5598f4e82a9ab99298c49d5fc23'}\n\n```{.r .fold-hide .cell-code}\n# Independent Poisson\n\n#' When specifying the mean, they can be specified cellwise, or by table, grouped by the nrow*ncol, in order of the columns\n#'\n#' @param mean means of the cells. If 1 number, all cells will have same \n#'\n#' @return\n#' @export\n#'\n#' @examples\nrpoisson_table <- function(num_tables = 1, mean = 5, nrow = 2,  ncol = 2) {\n  cells <- rpois(ncol * nrow * num_tables, lambda = mean)\n  vec_table <- split(cells, gl(num_tables, ncol*nrow))\n  vapply(vec_table, matrix, nrow = nrow, ncol = ncol, byrow = FALSE, FUN.VALUE = matrix(1:(ncol*nrow), nrow = nrow))\n}\n\n# 2x2 examples\npoisson_table_examples <- rpoisson_table(5, mean = 5, nrow = 2, ncol = 2)\napply(poisson_table_examples,\n      MARGIN = 3,\n      FUN = addmargins,\n      simplify = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$`1`\n         Sum\n     8 1   9\n     7 5  12\nSum 15 6  21\n\n$`2`\n         Sum\n    4  6  10\n    2  9  11\nSum 6 15  21\n\n$`3`\n         Sum\n    4  7  11\n    2  4   6\nSum 6 11  17\n\n$`4`\n         Sum\n    2  7   9\n    5  4   9\nSum 7 11  18\n\n$`5`\n        Sum\n    4 4   8\n    0 3   3\nSum 4 7  11\n```\n:::\n:::\n\n\n### Multinomial\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-3_1d938f58de3f5601081bc4322e49906b'}\n\n```{.r .cell-code}\n# grand total fixed\nrmultinom_table <- function(num_tables = 1, N = 20, nrow = 2, ncol = 2, p = c(1, 1, 1, 1)) {\n  stopifnot(length(p) == nrow * ncol)\n  # p is internally standardized by rmultinom\n  cells <- rmultinom(num_tables, N, p)\n  dim(cells) <- c(nrow, ncol, num_tables)\n  cells\n}\n```\n:::\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-4_c3e403f007d80a03ecaa00f44ead5472'}\n\n```{.r .cell-code}\n# examples\nmultinom_table_examples <- rmultinom_table(5, N = 20, nrow = 2, ncol = 2, p = c(1, 1, 1, 1))\napply(multinom_table_examples,\n      MARGIN = 3,\n      FUN = addmargins,\n      simplify = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n         Sum\n     4 7  11\n     7 2   9\nSum 11 9  20\n\n[[2]]\n         Sum\n    5  8  13\n    3  4   7\nSum 8 12  20\n\n[[3]]\n         Sum\n    1  7   8\n    8  4  12\nSum 9 11  20\n\n[[4]]\n         Sum\n    7  6  13\n    2  5   7\nSum 9 11  20\n\n[[5]]\n         Sum\n    4  5   9\n    4  7  11\nSum 8 12  20\n```\n:::\n:::\n\n\n\n### Binomial\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-5_e570d4e772e2886765a5f14ed36bf53f'}\n\n```{.r .cell-code}\n# Sample a 2x2 table\nrbinom_table <- function(num_tables = 1, row_n = c(10, 10), p = c(.5, .5)) {\n  vapply(1:num_tables, \n       FUN = function(x) {\n         a <- rbinom(length(row_n), row_n, p) # fix margins\n         binom_table <- cbind(a, b = row_n-a) # make other column by subtraction and combine\n         colnames(binom_table) <- NULL\n         binom_table}, \n       FUN.VALUE = matrix(rep(1.1, 4), nrow =2)) # expected 2x2 table\n}\n\nbinom_table_examples <- rbinom_table(5, row_n = c(10, 10), p = c(.5, .5))\napply(binom_table_examples,\n      MARGIN = 3,\n      FUN = addmargins,\n      simplify = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n         Sum\n     7 3  10\n     6 4  10\nSum 13 7  20\n\n[[2]]\n          Sum\n     5  5  10\n     5  5  10\nSum 10 10  20\n\n[[3]]\n         Sum\n     4 6  10\n     7 3  10\nSum 11 9  20\n\n[[4]]\n          Sum\n     5  5  10\n     5  5  10\nSum 10 10  20\n\n[[5]]\n         Sum\n     4 6  10\n     7 3  10\nSum 11 9  20\n```\n:::\n:::\n\n\n\n### Hypergeometric\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-6_e9a54325aef3b98ec998ef3298ecc665'}\n\n```{.r .cell-code}\n# Fixed margins\n# one hypergeometrical deviate will determine the entire table.\n# only doing 2x2 tables with sampling method\n\nrhyper_table <- function(num_tables, row_n = c(10, 10), col_n = c(10, 10))  {\n  a <- rhyper(num_tables, row_n[1], row_n[2], col_n[1])\n  vapply(a, \n       FUN = function(x) {\n         col1 <- c(x, col_n[1] - x)\n         col2 <- row_n - col1\n         contingency_table <- cbind(col1, col2)\n         colnames(contingency_table) <- NULL # get rid of column names\n         contingency_table\n       },\n       FUN.VALUE = matrix(c(0, 0, 0, 1.1), nrow = 2))}\n\n# examples\nhyper_table_examples <- rhyper_table(num_tables = 5, row_n = c(10, 10), col_n = c(10, 10))\napply(hyper_table_examples,\n      MARGIN = 3,\n      FUN = addmargins,\n      simplify = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n          Sum\n     4  6  10\n     6  4  10\nSum 10 10  20\n\n[[2]]\n          Sum\n     4  6  10\n     6  4  10\nSum 10 10  20\n\n[[3]]\n          Sum\n     3  7  10\n     7  3  10\nSum 10 10  20\n\n[[4]]\n          Sum\n     5  5  10\n     5  5  10\nSum 10 10  20\n\n[[5]]\n          Sum\n     5  5  10\n     5  5  10\nSum 10 10  20\n```\n:::\n:::\n\n\n:::\n\n## Measures of Association\n\nGiven tables, it's important to distinguish the properties of how to make comparisons, and summarize the information given. When dealing with percentages and ratios, it's sometimes difficult to have an intuitive understanding of meaning on the percentage scale. The example Professor Guanhua Chen gives to stimulate you for this thinking is from [cartalk](https://www.cartalk.com/radio/puzzler/porch-potatoes):\n\n> RAY: Potatoes are 99 percent water and one percent what? Potato. So say you take a bunch of potatoes, like 100 pounds of potatoes and you set them out on your back porch to dry out.\nTOM: Yeah, when they are dry they should weigh about a pound.\nRAY: Well, we’re not drying out completely. And as the potatoes dry out the water begins to evaporate. And after a while, enough water has evaporated so that they are now 98 percent water. If you were to weigh those potatoes at that moment...\nTOM: They'd be lighter.\nRAY: Yes, how much lighter? That's the question. Now you can solve this puzzler algebraically, and if you don't solve it algebraically, you are going to get the wrong answer.\nTOM: Really?\nRAY: Really.What's your answer, off the top of your head?\nTOM: 99 pounds.\nRAY: You are wrong.\n> Answer: \nRAY: Now, unencumbered by the thought process as usual, my brother guessed 99 pounds.\nTOM: Yeah.\nRAY: Now, when I guessed, off the top of my head, I guessed about 90 pounds.\nTOM: 'Cause it just feels right.\nRAY: But if you do the math, 1 percent of 100 --which is what the potato is-- is one pound. As we told you, that's 1 percent. So 2 percent, when it’s 98 percent water, two percent of the new weight of the mass is still going to be equal to that one pound, and 2 percent of 50 pounds is a pound. So the potato weight is now 50 pounds, not 100.\n\n* Risk Ratio: \n  - not symmetric\n* Odds Ratio\n  - symmetric, exposure gives information about disease and vice versa\n\n## Testing\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-7_408a56390961f3a4cf9bb5608e7b1111'}\n\n```{.r .cell-code}\ntribble(~test, ~parameter, ~sampling, ~pvalue, ~hypothesis,\n        \"pearson\", \"parameter\",\"two sample z proportion\", \"approximate\", \"association\",\n        \"cochran\", \"cell\", \"unconditional two sample\", \"approximate\", \"association\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 5\n  test    parameter sampling                 pvalue      hypothesis \n  <chr>   <chr>     <chr>                    <chr>       <chr>      \n1 pearson parameter two sample z proportion  approximate association\n2 cochran cell      unconditional two sample approximate association\n```\n:::\n:::\n\n\n\nA list of summary\n\nA list of tests:\n\n- Cochran summary test\n- (Cochran)-Mantel-Haenszel summary test\n- Generalized CMH Test\n- Cochran's Q test\n- Pearson Chi Squared\n- Cochran-Armitrage Trend Test\n- Cochran-Q Test\n- Mann-Whitney U test (Wilcoxon Rank Sum)\n- Wilcoson Signed Rank Test (paired samples)\n- Krustkal Wallis\n\nExact tests\n\n- Fisher Exact Test\n- Barnard Exact Test\n\nDependent \"Matched pairs\" tests\n\n- McNemar Test\n\n### Pearson\n\nGiven the following table,\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-8_639204ada22e347b46303a86e2ef8ca8'}\n\n```{.r .cell-code}\ntribble(~\"\", ~\"Disease\", ~\"No Disease\", ~\"\",\n        \"Exposed\", \"a\", \"b\", \"\\\\$n_1\\\\$\",\n        \"Not Exposed\" , \"c\", \"d\", \"\\\\$n_2\\\\$\",\n        \"\", \"\\\\$m_1\\\\$\", \"\\\\$m_2\\\\$\", \"N\") %>% \n  kable(\"html\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:left;\"> Disease </th>\n   <th style=\"text-align:left;\"> No Disease </th>\n   <th style=\"text-align:left;\">  </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Exposed </td>\n   <td style=\"text-align:left;\"> a </td>\n   <td style=\"text-align:left;\"> b </td>\n   <td style=\"text-align:left;\"> \\$n_1\\$ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Not Exposed </td>\n   <td style=\"text-align:left;\"> c </td>\n   <td style=\"text-align:left;\"> d </td>\n   <td style=\"text-align:left;\"> \\$n_2\\$ </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:left;\"> \\$m_1\\$ </td>\n   <td style=\"text-align:left;\"> \\$m_2\\$ </td>\n   <td style=\"text-align:left;\"> N </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\nAssume two sample proportional sampling model: A derivation from 2 sample proportion (pooled) z-test:\n\n$$\n\\begin{aligned}\nz^2 &= \\frac{(\\hat p_1 - \\hat p_2)^2}{\\hat p(1 - \\hat p)(\\frac{1}{n_1} + \\frac{1}{n_2})} \n&=\n\\end{aligned}\n$$\n\n\n$$\n\\begin{aligned}\nX^2 = \\sum_i^c \\frac{|O_i - E_i|}{E_i} \\sim \\chi^2_{c-1}\n\\end{aligned}\n$$\n\nResources:\n- [Pearson as Score Test](https://projecteuclid.org/download/pdf_1/euclid.lnms/1215091138)\n- [7 proofs of independence test](https://arxiv.org/pdf/1808.09171.pdf)\n\n\n### Barnard Exact Test\n\nBarnard is considered an \"unconditional\" approach to exact testing. contrast to Fisher exact test which is a \"conditional\" approach to testing the p-value\n\nBarnard is effectively a 2 stage test, given some observed table X, a \"p-value\" is the probability of observing a table more extreme than the observed. \n\nThe two stages are thus:\n\n1. determine which tables are more \"extreme\"\n2. calculated probability of those tables\n\nThus, the \"exactness\" part of the description refers to how a probability is calculated.\n\nMany methods have been proposed for stage 1\n\n- Suissa and Shuster (1985) use pooled and unpooled z statistic for two proportions.\n- Booschloo (1970) used the p-value from fisher's exact test to determine extremeness...\n- Santner Snell - difference in proportion\n\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-9_a52c6c11209c54ea17c43c1c0a0401b1'}\n\n```{.r .cell-code}\nX <- matrix(c(3, 0, 0, 3), nrow = 2)\n# row is fixed\nBarnardTest(X, method = \"z-pooled\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tZ-pooled Exact Test\n\ndata:  3 out of 3 vs. 0 out of 3\ntest statistic = 2.4495, first sample size = 3, second sample size = 3,\np-value = 0.03125\nalternative hypothesis: true difference in proportion is not equal to 0\nsample estimates:\ndifference in proportion \n                       1 \n```\n:::\n\n```{.r .cell-code}\n# z-pooled observed is...\n1 / sqrt(.25 * (1/3 + 1/3)) # observed test statistic\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.44949\n```\n:::\n\n```{.r .cell-code}\n# how to calculate p-value from this test statistic, sum of binomial products from extreme tables,\n# the null is that pi_1 = pi_2 = pi, since we don't know the actual value of pi, we take the supremum of these values\n\ntrue_pi <- seq(0, 1, .01)\n\npossible_p <- dbinom(3, 3, prob = true_pi) * dbinom(0, 3, prob = true_pi) + dbinom(0, 3, prob = true_pi) * dbinom(3, 3, prob = true_pi)\n\n\n# maximum occurs at .5\ntrue_pi[which.max(possible_p)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5\n```\n:::\n\n```{.r .cell-code}\n# thus, overall p-value is\nmax(possible_p) # .03125\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03125\n```\n:::\n:::\n\n\nNow we compare the methods for choosing \"extremeness\" with the assumed sampling mechanism.\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-10_46a7964aa9a70d90cb4fd86002a9dcda'}\n\n```{.r .cell-code}\nset.seed(1)\nfoo <- rbinom_table(1000, row_n = c(10, 10), p = c(.5, .5)) # null is no association\n\n# z pooled (score)\nbarnard_pooled <- apply(foo, MARGIN = 3,\n                        FUN = function (x) {BarnardTest(x, method = \"z-pooled\")$p.value})\n\nbarnard_unpooled <- apply(foo, MARGIN = 3,\n                        FUN = function (x) {BarnardTest(x, method = \"z-unpooled\")$p.value})\n\nbarnard_boschloo <- apply(foo, \n                          MARGIN = 3,\n                          FUN = function (x) {BarnardTest(x, method = \"boschloo\")$p.value})\n\n\nbarnard_csm <- apply(foo, \n                          MARGIN = 3,\n                          FUN = function (x) {BarnardTest(x, method = \"csm\")$p.value})\n\nbarnard_santner_snell <- apply(foo, \n                               MARGIN = 3,\n                               FUN = function (x) {BarnardTest(x, method = \"santner and snell\")$p.value})\n\n# for reference\nfisher_exact <-  apply(foo, \n                               MARGIN = 3,\n                               FUN = function (x) {fisher.test(x)$p.value})\n\n\n\n# comment out any that you don't want to appear in the plot\nbarnard <- bind_cols(pooled = barnard_pooled,\n                     unpooled = barnard_unpooled,\n                     boschloo = barnard_boschloo,\n                     csm = barnard_csm,\n                     santner_snell = barnard_santner_snell,\n                     fisher_exact = fisher_exact) %>% \n  pivot_longer(everything(), names_to = \"type\", values_to = \"p\")\n\n# there's some overplotting happening, dodge doesn't seem to work well for ecdf's\nbarnard %>% ggplot(aes(x = p, color = type)) +\n  stat_ecdf()  +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) +\n  labs(title = \"Barnard extremeness method comparison\")\n```\n\n::: {.cell-output-display}\n![](categorical_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nFor accuracy of the size of the test, it seems that csm is the most accurate, but also the most computationally intensive. We not that all the tests have more approriate \"size\" than the fisher_exact test.\n\nResources\n\n- [Explanation of difference between unconditional and conditional](https://www.researchgate.net/publication/242179503_Conditional_versus_Unconditional_Exact_Tests_for_Comparing_Two_Binomials)\n- [Exact Test Package Documentation](https://cran.r-project.org/web/packages/Exact/Exact.pdf) - `exact.test` function documentation has more information about barnard implmenetation.\n\n### McNemar\n\nMcNemar tests are used when there is some \"dichotomous trait\", for matched pairs. This means that the responses are statistically dependent. This is common for some longitudinal studies in which a single individual is asked two questions, and their answers are coded as locations in the table. Thus, the grand total, should be the number of pairs of data, not the total number of observations.\n\nFor example, suppose a person is asked if they voted democrat \n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-11_9643e223f916e4c33833b15a36f6d016'}\n\n```{.r .cell-code}\ntribble(~\"\", ~\"2008 democrat\", ~\"2008 republican\",\n        \"2004 democrat\", 175, 16,\n        \"2004 republican\", 54, 188) %>% \n  kbl()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">  </th>\n   <th style=\"text-align:right;\"> 2008 democrat </th>\n   <th style=\"text-align:right;\"> 2008 republican </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 2004 democrat </td>\n   <td style=\"text-align:right;\"> 175 </td>\n   <td style=\"text-align:right;\"> 16 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2004 republican </td>\n   <td style=\"text-align:right;\"> 54 </td>\n   <td style=\"text-align:right;\"> 188 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nMcNemar tests \"Marginal Homogeneity\", meaning that the probabilities of the margins are the same. $p_a + p_b = p_a + p_c$ and $p_c + p_d = p_b + p_d$. Thus, this means that we are testing $H_0: p_b = p_c$, $H_A: p_b \\neq p_c$. The score statistic is:\n\n$$\n\\begin{aligned}\nz_0^2 = \\frac{(b-c)^2}{b+c} \\sim \\chi^2_1\n\\end{aligned}\n$$\n\n\n<!-- This section needs clarification... -->\n\nVariance is\n\n$$\n\\begin{aligned}\n\\hat\\sigma_0 (d) = \\frac{b + c}{N^2}\n\\end{aligned}\n$$\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-12_25f7d7bd0d52dc0ef2c5878cdd44ef28'}\n\n```{.r .cell-code}\n# Presidential election, dependent table\npres <- matrix(c(175, 16,\n                      54, 188),\n                    ncol = 2,\n                    byrow = TRUE)\npres_mcnemar <- mcnemar.test(pres, correct = FALSE)\n# (54 - 16)^2 / (54 + 16) # 20.629\npres_mcnemar\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tMcNemar's Chi-squared test\n\ndata:  pres\nMcNemar's chi-squared = 20.629, df = 1, p-value = 5.576e-06\n```\n:::\n:::\n\n\n\nNotes\n\n- only the off diagonal matters for significance, where as the main diagonal \n\nA good reference is 11.1 in Categorical Data Analysis, 3rd edition by Agresti\n\n### Breslow-Day Test\n\na test of homogeneity.\n\n### Wilcox Test\n\nIs a test of location for two samples. This somewhat analogous to a nonparametric t-test.\n\n### Krustkal Wallis\n\nAlso a test of location for K samples. This is an extension of the Wilcox test that is also nonparametric. this can be thought of a nonparameteric anova.\n\n### Cochran-Armitrage Trend Test (ordinal)\n\nAppropriate for IxJ tables in which _both_ directions are ordinal. This assumes the tables are sampled by binomial fixed proportions for the two groups. We can kind of think of fitting some weighted regression to a surface that is determined by the scores along X and Y.\n\nIn the 2xJ case, this reduces to a Wilcoxon test, (also known as Mann-Whitney).\n\nThese tests are more powerful for testing specific hypotheses, like a specific trend. If we don't know what the trend is, then we may be better off testing with a pearson test which just looks for general association.\n\n\n\n\n\n\n\n\n### CMH testing\n\nThe CMH testing is technically supposed to be done on tables in strata. the data type is an I X J X K, in which we have K strata and an I X J contingency table in each.\n\n- you are not penalized for adding tables with sparse data with the CMH test statistic\n- this reduces to the N-1 adjusted pearson chisquared statistic for 1 strata\n- the test assumes that there is a common odds ratio to estimate, but in order to test the hypothesis we can use a Breslow-Day Test of Homogeneity\n- Conditional logistic regression gives a similar answer, `clogit` in package `survival` because similar to cox model as well.\n\n\n#### Generalized CMH Testing\n\n\nCMH is extended to IxJxK with possibly ordinal factors for I and J. An implementation of these statistics can be found: \n\n- `coin::cmh_test()`\n- `vcdExtra:CHMtest()`\n\n\n \n\n#### CMH - McNemar Equivalence\n\nIf you express the \"population averaged\" table and run McNemar, you will get the same statistic as expressing the data as a \"subject specific\" table for an individual per stratum.\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-15_a5ff7c39b132a4d4cd2ea0050b724a81'}\n\n```{.r .cell-code}\ntest <- matrix(c(5, 7, 3, 4), ncol = 2)\nchisq.test(test, correct = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in chisq.test(test, correct = FALSE): Chi-squared approximation may be\nincorrect\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test\n\ndata:  test\nX-squared = 0.0025703, df = 1, p-value = 0.9596\n```\n:::\n\n```{.r .cell-code}\n0.0025703 / 19 * 18 # the \"N-1\" chisq statistic in a 2 x 2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.002435021\n```\n:::\n\n```{.r .cell-code}\nfoo <- array(c(5, 7, 3, 4,\n               0, 0, 1, 1), dim = c(2, 2, 2))\nmantelhaen.test(foo, correct = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tMantel-Haenszel chi-squared test without continuity correction\n\ndata:  foo\nMantel-Haenszel X-squared = 0.0024351, df = 1, p-value = 0.9606\nalternative hypothesis: true common odds ratio is not equal to 1\n95 percent confidence interval:\n 0.1444191 6.2805369\nsample estimates:\ncommon odds ratio \n         0.952381 \n```\n:::\n\n```{.r .cell-code}\nbar <- array(c(5, 7, 3, 4,\n               0, 0, 1, 1,\n               1, 6, 0, 0), dim = c(2, 2, 3))\nmantelhaen.test(bar, correct = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tMantel-Haenszel chi-squared test without continuity correction\n\ndata:  bar\nMantel-Haenszel X-squared = 0.0024351, df = 1, p-value = 0.9606\nalternative hypothesis: true common odds ratio is not equal to 1\n95 percent confidence interval:\n 0.1444191 6.2805369\nsample estimates:\ncommon odds ratio \n         0.952381 \n```\n:::\n\n```{.r .cell-code}\nmantelhaen_2by2 <-  function(x) {\n  mantelhaen.test(array(c(x, 0, 0, 1, 1), dim = c(2, 2, 2)), correct = FALSE)\n}\n```\n:::\n\n\n\n### 2x2 tables Comparison {.tabset}\n\n#### Poisson Sampling\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-16_caa4edbe363587f4ae3832c5afbb703d'}\n\n```{.r .fold-hide .cell-code}\n# random poisson\nfoo <- rpoisson_table(num_tables = 1000, mean = 5, nrow = 2, ncol = 2)\n\n# pearson uncorrected\npearson_uncorrected_p <- \n  apply(foo, \n      MARGIN = 3,\n      function(x) {\n        suppressWarnings(chisq.test(x, correct = FALSE))$p.value})\n\n# pearson corrected\npearson_corrected_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(chisq.test(x, correct = TRUE))$p.value})\n\nfisher_exact_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(fisher.test(x))$p.value})\n\n# Pearson N-1 correction equivalent\ncmh_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          mantelhaen_2by2(x)$p.value\n        })\n\n# G-test (LRT)\nlrt_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(g.test(x))$p.value\n        })\n\n\ncontingency_p <- bind_cols(uncorrected = pearson_uncorrected_p,\n          corrected = pearson_corrected_p,\n          exact = fisher_exact_p,\n          cmh = cmh_p,\n          lrt = lrt_p)\n\ncontingency_p %>% \n  pivot_longer(everything(), names_to = \"test\", values_to = \"p\") %>% \n  ggplot(aes(p, color = test)) +\n  stat_ecdf(geom = \"step\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) + \n  labs(title = \"Tests of association\") #+\n```\n\n::: {.cell-output-display}\n![](categorical_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .fold-hide .cell-code}\n  coord_cartesian(xlim = c(0, .1), ylim = c(0, .1)) # for zooming in\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ggproto object: Class CoordCartesian, Coord, gg>\n    aspect: function\n    backtransform_range: function\n    clip: on\n    default: FALSE\n    distance: function\n    expand: TRUE\n    is_free: function\n    is_linear: function\n    labels: function\n    limits: list\n    modify_scales: function\n    range: function\n    render_axis_h: function\n    render_axis_v: function\n    render_bg: function\n    render_fg: function\n    setup_data: function\n    setup_layout: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    setup_params: function\n    train_panel_guides: function\n    transform: function\n    super:  <ggproto object: Class CoordCartesian, Coord, gg>\n```\n:::\n:::\n\n\n#### Binomial Sampling\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-17_cb65ac8693ed10feee596b6fed0ea0f4'}\n\n```{.r .fold-hide .cell-code}\nfoo <- rbinom_table(num_tables = 9000)\n\n# pearson uncorrected\npearson_uncorrected_p <- \n  apply(foo, \n      MARGIN = 3,\n      function(x) {\n        suppressWarnings(chisq.test(x, correct = FALSE))$p.value})\n\n# pearson corrected\npearson_corrected_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(chisq.test(x, correct = TRUE))$p.value})\n\nfisher_exact_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(fisher.test(x))$p.value})\n\n# Pearson N-1 correction equivalent\ncmh_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          mantelhaen_2by2(x)$p.value\n        })\n\n# G-test (LRT)\nlrt_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(g.test(x))$p.value\n        })\n\n\ncontingency_p <- bind_cols(uncorrected = pearson_uncorrected_p,\n          corrected = pearson_corrected_p,\n          exact = fisher_exact_p,\n          cmh = cmh_p,\n          lrt = lrt_p)\n\ncontingency_p %>% \n  pivot_longer(everything(), names_to = \"test\", values_to = \"p\") %>% \n  ggplot(aes(p, color = test)) +\n  stat_ecdf(geom = \"step\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) #+\n```\n\n::: {.cell-output-display}\n![](categorical_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .fold-hide .cell-code}\n  coord_cartesian(xlim = c(0, .1), ylim = c(0, .1)) # for zooming in\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ggproto object: Class CoordCartesian, Coord, gg>\n    aspect: function\n    backtransform_range: function\n    clip: on\n    default: FALSE\n    distance: function\n    expand: TRUE\n    is_free: function\n    is_linear: function\n    labels: function\n    limits: list\n    modify_scales: function\n    range: function\n    render_axis_h: function\n    render_axis_v: function\n    render_bg: function\n    render_fg: function\n    setup_data: function\n    setup_layout: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    setup_params: function\n    train_panel_guides: function\n    transform: function\n    super:  <ggproto object: Class CoordCartesian, Coord, gg>\n```\n:::\n:::\n\n\n#### Multinomial Sampling\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-18_2dabce5d16727c8b80bf234f83f1f59a'}\n\n```{.r .fold-hide .cell-code}\nfoo <- rmultinom_table(num_tables = 1000)\n\n# pearson uncorrected\npearson_uncorrected_p <- \n  apply(foo, \n      MARGIN = 3,\n      function(x) {\n        suppressWarnings(chisq.test(x, correct = FALSE))$p.value})\n\n# pearson corrected\npearson_corrected_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(chisq.test(x, correct = TRUE))$p.value})\n\nfisher_exact_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(fisher.test(x))$p.value})\n\n# Pearson N-1 correction equivalent\ncmh_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          mantelhaen_2by2(x)$p.value\n        })\n\n# G-test (LRT)\nlrt_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(g.test(x))$p.value\n        })\n\n\ncontingency_p <- bind_cols(uncorrected = pearson_uncorrected_p,\n          corrected = pearson_corrected_p,\n          exact = fisher_exact_p,\n          cmh = cmh_p,\n          lrt = lrt_p)\n\ncontingency_p %>% \n  pivot_longer(everything(), names_to = \"test\", values_to = \"p\") %>% \n  ggplot(aes(p, color = test)) +\n  stat_ecdf(geom = \"step\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) #+\n```\n\n::: {.cell-output-display}\n![](categorical_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .fold-hide .cell-code}\n  coord_cartesian(xlim = c(0, .1), ylim = c(0, .1)) # for zooming in\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ggproto object: Class CoordCartesian, Coord, gg>\n    aspect: function\n    backtransform_range: function\n    clip: on\n    default: FALSE\n    distance: function\n    expand: TRUE\n    is_free: function\n    is_linear: function\n    labels: function\n    limits: list\n    modify_scales: function\n    range: function\n    render_axis_h: function\n    render_axis_v: function\n    render_bg: function\n    render_fg: function\n    setup_data: function\n    setup_layout: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    setup_params: function\n    train_panel_guides: function\n    transform: function\n    super:  <ggproto object: Class CoordCartesian, Coord, gg>\n```\n:::\n:::\n\n\n#### Hypergeometric Sampling\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-19_5296bb3531d9be70d696069c7e4f5d95'}\n\n```{.r .fold-hide .cell-code}\nfoo <- rhyper_table(num_tables = 1000)\n\n# pearson uncorrected\npearson_uncorrected_p <- \n  apply(foo, \n      MARGIN = 3,\n      function(x) {\n        suppressWarnings(chisq.test(x, correct = FALSE))$p.value})\n\n# pearson corrected\npearson_corrected_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(chisq.test(x, correct = TRUE))$p.value})\n\nfisher_exact_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(fisher.test(x))$p.value})\n\n# Pearson N-1 correction equivalent\ncmh_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          mantelhaen_2by2(x)$p.value\n        })\n\n# G-test (LRT)\nlrt_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(g.test(x))$p.value\n        })\n\n\ncontingency_p <- bind_cols(uncorrected = pearson_uncorrected_p,\n          corrected = pearson_corrected_p,\n          exact = fisher_exact_p,\n          cmh = cmh_p,\n          lrt = lrt_p)\n\ncontingency_p %>% \n  pivot_longer(everything(), names_to = \"test\", values_to = \"p\") %>% \n  ggplot(aes(p, color = test)) +\n  stat_ecdf(geom = \"step\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) #+\n```\n\n::: {.cell-output-display}\n![](categorical_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .fold-hide .cell-code}\n  coord_cartesian(xlim = c(0, .1), ylim = c(0, .1)) # for zooming in\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ggproto object: Class CoordCartesian, Coord, gg>\n    aspect: function\n    backtransform_range: function\n    clip: on\n    default: FALSE\n    distance: function\n    expand: TRUE\n    is_free: function\n    is_linear: function\n    labels: function\n    limits: list\n    modify_scales: function\n    range: function\n    render_axis_h: function\n    render_axis_v: function\n    render_bg: function\n    render_fg: function\n    setup_data: function\n    setup_layout: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    setup_params: function\n    train_panel_guides: function\n    transform: function\n    super:  <ggproto object: Class CoordCartesian, Coord, gg>\n```\n:::\n:::\n\n\n\n## More than two categories\n\nThis section starts to get into 2 x I tables, and even more dimensions like, I X J X K tables, and how we analyze those tables. We'll start with an overview of the multinomial theory, which is fundamental in extending the binomial (2 categories) into multiple categories. The binomial is a special case of the multinomial distribution\n\n- `nnet::multinom`\n- `VGAM::vglm(family = multinom)`\n- `mlogit::mlogit`\n\n\nA common example we see in this exposition is housing data from library `MASS`\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-20_288a797da04d0b5d6b46a82ef3fd0091'}\n\n```{.r .cell-code}\nlibrary(MASS)\ndata(housing)\n\n# The array version\nhousing_arr <- xtabs(Freq~Sat + Infl + Type + Cont, data = housing)\n```\n:::\n\n\n\n\n### Poisson GLM Modeling\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-21_51eb49c4bd828dc108c66a5ad9c73dfa'}\n\n```{.r .cell-code}\n# simple glm model (satisfaction independent of Infl, Type, Cont)\nhouse_glm <- glm(Freq ~ Infl*Type*Cont + Sat, data = housing, family = poisson)\nsummary(house_glm) # high residual deviance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Freq ~ Infl * Type * Cont + Sat, family = poisson, \n    data = housing)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4.5551  -1.0612  -0.0593   0.6483   4.1478  \n\nCoefficients:\n                                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                        3.136e+00  1.196e-01  26.225  < 2e-16 ***\nInflMedium                         2.733e-01  1.586e-01   1.723 0.084868 .  \nInflHigh                          -2.054e-01  1.784e-01  -1.152 0.249511    \nTypeApartment                      3.666e-01  1.555e-01   2.357 0.018403 *  \nTypeAtrium                        -7.828e-01  2.134e-01  -3.668 0.000244 ***\nTypeTerrace                       -8.145e-01  2.157e-01  -3.775 0.000160 ***\nContHigh                          -2.200e-16  1.690e-01   0.000 1.000000    \nSat.L                              1.159e-01  4.038e-02   2.871 0.004094 ** \nSat.Q                              2.629e-01  4.515e-02   5.824 5.76e-09 ***\nInflMedium:TypeApartment          -1.177e-01  2.086e-01  -0.564 0.572571    \nInflHigh:TypeApartment             1.753e-01  2.279e-01   0.769 0.441783    \nInflMedium:TypeAtrium             -4.068e-01  3.035e-01  -1.340 0.180118    \nInflHigh:TypeAtrium               -1.692e-01  3.294e-01  -0.514 0.607433    \nInflMedium:TypeTerrace             6.292e-03  2.860e-01   0.022 0.982450    \nInflHigh:TypeTerrace              -9.305e-02  3.280e-01  -0.284 0.776633    \nInflMedium:ContHigh               -1.398e-01  2.279e-01  -0.613 0.539715    \nInflHigh:ContHigh                 -6.091e-01  2.800e-01  -2.176 0.029585 *  \nTypeApartment:ContHigh             5.029e-01  2.109e-01   2.385 0.017083 *  \nTypeAtrium:ContHigh                6.774e-01  2.751e-01   2.462 0.013811 *  \nTypeTerrace:ContHigh               1.099e+00  2.675e-01   4.106 4.02e-05 ***\nInflMedium:TypeApartment:ContHigh  5.359e-02  2.862e-01   0.187 0.851450    \nInflHigh:TypeApartment:ContHigh    1.462e-01  3.380e-01   0.432 0.665390    \nInflMedium:TypeAtrium:ContHigh     1.555e-01  3.907e-01   0.398 0.690597    \nInflHigh:TypeAtrium:ContHigh       4.782e-01  4.441e-01   1.077 0.281619    \nInflMedium:TypeTerrace:ContHigh   -4.980e-01  3.671e-01  -1.357 0.174827    \nInflHigh:TypeTerrace:ContHigh     -4.470e-01  4.545e-01  -0.984 0.325326    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 833.66  on 71  degrees of freedom\nResidual deviance: 217.46  on 46  degrees of freedom\nAIC: 610.43\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\n\nClearly there's probably some correlation between satisfaction and the other variables, let's check them out individually.\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-22_b83d59da6f3e471dae5878730084fff6'}\n\n```{.r .cell-code}\n# addterm will check each term individually, and test marginality\naddterm(house_glm, ~. + Sat:(Infl+Type+Cont), test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term additions\n\nModel:\nFreq ~ Infl * Type * Cont + Sat\n         Df Deviance    AIC     LRT   Pr(Chi)    \n<none>        217.46 610.43                      \nInfl:Sat  4   111.08 512.05 106.371 < 2.2e-16 ***\nType:Sat  6   156.79 561.76  60.669 3.292e-11 ***\nCont:Sat  2   212.33 609.30   5.126   0.07708 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nInfluence seems to have the largest impact, so we add this to the model, and type probably has a large\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-23_5c99fe13190006c911ddac2835819797'}\n\n```{.r .cell-code}\nhouse_glm1 <- update(house_glm, .~. + Sat:(Infl + Type + Cont))\nsummary(house_glm1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Freq ~ Infl + Type + Cont + Sat + Infl:Type + Infl:Cont + \n    Type:Cont + Infl:Sat + Type:Sat + Cont:Sat + Infl:Type:Cont, \n    family = poisson, data = housing)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6022  -0.5282  -0.0641   0.5757   1.9322  \n\nCoefficients:\n                                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                        3.135074   0.120112  26.101  < 2e-16 ***\nInflMedium                         0.248327   0.159979   1.552 0.120602    \nInflHigh                          -0.412645   0.184947  -2.231 0.025671 *  \nTypeApartment                      0.292524   0.157477   1.858 0.063231 .  \nTypeAtrium                        -0.792847   0.214413  -3.698 0.000218 ***\nTypeTerrace                       -1.018074   0.221263  -4.601 4.20e-06 ***\nContHigh                          -0.001407   0.169711  -0.008 0.993385    \nSat.L                             -0.098106   0.112592  -0.871 0.383570    \nSat.Q                              0.285657   0.122283   2.336 0.019489 *  \nInflMedium:TypeApartment          -0.017882   0.210496  -0.085 0.932302    \nInflHigh:TypeApartment             0.386869   0.233297   1.658 0.097263 .  \nInflMedium:TypeAtrium             -0.360311   0.304979  -1.181 0.237432    \nInflHigh:TypeAtrium               -0.036788   0.334793  -0.110 0.912503    \nInflMedium:TypeTerrace             0.185154   0.288892   0.641 0.521580    \nInflHigh:TypeTerrace               0.310749   0.334815   0.928 0.353345    \nInflMedium:ContHigh               -0.200060   0.228748  -0.875 0.381799    \nInflHigh:ContHigh                 -0.725790   0.282352  -2.571 0.010155 *  \nTypeApartment:ContHigh             0.569691   0.212152   2.685 0.007247 ** \nTypeAtrium:ContHigh                0.702115   0.276056   2.543 0.010979 *  \nTypeTerrace:ContHigh               1.215930   0.269968   4.504 6.67e-06 ***\nInflMedium:Sat.L                   0.519627   0.096830   5.366 8.03e-08 ***\nInflHigh:Sat.L                     1.140302   0.118180   9.649  < 2e-16 ***\nInflMedium:Sat.Q                  -0.064474   0.102666  -0.628 0.530004    \nInflHigh:Sat.Q                     0.115436   0.127798   0.903 0.366380    \nTypeApartment:Sat.L               -0.520170   0.109793  -4.738 2.16e-06 ***\nTypeAtrium:Sat.L                  -0.288484   0.149551  -1.929 0.053730 .  \nTypeTerrace:Sat.L                 -0.998666   0.141527  -7.056 1.71e-12 ***\nTypeApartment:Sat.Q                0.055418   0.118515   0.468 0.640068    \nTypeAtrium:Sat.Q                  -0.273820   0.149713  -1.829 0.067405 .  \nTypeTerrace:Sat.Q                 -0.032328   0.149251  -0.217 0.828520    \nContHigh:Sat.L                     0.340703   0.087778   3.881 0.000104 ***\nContHigh:Sat.Q                    -0.097929   0.094068  -1.041 0.297851    \nInflMedium:TypeApartment:ContHigh  0.046900   0.286212   0.164 0.869837    \nInflHigh:TypeApartment:ContHigh    0.126229   0.338208   0.373 0.708979    \nInflMedium:TypeAtrium:ContHigh     0.157239   0.390719   0.402 0.687364    \nInflHigh:TypeAtrium:ContHigh       0.478611   0.444244   1.077 0.281320    \nInflMedium:TypeTerrace:ContHigh   -0.500162   0.367135  -1.362 0.173091    \nInflHigh:TypeTerrace:ContHigh     -0.463099   0.454713  -1.018 0.308467    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 833.657  on 71  degrees of freedom\nResidual deviance:  38.662  on 34  degrees of freedom\nAIC: 455.63\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n\n```{.r .cell-code}\n# sum(residuals(house_glm1, type = \"pearson\")^2) / house_glm1$df.residual # dispersion estimate\nhouse_glm1$df.residual # nrow(housing) - length(coef(house_glm1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 34\n```\n:::\n\n```{.r .cell-code}\n# sum(residuals(house_glm1, type = \"deviance\")^2) / house_glm1$df.residual\n# 1 - pchisq(deviance(house_glm1), house_glm1$df.residual) # .267? what's the test here...\n```\n:::\n\n\nSee 202 for rescaling the predictions from this model to the probability scale (by the margin of satisfaction)\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-24_20bfb6f4b77c7a5123a4163321f653e9'}\n\n```{.r .cell-code}\nhnames <- lapply(housing[, -5], levels) # \nhouse_pm <- predict(house_glm1, expand.grid(hnames), type = \"response\") # poisson means exp(\\eta)\nhouse_pm <- matrix(house_pm, ncol = 3, byrow = T, dimnames = list(NULL, hnames[[1]])) # list the predictions into matrix form, columns being satisfaction\ncbind(expand.grid(hnames[-1]), house_pm / rowSums(house_pm)) # normalize by row, and attach the name\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Infl      Type Cont       Low    Medium      High\n1     Low     Tower  Low 0.3955687 0.2601077 0.3443236\n2  Medium     Tower  Low 0.2602403 0.2674072 0.4723526\n3    High     Tower  Low 0.1504958 0.1924126 0.6570916\n4     Low Apartment  Low 0.5427582 0.2308450 0.2263968\n5  Medium Apartment  Low 0.3945683 0.2622428 0.3431889\n6    High Apartment  Low 0.2551503 0.2110026 0.5338470\n7     Low    Atrium  Low 0.4294218 0.3220096 0.2485686\n8  Medium    Atrium  Low 0.2959630 0.3468082 0.3572289\n9    High    Atrium  Low 0.1865151 0.2719420 0.5415429\n10    Low   Terrace  Low 0.6453059 0.2178758 0.1368183\n11 Medium   Terrace  Low 0.5076883 0.2678600 0.2244517\n12   High   Terrace  Low 0.3676505 0.2413550 0.3909945\n13    Low     Tower High 0.2982776 0.2813636 0.4203589\n14 Medium     Tower High 0.1847507 0.2723332 0.5429161\n15   High     Tower High 0.1009787 0.1852058 0.7138155\n16    Low Apartment High 0.4375458 0.2669645 0.2954897\n17 Medium Apartment High 0.2974727 0.2836249 0.4189024\n18   High Apartment High 0.1794106 0.2128413 0.6077481\n19    Low    Atrium High 0.3319072 0.3570404 0.3110524\n20 Medium    Atrium High 0.2157414 0.3626615 0.4215970\n21   High    Atrium High 0.1283298 0.2684145 0.6032556\n22    Low   Terrace High 0.5471602 0.2650171 0.1878227\n23 Medium   Terrace High 0.4044226 0.3060991 0.2894783\n24   High   Terrace High 0.2729568 0.2570580 0.4699852\n```\n:::\n:::\n\n\n### Log Linear Models\n\nlog linear models with iterative proportional scaling is done with function `loglm`.\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-25_dfe587a2c9c02221f3fdcfc6db5c0073'}\n\n```{.r .cell-code}\nloglm(Freq ~ Infl*Type*Cont + Sat*(Infl + Type + Cont), data = housing)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nloglm(formula = Freq ~ Infl * Type * Cont + Sat * (Infl + Type + \n    Cont), data = housing)\n\nStatistics:\n                      X^2 df  P(> X^2)\nLikelihood Ratio 38.66222 34 0.2671359\nPearson          38.90831 34 0.2582333\n```\n:::\n:::\n\n\n### Multinomial Models\n\nThe example data we'll is use party affiliation:\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-26_6b229ec8ae2d455670bedcbfe47eda0e'}\n\n```{.r .cell-code}\n# array version of data\nparty <- array(c(132, 42, 176, 6, 127, 12,\n                 172, 56, 129, 4, 130, 15), \n               dim = c(2, 3, 2),\n               dimnames = list(race = c(\"white\", \"black\"),\n                               party = c(\"democrat\", \"independent\", \"republican\"),\n                               gender = c(\"male\", \"female\")))\nparty_df <- as.data.frame.table(party) # data frame version of data\n\n# Marginal Tables\nrace_party <- margin.table(party, margin = 1:2)\ngender_party <- margin.table(party, margin = c(3, 2))\nrace_gender <- margin.table(party, margin = c(1, 3))\n```\n:::\n\n\n#### nnet\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-27_5cb83ce6e1aa1cfd727802b25886158c'}\n\n```{.r .cell-code}\nparty_mod <- multinom(party ~ race + gender, weights = Freq, party_df) # democrat is the \"reference\"\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  12 (6 variable)\ninitial  value 1099.710901 \niter  10 value 1042.893269\nfinal  value 1042.891187 \nconverged\n```\n:::\n\n```{.r .cell-code}\nsummary(party_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nmultinom(formula = party ~ race + gender, data = party_df, weights = Freq)\n\nCoefficients:\n            (Intercept) raceblack genderfemale\nindependent   0.2855582 -2.278140   -0.5727764\nrepublican   -0.0497550 -1.118276   -0.2201972\n\nStd. Errors:\n            (Intercept) raceblack genderfemale\nindependent   0.1125979 0.3427945    0.1575210\nrepublican    0.1198046 0.2335145    0.1582522\n\nResidual Deviance: 2085.782 \nAIC: 2097.782 \n```\n:::\n:::\n\n\nHypothesis testing with nnet\n\n#### VGAM\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-28_f6853c515028d2f9d0b93398deadf474'}\n\n```{.r .cell-code}\n# VGAM\n# needs version in which \"stimulus factors\" are separated from \"response\" factors.\nhousing_wide <- housing %>% pivot_wider(names_from = \"Sat\", values_from = \"Freq\")\n```\n:::\n\n\n\nOur saturated dataset is one in which every cell is estimated with a parameter.\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-29_6d11130d78057fb500c17bb58da09696'}\n\n```{.r .cell-code}\n# saturated model\nhousing_vglm0 <- vglm(cbind(Low, Medium, High) ~ Infl*Type*Cont, data = housing_wide, family = multinomial)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some\nquantities such as z, residuals, SEs may be inaccurate due to convergence at a\nhalf-step\n```\n:::\n\n```{.r .cell-code}\ndeviance(housing_vglm0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -9.792167e-14\n```\n:::\n:::\n\n\nIn the saturated model, we see that our deviance is equal to 0 because it fits the data perfectly.\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-30_9b7eb0b909e536d4847d89d335fb3f6d'}\n\n```{.r .cell-code}\n# full two way interaction model\nhousing_vglm <- vglm(cbind(Low, Medium, High) ~ (Infl + Type + Cont)^2, data = housing_wide, family = multinomial)\nsummary(housing_vglm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nvglm(formula = cbind(Low, Medium, High) ~ (Infl + Type + Cont)^2, \n    family = multinomial, data = housing_wide)\n\nCoefficients: \n                             Estimate Std. Error z value Pr(>|z|)    \n(Intercept):1              -0.2728053  0.2532115  -1.077  0.28131    \n(Intercept):2              -0.3780064  0.2576046  -1.467  0.14227    \nInflMedium:1                0.1982869  0.3081018   0.644  0.51985    \nInflMedium:2               -0.0083922  0.3171262  -0.026  0.97889    \nInflHigh:1                 -0.9649735  0.3945466  -2.446  0.01445 *  \nInflHigh:2                 -0.8646670  0.3794916  -2.278  0.02270 *  \nTypeApartment:1             1.4992506  0.3153481   4.754 1.99e-06 ***\nTypeApartment:2             0.6555399  0.3307945   1.982  0.04751 *  \nTypeAtrium:1                0.6246612  0.4142176   1.508  0.13154    \nTypeAtrium:2                0.3751106  0.4204712   0.892  0.37233    \nTypeTerrace:1               1.2966645  0.4192741   3.093  0.00198 ** \nTypeTerrace:2               0.4378242  0.4595906   0.953  0.34077    \nContHigh:1                 -0.7397711  0.3116324  -2.374  0.01760 *  \nContHigh:2                 -0.2009809  0.3039959  -0.661  0.50853    \nInflMedium:TypeApartment:1 -1.3982019  0.3535169  -3.955 7.65e-05 ***\nInflMedium:TypeApartment:2 -0.5498060  0.3604313  -1.525  0.12716    \nInflHigh:TypeApartment:1   -0.9211659  0.4576832  -2.013  0.04415 *  \nInflHigh:TypeApartment:2   -0.3138506  0.4358689  -0.720  0.47149    \nInflMedium:TypeAtrium:1    -0.9955454  0.4790778  -2.078  0.03771 *  \nInflMedium:TypeAtrium:2    -0.1868591  0.4525047  -0.413  0.67965    \nInflHigh:TypeAtrium:1       0.1416714  0.5758309   0.246  0.80566    \nInflHigh:TypeAtrium:2       0.2132503  0.5362081   0.398  0.69085    \nInflMedium:TypeTerrace:1   -0.9030966  0.4563464  -1.979  0.04782 *  \nInflMedium:TypeTerrace:2    0.0081197  0.4836471   0.017  0.98661    \nInflHigh:TypeTerrace:1     -0.8443360  0.5889810  -1.434  0.15170    \nInflHigh:TypeTerrace:2     -0.2200667  0.5911664  -0.372  0.70970    \nInflMedium:ContHigh:1       0.0119167  0.2883493   0.041  0.96703    \nInflMedium:ContHigh:2      -0.0735225  0.3046628  -0.241  0.80930    \nInflHigh:ContHigh:1        -0.2258679  0.3504496  -0.645  0.51925    \nInflHigh:ContHigh:2         0.0318502  0.3496997   0.091  0.92743    \nTypeApartment:ContHigh:1    0.1350432  0.3218707   0.420  0.67481    \nTypeApartment:ContHigh:2   -0.0002221  0.3186735  -0.001  0.99944    \nTypeAtrium:ContHigh:1       0.3409407  0.4320950   0.789  0.43009    \nTypeAtrium:ContHigh:2       0.2975587  0.4139167   0.719  0.47221    \nTypeTerrace:ContHigh:1      1.1520381  0.4173381   2.760  0.00577 ** \nTypeTerrace:ContHigh:2      0.6313710  0.4353792   1.450  0.14701    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: log(mu[,1]/mu[,3]), log(mu[,2]/mu[,3])\n\nResidual deviance: 5.9443 on 12 degrees of freedom\n\nLog-likelihood: -102.5404 on 12 degrees of freedom\n\nNumber of Fisher scoring iterations: 3 \n\nNo Hauck-Donner effect found in any of the estimates\n\n\nReference group is level  3  of the response\n```\n:::\n\n```{.r .cell-code}\ndeviance(housing_vglm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.944319\n```\n:::\n\n```{.r .cell-code}\n# Lack of fit tests\n1 - pchisq(deviance(housing_vglm), df.residual(housing_vglm)) # deviance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9188628\n```\n:::\n\n```{.r .cell-code}\n1 - pchisq(sum(residuals(housing_vglm, type = \"pearson\")^2), df.residual(housing_vglm)) # pearson\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.917741\n```\n:::\n:::\n\n\nThe null hypothesis here is that the model is specified correctly. High p-values mean we fail to reject that the model is correct. In general, lack of fit tests are pretty bad tests for telling us any information. We would prefer to do some manual model searching.\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-31_e6a37fc15e34de571ed1bb12ff4dc769'}\n\n```{.r .cell-code}\ndrop1(housing_vglm, test = \"LRT\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\ncbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n          Df Deviance    AIC     LRT Pr(>Chi)  \n<none>         5.9443 277.08                   \nInfl:Type 12  27.0024 274.14 21.0581  0.04954 *\nInfl:Cont  4   6.8284 269.96  0.8841  0.92684  \nType:Cont  6  15.2341 274.37  9.2898  0.15793  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe results here say that we could probably drop `Infl:Cont` and `Type:Cont`. In fact, dropping `Infl:Cont`, we would get the biggest drop in AIC, indicating better model fit for number of parameters we estimate.\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-32_a0193cf2738ac122e3dd0988cd83bebc'}\n\n```{.r .cell-code}\nhousing_vglm1 <- update(housing_vglm, .~. - Infl:Cont)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some\nquantities such as z, residuals, SEs may be inaccurate due to convergence at a\nhalf-step\n```\n:::\n\n```{.r .cell-code}\ndrop1(housing_vglm1, test = \"LRT\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\ncbind(Low, Medium, High) ~ Infl + Type + Cont + Infl:Type + Type:Cont\n          Df Deviance    AIC     LRT Pr(>Chi)  \n<none>         6.8284 269.96                   \nInfl:Type 12  28.2559 267.39 21.4275  0.04446 *\nType:Cont  6  16.1072 267.24  9.2788  0.15850  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nDoing it again shows we could again get lower AIC by dropping either parameter.... an automated way of doing this can be done through `step4vglm`\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-33_e8949ee97bbab76cadd3cd88f9b55128'}\n\n```{.r .cell-code}\n# Forward-Backward Step selection on 2-way interaction model\nhousing_vglm_step <- step4vglm(housing_vglm, direction = \"both\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=277.08\ncbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n\n            Df Deviance    AIC\n- Infl:Cont  4   6.8284 269.96\n- Infl:Type 12  27.0024 274.14\n- Type:Cont  6  15.2341 274.37\n<none>           5.9443 277.08\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some\nquantities such as z, residuals, SEs may be inaccurate due to convergence at a\nhalf-step\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nStep:  AIC=269.96\ncbind(Low, Medium, High) ~ Infl + Type + Cont + Infl:Type + Type:Cont\n\n            Df Deviance    AIC\n- Type:Cont  6  16.1072 267.24\n- Infl:Type 12  28.2559 267.39\n<none>           6.8284 269.96\n+ Infl:Cont  4   5.9443 277.08\n\nStep:  AIC=267.24\ncbind(Low, Medium, High) ~ Infl + Type + Cont + Infl:Type\n\n            Df Deviance    AIC\n- Infl:Type 12   38.662 265.80\n<none>           16.107 267.24\n+ Type:Cont  6    6.828 269.96\n+ Infl:Cont  4   15.234 274.37\n- Cont       2   32.871 280.01\n\nStep:  AIC=265.8\ncbind(Low, Medium, High) ~ Infl + Type + Cont\n\n            Df Deviance    AIC\n<none>           38.662 265.80\n+ Infl:Type 12   16.107 267.24\n+ Type:Cont  6   28.256 267.39\n+ Infl:Cont  4   37.472 272.61\n- Cont       2   54.722 277.86\n- Type       6  100.889 316.03\n- Infl       4  147.780 366.92\n```\n:::\n\n```{.r .cell-code}\nhousing_vglm_step@post$anova # shows the steps that the algorithm took\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Step Df   Deviance Resid. Df Resid. Dev      AIC\n1             NA         NA        12   5.944319 277.0807\n2 - Infl:Cont  4  0.8840624        16   6.828381 269.9648\n3 - Type:Cont  6  9.2787763        22  16.107157 267.2436\n4 - Infl:Type 12 22.5550474        34  38.662205 265.7986\n```\n:::\n:::\n\n\nThe steps the algorithm is saved in the slot `@post$anova`. We can see that the additive model was selected, dropping all the interactions.\n\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-34_dfb7a8ce4e734092f81665821ebe1446'}\n\n```{.r .cell-code}\n# additive model\nhousing_vglm2 <- vglm(cbind(Low, Medium, High) ~ Infl + Type + Cont, data = housing_wide, family = multinomial)\n\nanova(housing_vglm2, housing_vglm, type = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: cbind(Low, Medium, High) ~ Infl + Type + Cont\nModel 2: cbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)  \n1        34     38.662                       \n2        12      5.944 22   32.718  0.06595 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nthere's weak evidence that those dropped coefficients were not zero... so in favor of the more parsimonious and interpretable model, we choose the additive model. Now we do some diagnostics, show the mosaic plot of the pearson chisq values.\n\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-35_833c982d68a019707aec7ce3767b605a'}\n\n```{.r .cell-code}\nsum(residuals(housing_vglm2, \"pearson\")^2) # asymptotically the same\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 38.91043\n```\n:::\n\n```{.r .cell-code}\ndeviance(housing_vglm2) # pretty darn close\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 38.6622\n```\n:::\n\n```{.r .cell-code}\n# grab standardized residuals... I think theses are on the raw scale, need to derive\nhousing_vglm2_stdres <- housing_wide %>% \n  dplyr::select(Infl,Type, Cont) %>% \n  bind_cols(residuals(housing_vglm2, \"stdres\")) %>% \n  pivot_longer(Low:High, names_to = \"Sat\", values_to = \"stdres\")\n\nfoo <- xtabs(stdres~Sat + Infl + Type, data=housing_vglm2_stdres)\n\nfoo\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n, , Type = Tower\n\n        Infl\nSat             Low     Medium       High\n  High    2.3368820  1.7993578  7.6329517\n  Low    -3.1561606 -1.7459604 -5.4886972\n  Medium  0.7896368 -0.1247062 -2.5826432\n\n, , Type = Apartment\n\n        Infl\nSat             Low     Medium       High\n  High   -8.7451504  1.0581085  7.6844538\n  Low     9.5841342 -1.4240255 -5.7464336\n  Medium -0.5702533  0.3521384 -2.3637276\n\n, , Type = Atrium\n\n        Infl\nSat             Low     Medium       High\n  High   -2.3120155  0.8251904  2.0911215\n  Low     0.4934728 -3.1374902 -2.6638237\n  Medium  2.0341706  2.4451851  0.5348104\n\n, , Type = Terrace\n\n        Infl\nSat             Low     Medium       High\n  High   -7.1933577 -4.3805628  2.2529711\n  Low     8.6764258  2.8198862 -1.6825344\n  Medium -1.3182128  1.8356588 -0.6954066\n```\n:::\n\n```{.r .cell-code}\nmosaicplot(foo)\n```\n\n::: {.cell-output-display}\n![](categorical_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-36_04f38c22b81b028b03ef5638bd6e71d7'}\n\n```{.r .cell-code}\nhousing_wide_matrix <- housing_wide %>% dplyr::select(Low:High) %>% \n  data.matrix()\nsat_margin <- housing_wide_matrix %>% rowSums()\n\nhousing_vglm2_predicted <- fitted(housing_vglm2) * sat_margin\n(housing_wide_matrix - housing_vglm2_predicted) / sqrt(housing_vglm2_predicted)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              Low      Medium         High\n [1,] -1.27131702  0.65442726  0.793847557\n [2,]  2.05554037 -0.52448903 -1.131108776\n [3,]  0.48542299  0.00980894 -0.237618822\n [4,]  0.83488118 -0.06530752 -1.226738951\n [5,] -0.52159491  0.72901366 -0.077987991\n [6,]  0.19903520 -0.58897340  0.232680471\n [7,] -0.20002910 -0.40632189  0.725380811\n [8,] -0.09968461 -0.54894903  0.631617806\n [9,]  0.93631742  0.41590003 -0.844215602\n[10,] -0.44816554 -0.29018330  1.339493961\n[11,] -1.27460531  0.60886282  1.251820928\n[12,] -0.50068951 -0.23393207  0.669307443\n[13,] -1.50554299 -0.15670496  1.396421868\n[14,]  0.57743627  0.25994911 -0.520953337\n[15,] -0.07366774 -0.30940893  0.185311590\n[16,]  0.57671852  0.21220772 -0.903490751\n[17,] -0.71913615 -0.80963872  1.272211462\n[18,] -0.77139003  0.70614331  0.001230827\n[19,] -0.19903792  0.10678590  0.091194172\n[20,] -0.59885245  0.37522109  0.080380548\n[21,]  0.96158954 -0.06254569 -0.401788912\n[22,]  0.85710435 -0.33167080 -1.068931434\n[23,]  0.91913589  0.24740415 -1.340806857\n[24,] -0.60596707 -0.06819771  0.512236271\n```\n:::\n\n```{.r .cell-code}\nresiduals(housing_vglm2, type = \"response\") + fitted(housing_vglm2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          Low    Medium      High\n1  0.30000000 0.3000000 0.4000000\n2  0.36956522 0.2391304 0.3913043\n3  0.17543860 0.1929825 0.6315789\n4  0.60396040 0.2277228 0.1683168\n5  0.36440678 0.2966102 0.3389831\n6  0.26530612 0.1836735 0.5510204\n7  0.40625000 0.2812500 0.3125000\n8  0.28571429 0.2857143 0.4285714\n9  0.27272727 0.3181818 0.4090909\n10 0.58064516 0.1935484 0.2258065\n11 0.36585366 0.3170732 0.3170732\n12 0.30434783 0.2173913 0.4782609\n13 0.20000000 0.2714286 0.5285714\n14 0.21250000 0.2875000 0.5000000\n15 0.09677419 0.1612903 0.7419355\n16 0.46706587 0.2754491 0.2574850\n17 0.26815642 0.2513966 0.4804469\n18 0.14705882 0.2450980 0.6078431\n19 0.31746032 0.3650794 0.3174603\n20 0.17857143 0.3928571 0.4285714\n21 0.18421053 0.2631579 0.5526316\n22 0.61290323 0.2473118 0.1397849\n23 0.47692308 0.3230769 0.2000000\n24 0.20833333 0.2500000 0.5416667\n```\n:::\n\n```{.r .cell-code}\nobs_p <- housing_wide_matrix / sat_margin\nfit_p <- fitted(housing_vglm2)\n\nresiduals(housing_vglm2, type = \"response\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           Low        Medium          High\n1  -0.09556873  0.0398922904  5.567644e-02\n2   0.10932496 -0.0282767182 -8.104824e-02\n3   0.02494279  0.0005699035 -2.551270e-02\n4   0.06120222 -0.0031222147 -5.808001e-02\n5  -0.03016154  0.0343673813 -4.205846e-03\n6   0.01015582 -0.0273291797  1.717336e-02\n7  -0.02317183 -0.0407595729  6.393140e-02\n8  -0.01024868 -0.0610938738  7.134255e-02\n9   0.08621220  0.0462397822 -1.324520e-01\n10 -0.06466070 -0.0243274212  8.898812e-02\n11 -0.14183466  0.0492131823  9.262148e-02\n12 -0.06330269 -0.0239637078  8.726640e-02\n13 -0.09827758 -0.0099349949  1.082126e-01\n14  0.02774930  0.0151667891 -4.291609e-02\n15 -0.00420447 -0.0239154916  2.811996e-02\n16  0.02952007  0.0084845677 -3.800464e-02\n17 -0.02931623 -0.0322282664  6.154450e-02\n18 -0.03235176  0.0322567566  9.500771e-05\n19 -0.01444687  0.0080390048  6.407869e-03\n20 -0.03717000  0.0301956212  6.974382e-03\n21  0.05588069 -0.0052566447 -5.062404e-02\n22  0.06574300 -0.0177052766 -4.803772e-02\n23  0.07250046  0.0169777976 -8.947826e-02\n24 -0.06462348 -0.0070579688  7.168145e-02\n```\n:::\n\n```{.r .cell-code}\n# varfun <- object@family@charfun\n# vfun <- varfun(x = NULL, eta = predict(object), \n#                   extra = object@extra, varfun = TRUE)\n#                 ans <- (y - E1)/sqrt(vfun * (1 - c(hatvalues(object))))\n# \n# 1 - hatvalues(housing_vglm2)\n# \n# varfun <- housing_vglm2@family@charfun\n# vfun <- varfun(x = NULL, eta = predict(housing_vglm2), extra = housing_vglm2@extra, varfun = TRUE)\n# \n# housing_vglm2@family@vfamily\n# \n# w <- weights(object, type = \"prior\")\n#                 x <- y * c(w)\n#                 E1 <- E1 * c(w)\n#                 if (any(x < 0) || anyNA(x)) stop(\"all entries of 'x' must be nonnegative and finite\")\n#                 if ((n <- sum(x)) == 0) stop(\"at least one entry of 'x' must be positive\")\n#                 if (length(dim(x)) > 2L) stop(\"invalid 'x'\")\n#                 if (length(x) == 1L) stop(\"'x' must at least have 2 elements\")\n#                 sr <- rowSums(x)\n#                 sc <- colSums(x)\n#                 E <- outer(sr, sc, \"*\")/n\n#                 v <- function(r, c, n) c * r * (n - r) * (n - \n#                   c)/n^3\n#                 V <- outer(sr, sc, v, n)\n#                 dimnames(E) <- dimnames(x)\n#                 ans <- stdres <- (x - E)/sqrt(V)\n# rowSums(fitted)\n# \n# \n# housing_vglm2@y # observed\nplot(housing_vglm2)\n```\n\n::: {.cell-output-display}\n![](categorical_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](categorical_files/figure-html/unnamed-chunk-36-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](categorical_files/figure-html/unnamed-chunk-36-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](categorical_files/figure-html/unnamed-chunk-36-4.png){width=672}\n:::\n\n```{.r .cell-code}\ncoef(housing_vglm2, matrix = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              log(mu[,1]/mu[,3]) log(mu[,2]/mu[,3])\n(Intercept)            0.1387428         -0.2804860\nInflMedium            -0.7348632         -0.2884673\nInflHigh              -1.6126311         -0.9476957\nTypeApartment          0.7356317          0.2999430\nTypeAtrium             0.4079781          0.5393484\nTypeTerrace            1.4123277          0.7457572\nContHigh              -0.4818270         -0.1209751\n```\n:::\n:::\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-37_97216e781eeb0988a522a57a67cc2adb'}\n\n```{.r .cell-code}\n# the predicted probabilities by each of the covariates.\nbind_cols(housing_wide %>% dplyr::select(Infl, Type, Cont),\n          fitted(housing_vglm2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 24 × 6\n   Infl   Type      Cont    Low Medium  High\n   <fct>  <fct>     <fct> <dbl>  <dbl> <dbl>\n 1 Low    Tower     Low   0.396  0.260 0.344\n 2 Medium Tower     Low   0.260  0.267 0.472\n 3 High   Tower     Low   0.150  0.192 0.657\n 4 Low    Apartment Low   0.543  0.231 0.226\n 5 Medium Apartment Low   0.395  0.262 0.343\n 6 High   Apartment Low   0.255  0.211 0.534\n 7 Low    Atrium    Low   0.429  0.322 0.249\n 8 Medium Atrium    Low   0.296  0.347 0.357\n 9 High   Atrium    Low   0.187  0.272 0.542\n10 Low    Terrace   Low   0.645  0.218 0.137\n# … with 14 more rows\n# ℹ Use `print(n = ...)` to see more rows\n```\n:::\n:::\n\n::: {.cell hash='categorical_cache/html/unnamed-chunk-38_aaa9e3df8b2ae1488174d38c0ee855b2'}\n\n```{.r .cell-code}\nanova(housing_vglm1, housing_vglm, type = 1) # score tests not available in VGAM currently\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: cbind(Low, Medium, High) ~ Infl + Type + Cont + Infl:Type + Type:Cont\nModel 2: cbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)\n1        16     6.8284                     \n2        12     5.9443  4  0.88406   0.9268\n```\n:::\n\n```{.r .cell-code}\ndeviance(housing_vglm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.944319\n```\n:::\n\n```{.r .cell-code}\ndropterm(housing_vglm, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSingle term deletions\n\nModel:\ncbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n          Df    AIC     LRT Pr(Chi)  \n<none>       277.08                  \nInfl:Type 12 274.14 21.0581 0.04954 *\nInfl:Cont  4 269.96  0.8841 0.92684  \nType:Cont  6 274.37  9.2898 0.15793  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n## Evaluating Results\n\nThere are a lot of multiclass metrics that are difficult to keep straight, especially as it comes to confusion matrix.\n\n<figure>\n<img src=\"classification/classification_metrics.jpg\" style=\"width:100%\"></img>\n<figcaption></figcaption>\n</figure>\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}