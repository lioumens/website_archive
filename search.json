[
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Personal Documentation",
    "section": "0.1 Quarto",
    "text": "0.1 Quarto\nWelcome to my documentation."
  },
  {
    "objectID": "matrix/matrix.html",
    "href": "matrix/matrix.html",
    "title": "2  Matrix Math",
    "section": "",
    "text": "3 Basic Matrices\nLet j be the vector of all 1’s, and J be the matrix of all ones.\n\\begin{aligned}\njj' &= J \\\\\nj'x  &= \\sum x \\\\\nj'A  &= \\operatorname{colsums}(A) \\\\\nAj &= \\operatorname{rowsums}(A) \\\\\nJx &= \\begin{bmatrix}\n  \\sum x \\\\\n  \\vdots \\\\\n  \\sum x \\\\\n\\end{bmatrix}\n\\end{aligned}\nmatrices in which each matrix row sums to 1.\nThis section aims to quantify and give intuition behind the following matrix norms."
  },
  {
    "objectID": "matrix/matrix.html#example-spectrums",
    "href": "matrix/matrix.html#example-spectrums",
    "title": "2  Matrix Math",
    "section": "4.1 Example Spectrums",
    "text": "4.1 Example Spectrums"
  },
  {
    "objectID": "matrix/matrix.html#spectral-radius-bounds",
    "href": "matrix/matrix.html#spectral-radius-bounds",
    "title": "2  Matrix Math",
    "section": "4.2 Spectral Radius Bounds",
    "text": "4.2 Spectral Radius Bounds\n\n\n\n\n\n\nTheorem 2.3: Adjacency Matrix Spectral Radius Bound (Hong, Shu, and Fang 2001)\n\n\n\n\n\\begin{aligned}\n\\rho(A) \\leq \\frac{d_{min} -1 + \\sqrt{(d_{min} + 1)^2 + 4(2m-nd_{min})}}{2}\n\\end{aligned}\n\n\nd_{min} is minimum degree on the graph\nn,m is number of nodes, edges in the graph\nequality is reached for regular graphs, or bi-degreed graphs of either d_{min} or n-1\n\nMore simply, for a simply connected graph d_{min} = 1 the expression simplifies\n\n\\begin{align*}\n\\rho(A) \\leq \\sqrt{1 + 2m - n}\n\\end{align*}\n\\tag{4.1}\n\nequality reached for complete or star graph\n\n\n\n\n\nCode\nhong_upper_adj_spectral <- function(d_min, n, m, easy = FALSE) {\n  if (easy) return(sqrt(1 + 2*m - n))\n  (d_min - 1 + sqrt((d_min + 1)^2 + 4 * (2 * m - n * d_min))) / 2\n}\n\n\n\n\nCode\ng5_list <- read_file6(\"data/graph5c.g6\", type = \"igraph\")\n\nspectrum(g5_list[[5]], which = list(pos = \"LM\", howmany = 1))$values\n\n\n[1] 2.135779\n\n\nCode\neigen(as_adj(g5_list[[2]]))$values[1]\n\n\n[1] 1.847759\n\n\nCode\ng5_hong_df <- tibble(g = g5_list) |> \n  rownames_to_column(\"g_id\") |> \n  rowwise() |> \n  mutate(min_deg = min(degree(g)),\n         n_edge = ecount(g),\n         n_vertex = vcount(g),\n         spectral_radius = eigen(as_adj(g))$values[1],\n         upper_spectral = hong_upper_adj_spectral(min_deg, n_vertex, n_edge),\n         easy_upper_spectral = hong_upper_adj_spectral(min_deg, n_vertex, n_edge, easy = TRUE)) |> \n  arrange(upper_spectral, easy_upper_spectral, spectral_radius)\n\ng5_hong_df |> ggplot(aes(factor(g_id, unique(g_id)), spectral_radius)) +\n  geom_point(aes(color = \"Spectral Radius\")) +\n  geom_line(aes(y = upper_spectral, group = 1, color = \"Hong Upper Bound\", linetype = \"Hong Upper Bound\")) +\n  geom_line(aes(y = easy_upper_spectral, group = 1, color = \"Hong Easy Upper Bound\", linetype = \"Hong Easy Upper Bound\")) +\n  guides(colour = guide_legend(override.aes = list(shape = c(26, 26, 19), linetype = c(1, 2, 0)))) +\n  scale_color_manual(\"\", values = c(\"red\", \"red\", \"black\")) +\n  scale_shape(guide = \"none\") + \n  scale_linetype(guide = \"none\") +\n  labs(x = \"graph\",\n       y = \"Adj. Spectral Radius\",\n       title = \"Hong Spectral Radius of Adjacency for all connected 5-graphs\")"
  },
  {
    "objectID": "matrix/matrix.html#example-spectrums-1",
    "href": "matrix/matrix.html#example-spectrums-1",
    "title": "2  Matrix Math",
    "section": "5.1 Example Spectrums",
    "text": "5.1 Example Spectrums\n\nComponentsComplete GraphStar GraphBipartite GraphPath\n\n\nIf there are 2 connected components, 2 of the eigenvalues will be 0.\n\n\nCode\ng1 <- graph(~1-2-3-1-4-5-6-4)\nL1 <- laplacian_matrix(g1)\nplot(g1)\n\n\n\n\n\nCode\neigen(L1)$values |> zapsmall()\n\n\n[1] 4.561553 3.000000 3.000000 3.000000 0.438447 0.000000\n\n\nCode\ng2 <- graph(~1-2-3-1, 4-5-6-4)\nplot(g2)\n\n\n\n\n\nCode\nL2 <- laplacian_matrix(g2)\neigen(L2)$values |> zapsmall()\n\n\n[1] 3 3 3 3 0 0\n\n\n\n\n\n\nCode\ng <- make_full_graph(5)\nL <- laplacian_matrix(g)\nplot(g)\n\n\n\n\n\nCode\neigen(L)$values |> zapsmall()\n\n\n[1] 5 5 5 5 0\n\n\n\n\n\n\nCode\ng <- make_star(5, mode = \"undirected\")\nL <- laplacian_matrix(g)\nplot(g)\n\n\n\n\n\nCode\neigen(L)$values |> zapsmall()\n\n\n[1] 5 1 1 1 0\n\n\n\n\n\n\nCode\ng <- make_full_bipartite_graph(5, 3)\nL <- laplacian_matrix(g, normalized = F)\nplot(g)\n\n\n\n\n\nCode\neigen(L)$values |> zapsmall()\n\n\n[1] 8 5 5 3 3 3 3 0\n\n\n\n\n\n\nCode\ng <- make_graph(~1-2-3-4-5-6-7-8-9-10)\nL <- laplacian_matrix(g, normalized = F)\nplot(g)\n\n\n\n\n\nCode\neigen(L)$values |> zapsmall()\n\n\n [1] 3.902113 3.618034 3.175571 2.618034 2.000000 1.381966 0.824429 0.381966\n [9] 0.097887 0.000000"
  },
  {
    "objectID": "matrix/matrix.html#spectral-radius-bounds-1",
    "href": "matrix/matrix.html#spectral-radius-bounds-1",
    "title": "2  Matrix Math",
    "section": "5.2 Spectral Radius Bounds",
    "text": "5.2 Spectral Radius Bounds\n\n5.2.1 Simple\nThere are 3 simple bounds on the Laplacian presented by (Anderson and Morley 1985),\n\nnumber of nodes in graph\n\n \\lambda_1(L) \\leq n\n\nequality reached on complete graph, for example.\n\n\nmax degree\n\n \\lambda_1(L) \\leq 2d_{max}\n\nequality nears on a path graph.\n\n\nmaximal ends of an edge\n\n \\lambda_1(L) \\leq \\max_{i\\sim j} w_i + w_j \n\nwhere maximum is over edges in graph, and w_i + w_j is the sum of weights of that edge’s endpoints.\nI believe this is true over any weighted, connected, undirected graph\nequality is reached when bipartite graph"
  },
  {
    "objectID": "matrix/matrix.html#laplacian-for-distributed-summation",
    "href": "matrix/matrix.html#laplacian-for-distributed-summation",
    "title": "2  Matrix Math",
    "section": "5.3 Laplacian for Distributed Summation",
    "text": "5.3 Laplacian for Distributed Summation\n\n\n\n\n\n\nSource\n\n\n\nThis insight comes from Sivan Toledo, where he has a nice description of the problem.\n\n\nIf the eventual algorithm goal is to have the sum of the graph sitting on every node, the repeated application of a matrix multiplication should converge to the matrix of all ones J. The spectrum of J is simply n and the rest zeros. \\frac{1}{n}J is special because it sets each node to have the mean, and the single non-zero eigenvalue is 1.\n\n\nCode\neigen(matrix(rep(1/4, 16), nrow = 4))$values |> zapsmall()\n\n\n[1] 1 0 0 0\n\n\nSuppose the initial state of the graph y is values at each of the n nodes of the graph.\n\n\\begin{aligned}\nn(I - \\frac{1}{n}L)^ky\n\\end{aligned}\n\nAs k increases, we’d expect that the matrix spectrum converges to match J, since L has a zero eigenvalue, one eigen value limit is 1. And since the Laplacian has spectral radius upper bounded by number of nodes,\n\n\nCode\nset.seed(1)\ng <- sample_gnp(10, .5)\nA <- g %>% as_adj()\nL <- Diagonal(x = rowSums(A)) - A\n\n\n\n\nCode\nJ <- Matrix(1, nrow = 10, ncol = 10)\nJ %*% cbind(1:10)\n\n\n10 x 1 Matrix of class \"dgeMatrix\"\n      [,1]\n [1,]   55\n [2,]   55\n [3,]   55\n [4,]   55\n [5,]   55\n [6,]   55\n [7,]   55\n [8,]   55\n [9,]   55\n[10,]   55\n\n\n\n\nCode\nJe <- eigen(J)\nLn <- laplacian_matrix(g, normalized = TRUE)\neigen(Ln)\n\n\neigen() decomposition\n$values\n [1] 1.647271e+00 1.467555e+00 1.450327e+00 1.166667e+00 1.084726e+00\n [6] 1.000000e+00 9.451136e-01 7.853920e-01 4.529494e-01 2.442491e-15\n\n$vectors\n             [,1]        [,2]         [,3]          [,4]        [,5]\n [1,] -0.16557306 -0.35884141  0.414024644  4.378811e-16 -0.41543869\n [2,]  0.01245928  0.32716246 -0.261033301  7.071068e-01 -0.26421090\n [3,]  0.07815309  0.07003764  0.522362456 -2.958733e-16  0.30735544\n [4,]  0.49476837 -0.29623231 -0.126497326  3.013764e-16 -0.34924549\n [5,] -0.26126348 -0.45016699 -0.226430018  3.655149e-16 -0.17830741\n [6,]  0.01453893 -0.34738481 -0.412927706 -6.383782e-16  0.64394097\n [7,]  0.29116939 -0.06983555  0.410643861 -1.133005e-16  0.11871973\n [8,]  0.39734608  0.39945650 -0.007375584 -5.551115e-17  0.08547476\n [9,]  0.01245928  0.32716246 -0.261033301 -7.071068e-01 -0.26421090\n[10,] -0.64049820  0.27700977  0.113930213 -2.030681e-18  0.05918033\n               [,6]        [,7]       [,8]        [,9]      [,10]\n [1,] -3.131423e-15  0.23529056  0.5901861  0.09578594 -0.2948839\n [2,] -3.906316e-15  0.22258023 -0.1102821  0.25044926 -0.3611576\n [3,] -6.324555e-01 -0.02451212 -0.2642891  0.25347743 -0.2948839\n [4,] -3.162278e-01  0.13949439 -0.2339562 -0.57753213 -0.1474420\n [5,]  6.578071e-15 -0.66595972 -0.2431392  0.18631243 -0.3296902\n [6,] -1.421198e-15  0.30002682  0.2257963 -0.02773400 -0.3900947\n [7,]  7.071068e-01  0.04045948 -0.3410158 -0.03156311 -0.3296902\n [8,]  6.679915e-15 -0.53861397  0.5160385 -0.17841799 -0.2948839\n [9,] -2.959103e-15  0.22258023 -0.1102821  0.25044926 -0.3611576\n[10,] -1.665335e-16  0.01531268 -0.1004178 -0.63187856 -0.2948839\n\n\nCode\n# not correct\n# M <- diag(10) - L / (diag(L) + 1) # direct averaging of over all nodes ***\n# M <- diag(L) - L/2 # average across edges?\n\n# correct\nM <- diag(10) - L / 10\n\nmat_pow <- function(M, t = 10) {\n  if (t == 1) {return(M)}\n  return(M %*% mat_pow(M, t - 1))\n}\n# mat_pow(M, 100)\n\ny <- cbind(1:10)\nfor (i in 1:100) {\n  y <- M %*% y\n}\ny\n\n\n10 x 1 Matrix of class \"dgeMatrix\"\n          [,1]\n [1,] 5.499981\n [2,] 5.499980\n [3,] 5.499978\n [4,] 5.500109\n [5,] 5.499981\n [6,] 5.499988\n [7,] 5.499989\n [8,] 5.499992\n [9,] 5.499980\n[10,] 5.500024"
  },
  {
    "objectID": "matrix/matrix.html#normalized-laplacian",
    "href": "matrix/matrix.html#normalized-laplacian",
    "title": "2  Matrix Math",
    "section": "5.4 Normalized Laplacian",
    "text": "5.4 Normalized Laplacian\nThe primary reason for looking at the normalized laplacian is because it removes dependence on the number of nodes in the graph, which would change bounds. Rather, the eigenvalues of a normalized laplacian will range from 0 \\leq 2, reaching 2 for bipartite graphs.\n\n\nCode\nD <- 1 / sqrt(diag(L))\nNL <- Diagonal(x = D) %*% L %*% Diagonal(x = D)\nNL\n\n\n10 x 10 sparse Matrix of class \"dgCMatrix\"\n                                                                            \n [1,]  1.0000000 -0.2041241  .          .    .         -0.1889822  .        \n [2,] -0.2041241  1.0000000 -0.2041241  .   -0.1825742 -0.1543033 -0.1825742\n [3,]  .         -0.2041241  1.0000000  .   -0.2236068 -0.1889822  .        \n [4,]  .          .          .          1.0  .          .          .        \n [5,]  .         -0.1825742 -0.2236068  .    1.0000000  .         -0.2000000\n [6,] -0.1889822 -0.1543033 -0.1889822  .    .          1.0000000 -0.1690309\n [7,]  .         -0.1825742  .          .   -0.2000000 -0.1690309  1.0000000\n [8,] -0.2500000  .          .          .   -0.2236068 -0.1889822  .        \n [9,] -0.2041241 -0.1666667 -0.2041241  .   -0.1825742 -0.1543033 -0.1825742\n[10,]  .          .          .         -0.5  .         -0.1889822 -0.2236068\n                                      \n [1,] -0.2500000 -0.2041241  .        \n [2,]  .         -0.1666667  .        \n [3,]  .         -0.2041241  .        \n [4,]  .          .         -0.5000000\n [5,] -0.2236068 -0.1825742  .        \n [6,] -0.1889822 -0.1543033 -0.1889822\n [7,]  .         -0.1825742 -0.2236068\n [8,]  1.0000000  .         -0.2500000\n [9,]  .          1.0000000  .        \n[10,] -0.2500000  .          1.0000000\n\n\n\n5.4.1 Example Spectrums of Normalized Laplacian\n\nRingPathCompleteBipartite\n\n\n\n\nCode\ng <- make_ring(5)\nplot(g)\n\n\n\n\n\nCode\nN <- laplacian_matrix(g, normalized = TRUE)\n# 1 - cos(2 * pi * 0:4 / 5) |> sort() # exact\neigen(N)$values |> zapsmall()\n\n\n[1] 1.809017 1.809017 0.690983 0.690983 0.000000\n\n\n\n\n\n\nCode\ng <- graph(~1-2-3-4-5)\nN <- laplacian_matrix(g, normalized = TRUE)\n# 1 - cos(pi * 0:4 / 4) |> sort() # exact\neigen(N)$values |> zapsmall()\n\n\n[1] 2.0000000 1.7071068 1.0000000 0.2928932 0.0000000\n\n\n\n\n\n\nCode\ng <- make_full_graph(5)\nN <- laplacian_matrix(g, normalized = TRUE)\neigen(N)$values |> zapsmall() # n / n-1\n\n\n[1] 1.25 1.25 1.25 1.25 0.00\n\n\n\n\n\n\nCode\ng <- make_full_bipartite_graph(6, 4)\nN <- laplacian_matrix(g, normalized = TRUE)\neigen(N)$values |> zapsmall() # 0, 1 (n + m - 2), 2\n\n\n [1] 2 1 1 1 1 1 1 1 1 0"
  },
  {
    "objectID": "matrix/matrix.html#fiedler-bounds-normalized-laplacian",
    "href": "matrix/matrix.html#fiedler-bounds-normalized-laplacian",
    "title": "2  Matrix Math",
    "section": "5.5 Fiedler Bounds (normalized Laplacian)",
    "text": "5.5 Fiedler Bounds (normalized Laplacian)\nFiedler Eigenvalue is the smallest non-zero eigenvalue.\n\n5.5.1 Simple\nFor k-regular graph, and diameter > 4, we have\n\n\\begin{aligned}\n\\limsup \\lambda_{n-1} \\leq 1 - 2\\frac{\\sqrt{k-1}}{k}\n\\end{aligned}\n\n\nequality is reached for ramanujan graphs\nnote that the conditions exclude bipartite graphs\nthe fact that it’s “lim sup” makes this quite useless, because it’s asymptotically toward infinity”. In fact, for some regular 4 graphs, 14 node, diameter 5 graphs, the Fiedler value is still well above this bound.\n\nRather, a more general (and useful upper bound):\n\n\n\n\n\n\nLemma 1.14: Fiedler Upper Bound (dia \\geq 4) (Chung 1997)\n\n\n\nLet G be a graph with diameter D \\geq 4, and let k denote the maximum degree of G. Then,\n\n\\begin{aligned}\n\\lambda_{n-1} \\leq 1 - 2\\frac{\\sqrt{k-1}}{k}\\left(1 - \\frac{2}{D}\\right) + \\frac{2}{D}\n\\end{aligned}\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy contracting weighted graphs, and using Rayleigh quotient to upper bound.\n\n\n\n\n\nNote that the original reference (Nilli 1991) , studies this problem for non-standardized Laplacian.\n\n\nCode\nupper_fiedler_nilli <- function(k, D) {\n  if (D < 4) abort(\"upper bound only valid when diameter greater than 4\")\n  return(1 - 2 * sqrt(k -1) / k * (1 - 2 / D) + 2 / D)\n}\n\n\nLet’s generate all permutations of connected 8 graphs, and pick out those with diameter greater than 4. All graph generation is done with the program geng and filtered with pickg (McKay and Piperno 2013).\n\n\nCode\ng8_dia4plus_list <- read_file6(\"data/graph8cdia4p.g6\", type = \"igraph\")\n\ng8_dia4plus_df <- tibble(g = g8_dia4plus_list) |> \n  rownames_to_column(\"g_id\") |> \n  rowwise() |> \n  mutate(fiedler = net_fiedler(g, normalize = T),\n         k = max(degree(g)),\n         D = diameter(g),\n         upper_fiedler = upper_fiedler_nilli(k, D)) |> \n  arrange(desc(upper_fiedler), desc(fiedler))\n  \ng8_dia4plus_df |> \n  ggplot(aes(x = factor(g_id, levels = unique(g_id)),\n             y = fiedler, color = \"Fiedler Value\")) +\n  geom_point(size = .5, alpha = .7) +\n  geom_line(aes(y = upper_fiedler, group = upper_fiedler, color = \"Upper Bound\")) +\n  labs(color = \"Color\",\n       title = \"Nilli Bound on Fiedler Value for connected-8 Graphs\",\n       x = \"Unique Graph Combinations\",\n       y = \"Fiedler Value\") +\n  scale_color_manual(values = c(\"black\", \"red\")) +\n  scale_x_discrete(breaks = NULL, labels = NULL) + \n  theme_minimal()\n\n\n\n\n\n\n\nCode\n# files with regular, diameter larger than 4\n# These are all 4 regular graphs, connected, 13/14 nodes, d\nreg4_dia4plus_files <- c(\"data/reg4c13dia4.g6\", \"data/reg4c13dia5.g6\", \"data/reg4c14dia4.g6\", \"data/reg4c14dia5.g6\") \nreg4_dia4plus_list <- reg4_dia4plus_files |> \n  map(read_file6, type = \"igraph\")\n\nreg4_dia4plus_df <- tibble(g = reg4_dia4plus_list, path = reg4_dia4plus_files)  |> \n  unnest_longer(g, indices_include = TRUE, indices_to = \"graph_id\") |> \n  rowwise() |> \n  mutate(match = list(str_match(path, \"data/reg4c(\\\\d*)dia(\\\\d*)\")),\n         nodes = match[2],\n         dia = match[3],\n         k = 4,\n         fiedler = net_fiedler(g, normalized = T),\n         upper_fiedler = 1 - 2*sqrt(4-1) / 4) |> \n  select(-c(match, path)) |> arrange(desc(dia), nodes)\n\nreg4_dia4plus_df <- reg4_dia4plus_df |> mutate(dia = as.numeric(dia),\n                           islower = upper_fiedler > fiedler,\n                           other_upper = 1 - 2 * sqrt(4 - 1)/4 * (1 - 2 / dia) + 2/dia,\n                           other_islower = other_upper > fiedler) |> \n  arrange(islower, other_islower, desc(dia))\n\n# among regular graphs, 13-14 nodes, 4+ diameter\nreg4_dia4plus_df\n\n\n# A tibble: 4,271 × 10\n# Rowwise: \n   g        graph_id nodes   dia     k fiedler upper_f…¹ islower other…² other…³\n   <list>      <int> <chr> <dbl> <dbl>   <dbl>     <dbl> <lgl>     <dbl> <lgl>  \n 1 <igraph>        1 13        4     4   0.203     0.134 FALSE      1.07 TRUE   \n 2 <igraph>        2 13        4     4   0.250     0.134 FALSE      1.07 TRUE   \n 3 <igraph>        3 13        4     4   0.207     0.134 FALSE      1.07 TRUE   \n 4 <igraph>        4 13        4     4   0.205     0.134 FALSE      1.07 TRUE   \n 5 <igraph>        5 13        4     4   0.204     0.134 FALSE      1.07 TRUE   \n 6 <igraph>        6 13        4     4   0.257     0.134 FALSE      1.07 TRUE   \n 7 <igraph>        7 13        4     4   0.255     0.134 FALSE      1.07 TRUE   \n 8 <igraph>        8 13        4     4   0.257     0.134 FALSE      1.07 TRUE   \n 9 <igraph>        9 13        4     4   0.188     0.134 FALSE      1.07 TRUE   \n10 <igraph>       10 13        4     4   0.192     0.134 FALSE      1.07 TRUE   \n# … with 4,261 more rows, and abbreviated variable names ¹​upper_fiedler,\n#   ²​other_upper, ³​other_islower\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nreg4_dia4plus_df |> unite(col = \"g_uid\", !!!c(\"nodes\", \"dia\", \"graph_id\"), remove = FALSE) |>\n  ggplot(aes(fct_reorder2(g_uid, fiedler, dia, .desc = T), fiedler)) +\n  geom_point() + \n  geom_point(aes(y = upper_fiedler), color = \"red\") +\n  geom_point(aes(y = other_upper), color = \"red\")\n\n\n\n\n\nCode\n# reg4_dia4plus_df\n# reg4_dia4plus_df # well... they are all lower bounded? if it's an infinite family of regular graphs... it should get closer right? b/c it's lim sup?\n\n\n\n\n5.5.2 By Volume (global)\nA loose lower bound for the smallest non-trivial eigenvalue (Chung 1997, 7):\n\n\\begin{aligned}\n\\lambda_{n-1} \\geq \\frac{1}{D\\operatorname{vol}(G)}\n\\end{aligned}\n where D is the diameter of the graph, and volume of the graph is the sum of degrees for each node.\n\n\n\n\n\n\nDefinition of Volume\n\n\n\nNote that Chung (1997) uses the definition \\operatorname{vol}(G) = \\sum_{x\\in S}d_x where d_x is the degree of vertex x. Other references use \\operatorname{vol}(G) = |G| which is the number of nodes, or giving each node weight 1.\n\n\nLet’s find the Fiedler value of every graph of size 5 and graph them against the bound,\n\n\nCode\ng5_list <- read_file6(\"data/graph5c.g6\", type = \"igraph\")\n\n# fiedler values\nfiedler5 <- lapply(g5_list, function(g) {\n  vol <- sum(degree(g)) # conservative\n  # vol <- vcount(g)\n  M <- laplacian_matrix(g, normalized = TRUE)\n  dia <- diameter(g, directed = FALSE)\n  list(g = list(g),\n       fiedler = eigen(M)$values[4],\n       vol = vol,\n       dia = dia)\n})\n\ngraph_fiedler <- tibble(gg = fiedler5) |> unnest_wider(gg) |> unnest(cols = c(g)) |> \n  mutate(bound = 1/ (dia * vol),\n         diff = fiedler - bound) |> \n  arrange(diff)\n\n\n# calculate the lower bound.\nfiedler_bound_line <- tibble(dia = rep(list(seq(1, 4, .1)), 3), vol = c(5, 10, 20)) |> rowwise() |> \n  mutate(bound = list(1 / vol / dia)) |> \n  unnest_longer(col = c(dia, bound))\n\n\n# The bound is generally quite bad for graphs \ngraph_fiedler |> ggplot(aes(dia, fiedler, color = vol)) +\n  geom_line(data = fiedler_bound_line, mapping = aes(dia, bound, color = vol, group = vol)) +\n  geom_point() +\n  labs(title = \"Simple Fiedler Bound\")\n\n\n\n\n\nEven using the vertex count definition for volume (less conservative), the bound is quite low for most of the Fiedler values.\n\n\n5.5.3 By Cheeger’s (Sparsest Cut)\nCheeger’s constant is loosely defined in english, as the minimal ratio, of cost of cutting edges, to the size of sets it cuts off. That is, a “dumbbell” shape graph, where large vertex sets are on both side, and only cutting 1 edge in the middle would have a very very low cheeger constant.\n\n\\begin{aligned}\nh_G &= \\min_S \\frac{|\\delta S|}{\\min \\{|S|,|\\bar S| \\}} \\\\&= \\frac{\\text{cutting edges cost}}{\\text{vertex set volume}}\n\\end{aligned}\n\nCalculating Cheeger’s constant is an NP-Hard problem, meaning that the problem is likely non-polynomial for solution and checking.\nThe bounds on Fiedler’s value, with cheegers constants have the form,\n\n\\begin{aligned}\n\\frac{h_G^2}{2d_{max}} \\leq \\lambda_{n-1}  \\leq 2h_G\n\\end{aligned}\n\n\n\nCode\n# graph_boundary_edges <- function(g, vs) {\n#   if(!inherits(g, \"igraph\")) {\n#     abort(\"`g` must be of class `igraph`\")\n#   }\n#   inc_edges <- igraph::incident_edges(g, vs)\n#   purrr::reduce()\n# }\n\n# graph_interior_edges <- function(g, vs) {\n#   if(!inherits(g, \"igraph\")) {\n#     abort(\"`g` must be of class `igraph`\")\n#   }\n#   inc_edges <- igraph:incident_edges(g, vs)\n# }\n\n\n# forget the functions... just do on matrices\n\n\n\n\nCode\n#TODO: implemented assuming undirected graph, generalize?\ncheeger <- function(A, S) {\n  # calculate vertex boundary by taking neighbors minus initial set\n  adj_vec_set <- A[S,, drop = F] |> colSums()\n  ego1 <- which(adj_vec_set > 0)\n  dS <- setdiff(ego1, S) # vertex boundary\n  \n  # calculate edge boundary by subsetting matrix from S to !S\n  bridge_matrix <- A[S, dS, drop = F]\n  \n  bridge_edges <- bridge_matrix |> \n    as(\"lgCMatrix\") |> # also assumes unweighted here when converting to logical\n    which(arr.ind = T) \n  bridge_edges[,\"row\"] <- S[bridge_edges[,\"row\"]] # convert indices\n  bridge_edges[,\"col\"] <- dS[bridge_edges[,\"col\"]]\n  vol_dS <- sum(bridge_matrix)\n  min_vol_S <- min(length(S), nrow(A) - length(S)) # use length as volume metric\n  return(vol_dS / min_vol_S)\n}\n\n# generate subsets for each graph\nall_comb <- mapply(function(x) combn(5, x, simplify = F), 1:5)\nall_comb_df <- tibble(elem = all_comb) |>\n  unnest_longer(elem)\n\ngraph_df <- g5_list |> lapply(as_adj) |>\n  tibble(A = _) |> rownames_to_column(var = \"id\") |> \n  mutate(id = as.numeric(id))\n\ncheeger_df <- full_join(graph_df, all_comb_df, by = character()) |> rowwise() |> \n  mutate(cheeger_values = cheeger(A, elem))\n\ncheeger_constant_df <- cheeger_df |> filter(cheeger_values > 0) |>  group_by(id) |> arrange(id, cheeger_values) |> slice(1) |> ungroup()\n\n# cheeger_constant_df |> slice(2) |> unlist()\n\nfiedler_value_df <- tibble(gg = fiedler5) |> unnest_wider(gg) |> unnest(cols = c(g)) |> \n  rownames_to_column(var = \"id\") |> \n  rowwise() |> \n  mutate(id = as.numeric(id),\n         max_deg = max(degree(g))) |> \n  select(id, fiedler, dia, vol, max_deg) |> \n  ungroup()\n\n\ncheeger_fiedler_df <- cheeger_constant_df |> select(id, elem, cheeger_values) |> left_join(fiedler_value_df, by = \"id\") |> \n  mutate(upper_cheeger = cheeger_values * 2,\n         lower_cheeger = cheeger_values^2 / 2/ max_deg) # dividing by max deg\n# cheeger_fiedler_df |> mutate(inbound = fiedler > lower_cheeger & fiedler < upper_cheeger)\n\ncheeger_fiedler_df |>\n  ggplot() +\n  geom_point(aes(id, lower_cheeger), color = \"red\") +\n  geom_point(aes(id, fiedler, color = vol)) +\n  geom_point(aes(id, upper_cheeger), color = \"red\") +\n  labs(y = \"Eigenvalue\",\n       x = \"Graph ID\",\n       title = \"\")\n\n\n\n\n\nCode\n# many variations of cheeger's unfortunately\n# seems incorrect for"
  },
  {
    "objectID": "matrix/matrix.html#laplacian-decomposition-as-incidence-matrix",
    "href": "matrix/matrix.html#laplacian-decomposition-as-incidence-matrix",
    "title": "2  Matrix Math",
    "section": "5.6 Laplacian decomposition as incidence matrix",
    "text": "5.6 Laplacian decomposition as incidence matrix\nIf we define an incidence matrix as a |V| \\times |E| matrix, in which each column has a 1 and -1 for in positions the edge connects the vertices,\n\n\nCode\n# ve_incidence <- function(g) {\n#   stopifnot(class(g) == \"igraph\")\n#   g %>% get.adjedgelist() %>% map_dbl(~-onehot(.x, n = ecount(g)))\n# }\n\n\nonehot <- function(x, n = max(x)) {\n  y <- vector(mode = \"numeric\",length = n)\n  y[x] <- 1\n  return(y)\n}\n\n\n# edge list\nve_incidence_matrix <- function(g) {\n  stopifnot(class(g) == \"igraph\")\n  onehot_edge <- function(x, n = max(x)) {\n    y <- vector(mode = \"numeric\",length = n)\n    y[x * sign(x)] <- sign(x)\n    return(y)\n  }\n  g_el <- get.edgelist(g) \n  g_el[,2] <- -g_el[,2]\n  g_el %>% apply(1, onehot_edge, n = vcount(g))\n}\n\nset.seed(1)\ng <- sample_gnp(10, .5)\nA <- g %>% as_adj()\nL <- Diagonal(x = rowSums(A)) - A\nB <- ve_incidence_matrix(g) # incidence matrix\n\n\nOur incidence matrix B looks like\n\n\nB = \\begin{bmatrix} 1 &0 &0 &0 &1 &0 &0 &0 &0 &0 &1 &0 &0 &1 &0 &0 &0 &0 &0 &0 &0 &0 &0 \\\\-1 &1 &1 &0 &0 &1 &0 &1 &0 &0 &0 &0 &0 &0 &1 &0 &0 &0 &0 &0 &0 &0 &0 \\\\0 &-1 &0 &1 &0 &0 &1 &0 &0 &0 &0 &0 &0 &0 &0 &1 &0 &0 &0 &0 &0 &0 &0 \\\\0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &1 &0 &0 &0 \\\\0 &0 &-1 &-1 &0 &0 &0 &0 &1 &0 &0 &1 &0 &0 &0 &0 &1 &0 &0 &0 &0 &0 &0 \\\\0 &0 &0 &0 &-1 &-1 &-1 &0 &0 &1 &0 &0 &1 &0 &0 &0 &0 &1 &0 &0 &1 &0 &0 \\\\0 &0 &0 &0 &0 &0 &0 &-1 &-1 &-1 &0 &0 &0 &0 &0 &0 &0 &0 &1 &0 &0 &1 &0 \\\\0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &-1 &-1 &-1 &0 &0 &0 &0 &0 &0 &0 &0 &0 &1 \\\\0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &-1 &-1 &-1 &-1 &-1 &-1 &0 &0 &0 &0 \\\\0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &-1 &-1 &-1 &-1 \\\\ \\end{bmatrix}\n\n\n\n\nL= \\begin{bmatrix} 4 &-1 &0 &0 &0 &-1 &0 &-1 &-1 &0 \\\\-1 &6 &-1 &0 &-1 &-1 &-1 &0 &-1 &0 \\\\0 &-1 &4 &0 &-1 &-1 &0 &0 &-1 &0 \\\\0 &0 &0 &1 &0 &0 &0 &0 &0 &-1 \\\\0 &-1 &-1 &0 &5 &0 &-1 &-1 &-1 &0 \\\\-1 &-1 &-1 &0 &0 &7 &-1 &-1 &-1 &-1 \\\\0 &-1 &0 &0 &-1 &-1 &5 &0 &-1 &-1 \\\\-1 &0 &0 &0 &-1 &-1 &0 &4 &0 &-1 \\\\-1 &-1 &-1 &0 &-1 &-1 &-1 &0 &6 &0 \\\\0 &0 &0 &-1 &0 &-1 &-1 &-1 &0 &4 \\\\ \\end{bmatrix}\n\n\nJust to see the sparsity pattern of B, (we try spam’s display routine)"
  },
  {
    "objectID": "matrix/matrix.html#laplacian-stochastic-matrix",
    "href": "matrix/matrix.html#laplacian-stochastic-matrix",
    "title": "2  Matrix Math",
    "section": "5.7 Laplacian Stochastic Matrix",
    "text": "5.7 Laplacian Stochastic Matrix\nI - D^{\\dagger}A"
  },
  {
    "objectID": "stochastic/stochastic.html#gamblers-ruin",
    "href": "stochastic/stochastic.html#gamblers-ruin",
    "title": "3  Stochastic",
    "section": "3.1 Gamblers Ruin",
    "text": "3.1 Gamblers Ruin\nThe random walk we’re simulating is a symmetric random walk from some initial location, and calculating the expected hitting time of crossing some upper or lower boundary\n\n\n\n\n\nCode\nset.seed(20)\nk <- 10\nrw <- rw_sim(a, b, p, k, return_path = TRUE)\n\nqplot(0:(length(rw) - 1), rw, geom = \"line\", xlab = \"time\", ylab = \"state\", linetype = \"random walk path\") + \n  geom_hline(yintercept = b, color = \"tomato1\") + \n  geom_hline(yintercept = a, color = \"tomato1\") +\n  annotate(\"point\", x = 0, y = 10, size = 2, color = \"lightblue\") +\n  annotate(\"label\", label = \"k - initial state\", x = 3, y = 9, fill = \"lightblue\", size = 2) +\n  annotate(\"point\", x = 44, y = 0, size = 2, color = \"lightblue\") +\n  annotate(\"label\", label = \"T - hitting time\", x = 41, y = 1, fill = \"lightblue\", size = 2) +\n  scale_linetype_manual(values = c(3)) + \n  labs(linetype = \"\",\n       title = \"Hitting Times of Random Walk\") + \n  lims(y = c(a, b)) + \n  theme_minimal() + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\nBy theory, for the symmetric random walk, (see Durrett)\n\n\\begin{aligned}\nE[T|X_0 = k] = (k - a)(b - k)\n\\end{aligned}\n\n\n\nCode\nhitting_times <- replicate(10000,\n                           rw_sim(a,b,p, k))\nmean(hitting_times)\n\n\n[1] 101.0178"
  },
  {
    "objectID": "stochastic/stochastic.html#stochastic-processes",
    "href": "stochastic/stochastic.html#stochastic-processes",
    "title": "3  Stochastic",
    "section": "3.2 Stochastic Processes",
    "text": "3.2 Stochastic Processes\nA great overview of Differential Equations in R is covered by\n\n“sde” - stochastic differential equations\n“Sim.DiffProc”- simulate diffusion processes\n“ReacTran” - functions for generating finite differences on a grid\n\n\n3.2.1 Wiener Process\nThe most basic wiener process takes the form, which describes Brownian motion. This differential equation models 1 dimensional diffusion, and to see this, we can imagine the probability distribution over time. Each of the sample paths are a random walk with gaussian increment with proportional\n\ndx = dW\n\n\nx is the position, which is a function of time\ndW is the Wiener Noise, gaussian distribution with\n\n\n\nCode\nx0 <- 0 # initial position\nt0 <- 0 # initial time\ndt <- .01\nnt <- 100 # how many time steps to take, \ndx <- rnorm(nt, 0, dt) # sample steps\nx <- cumsum(c(x0, dx)) # sample path\n\nrwiener <- function(x0 = 0, t0 = 0, dt = .01, nt = 100) {\n  dx <- rnorm(nt, 0, sqrt(dt)) # sample steps\n  cumsum(c(x0, dx)) # sample path\n}\n\nset.seed(1)\nwiener_paths <- replicate(2000, rwiener())\nwiener_ts <- ts(wiener_paths, start = t0, deltat = dt)\nts.plot(wiener_ts, col = rgb(0,0,0,alpha = .03), ylim = c(-4, 4), main = \"Wiener Sample Paths\")\n\n\n\n\n\nInstead of the sample path approach, if we instead think about the probability density of our position as a function of time, we can see that the probability function becomes more diffuse over time. Thus, it’s not surprising that we can describe the diffusion with a PDE through the Fokker-Plank Equation.\n\n\nCode\n# code to create animations\nx <- seq(-3, 3, by = .01)\n# animation of density\nsaveGIF({\n    for (i in 2:100)  {\n      hist(wiener_ts[i,], xlim = c(-3, 3), ylim = c(0, 10),\n           freq = F,\n           breaks = 40,\n           main = paste0(\"Density of Sample Paths, Time = \", i*dt),\n           xlab = \"x\")\n      lines(x, dnorm(x, sd = sqrt(dt * i)), col = \"red\")\n      legend(\"topright\", legend = c(\"theoretical density\"), col = 2, lty = 1, bty = \"n\")\n    }\n}, movie.name = \"test.gif\", loop = T, interval = .01)\n\n# along the sample paths\nsaveGIF({\n    for (i in 2:100)  {\n      ts.plot(wiener_ts, col = rgb(0,0,0,alpha = .03), ylim = c(-4, 4), main = \"Wiener Sample Paths\")\n      abline(v = i * dt, col = \"red\")\n    }\n}, movie.name = \"sample.gif\", loop = T, interval = .01)\n\n\n\n\n\n\n\n\n\n\n\n\nThe associated Fokker Plank equation associated with this stochastic differential equation, is simply the heat equation.\n\n\\frac{\\partial}{\\partial t} P(x, t) =\\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}P(x,t)\n\n\nInitial condition: P(x, 0) = \\delta(x)\n\\delta(x) is the delta-dirac function, which has infinite mass at 0.\nBoundary Condition: P(a, t) = P(b, t) = 0\n\nsince our simulation has to occur on some bounded region [a,b], we just set the simulation to have absorbing boundaries.\n\n\nSolving the PDE with initial value conditions analytically, we find that the solution to this eigenvalue problem is\n\n\\begin{aligned}\nP(x, t) = \\frac{1}{\\sqrt{2\\pi t}}\\exp \\left(\\frac{x^2}{2t}\\right)\n\\end{aligned}\n\nWe can also calculate the solution by finite differencing. We can solve the PDE and show that our solutions match the rate given by the SDE formulation of the PDE. We use code from the vignette of the ReacTran R package. ReacTran package uses the method of lines for solving PDE’s, in which we set up a discretized grid, and solve the ODE as a vector\n\n\nCode\n# ReacTran uses the\nN <- 601\nxgrid <- setup.grid.1D(x.up = -3, x.down = 3, N = N) # grid of values\nx <- xgrid$x.mid\nD.coeff <- .5 # diffusion coefficient from solving FP\n\n# defines the diffusion (the derivative with respect to time)\n# Since our function has no time dependence, we only need to calculate the derivatives for the next step\nDiffusion <- function (t, Y, parms){\n  tran <- tran.1D(C = Y, C.up = 0, C.down = 0, # dirchlet boundary conditions, set to 0\n                D = D.coeff, dx = xgrid)\n  list(dY = tran$dC)\n}\n\n# Set initial condition of the differential equation, we approximate the \nYini <- c(rep(0, 300), 100, rep(0, 300)) # very tall initial mass\ntt <- seq(t0, dt * nt, dt) #times to simulate\n\n# solve heat equation\nout <- ode.1D(y = Yini, times = tt, func = Diffusion, parms = NULL, dimens = N)\n\n# library(tidyverse)\ncolorBreaks = 10^(seq(-2, 2, length.out = 255)) # different\nplot(raster(t(out[,-1]), xmn =0, xmx = 1, ymn = -3, ymx = 3),\n     asp = NA,\n     breaks = colorBreaks,\n     col = rev(hcl.colors(length(colorBreaks) - 1L, palette = \"Rocket\")), legend = FALSE,\n     xlab = \"time\", ylab = \"x\", interpolate = TRUE)\n\n\n\n\n\n\n\n3.2.2 Ornstein-Uhlenbeck Process\nAdding a drift term to the stochastic equation, gives the stochastic equation a mean. The negative in front of the drift implies that it will always regress to 0, because it’s a function of x (kind of like a spring constant). The \\theta parameter controls how strong the restoring force is.\n\ndx = -\\theta x\\, dt + \\sigma \\, dW\n\n\nx is position, which is a function of time.\ndW is the wiener process.\n\\theta is the rate of return to the mean (0)\nVariance of\n\nThe following shows the effect of the parameters \\theta = 3 and \\sigma = .5 with initial state x_0 = 5. We also show the process with a different initial state x_0 = -3 to show the restoring effect to the mean.\n\n\nCode\n# OU function\nou_paths <- function(npaths = 1, theta = 3, sigma = .5, x0 = 5, nt = 100, t0 = 0, dt = .01) {\n  sde_path_ou <- function() {\n    x <- vector(mode = \"numeric\", length = nt + 1)\n    x[1] <- x0\n    wiener_noise <- rnorm(nt, sd = sqrt(dt))\n    for (i in 1:nt) {\n      dx <- -theta * x[i] * dt + sigma * wiener_noise[i]\n      x[i+1] <- x[i] + dx\n    }\n    return(x)\n  }\n  ts(replicate(npaths, sde_path_ou()), start = t0, deltat = dt)\n}\n\ntheta <- 3\nsig <- .5\ny0 <- 5\nnt <- 100\ndt <- .01\nt0 <- 0\n\nset.seed(1)\nou_ts <- ou_paths(npaths = 500, theta = theta, sigma = sig, x0 = y0, nt = nt, t0 = t0, dt = dt)\n\nts.plot(ou_ts, col = rgb(0,0,0,alpha = .05)) # plot\n\ntt <- seq(0, 1, .01)\ntheory_mean <- y0*exp(-theta*tt)\ntheory_var <- sig^2 / 2 / theta * (1 - exp(-2 * theta * tt))\n\nlines(tt, theory_mean, col = \"red\")\nlines(tt, theory_mean + 2 * sqrt(theory_var), lty = 2, col = \"red\")\nlines(tt, theory_mean - 2 * sqrt(theory_var), lty = 2, col = \"red\")\nlegend(\"topright\", legend = c(\"mean\", \"\\u00B1 2sd\"), col = 2, lty = 1:2)\n\n\n\n\n\nBecause of this restoring property to the mean, a closely related stochastic process is known as the Vasicek model (commonly used to model interest rates), which adds another parameter to control what the mean is.\n\ndx = \\theta(\\mu - x)\\, dt + \\sigma \\, dW\n\n\nreduces to the 0 mean OU process when \\mu = 0.\nThe long term variance of this model is also\n\nThis process is important because it’s the continuous time analogue of a discrete time AR1 process.\nSimilarly, the associated Fokker-Plank Equation for this SDE is:\n\n\\begin{aligned}\n\\frac{\\partial P(x, t)}{\\partial t} = -\\mu \\frac{\\partial P(x, t)}{\\partial x} + \\frac{\\sigma^2}{2}\\frac{\\partial^2P(x, t)}{\\partial x^2}\n\\end{aligned}\n\n\n\nCode\n# simulation parameters\nt0 <- 0                       # time start\ndt <- .01                     # time step \ntn <- 1                       # time end\nnt <- tn/dt                   # number of time steps\ntgrid <- seq(t0, nt * dt, dt) # time grid\n\nx0 <- 6                       # space start\nxn <- -2                    # space end\nnx <- 800                     # number of grid points\nxgrid <- setup.grid.1D(x.up = x0, x.down = xn, N = nx) # space grid\nx <- xgrid$x.int\n\nsig <- .5            # Parameters from OU simulation\nmu <- -3             # Parameters from OU simulation  \nD_coef <- sig^2 / 2  # Diffusion function\n\ny0 <- c(rep(0, 100), 100, rep(0, 699))                 # initial condition\n\n# advection-diffusion (method of lines)\nadvec <- function(t, Y, parms) {\n  trans <- tran.1D(C = Y, D = D_coef, v = mu*x, C.up = 0, C.down = 0, dx = xgrid)\n  return(list(dY = trans$dC))\n}\n\n# solve advec/diffusion equation\nout <- ode.1D(y = y0, times = tgrid, func = advec, parms = NULL, dimens = nx)\n\n# plot solution\ncolorBreaks = 10^(seq(-3, 3, length.out = 255)) # different to capture more drift in lower parameters\nplot(raster(t(out[,-1]), xmn = 0, xmx = 1, ymn = xn, ymx = x0),\n     asp = NA,\n     breaks = colorBreaks,\n     col = rev(hcl.colors(length(colorBreaks) - 1L, palette = \"Rocket\")), legend = FALSE,\n     xlab = \"time\", ylab = \"x\", interpolate = TRUE)\n\n\n\n\n\nCode\n# rcOU(n=1, Dt=0.1, x0=1, theta=c(0,2,1))\n\n\n\n\n\n\n\n\nODE for Mean/Variance of OU process\n\n\n\nWe can get a First Order ODE characterization of the mean and variance for the Ornstein-Uhlenbeck process.\n\\begin{aligned}\n\\frac{d\\langle x \\rangle}{dt} = -\\theta \\langle x \\rangle \\\\\n\\langle x\\rangle = \\langle x_0 \\rangle e^{-\\theta t}\n\\end{aligned}\n\n\\frac{dV}{dt} = -2 \\theta V + \\sigma^2\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\ndx = -\\theta x\\, dt + \\sigma\\, dW\n First we note the expression for the derivative of variance, and calculate the necessary components.\n\n\\begin{align*}\n\\frac{dV}{dt} &= \\frac{d\\langle x^2\\rangle}{dt} - \\frac{d \\langle x\\rangle^2}{dt} \\\\\n&= \\frac{d\\langle x^2\\rangle}{dt} - 2 \\langle x\\rangle \\frac{d \\langle x\\rangle}{dt}\n\\end{align*}\n\nHence, we need to evaluate the mean differentials to the second order, from\n\n\\begin{align}\nd\\langle x\\rangle &= -\\theta \\langle x\\rangle \\, dt\\\\\ndx^2 &= (x + dx)^2 - x^2 \\\\\n&=2x\\,dx + (dx)^2 \\\\\n&= (-2\\theta x^2 + \\sigma^2)\\,dt + \\sigma \\, dW \\\\\nd\\langle x^2\\rangle &= (-2\\theta \\langle x^2\\rangle + \\sigma^2)\\, dt\n\\end{align}\n\nWe’ve used that the rules of Ito’s calculus, that dt\\,dW = 0, (dW)^2 = dt, (dt)^2 =0. Plugging in the values and simplifying gives us the desired result.\n\n\n\n\n\n\n\nCode\ntheta <- 3\nsig <- .5\n\n# simplest first order ode\nou_mean <- function(t, y, parms) {\n  dy <- -theta * y\n  return(list(dy))\n}\n\n\nx0 <- 5 # initial mean\ntt <- seq(0, 1, by = .01)\nout_mean <- ode(x0, tt, ou_mean, parms = NULL)\n\ncbind(out_mean[,2],\n      5 * exp(-3* tt)) |> head() # matches\n\n\n         [,1]     [,2]\n[1,] 5.000000 5.000000\n[2,] 4.852224 4.852228\n[3,] 4.708817 4.708823\n[4,] 4.569653 4.569656\n[5,] 4.434600 4.434602\n[6,] 4.303538 4.303540\n\n\nCode\n# function coding differential equation \nou_var <- function(t, v, parms) {\n  dv <- -2 * theta * v + sig^2\n  return(list(dv))\n}\n\ny0 <- 0 # initial variance\ntt <- seq(0, 1, by = .01)\n\nout <- ode(y0, tt, ou_var, parms = NULL)\n\ncbind(out[,2],\n      sig^2 / 2 / theta * (1 - exp(-2 * theta * tt))) |> head() # matches!\n\n\n            [,1]        [,2]\n[1,] 0.000000000 0.000000000\n[2,] 0.002426880 0.002426478\n[3,] 0.004712408 0.004711648\n[4,] 0.006864444 0.006863741\n[5,] 0.008891073 0.008890506\n[6,] 0.010799760 0.010799241\n\n\n\n\n3.2.3 General Linear SDE\n\ndx = -\\gamma x \\, dt + g x\\, dW"
  },
  {
    "objectID": "stochastic/stochastic.html#reaction-diffusion-equations",
    "href": "stochastic/stochastic.html#reaction-diffusion-equations",
    "title": "3  Stochastic",
    "section": "3.3 Reaction Diffusion Equations",
    "text": "3.3 Reaction Diffusion Equations\nThese are non-linear differential equations, and a system of them. We can start describing the reactions of SIR model as sets of nonlinear differential equations. There are a number of famous examples of these, we’ll study the Grey-Scott system, then the SIR system, and hopefully we’ll see the reaction diffusion nonlinearity around the boundaries of the different stable sets.\n\n3.3.1 Grey-Scott Model\nA really cool web simulation of the phenomena I want to recreate can be found by Karl Sims, Reaction Diffusion Tutorial. Luckily, someone else has already implemented a version of this, and we’ll just borrow their code (Fronkonstin).\n\nThe Grey-Scott Model describes the following irreversible, reactions of three compound U, V, P and P is an inert product.\n\n\\begin{align*}\nU + 2V &\\rightarrow 3V \\\\\nV &\\rightarrow P\n\\end{align*}\n\nWe will use the simulation parameters from Pearson (1993), in particular, the equations that result from this reaction diffusion is\n\n\\frac{\\partial U}{\\partial t} = D_u \\nabla^2U - UV^2 + F(1 - U) \\\\\n\\frac{\\partial V}{\\partial t} = D_v \\nabla^2V + UV^2 - (F + k)V\n\n\nD_u = 2 \\times 10^{-5}\nD_v = 10^{-5}\nperiodic boundary condition\nF and k are known as the feed and kill rates of the reactants. since concentration ranges between 0 and 1, the reaction term in the first equation is positive, and then F controls how much of reactant U is being introduced.\n\n\n\nCode\ngray_scott <- function(U0 = NULL, V0 = NULL,\n                       feed_rate = 0.0545,\n                       kill_rate = 0.062,\n                       N = 256,\n                       tN = 2000,\n                       D.u = 1,\n                       D.v = .5,\n                       save_frame_freq = 20,\n                       video_file = \"gray_scott.mp4\",\n                       pic_dir = NULL,\n                       init_strategy = c(\"random\"),\n                       seed = 1,\n                       ...) {\n  set.seed(seed)\n  pct <- proc.time()\n  init_strategy <- match.arg(init_strategy)\n  if (missing(U0) | missing(V0)) {\n    #TODO: implement different initialization strategies for different patterns\n    U0 <- matrix(1, nrow = N, ncol = N)\n    V0 <- matrix(0, nrow = N, ncol = N)\n    V0[sample(N^2, ceiling(N^2 / 20))] <- 1 # 10% of cells\n    } else if (!all(c(dim(U0), dim(V0)) == N)) {\n      rlang::abort(\"Initial Matrix must be a grid with dimension N\")\n    }\n  \n  U <- U0\n  V <- V0\n  # 9 point stencil for Laplacian\n  # yuvj420p pix format used: https://superuser.com/questions/1273920/deprecated-pixel-format-used-make-sure-you-did-set-range-correctly\n  L <- matrix(c(0.05, 0.2, 0.05, \n                0.2,  -1, 0.2,\n                0.05, 0.2, 0.05), nrow = 3)\n  \n  \n  if (missing(pic_dir))\n    pic_dir <- tempdir() \n  else \n    pic_dir <- fs::dir_create(fs::path_wd(pic_dir))\n\n  # clean directory  \n  if (length(Sys.glob(fs::path(pic_dir, \"*.jpg\"))) > 0 ) {\n    rlang::abort(sprintf(\"%s not empty, please clean out *.jpg files to prevent overwriting!\", pic_dir))\n  }\n\n  jpeg_file <- fs::path(pic_dir, sprintf(\"plot%06d.jpg\", 0))\n  jpeg(jpeg_file)\n      # setup base plot graphical parameters\n      par(bty = \"n\", oma = rep(0, 4), mar = rep(0, 4))\n      plot(raster(V0),\n           axes = F, bty = \"n\", frame.plot = F,\n           interpolate = F,\n           breaks = hist(V0, breaks = 256, plot = F)$breaks,\n           col = hcl.colors(256,  palette = \"ag_sunset\"),\n           legend = FALSE)\n      \n      dev.off()\n  \n  p <- progressr::progressor(tN / save_frame_freq)\n  for (i in 1:tN) {\n    dU <- D.u * filter2(U, L) - U * V^2 + feed_rate * (1 - U)\n    dV <- D.v * filter2(V, L) + U * V^2 - (feed_rate + kill_rate) * V\n    U <- U + dU\n    V <- V + dV\n    # save frame in temp folder\n    if (i %% save_frame_freq == 0) {\n      p(message = sprintf(\"Timestep: %g\", i))\n      jpeg_file <- fs::path(pic_dir,\n                            sprintf(\"plot%06d.jpg\", i))\n      jpeg(jpeg_file)\n      # setup base plot graphical parameters\n      par(bty = \"n\", oma = rep(0, 4), mar = rep(0, 4))\n      plot(raster(V),\n           axes = F, bty = \"n\", frame.plot = F,\n           interpolate = T,\n           breaks = hist(V, breaks = 256, plot = F)$breaks,\n           col = hcl.colors(256,  palette = \"ag_sunset\"),\n           legend = FALSE)\n      dev.off()\n    }\n  }\n  \n  # create video directory\n  fs::dir_create(fs::path_wd(fs::path_dir(video_file)))\n  \n  #TODO: check if ffmpeg available on system?\n  ffmpeg_cmd <- sprintf('ffmpeg -y -f image2 -pattern_type glob -i \"%s/*.jpg\" -framerate 60 -c:v libx264 -crf 20 -filter:v \"format=yuvj420p\" %s', pic_dir, fs::path_wd(video_file))\n  # run ffmpeg command\n  system(ffmpeg_cmd)\n  cat(paste0(\"Running command: \", ffmpeg_cmd, \"\\n\"))\n  proc.time() - pct # elapsed wall time\n}\n# relative to this script (when running commands in notebook)\nwith_progress(\n  gray_scott(video_file = \"vid/gray_scott.mp4\", pic_dir = \"gray_pic\"), handlers = handlers(\"progress\")) # for text updates\n\n\n\n\n\n\n\nGray Scott Reaction Diffusion, f=.0545, k=.062\n\n\n\n\n3.3.2 SIR Diffusion\n\n\n\nhere’s we’re trying to upsample the values that come out of the\nThe SIR image has a slightly different quality to it in that the lines around the image are thinner\n\n\n\n\n\nSIR Reaction Diffusion, f=.0545, k=.062\n\n\n\n\n3.3.3 SIS system\nI’m curious if the SIS system without vital dynamics will show any interesting patterns. We know there must be an absorbing state, but this system can also reach an endemic state based on the reproduction number. If we add diffusion into the system we should get interesting results, at the very least some cool videos.\n\n\n3.3.4 Brusselator\n\n\n3.3.5 Lorentz Attractor\n\n\n\n\nPearson, John E. 1993. “Complex Patterns in a Simple System.” Science 261 (5118): 189–92. https://doi.org/10.1126/science.261.5118.189."
  },
  {
    "objectID": "meta/meta.html",
    "href": "meta/meta.html",
    "title": "4  Meta Analysis",
    "section": "",
    "text": "5 Effect Size\nIn order to pool studies, the most important item is the effect size. Largely effect size can be categorized into relative or absolute. Another breakdown is whether the effect size is standardized or not.\nNow that we have an understanding of how effect sizes matter for different variables, how do we actually pool the studies?\nWe’ll follow along with the Chapter 4: Doing Meta Analysis in R.\nRandom effect weighting adds and additional\nConsider a Knapp and Hartung (2003) adjustment, changing tests to use t-distribution for the extra parameter estimated in the random effect model. Sidik and Jonkman (2005) evaluates the estimators and finds Knapp Hartung variance estimates to be the best protection against error. The main problem is estimating the weights used in the meta-analyses."
  },
  {
    "objectID": "meta/meta.html#mean",
    "href": "meta/meta.html#mean",
    "title": "4  Meta Analysis",
    "section": "5.1 Mean",
    "text": "5.1 Mean"
  },
  {
    "objectID": "meta/meta.html#median",
    "href": "meta/meta.html#median",
    "title": "4  Meta Analysis",
    "section": "5.2 Median",
    "text": "5.2 Median"
  },
  {
    "objectID": "meta/meta.html#proportion",
    "href": "meta/meta.html#proportion",
    "title": "4  Meta Analysis",
    "section": "5.3 Proportion",
    "text": "5.3 Proportion"
  },
  {
    "objectID": "meta/meta.html#logit-proportion",
    "href": "meta/meta.html#logit-proportion",
    "title": "4  Meta Analysis",
    "section": "5.4 Logit Proportion",
    "text": "5.4 Logit Proportion"
  },
  {
    "objectID": "meta/meta.html#correlation",
    "href": "meta/meta.html#correlation",
    "title": "4  Meta Analysis",
    "section": "5.5 Correlation",
    "text": "5.5 Correlation\nStandard error formulas are not trivial. Even something like the correlation coefficients have been glossed over and never revisited. We’ve said “good enough”! The commonly reported values are given here:\n\n(1 - \\rho^2)^2 / n is reported by Hald (2008) pp. 126\n\\frac{(1-\\rho^2)^2}{n-1} is reported by Dingman and Perry (1956)\n\\frac{(1-\\rho^2)^2}{n-2} is reported by\n\\frac{\\rho^2}{n}\\left(\\frac{M_{22}}{\\rho^2\\sigma_x^2 \\sigma_y^2} + \\frac{M_{40}}{4\\sigma_x^4} + \\frac{M_{04}}{4\\sigma_y^4} - \\frac{M_{13}}{\\rho \\sigma_x\\sigma_y^3} - \\frac{M_{31}}{\\rho \\sigma_x^3\\sigma_y1} + \\frac{M_{22}}{\\rho \\sigma_x^2\\sigma_y^2}\\right) is reported by Sheppard and Forsyth (1899) (in the bivariate normal case, this reduces to the first case)\n\n\n\nCode\nsample_cor_norm <- function(mean1, mean2, sd1, sd2, n) {\n  x <- rnorm(n, mean1, sd2)\n  y <- rnorm(n, mean2, sd2)\n  cor(x, y)\n}\nsample_cor_norm_std <- partial(sample_cor_norm, mean1 = 0, mean2=0, sd1=1, sd2=1)\n\n# resampling simulation, (for n = 10, 20, 30, repeat the sampling 10 times.)\ndat_resample_cor_norm_std <- seq(10, 410, 20) |> \n  enframe(name = NULL, value = \"n\") |> \n  rowwise() |> \n  mutate(sample_cor = list(replicate(10, sample_cor_norm_std(n))), # increase replicates to make it closer to truth\n         sample_cor_se = sd(sample_cor))\n\ntheoretical_resample_cor_se <- tibble(x = seq(3, 410),\n                             y = 1/sqrt(x),\n                             y1 = 1/sqrt(x -1),\n                             y2 = 1/sqrt(x-2)) |> \n  pivot_longer(cols = y:y2)\n\ndat_resample_cor_norm_std |> ggplot() +\n  geom_point(aes(n, sample_cor_se)) +\n  geom_line(data = theoretical_resample_cor_se,\n            aes(x, value, linetype = name), color = \"red\")\n\n\n\n\n\nIt doesn’t really matter the denominator, it’s not a big difference. This looks at the actual values of sampling correlation coefficient as n increases.\n\n\nCode\n# Alternative way of looking at the drop off...\ndat_cor_norm_std <- 5:200 |> \n  enframe(name = NULL, value = \"n\") |> \n  rowwise() |> \n  mutate(sample_cor = sample_cor_norm_std(n))\n\ntheoretical_cor_se <- tibble(x = seq(5,200, .5),\n                             y2 = 2/sqrt(x),\n                             y_2 = -2/sqrt(x)) |> \n  pivot_longer(y2:y_2)\ndat_cor_norm_std |> ggplot(aes(n, sample_cor)) +\n    geom_line(data = theoretical_cor_se,\n            mapping = aes(x, value, group = name), linetype = 2, color = \"red\") +\n  geom_line(color = \"grey50\") +\n  theme_minimal() +\n  lims(y = c(-.8, .8)) +\n  labs(y = \"Sample Pearson Correlation\",\n       title = \"2-SE Envelope of Sample Correlation as function of n\")\n\n\nWarning: Removed 6 row(s) containing missing values (geom_path).\n\n\n\n\n\nWe have the luxury of just resampling from our target population, so we don’t really have to bootstrap, but evaluating how good the bootstrap is in various situations is probably worth looking into as well.\nNow we’re curious how bad these distributions can get if the distributions are skewed. Let’s do a correlated example with exponential distributions.\n\n\nCode\n# simulating function for correlated variables\nsample_cor_exp <- function(lambda1, lambda2, n) {\n  x <- rexp(n, lambda1)\n  y <- rexp(n, lambda2)\n  cor(x, x + y) # expected correlation is 1/lambda1 + 1/lambda2\n}\n\ndat_resample_cor_exp <- seq(10, 410, 20) |> \n  enframe(name = NULL, value = \"n\") |> \n  rowwise() |> \n  mutate(sample_cor = list(replicate(50, sample_cor_exp(2, 3, n))), # increase replicates to make it closer to truth\n         sample_cor_se = sd(sample_cor))\n\nrho <- 5/6\ntheoretical_resample_cor_se <- tibble(x = seq(3, 410),\n                             y = (1 - rho^2)/sqrt(x))\n\ntheoretical_resample_cor_se |> ggplot() +\n  geom_line(mapping = aes(x = x, y = y), \n            color = \"red\",\n            linetype = 2) +\n  geom_point(data = dat_resample_cor_exp,\n             mapping = aes(n, sample_cor_se)) +\n  labs(y=\"standard error\",\n       title = \"Standard Errors vs \\u03C1 = 5/6\")\n\n\n\n\n\nOverall, when the distribution is skewed, we’re notably underestimating the standard error almost across the board.\n\n\nCode\n# envelope graph\ndat_sample_cor_exp <- 5:200 |> \n  enframe(name = NULL, value = \"n\") |> \n  rowwise() |> \n  mutate(sample_cor = sample_cor_exp(2, 3, n)) # 5/6 cor theoretical\n\n# theoretical envelop, upper lower 2nd std dev, around (d)\nrho <- 5/6\ntheoretical_cor_se <- tibble(x = seq(5,200, .5),\n                             y_upper = 2 * (1 - rho^2)/sqrt(x) + rho,\n                             y_lower = -2 * (1 - rho^2)/sqrt(x) + rho) |> \n  pivot_longer(starts_with(\"y\"))\n\ndat_sample_cor_exp |> ggplot() +\n  geom_line(aes(x = n, y= sample_cor), color = \"grey50\") +\n  geom_line(data = theoretical_cor_se, \n            mapping = aes(x = x, value, group = name),\n            color = \"red\") +\n  theme_minimal() +\n  labs(title = \"2-SE Envelope of Sample Correlation of Exponential RV when \\u03C1 = 5/6\")\n\n\n\n\n\nWhen the distributions are skewed, it seems they peak the bottom a little more, but this matches the story above in which we are underestimating the standard error a little, because it’s poking out a little more all over.\n\n\n5.5.1 Z transformed Pearson"
  },
  {
    "objectID": "meta/meta.html#diversity-indices",
    "href": "meta/meta.html#diversity-indices",
    "title": "4  Meta Analysis",
    "section": "5.6 Diversity Indices",
    "text": "5.6 Diversity Indices\n\nShannon diversity\nHill Diversity Indices\n\nIt has been noted that there are biases when bootstrapping these diversity indices.1 Furthermore, there have been attempts to quantify the standard error as well.2\n\n5.6.1 Hill/Renyi Diversity\n\n\\begin{align*}\nD_a &= \\left(\\sum_i^S p_i^a\\right)^{\\frac{1}{1-a}} \\\\\nD_0 &= \\sum_i^S p_i^0 = S & \\text{Species Richness}\\\\\nD_1 &= \\exp\\left(-\\sum_i p_i \\log p_i\\right) & \\text{Exp(Shannon-Wiener Entropy Index)} \\\\\nD_2 &= \\frac{1}{\\sum_i^S p_i^2} & \\text{Simpson's Reciprocol Index}\n\\end{align*}\n\n\n\n\nLimit details for D_1\n\nWhen a = 1, the exponent becomes 1/0, so we must use the limiting definition as a\\rightarrow 1 to see how the diversity metric reduces to entropy.\n\n\\begin{align*}\n\\lim_{a\\rightarrow 1} \\left(\\sum_{i}^S p_i^a\\right)^{\\frac{1}{1-a}} &= \\lim_{a\\rightarrow 1} \\exp\\left(\\frac{1}{1-a}\\log\\left(\\sum_{i}^S p_i^a\\right)\\right) \\\\\n&= \\exp\\left(\\lim_{a\\rightarrow 1}\\frac{\\log\\left(\\sum_{i}^S p_i^a\\right)}{1-a}\\right) \\\\\n&= \\exp\\left(\\lim_{a\\rightarrow 1}\\frac{\\sum_i^S p^a_i \\log_i p_i}{-\\sum_i^S p_i^a}\\right) \\\\\n&= \\exp\\left(-\\sum_i^S p \\log_i p_i\\right)\n\\end{align*}\n\nwhere we’ve used L’hopitals rule in step 2, and \\sum_i^S p_i = 1 in step 3.\n\n\n\n\n5.6.2 Species Diversity\nThe most basic metric, basically binarizing the counts, and saying these are all the species that we see.\n\n\n5.6.3 Shannon diversity\nShannon diversity is defined as:\n\n\\begin{align*}\nH = -\\sum p_i \\log p_i\n\\end{align*}\n\n\np_i is the proportion among all the species, so \\sum p_i = 1.\nH is maximized when proportions are the same among proportions\nSpecies with count 0 do not contribute to entropy (they also don’t change the proportion of existing species).\n\nHowever if using Shannon Equitability Index, the total species count does matter.\n\n\nIs Shannon diversity dependent on number of species? Assume uniform distribution.\n\n\nCode\nS <- 2:200\nH <- S |> map_dbl(~entropy(rep(20, .x)))\nH_equitable <- H / log(S)\ntibble(S, H, H_equitable) |> ggplot() +\n  geom_line(aes(S, H, color = \"Shannon Index (H)\")) +\n  geom_line(aes(S, H_equitable, color = \"Equitable Shannon Index\")) +\n  theme_minimal() +\n  labs(y = \"Entropy\",\n       title = \"Uniform distribution of species\")\n\n\n\n\n\nWhat’s the sampling distribution of Shannon’s diversity index?\n\n\nCode\n#' simulate uniform counts\n#'\n#' @param S number of species\n#' @param n sequencing depth\n#'\n#' @return\n#' @export\n#'\n#' @examples\nsim_uniform_species <- function(S, n = S*5){\n  y <- rdunif(n, S)\n  table(y)[as.character(1:S)] %|% 0L |> as.numeric()\n}\n\n# gives sample community matrix, with uniform distribution across both margins.\nsim_uniform_community_matrix <- function(S, sample_n = 10, sample_min = 100, sample_max = 400) {\n  # sequencing effort will vary between samples\n  n <- runif(sample_n, sample_min, sample_max)\n\n  # each line is a count from samples\n  mapply(n, FUN = sim_uniform_species, S = S) |> \n    t()\n}\n\n\n# Bias of entropy estimation as a function of sequencing depth.\nentropy_uniform_se <- expand.grid(n = c(2, 5, 10, 50, 200, 500, 1000),\n                                  S = 2:200) |> rowwise() |> \n  mutate(H = entropy(sim_uniform_species(S, n)))\n\nS <- seq(2:200)\nH_theoretical <- S |> map_dbl(~entropy(rep(10, .x)))\ntheoretical_entropy_uniform <- tibble(S, H_theoretical)\n\nentropy_uniform_se |> ggplot() +\n  geom_line(aes(S, H, color = n, group = n)) +\n  geom_line(data = theoretical_entropy_uniform,\n            mapping = aes(S, H_theoretical)) +\n  scale_color_gradient(trans = \"log\", breaks = c(2, 10, 100, 1000))\n\n\n\n\n\nThe bias it seems can be quite massive if you choose different sequencing efforts. Thus, we must standardize our metric before comparing alpha diversity.\nNormalization Techniques (Slides with more3)\n\nSubsampling to a common depth (rarefaction)\nEqualize depths by scaling OTU counts to common depth (different from rarefaction, scaling vs subsampling)\nTransform counts into relative abundance (denominator is each sample)\nNormalize data based on 16S\n\nNow we’ll try to quantify this bias in the Shannon Diversity Metric, using different estimators of the uniform.\n\n\nCode\n# Chao Shen entropy estimator\nS <- 100\nn <- 50:1000\n\nset.seed(1)\ndat_entropy_bias <- expand_grid(n, S) |> \n  rowwise() |> \n  mutate(y = list(sim_uniform_species(S, n)),\n         entropy_est = list(list(H_ML = entropy(y),\n                            H_MM = entropy(y, method = \"MM\"),\n                            H_Jeffreys = entropy(y, method = \"Jeffreys\"),\n                            H_Laplace = entropy(y, method = \"Laplace\"),\n                            H_minimax = entropy(y, method = \"minimax\"),\n                            H_CS = entropy(y, method = \"CS\"),\n                            H_shrink = entropy(y, method = \"shrink\", verbose = FALSE))),\n         H_Z = Entropy.z(y)) |> \n  unnest_wider(entropy_est) |> \n  pivot_longer(cols = starts_with(\"H\"), names_to = \"H_type\" , values_to = \"H\")\n\ndat_entropy_true <- tibble(n, H_theory = entropy(rep(1, 100)))\n\ng <- dat_entropy_bias |> ggplot() +\n  geom_line(data = dat_entropy_true,\n            mapping = aes(n, H_theory), color = \"red\") + \n  geom_line(aes(n, H, color = H_type), alpha = .5)\n\nconfig(ggplotly(g), modeBarButtonsToRemove = c(\"zoom\", \"pan\", \"select\", \"zoomIn\", \"zoomOut\",\n                                               \"autoScale\", \"select2d\", \"hoverClosestCartesian\", \"hoverCompareCartesian\"), displaylogo = FALSE)\n\n\n\n\n\n\nThe H_ML is the standard plug in estimator of the entropy. We can see that most of them underestimate the true entropy, but the shrinkage estimator does surprisingly well. This is something to look into, I think it shrinks toward the uniform distribution anyway, so it’s not really a fair comparison. overall we note that:\n\nH_CS probably has the highest standard error\n\n\n\n5.6.4 Rarefaction (aside)\nThere is a difference in terminology rarefaction and rarifying. Rarefaction curves are the curves, plotting affect of the alpha diversity metric against the sampling effort. rarifying is the practice of subsampling to a similar depth.\nThe main idea, is that having more sample counts means that more species are likely to be represented. Thus, in order to make samples comparable, we should calculate an “expected species” count with some sort of common effort. Rarefied estimators for species counts are statistically inadmissable (McMurdie and Holmes 2014), due to the simple fact that we’re throwing away data, and thus artificially increasing standard error when we don’t need to. However, it still remains a common technique so understanding why it’s not optimal is still relevant. Since rarefying is basically based on hypergeometric subsampling, exact formulas for standard error and expected value exist.\nIn general, this is what rarefied curves look like:\n\n\nCode\ndata(BCI)\nrarecurve(BCI)\nabline(v=300, col = 2)\n\n\n\n\n\nA single curve is one sample (row), and plots the expected species count as a function of sub-sampling depth.\n\n\nCode\nset.seed(1)\nuniform_cm <- sim_uniform_community_matrix(100)\n# rarefying gives expected species diversity.\nrarefy(uniform_cm, 118, se = T)\n\n\n        [,1]     [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\nS  67.860019 68.13418 68.816150 69.537161 70.031099 69.334350 68.897680\nse  2.417449  2.72915  2.953008  3.075216  2.255168  3.063407  3.047504\n        [,8]      [,9] [,10]\nS  69.540933 69.126184    69\nse  3.020944  2.948892     0\nattr(,\"Subsample\")\n[1] 118\n\n\nCode\n# sum(uniform_cm[10,] != 0) # 69, since this is actual data for 10, the se is zero.\nuniform_drare <- drarefy(uniform_cm, 118) # margin sums are the expected species count, so each cell is the probability of seeing that species \n\n# subsample first one 179 down to 118.\ncbind(uniform_drare[1, ],\n      1 - exp(lchoose(179 - uniform_cm[1,], 118) - lchoose(179, 118))) |> head()\n\n\n      [,1]      [,2]\n 0.8851296 0.8851296\n 0.9617099 0.9617099\n 0.0000000 0.0000000\n 0.0000000 0.0000000\n 0.6592179 0.6592179\n 0.6592179 0.6592179\n\n\ndrarify formula counts subsampling arrangements without replacement. Let’s say sample 1 had 200 counts total, and we wanted to subsample down to 120. Then, we the total number of subsamples is taking those 120 of those 200 counts. We can imagine that we’re pulling from a pool of S_1, S_2, S_2, S_3, for which the sample counts would be c(1, 2, 1).\n\n\\begin{align*}\nP(\\text{at least one }S_1 \\text{ sampled}) =1 - \\frac{\\binom{200 - S_1}{120}}{\\binom{200}{120}}\n\\end{align*}\n\n\n\nCode\nrarecurve(uniform_cm, sample = 118)\n\n\n\n\n\nWhen all the curves are coming from the same distribution, it’s reasonable that the rarefied curves will all look the same."
  },
  {
    "objectID": "meta/meta.html#estimators-for-species-diversity",
    "href": "meta/meta.html#estimators-for-species-diversity",
    "title": "4  Meta Analysis",
    "section": "5.7 Estimators for Species Diversity",
    "text": "5.7 Estimators for Species Diversity\n\nbreakaway - estimates through nonlinear regression of probability ratios, fraction of singleton’s to empty, doubles to singles, etc. Kemp has characterized the distribution of these ratios.\nDivNet Paper\nEntropy Estimators\n\nAsymptotic Normality of Entropy Estimator\nEntropic Representation and Estimation of Diversity Indices"
  },
  {
    "objectID": "meta/meta.html#entropy",
    "href": "meta/meta.html#entropy",
    "title": "4  Meta Analysis",
    "section": "5.8 Entropy",
    "text": "5.8 Entropy\n\n\nCode\n# se calculated by delta method (for binomial at least)\nentropy_delta_se <- function(n, p) {\n  sqrt(p * (1 - p) / n * (1 + log(p))^2)\n}\n\np <- c(.01, .1, .3, .5)\nn <- seq(5, 200, 15)\nsim_df <- expand_grid(n, p)\nentropy_se_df <- sim_df |> rowwise() |> \n  mutate(y = list(rbinom(200, n, p)),\n         p_hat = list(y / n),\n         sample_entropy = list(p_hat * log(p_hat)),\n         se_sample_entropy = sd(sample_entropy, na.rm = TRUE),\n         delta_se = entropy_delta_se(n, p))\n\n# deltamethod(~ 1 / (x1 + x2))\n# deriv(expression(x * log(x)), \"x\", function.arg = TRUE)\n# deriv(~x * log(x), \"x\")\n\n\nentropy_se_df |> ggplot(aes(n, se_sample_entropy, color = factor(p))) +\n  geom_point() +\n  geom_line(mapping = aes(n, delta_se),\n            linetype = 2) +\n  facet_wrap(~p, nrow = 2)"
  },
  {
    "objectID": "meta/meta.html#group-differences",
    "href": "meta/meta.html#group-differences",
    "title": "4  Meta Analysis",
    "section": "5.9 Group Differences",
    "text": "5.9 Group Differences\nThere’s a lot of these, so here’s a quick list\n\n5.9.1 Mean Difference\n\n\n5.9.2 Standardized Mean Difference (Cohen’s d)\nNote that the standard t statistic is defined as \\frac{\\sqrt{n}(\\bar x_2 - \\bar x_1)}{s_p} which differs from d by a factor of \\sqrt{n}.\n\nAdjustments\n\n\nName\n\nEstimator\nSE\n\n\n\n\nMD\n\n\\bar x_2 - \\bar x_1\n\ns_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\n\n\nSMD\nCohen’s d\n\\frac{\\bar x_2 - \\bar x_1}{s_{p}}\n\n\\sqrt{\\frac{n_1 + n_2}{n_1n_2} + \\frac{d^2}{2(n_1 + n_2)}}\n\nSO Derivation of SE\n\n\n\nHedge’s g\n\n\n\n\n(log) Risk Ratio\n\n\\log\\left(\\frac{a/n_{1}}{b/n_2}\\right)\n\n\n\n(log) Odds Ratio\n\n\\log\\left(\\frac{a/b}{c/d}\\right)\n\n\n\n(log) Incidence Risk Ratio\n\n\n\n\n\nHazard Ratio\n\n\n\n\n\n\n\nreliability / test-retest / attenuated/ error in measurement models\n\nThese types of estimators standardize by \\sqrt{r_{xx}}, when available and many variants thereafter\n\nSmall Sample Adjustments\n\nGenerally small samples have quite a large bias"
  },
  {
    "objectID": "bootstrap/bootstrap.html#estimating-bias",
    "href": "bootstrap/bootstrap.html#estimating-bias",
    "title": "5  Bootstrap",
    "section": "5.1 Estimating Bias",
    "text": "5.1 Estimating Bias\n\n\\begin{aligned}\n\\text{Bias}_F(\\hat \\theta, \\theta) = E_F[\\hat \\theta] - \\theta\n\\end{aligned}\n\nWe note that \\hat\\theta = s(\\mathbf{x}) is function of the data \\hat\\theta = s(\\mathbf{x}), whereas \\theta = t(F) is a function of the underlying true distribution. In order to use bootstrap to estimate the bias of an estimator, we use average of bootstrap samples to estimate the expected value, while we use the empirical distribution to estimate the true parameter.\n\n5.1.1 Example (sample standard deviation)\n\n\nCode\nset.seed(1)\nx <- rnorm(30) # \"Empirical Distribution\" from normal distribution\nB <- 1000 # Number of bootstrap samples to take.\ngen_boot <- function(x) {\n  xb <- sample(x, replace = TRUE)\n  sum((xb - mean(xb))^2 / length(xb)) \n}\n\n# bootstrap expected value\nthetahat <- mean(replicate(B, gen_boot(x)))\ntheta <- sum((x - mean(x))^2 / length(x)) # Distributional \"real value\"\n\ncbind(thetahat - theta,\n      1/30) # Pretty close to theoretical \"bias\"\n\n\n           [,1]       [,2]\n[1,] -0.0298494 0.03333333\n\n\nI’m sure if you increase the number of replicates"
  },
  {
    "objectID": "causal/causal.html",
    "href": "causal/causal.html",
    "title": "6  Causal Statistics",
    "section": "",
    "text": "7 Propensity Scores\nWhat are propensity scores? A propensity score is defined as the probability of a treatment assignment conditional on some background observed covariates.\nThis allows one to mimic some of the conditions of a randomized controlled trial. In a randomized control trial, one can be fairly certain that each group is balanced due to the law of large numbers.\n\n\\begin{aligned}\n\\Pr(T = 1 \\, | X=x )\n\\end{aligned}\n For more background information see Matching Methods.\n\n\n8 Matching\nThere are many ways to match:\n\n\n9 R Implementation\nThe package MatchIt provides the function matchit to attempt matching of covariates automatically. There are many methods to matching covariates. The main page of the package can be found here. The package also uses Zelig for additional features."
  },
  {
    "objectID": "color/color.html#viewing-color-palettes",
    "href": "color/color.html#viewing-color-palettes",
    "title": "Appendix A — Colors in R",
    "section": "A.1 Viewing Color Palettes",
    "text": "A.1 Viewing Color Palettes\n\n\nCode\nhcl.pals(type = \"sequential\")\n\n\n [1] \"Grays\"         \"Light Grays\"   \"Blues 2\"       \"Blues 3\"      \n [5] \"Purples 2\"     \"Purples 3\"     \"Reds 2\"        \"Reds 3\"       \n [9] \"Greens 2\"      \"Greens 3\"      \"Oslo\"          \"Purple-Blue\"  \n[13] \"Red-Purple\"    \"Red-Blue\"      \"Purple-Orange\" \"Purple-Yellow\"\n[17] \"Blue-Yellow\"   \"Green-Yellow\"  \"Red-Yellow\"    \"Heat\"         \n[21] \"Heat 2\"        \"Terrain\"       \"Terrain 2\"     \"Viridis\"      \n[25] \"Plasma\"        \"Inferno\"       \"Rocket\"        \"Mako\"         \n[29] \"Dark Mint\"     \"Mint\"          \"BluGrn\"        \"Teal\"         \n[33] \"TealGrn\"       \"Emrld\"         \"BluYl\"         \"ag_GrnYl\"     \n[37] \"Peach\"         \"PinkYl\"        \"Burg\"          \"BurgYl\"       \n[41] \"RedOr\"         \"OrYel\"         \"Purp\"          \"PurpOr\"       \n[45] \"Sunset\"        \"Magenta\"       \"SunsetDark\"    \"ag_Sunset\"    \n[49] \"BrwnYl\"        \"YlOrRd\"        \"YlOrBr\"        \"OrRd\"         \n[53] \"Oranges\"       \"YlGn\"          \"YlGnBu\"        \"Reds\"         \n[57] \"RdPu\"          \"PuRd\"          \"Purples\"       \"PuBuGn\"       \n[61] \"PuBu\"          \"Greens\"        \"BuGn\"          \"GnBu\"         \n[65] \"BuPu\"          \"Blues\"         \"Lajolla\"       \"Turku\"        \n[69] \"Hawaii\"        \"Batlow\"       \n\n\nCode\nhcl.colors(256, palette = \"ag_sunset\")\n\n\n  [1] \"#4B1D91\" \"#4D1C92\" \"#4F1C92\" \"#511B92\" \"#531B93\" \"#551B93\" \"#571A93\"\n  [8] \"#591A94\" \"#5B1A94\" \"#5C1994\" \"#5E1995\" \"#601895\" \"#611895\" \"#631895\"\n [15] \"#651796\" \"#661796\" \"#681696\" \"#6A1697\" \"#6B1697\" \"#6D1597\" \"#6E1597\"\n [22] \"#701598\" \"#711498\" \"#731498\" \"#741498\" \"#761399\" \"#771399\" \"#791399\"\n [29] \"#7A1299\" \"#7C1299\" \"#7D129A\" \"#7F119A\" \"#80119A\" \"#81119A\" \"#83109A\"\n [36] \"#84109B\" \"#86109B\" \"#87109B\" \"#88109B\" \"#8A109B\" \"#8B0F9B\" \"#8C0F9B\"\n [43] \"#8E0F9C\" \"#8F0F9C\" \"#900F9C\" \"#910F9C\" \"#930F9C\" \"#940F9C\" \"#950F9C\"\n [50] \"#960F9C\" \"#980F9C\" \"#990F9C\" \"#9A109C\" \"#9B109C\" \"#9D109C\" \"#9E109D\"\n [57] \"#9F119D\" \"#A0119D\" \"#A1119D\" \"#A3119D\" \"#A4129D\" \"#A5129D\" \"#A6139D\"\n [64] \"#A7139C\" \"#A9149C\" \"#AA149C\" \"#AB159C\" \"#AC159C\" \"#AD169C\" \"#AE169C\"\n [71] \"#AF179C\" \"#B0179C\" \"#B2189C\" \"#B3199C\" \"#B4199C\" \"#B51A9C\" \"#B61B9B\"\n [78] \"#B71B9B\" \"#B81C9B\" \"#B91D9B\" \"#BA1D9B\" \"#BB1E9B\" \"#BC1F9A\" \"#BD209A\"\n [85] \"#BE209A\" \"#BF219A\" \"#C0229A\" \"#C12399\" \"#C22499\" \"#C32499\" \"#C42599\"\n [92] \"#C52698\" \"#C62798\" \"#C72898\" \"#C82897\" \"#C92997\" \"#CA2A97\" \"#CB2B96\"\n [99] \"#CC2C96\" \"#CD2D96\" \"#CE2E95\" \"#CF2E95\" \"#D02F95\" \"#D13094\" \"#D23194\"\n[106] \"#D23293\" \"#D33393\" \"#D43493\" \"#D53592\" \"#D63692\" \"#D73691\" \"#D83791\"\n[113] \"#D93890\" \"#D93990\" \"#DA3A8F\" \"#DB3B8F\" \"#DC3C8E\" \"#DD3D8E\" \"#DE3E8D\"\n[120] \"#DE3F8D\" \"#DF408C\" \"#E0418B\" \"#E1428B\" \"#E2438A\" \"#E24489\" \"#E34589\"\n[127] \"#E44588\" \"#E54687\" \"#E54787\" \"#E64886\" \"#E74985\" \"#E84A85\" \"#E84B84\"\n[134] \"#E94C83\" \"#EA4D82\" \"#EB4E82\" \"#EB4F81\" \"#EC5080\" \"#EC527F\" \"#EC537E\"\n[141] \"#ED557E\" \"#ED567D\" \"#ED587C\" \"#ED597B\" \"#EE5A7B\" \"#EE5C7A\" \"#EE5D79\"\n[148] \"#EE5E78\" \"#EE6078\" \"#EF6177\" \"#EF6276\" \"#EF6475\" \"#EF6575\" \"#EF6674\"\n[155] \"#F06873\" \"#F06972\" \"#F06A72\" \"#F06B71\" \"#F06D70\" \"#F16E6F\" \"#F16F6F\"\n[162] \"#F1706E\" \"#F1716D\" \"#F1736D\" \"#F1746C\" \"#F1756B\" \"#F2766B\" \"#F2776A\"\n[169] \"#F27969\" \"#F27A69\" \"#F27B68\" \"#F27C68\" \"#F27D67\" \"#F27F66\" \"#F28066\"\n[176] \"#F28165\" \"#F28265\" \"#F38364\" \"#F38464\" \"#F38563\" \"#F38763\" \"#F38862\"\n[183] \"#F38962\" \"#F38A61\" \"#F38B61\" \"#F38C61\" \"#F38D60\" \"#F38E60\" \"#F38F5F\"\n[190] \"#F3915F\" \"#F3925F\" \"#F3935F\" \"#F3945E\" \"#F3955E\" \"#F3965E\" \"#F3975E\"\n[197] \"#F3985E\" \"#F3995E\" \"#F39A5E\" \"#F39B5D\" \"#F39D5D\" \"#F39E5D\" \"#F29F5D\"\n[204] \"#F2A05E\" \"#F2A15E\" \"#F2A25E\" \"#F2A35E\" \"#F2A45E\" \"#F2A55E\" \"#F2A65F\"\n[211] \"#F2A75F\" \"#F2A85F\" \"#F1A960\" \"#F1AA60\" \"#F1AB61\" \"#F1AC61\" \"#F1AD62\"\n[218] \"#F1AE62\" \"#F1AF63\" \"#F1B063\" \"#F0B164\" \"#F0B265\" \"#F0B465\" \"#F0B566\"\n[225] \"#F0B667\" \"#F0B768\" \"#EFB869\" \"#EFB96A\" \"#EFBA6A\" \"#EFBB6B\" \"#EFBC6C\"\n[232] \"#EEBD6E\" \"#EEBE6F\" \"#EEBE70\" \"#EEBF71\" \"#EDC072\" \"#EDC173\" \"#EDC275\"\n[239] \"#EDC376\" \"#EDC477\" \"#ECC579\" \"#ECC67A\" \"#ECC77C\" \"#ECC87D\" \"#EBC97F\"\n[246] \"#EBCA81\" \"#EBCB82\" \"#EACC84\" \"#EACD86\" \"#EACE88\" \"#EACF8A\" \"#E9D08C\"\n[253] \"#E9D18F\" \"#E8D191\" \"#E8D295\" \"#E7D39A\"\n\n\nCode\nplot(0, \n     type = \"n\",\n     axes = F,\n     xlab = \"\", ylab = \"\",\n     xlim = c(0, 1),\n     ylim = c(0, 1),\n     xaxs = \"i\",\n     yaxs = \"i\",\n     omi = rep(0, 4),\n     oma = rep(0, 4),\n     mar = rep(0, 4),\n     frame.plot = F)\nrect(0, .5, .8, 1, col = terrain.colors(256))\n\n\n\n\n\n\n\nCode\ngrid.raster(seq(0, 1, .01))\n\n\n\n\n\n\n\nCode\n# you can color the matrix directly as well, and generate the raster that way\nredGradient <- matrix(hcl(0, 80, seq(50, 80, 10)),\n                      nrow=4, ncol=5)\n\ngrid.raster(redGradient)"
  }
]