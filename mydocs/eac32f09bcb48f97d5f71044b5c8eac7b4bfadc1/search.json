[
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Personal Documentation",
    "section": "0.1 Quarto",
    "text": "0.1 Quarto\nWelcome to my documentation."
  },
  {
    "objectID": "advanced/advanced.html#plotting-in-r",
    "href": "advanced/advanced.html#plotting-in-r",
    "title": "2  Advanced R",
    "section": "2.1 Plotting in R",
    "text": "2.1 Plotting in R\nThe interface of plotting is powered by the package grid\n\n2.1.1 grid package\n\ngrobs : graphical object\nviewports: defining regions where to plot\n\n\n\nCode\ngrid.circle(x=seq(0.1, 0.9, length=100), # position\n            y=0.5 + 0.4*sin(seq(0, 2*pi, length=100)),\n            r=abs(0.1*cos(seq(0, 2*pi, length=100))))\n\n\n\n\n\n\nviewports\nviewports are “drawing contexts”, basically defining regions of plotting.\n\n\nCode\n# An easy first example\ngrid.newpage()\nvp1 <- viewport(x = .5, y = .5,\n                height = .9, width = .9,\n                xscale=c(0,1), yscale=c(0,1))\n\nfor (i in 1:5) {\n pushViewport(vp1)\n grid.rect()\n}\n\n\n\n\n\nCode\n# current.vpTree() # check the drawing context\n\n\nWhat’s going on here?!\nThe viewports are stored in a tree structure, and the active context of the viewport\nExample of recreating a scatter plot\n\n\nCode\nx <- runif(10)\ny <- runif(10)\nplot(x, y)\n\n\n\n\n\nLooking above, we can decompose the plot into 9 sections,\n\n\nCode\n# first layer\nmain_vp <- viewport(layout = grid.layout(3, 3,\n                                         widths = unit(c(5, 1, 2), c(\"lines\", \"null\", \"lines\")),\n                                         heights = unit(c(5, 1, 2), c(\"lines\", \"null\", \"lines\"))),\n                    name = \"main\")\n\n# second layer \nmr_vp <- viewport(layout.pos.row = 2, layout.pos.col = 3, name = \"margin_right\")\nmb_vp <- viewport(layout.pos.row = 3, layout.pos.col = 2, name = \"margin_bottom\")\nmt_vp <- viewport(layout.pos.row = 1, layout.pos.col = 2, name = \"margin_top\")\nml_vp <- viewport(layout.pos.row = 2, layout.pos.col = 3, name = \"margin_left\")\ncenter_vp <- viewport(layout.pos.row = 2, layout.pos.col = 2, name = \"center\",\n                      xscale = extendrange(x),\n                      yscale = extendrange(y))\n\n# create the viewport\nsplot <- vpTree(main_vp, vpList(mr_vp, mb_vp, mt_vp, ml_vp, center_vp))\nsplot # Currently just the description, and not associated with the graphics device\n\n\nviewport[main]->(viewport[margin_right], viewport[margin_bottom], viewport[margin_top], viewport[margin_left], viewport[center]) \n\n\nCode\n# viewport of current device\ncurrent.vpTree() # current vp tree of graphic device\n\n\nviewport[ROOT] \n\n\nCode\ncurrent.viewport() # This is the current viewport LAYER (in the tree) of our graphics device\n\n\nviewport[ROOT] \n\n\nCode\n# In order to make the viewport splot active, we must add it to the vpTree of our device\npushViewport(splot)\ncurrent.vpTree() # notice how our tree has changed\n\n\nviewport[ROOT]->(viewport[main]->(viewport[margin_right], viewport[margin_top], viewport[margin_left], viewport[margin_bottom], viewport[center])) \n\n\nCode\ncurrent.viewport() # our current layer is center\n\n\nviewport[center] \n\n\nCode\ngrid.points(x, y) # drawn to center, for some reason this isn't showing up\ngrid.xaxis()\ngrid.yaxis()\ngrid.rect()\n\n# navigate to bottom margin\nseekViewport(\"margin_bottom\")\ngrid.text(\"Random X\", y=unit(1, \"lines\"))\n\nseekViewport(\"margin_left\")\ngrid.text(\"Random Y\", x=unit(1, \"lines\"), rot = 90)\n\n\n\n\n\nRun all the code all at once, rmarkdown does some weird stuff with viewports\n\n\nCode\npushViewport(viewport())\nupViewport()\npushViewport(viewport(name = \"A\"))\nupViewport()\npushViewport(viewport(name = \"B\"))\nupViewport()\n\n\n\n\n\nCode\ncurrent.vpTree()\n\n\nviewport[ROOT]->(viewport[GRID.VP.2], viewport[A], viewport[B])"
  },
  {
    "objectID": "advanced/advanced.html#gtable",
    "href": "advanced/advanced.html#gtable",
    "title": "2  Advanced R",
    "section": "2.2 gtable",
    "text": "2.2 gtable\nUnofficial Guide\nThe important structures of gtable object are\n\ngrobs:d\n\n\n\nCode\n# creating a gtable\ngtable(unit(1:3, c(\"cm\")),\n       unit(5, \"cm\"))\n\n\nTableGrob (1 x 3) \"layout\": 0 grobs\n\n\nCode\na <- rectGrob(gp = gpar(fill = \"red\"))\n\na <- rectGrob(gp = gpar(fill = \"red\"))\nb <- grobTree(rectGrob(), textGrob(\"new\\ncell\"))\nc <- ggplotGrob(qplot(1:10,1:10))\nd <- linesGrob()\nmat <- matrix(list(a, b, c, d), nrow = 2)\ng <- gtable_matrix(name = \"demo\", grobs = mat, \n                   widths = unit(c(2, 4), \"cm\"), \n                   heights = unit(c(2, 5), c(\"in\", \"lines\")))\ng\n\n\nTableGrob (2 x 2) \"demo\": 4 grobs\n  z     cells name                 grob\n1 1 (1-1,1-1) demo   rect[GRID.rect.14]\n2 2 (2-2,1-1) demo gTree[GRID.gTree.15]\n3 3 (1-1,2-2) demo       gtable[layout]\n4 4 (2-2,2-2) demo lines[GRID.lines.57]"
  },
  {
    "objectID": "advanced/advanced.html#gridextra",
    "href": "advanced/advanced.html#gridextra",
    "title": "2  Advanced R",
    "section": "2.3 gridextra",
    "text": "2.3 gridextra\n\n\nCode\ndummy_grob <- function(id)  {\n  grobTree(rectGrob(gp=gpar(fill=id, alpha=0.5)), textGrob(id))\n}\ngs <- lapply(1:9, dummy_grob)\ngrid.arrange(ncol=4, grobs=gs, \n               top=\"top\\nlabel\", bottom=\"bottom\\nlabel\", \n               left=\"left\\nlabel\", right=\"right\\nlabel\")"
  },
  {
    "objectID": "advanced/advanced.html#r-macos",
    "href": "advanced/advanced.html#r-macos",
    "title": "2  Advanced R",
    "section": "2.4 R MacOS",
    "text": "2.4 R MacOS\n\n2.4.1 Matrix Operations\n\nWhich BLAS library is used by R?\nUsing the faster veclib dynamically linked library\nA tutorial/discussion from 2019 about rebuilding R binary with openBLAS and openMP\n\nThere are two common libraries for matrix operations, LAPACK and BLAS. There are many varieties of these two libraries,\n\n\nCode\nsessionInfo() # Shows library used for matrix products\n\n\nR version 4.2.0 (2022-04-22)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur/Monterey 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] gridExtra_2.3   gtable_0.3.0    forcats_0.5.1   stringr_1.4.0  \n [5] dplyr_1.0.9     purrr_0.3.4     readr_2.1.2     tidyr_1.2.0    \n [9] tibble_3.1.8    ggplot2_3.3.6   tidyverse_1.3.1\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.1.2  xfun_0.31         haven_2.5.0       colorspace_2.0-3 \n [5] vctrs_0.4.1       generics_0.1.3    htmltools_0.5.3   yaml_2.3.5       \n [9] utf8_1.2.2        rlang_1.0.6       pillar_1.8.0      glue_1.6.2       \n[13] withr_2.5.0       DBI_1.1.3         dbplyr_2.2.0      modelr_0.1.8     \n[17] readxl_1.4.0      lifecycle_1.0.1   munsell_0.5.0     cellranger_1.1.0 \n[21] rvest_1.0.2       htmlwidgets_1.5.4 codetools_0.2-18  evaluate_0.15    \n[25] labeling_0.4.2    knitr_1.39        tzdb_0.3.0        fastmap_1.1.0    \n[29] fansi_1.0.3       broom_0.8.0       backports_1.4.1   scales_1.2.0     \n[33] jsonlite_1.8.0    farver_2.1.1      fs_1.5.2          hms_1.1.1        \n[37] digest_0.6.29     stringi_1.7.8     cli_3.4.1         tools_4.2.0      \n[41] magrittr_2.0.3    crayon_1.5.1      pkgconfig_2.0.3   ellipsis_0.3.2   \n[45] xml2_1.3.3        reprex_2.0.1      lubridate_1.8.0   assertthat_0.2.1 \n[49] rmarkdown_2.14    httr_1.4.3        rstudioapi_0.13   R6_2.5.1         \n[53] compiler_4.2.0   \n\n\nOf note, R binary versus the CRAN version of the R binary have different libraries bundled together. For older versions of Rstudio, there is a BLAS library that is provided with Apple that sometimes is faster. Ultimately, R will look for a symlinked library in the lib/ folder.\n\n\nCode\n# Install openblas\nbrew install openblas\n\n# Some potentially useful symlinking commands\nln -sf /usr/local/opt/openblas/lib/libopenblas.dylib /usr/local/Cellar/r/4.1.2/lib/R/lib/libRblas.dylib\nln -sf /usr/local/opt/openblas/lib/libopenblas.dylib /usr/local/Cellar/r/4.1.2/lib/libRblas.dylib"
  },
  {
    "objectID": "advanced/advanced.html#metaprogramming",
    "href": "advanced/advanced.html#metaprogramming",
    "title": "2  Advanced R",
    "section": "2.5 Metaprogramming",
    "text": "2.5 Metaprogramming\n\n2.5.1 Expressions\n\nconstants\nsymbols\n\n\n\nCode\nlibrary(rlang)\n\n\n\nAttaching package: 'rlang'\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int,\n    flatten_lgl, flatten_raw, invoke, splice\n\n\nCode\n# two ways of creating a symbol\nsym(\"x\") # symbol\n\n\nx\n\n\nCode\nclass(expr(x)) # symbol\n\n\n[1] \"name\"\n\n\nCode\nquo(x)\n\n\n<quosure>\nexpr: ^x\nenv:  global\n\n\nCode\n?quo\n\n\nHelp on topic 'quo' was found in the following packages:\n\n  Package               Library\n  tidyselect            /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n  rlang                 /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n  ggplot2               /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n  dplyr                 /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n\nUsing the first match ...\n\n\nCode\nexpr(\"x\") # expression\n\n\n[1] \"x\"\n\n\nCode\nas_string(sym(\"x\")) \n\n\n[1] \"x\"\n\n\nCode\nas_name(sym(\"x\")) # character\n\n\n[1] \"x\"\n\n\nCode\nclass(as_name(\"x\")) # character\n\n\n[1] \"character\"\n\n\n\n\n2.5.2 Quosures\nQuosures are special type of defused expression that also keeps track of the original context the expression was written in.\n\nquo\n`quo is used to capture local expressions, and also track the environment the expression was written.\n\n\nenquo\n\n{% raw %} enquo is used to defuse function arguments, the {{ is short for this common pattern. The arguments need to be defused because otherwise R will try to evaluate the expression in its original environment. {% endraw %}\n\n\nCode\nmy_function <- function(var) {\n  var <- enquo(var)\n  their_function(!!var)\n}\n\n# Equivalently\nmy_function <- function(var) {\n  their_function({{ var }})\n}"
  },
  {
    "objectID": "approximation/approximation.html#edgeworth-expansion",
    "href": "approximation/approximation.html#edgeworth-expansion",
    "title": "3  Approximation",
    "section": "3.1 Edgeworth Expansion",
    "text": "3.1 Edgeworth Expansion\nEdgeworth expansion is an improvement to the normal approximation that includes skewness (degree 2) and kurtosis (degree three). The first degree approximation amounts to the normal approximation.\nSuppose we have a general distribution X_i with mean \\mu and variance \\sigma^2. If we standardize the sum, we have a statistic that is approximately N(0, 1)\n\n\\begin{aligned}\nZ = \\frac{1}{\\sqrt{n}} \\sum_i \\frac{X_i - \\mu}{\\sigma}\n\\end{aligned}\n Edgeworth expansion of the pdf of Z is\n\n\\begin{aligned}\nf_Z(z) = \\underbrace{\\underbrace{\\underbrace{\\phi(z)}_{\\text{degree 1}} + \\frac{\\lambda_3}{6\\sqrt{n}}H_3(z)\\phi(z)}_{\\text{degree 2}} + \\left(\\frac{\\lambda_4}{24n}H_4(z)\\phi(z) + \\frac{\\lambda_3^2}{72n}H_6(z)\\phi(z)\\right)}_{\\text{degree 3}} + o\\left(\\frac{1}{n}\\right)\n\\end{aligned}\n where:\n\n\\lambda_r denotes the cumulant of Z, related to the cumulants of X_i by \\lambda_r = \\kappa_r / \\sigma^r if cumulants of X_i are represented by \\kappa_r\nH_r are hermite polynomials.\n\nWe look at how well this approximation works for the sum of chisq random variables. The cumulants of the chisq variables.\nSuppose X_i \\sim \\chi^2_2\nIf we study the mean, the true distribution is \\bar{X_i} \\sim \\Gamma(\\frac{n * df}{2}, \\frac{n}{2}).\n\n\nCode\nset.seed(1)\nn <- 3\n\n# Sampled distribution\nrand_x <- rchisq(1000, 2) + rchisq(1000, 2) + rchisq(1000, 2)\nrand_x_mean <- rand_x / n\n\n# True distribution\n# Gamma(3, 3/2)\nx <- seq(0, 10, by=.1)\ntrue_y <- dgamma(x, 3, 3/2)\n\n# Normal Approximation\n# N(2, 4/3)\nnorm_approx_y <- dnorm(x, mean = 2, sd = sqrt(4/3))\n\n\nFor the edgeworth expansion, since we’re studying the mean \\bar X, this is a transformation of Z, that is,\n\n\\begin{aligned}\n\\bar X = \\frac{(Z\\sqrt{n\\sigma^2} + n\\mu)}{n}\n\\end{aligned}\n so the inverse formula that we use in the pdf transformation formula,\n\n\\begin{aligned}\nZ = \\frac{n\\bar X - n\\mu}{\\sqrt{n\\sigma^2}}\n\\end{aligned}\n The transformation formula would then say,\n\n\\begin{aligned}\nf_{\\bar X}(x) &= \\underbrace{f_Z\\left(\\frac{nx - n\\mu}{\\sqrt{n\\sigma^2}}\\right)}_{\\text{approximated by Edgeworth}} \\left| \\frac{\\sqrt{n}}{\\sigma}\\right|\n\\end{aligned}\n\n\n\nCode\nherm <- Vectorize(function(x, degree = 1 ) {\n  switch(degree,\n         x,\n         x^2 -1,\n         x^3 - 3*x,\n         x^4 - 6*x^2 + 3,\n         x^5 - 10 * x^3 + 15 * x,\n         x^6 - 15*x^4 + 45*x^2 - 15)\n}, \"x\")\n\n\n# Edgeworth expansion, degree 2\nedgeworth_mean2 <- function(x, n, mu, sigma2, lambda3){\n  z <- sqrt(n) * (x - mu) / sqrt(sigma2)\n  sqrt(n / sigma2) * dnorm(z) * (1 + lambda3 / (6 * sqrt(n)) * herm(z, 3))\n}\n\n# Edgeworth expansion, degree 3\nedgeworth_mean3 <- function(x, n, mu, sigma2, lambda3, lambda4) {\n  z <- sqrt(n) * (x - mu) / sqrt(sigma2) # transformation\n  sqrt(n / sigma2) * # transformation scaling\n    dnorm(z) * (1 +  # degree 1 terms\n      lambda3 / (6 * sqrt(n)) * herm(z, 3) + # degree 2 terms\n      lambda4 / (24 * n) * herm(z, 4) + # degree 3 terms\n      lambda3^2 / (72 * n) * herm(z, 6)) # degree 3 terms\n}\n\n\n\n\nCode\n# Creating unified plot with all the \nhist(rand_x_mean, breaks=50, freq = FALSE, main = \"Mean of chisq (Edgeworth)\", xlab = \"x\")\nlines(x, true_y, lwd = 2)\nlines(x, norm_approx_y, lty = 2, col = 2, lwd = 2)\nlines(x, edgeworth_mean2(x, 3, 2, 4, 2), lty = 2, col = 3, lwd = 2)\nlines(x, edgeworth_mean3(x, 3, 2, 4, 2, 6), lty = 2, col = 4, lwd = 2)\nlegend(\"topright\", \n       legend = c(\"True Distribution\", \n                  \"Normal Approximation\", \n                  \"Edgeworth (2 deg)\", \n                  \"Edgeworth (3 deg)\"),\n       col = c(1, 2, 3, 4), \n       lty = c(1, 2, 2, 2))\n\n\n\n\n\n\n3.1.1 R Packages\n\nEW\n\ndoes finite differencing for the moment estimators\nonly two functions, edgeworth and saddlepoint approximations\n\nEQL (extended quasi likelihood)\n\ndoes extended quasi-likelihood and saddlepoint approximations too\nin my opinion, very well written and clear readable code. Used source code to figure out my own errors.\n\n\n\n\nCode\nlibrary(EQL)\n# Usage for our example\new <- edgeworth(x, n = 3, mu = 2, sigma2 = 4, rho3 = 2, rho4 = 6, type = \"mean\", deg = 3)\n\n# They match with our results\nhist(rand_x_mean, breaks=50, freq = FALSE, main = \"Mean of chisq (Edgeworth)\", xlab = \"x\")\nlines(x, true_y)\nlines(x, edgeworth_mean3(x, 3, 2, 4, 2, 6), col = 2)\nlines(x, ew$approx, lty = 2, lwd = 2, col = 3)\n\n\n\n\n\n\n\n3.1.2 Further Resources\n\nhttp://personal.psu.edu/drh20/asymp/fall2004/lectures/edgeworth.pdf\nWriting Projects"
  },
  {
    "objectID": "approximation/approximation.html#saddlepoint-approximation",
    "href": "approximation/approximation.html#saddlepoint-approximation",
    "title": "3  Approximation",
    "section": "3.2 Saddlepoint Approximation",
    "text": "3.2 Saddlepoint Approximation\n\nhttps://stats.stackexchange.com/questions/191492/how-does-saddlepoint-approximation-work/191781\nPreliminary Asymptotics"
  },
  {
    "objectID": "bayesian/bayesian.html#introduction-to-bayesian-modeling",
    "href": "bayesian/bayesian.html#introduction-to-bayesian-modeling",
    "title": "4  Bayesian",
    "section": "4.1 Introduction to Bayesian Modeling",
    "text": "4.1 Introduction to Bayesian Modeling\n\n4.1.1 Software Ecosystem\nThe software mostly is split up by the methodology that each method uses. Seems like results will vary, and syntax also will vary. Trade-offs on speed and use cases.\n\nR Software\n\nINLA - integrated nested laplace approximations. Uses R formula syntax\n\nnot on CRAN because it uses a C library that is machine dependent\noption to use pardiso optimizer, for which you need a license.\n\nBUGS - MCMC based sampling\n\nhas many different versions, seems easier to install on windows\n\nJAGS\n\nthe original MCMC based solutions, has an R implementation with “rjags”\nStands for “Just Another Gibbs Sampler”\ntext file specification of the model\nreimplementation of “BUGS”\n\nrstan - does some magical Hamiltonion Monte Carlo, which supp\n\nbrms - uses STAN as backend, but allows linear and nonlinear models with R formula syntax. Offers more flexibility than rstanarm\nrstanarm - meant as a drop in replacement using STAN for many base R and lme4 models. The main difference is that these models are precompiled in STAN code, while brms will compile everytime into new STAN code, thus will be slower for basic models. However, what you trade in slowness you get flexibility.\nblavaan - latent variable modeling, like SEM modeling, latent growth curve models, confirmatory factor analysis."
  },
  {
    "objectID": "bayesian/bayesian.html#inla",
    "href": "bayesian/bayesian.html#inla",
    "title": "4  Bayesian",
    "section": "4.2 INLA",
    "text": "4.2 INLA\nBroadly speaking, INLA is for a broad class of models used for Guassian markov random fields (GMRF). INLA tends to be faster than other methods, because the software is based on Laplace Approximation. Because of that, it is also a deterministic method unlike stochastic MCMC based software like STAN or BUGS/JAGS\n\n4.2.1 Installation Issues\nThe standard installation as recommended by the website is:\n\n\nCode\ninstall.packages(\"INLA\",repos=c(getOption(\"repos\"),INLA=\"https://inla.r-inla-download.org/R/testing\"), dep=TRUE) # R > 4.1\n\n\nBut I keep getting an error:\n\nError in inla.call.builtin() : INLA installation error; no such file\n\nso i tried installing directly from github with devtools.\n\n/usr/local/lib/R/4.1/site-library/INLA/bin/mac/64bit/inla.run: line 125: /usr/local/lib/R/4.1/site-library/INLA/bin/mac/64bit/inla: No such file or directory\n\nThe “devel” version unfortunately gives the following error, so trying the stable version.\n\n\nCode\ndevtools::install_github(repo = \"https://github.com/hrue/r-inla\", ref = \"stable\", subdir = \"rinla\", build = FALSE)\n\n\nThe stable version also didn’t work, was giving some dynamic library errors, finally tried installing manually and it seemed to work. INLA Binaries for R 4.1. The thread that gave me idea of installing manually.\nIn order to install it manually,\n\nDownload the precompiled binary here: https://inla.r-inla-download.org/R/testing/bin/macosx/contrib/4.1/\n\n\nback up directories if you need a different binary\n\n\nFrom R, run install.packages(\"pathtobinary.tgz\", repos = NULL)\n\n\nPardiso License\nPardiso is parallel computing library, stands for “Parallel Sparse Direct Solver”, and academics can get a license, just need to set the $PARDISO_LIC_PATH to where you download the license\nThere might be some useful documentation on how R finds its matrix libraries.\nSee the Advanced R sections for more on how R finds its libraries. Ultimnately, after some permissioning issues, we have a working example.\n\n\n\n4.2.2 INLA Basics\ninla() - this is the main function that is used from the package.\n\nformula - formula object\ndata - the dataframe\nfamily - string or vector to\n\n\n\n4.2.3 INLA Examples (simulated)\n\nLinear Regression\nWe first simulate some data, and plot the points that\n\n\nCode\n# create the simulated data\nN <- 100 # 500, 5000, 25000, 100000\nx <- rnorm(N, mean = 6, sd = 2)\ny <- rnorm(N, mean = x, sd = 1)\ndat_simple <- list(x = x, y = y, N = N)\n\n\n\n\nCode\n# Basic fit of model, assuming gaussian link\nmod <- inla(y ~ x,\n  family = \"gaussian\",\n  data = dat_simple,\n  control.predictor = list(link = 1)\n)\n\n\nYou can plot the posterior mean for each of the observations, but this plot is not particularly useful.\n\n\nCode\n# attributes(mod)\n# ?plot.inla\n# methods(\"inla\")\nplot(mod, plot.prior = TRUE, single=TRUE) # Plotting the posterior mean for fitted values, with index on axis.\n\n\nIn order to access the estimated coefficients and standard deviations of each parameter, we can use\n\n\nCode\nmod$summary.fixed\nmod$summary.random\nmod$summary.fitted.values\n\n\n\n\nCode\n# Plot solution from INLA, with prediction bounds\ng_inla_slr <- data.frame(dat_simple) %>% ggplot(aes(x, y)) +\n  geom_point() +\n  geom_ribbon(aes(y = mean, ymin = `0.025quant`, ymax=`0.975quant`),\n              data = mod$summary.fitted.values,\n              alpha = .1) +\n  geom_smooth(aes(y = mean), formula = y~x, method=\"lm\", data = mod$summary.fitted.values) +\n  labs(title = \"INLA\")\n  \n# Plot solution from lm\ng_smooth_slr <- data.frame(dat_simple) %>% ggplot(aes(x, y)) + \n  geom_point() + \n  geom_smooth(formula = y~x, method=\"lm\") +\n  labs(title = \"LM\")\n  \n\ng_inla_slr + g_smooth_slr\n\n\n\n\n\nClearly the INLA solution is very very similar to the one produced by LM as we expected.\n\n\n\n4.2.4 Laplace Approximation\nLet’s see an example with a closed form solution conjugate families. A good example of one is the Poisson-Gamma.\n\n\\begin{aligned}\nY | \\lambda &\\sim Poisson(\\lambda) \\\\\n\\lambda &\\sim \\Gamma(\\alpha, \\beta)\n\\end{aligned}\n Suppose we observe that y = 6, and we set a prior \\alpha=\\beta=1.\nThe update for poisson-gamma upon observing y_1, y_2, y_3, \\dots, y_n, (note this is using the rate parameterization, \\beta^\\alpha in the numerator)\n\n\\begin{aligned}\n\\Gamma(\\alpha, \\beta) \\longrightarrow \\Gamma(\\alpha + \\sum_i y_i \\, , \\beta + n)\n\\end{aligned}\n Hence, the true posterior is \\Gamma(1+6, 1 +1), which has density,\n\n\\begin{aligned}\np(\\lambda | Y=6) = \\frac{2^7}{\\Gamma(2)}\\exp(-2 \\lambda)\\lambda^{7 - 1}\n\\end{aligned}\n If we didn’t know this, then we would be trying to approximate the integral in the denominator of bayes rule, we do this with Laplace’s Approximation,\n\n\\begin{aligned}\np(\\lambda|y) &= \\frac{f(y|\\lambda) \\xi(\\lambda)}{\\int f(y|\\lambda)\\xi(\\lambda)\\, d\\lambda} \\\\\n\\end{aligned}\n to approximate the bottom, we have\n\n\\begin{aligned}\n\\int f(y|\\lambda)\\xi(\\lambda)\\, d\\lambda \\approx f(y|\\lambda_0)\\xi(\\lambda_0)\\sqrt{\\frac{2\\pi}{-h''(\\lambda_0)}}\n\\end{aligned}\n\nwhere h = \\log \\left[f(y|\\lambda) \\xi(\\lambda)\\right], and h''(\\lambda_0) is the second derivative evaluated at \\lambda_0, the maximum of the integrand f(y|\\lambda) \\xi(\\lambda). Finding the maximum can be done with an optimization procedure, optim or it can be done analytically.\nHere’s a dump of the calculations!\n\n\\begin{aligned}\nf(y=6|\\lambda)\\xi(\\lambda) &= \\frac{e^{-\\lambda} \\lambda^6}{6!}\\exp(-\\lambda) \\\\\nh(\\lambda) &= -\\log 6! -2\\lambda + 6 \\log \\lambda \\\\\nh'(\\lambda) &= \\frac{6}{\\lambda} -2\n\\end{aligned}\n\nSetting h'(\\lambda) = 0, we find the maximum of the integrand to be \\lambda_0 = 3\n\n\\begin{aligned}\nh''(\\lambda) &= -\\frac{6}{\\lambda^2} \\\\\n-h''(\\lambda)^{-1} &= \\frac{\\lambda^2}{6}\n\\end{aligned}\n Evaluating the maximum, we get -h''(\\lambda_0)^{-1} = 1.5. Now we’re ready to code it up!\n\n\nCode\n# Poisson-Gamma Update\n# gamma(a, b) -> gamma(a + \\sum_i x_i, b + n)\n\n# Let prior be gamma(1, 1)\n# Observe y = 6\n# True Posterior is gamma(1 + 6, 1 + 1)\n\nlgrid <- seq(0, 10, .01)\n\nplot(lgrid, dgamma(lgrid, 1, 1), type=\"l\", xlab=\"lambda\", ylab=\"density\")\nlines(lgrid,dgamma(lgrid, 7, 2), type=\"l\", col=2)\nlegend(\"topright\",legend =  c(\"Prior\", \"Posterior\"),  col = c(1, 2),lty=1)\n\n\n\n\n\n\n\nCode\nest_mode <- (6) / 2 # (alpha + x) / beta\nest_cov <- 1 / ((1 + 6 - 1) / est_mode^2) # (alpha + x - 1) / mode^2\nest_cov\n\n\n[1] 1.5\n\n\nCode\n# integral approximation is\n# likelihood * prior * constant based on curvurature of function\n# The constant in the denominator of Bayes rule, also the integral approximation\nZ <- dpois(6, lambda = est_mode) * dgamma(est_mode, 1, 1) * sqrt(2 * pi * est_cov)\n\nposterior <- function(lambda) {\n  dpois(6, lambda = lambda) * dgamma(lambda, 1, 1) / Z\n}\n\n\n\n\nCode\nplot(lgrid, dgamma(lgrid, 1, 1), type=\"l\", xlab=\"lambda\", ylab=\"density\",\n     main = \"Poisson-Gamma Example with Laplace Approximation\")\nlines(lgrid,dgamma(lgrid, 7, 2), type=\"l\", col=2, lwd=3)\nlines(lgrid, posterior(lgrid), type = \"l\", col=3)\nlines(lgrid, dnorm(lgrid, mean=est_mode, sd = sqrt(est_cov)), type = \"l\", col=3, lty=2) # The approximating normal distribution used in laplace approximation, centered on posterior mean\nlegend(\"topright\", legend = c(\"Prior\", \"True Posterior\", \"Posterior with Laplace approximated Integral\", \"Laplace Approximation\"), \n       lty = c(1, 1, 1, 2),\n       col = c(1, 2, 3, 3),\n       cex = .8)\n\n\n\n\n\n\n\nCode\n# A different way of visualizing the above information\nplot(dgamma(0:10, 7, 2), posterior(0:10), pch=19, cex=.2, xlab = \"True Posterior density, Gamma(7,2)\",\n     ylab = \"posterior density by laplace approximation\",\n     main = \"Comparison of True Posterior and Laplace Approximation\")\nabline(0,1, col=2)\nlegend(\"bottomright\", col = c(1,2), pch = c(19,-1), lty=c(0, 1), legend = c(\"evaluated densities\", \"line of equality\"))\n\n\nResources used when writing this section - https://www.sumsar.net/blog/2013/11/easy-laplace-approximation/ - https://bookdown.org/rdpeng/advstatcomp/laplace-approximation.html - https://en.wikipedia.org/wiki/Laplace%27s_method\n\n\n4.2.5 Resources for INLA\n\nWorked Examples for Bayesian Analysis with INLA from Faraway - author of the “Extending the Linear Model” book, this site also includes many mixed effects model examples."
  },
  {
    "objectID": "bayesian/bayesian.html#stan",
    "href": "bayesian/bayesian.html#stan",
    "title": "4  Bayesian",
    "section": "4.3 STAN",
    "text": "4.3 STAN\nSTAN is sophisticated because it uses the No U-Turn Sampling (NUTS), and thus has a faster convergence.\n\nthere’s a “warm-up” rather than a “burn-in” for the sampling.\nfaster for a complex model\n\n\n4.3.1 rstanarm\nThe functions offered in rstanarm, and quick explanation\n\nstan_lm - basic linear model\nstan_aov - calls stan_lm in the backend\nstan_lmer - calls\n\n\n\nCode\ndata(\"weightgain\", package = \"HSAUR3\")\n# Standard Frequentist Method\nfmod <- aov(weightgain ~ source * type, data = weightgain)\n\n\n# Bayesian aov\nbmod <- stan_aov(weightgain ~ source * type, data = weightgain,\n         prior = R2(location = .5),\n         adapt_delta = .999,\n         seed = 1234)\n\n\n\nSAMPLING FOR MODEL 'lm' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.55 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.286155 seconds (Warm-up)\nChain 1:                0.22461 seconds (Sampling)\nChain 1:                0.510765 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'lm' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 9e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.276645 seconds (Warm-up)\nChain 2:                0.263396 seconds (Sampling)\nChain 2:                0.540041 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'lm' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.322498 seconds (Warm-up)\nChain 3:                0.182252 seconds (Sampling)\nChain 3:                0.50475 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'lm' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 9e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.30836 seconds (Warm-up)\nChain 4:                0.199297 seconds (Sampling)\nChain 4:                0.507657 seconds (Total)\nChain 4: \n\n\nCode\nsummary(bmod)\n\n\n\nModel Info:\n function:     stan_aov\n family:       gaussian [identity]\n formula:      weightgain ~ source * type\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 40\n\nEstimates:\n                       mean   sd    10%   50%   90%\n(Intercept)           98.7    4.4  93.1  98.7 104.4\nsourceCereal         -12.7    6.2 -20.6 -12.7  -4.8\ntypeLow              -18.7    6.2 -26.4 -18.8 -10.4\nsourceCereal:typeLow  16.9    8.8   5.4  17.1  27.8\nsigma                 14.8    1.8  12.8  14.7  17.2\nlog-fit_ratio          0.0    0.1  -0.1   0.0   0.2\nR2                     0.2    0.1   0.1   0.2   0.3\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 87.3    3.4 83.0  87.2  91.6 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                     mcse Rhat n_eff\n(Intercept)          0.1  1.0  1492 \nsourceCereal         0.1  1.0  1853 \ntypeLow              0.2  1.0  1422 \nsourceCereal:typeLow 0.2  1.0  2093 \nsigma                0.0  1.0  2533 \nlog-fit_ratio        0.0  1.0  2240 \nR2                   0.0  1.0  1733 \nmean_PPD             0.1  1.0  4009 \nlog-posterior        0.1  1.0   850 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nCode\n# Bayesian lmer method\n\nbmmod <- stan_lmer(weightgain ~ 1 + (1|source) + (1 | type) + (1 | source:type),\n                   data = weightgain,\n                   prior_intercept = cauchy(),\n                   prior_covariance = decov(shape = 2, scale = 2),\n                   adapt_delta = 0.999, seed = 12345)\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 7.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.78 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 6.87361 seconds (Warm-up)\nChain 1:                5.0979 seconds (Sampling)\nChain 1:                11.9715 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 6.09438 seconds (Warm-up)\nChain 2:                5.36233 seconds (Sampling)\nChain 2:                11.4567 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2.8e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 7.68205 seconds (Warm-up)\nChain 3:                7.34715 seconds (Sampling)\nChain 3:                15.0292 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 6.01372 seconds (Warm-up)\nChain 4:                7.2315 seconds (Sampling)\nChain 4:                13.2452 seconds (Total)\nChain 4: \n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\n\nCode\ntibble(freq = coef(fmod),\n       bayes = coef(bmod))\n\n\n# A tibble: 4 × 2\n   freq bayes\n  <dbl> <dbl>\n1 100    98.7\n2 -14.1 -12.7\n3 -20.8 -18.8\n4  18.8  17.1\n\n\n\n\n4.3.2 Resources for STAN\n\nFaraway worked examples in STAN - largely examples from his books, and pretty basic analyses."
  },
  {
    "objectID": "bayesian/bayesian.html#jags",
    "href": "bayesian/bayesian.html#jags",
    "title": "4  Bayesian",
    "section": "4.4 JAGS",
    "text": "4.4 JAGS\nJAGS stands for “Just another Gibbs Sampler”, and operates by MCMC.\n\n4.4.1 Worked Examples\n\nLinear Regression (simulated)\nWe’ll run the same model that did for INLA, a SLR.\n\n\nCode\n# https://www.precision-analytics.ca/articles/a-gentle-inla-tutorial/\nN <- 100 # 500, 5000, 25000, 100000\nx <- rnorm(N, mean = 5, sd = 1)\nnu <- rnorm(N, 0, 0.1)\nmu <- exp(1 + 0.5 * x + nu)\ny <- rpois(N, mu)\n\n\n\n\nCode\n# https://www.precision-analytics.ca/articles/a-gentle-inla-tutorial/\n\n# Save BUGS style specification to .txt file\ncat(\"model {\n  for(i in 1:N) {\n    y[i] ~ dnorm(mu[i], tau)\n    mu[i] <- alpha + beta * x[i]\n  }\n  alpha ~ dnorm(0, 0.001)\n  beta  ~ dnorm(0, 0.001)\n  tau   ~ dgamma(0.01, 0.01)\n}\", file=\"jags_slr.txt\")\n\n# Initialize the Model \njags_mod_slr <- jags.model(\n  file = \"jags_slr.txt\",\n  data = dat_simple,\n  n.chains = 3,\n)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 100\n   Unobserved stochastic nodes: 3\n   Total graph size: 407\n\nInitializing model\n\n\nCode\n# Run Jags and save posterior samples\nparams <- c(\"alpha\", \"beta\", \"tau\")\nsamps <- coda.samples(jags_mod_slr, params, n.iter=1000)\n\nsummary(samps)\n\n\n\nIterations = 1:1000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 1000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean      SD Naive SE Time-series SE\nalpha 0.2382 0.50615 0.009241       0.039769\nbeta  0.9858 0.08229 0.001502       0.006511\ntau   0.9590 0.14776 0.002698       0.004212\n\n2. Quantiles for each variable:\n\n         2.5%      25%    50%    75%  97.5%\nalpha -0.4555 -0.02924 0.2080 0.4334 0.9664\nbeta   0.8656  0.95217 0.9898 1.0312 1.1020\ntau    0.6885  0.86472 0.9578 1.0522 1.2467\n\n\n\n\nCode\nplot(samps)\n\n\n\n\n\nThese are the mixing plots and show posterior sample draws, as well as the density of those draws. We can see that the mixing times, and such stabilize quite quickly. We can see in the trace three dotted lines, because we requested three chains."
  },
  {
    "objectID": "bayesian/bayesian.html#cookbook",
    "href": "bayesian/bayesian.html#cookbook",
    "title": "4  Bayesian",
    "section": "4.5 Cookbook",
    "text": "4.5 Cookbook\n\n4.5.1 Conditional Logistic Regression Model\nConditional Logistic Regression Example with INLA\n\n\n4.5.2 Conditional Sampling\nConditional Sampling from INLA"
  },
  {
    "objectID": "bayesian/bayesian.html#additional-resources",
    "href": "bayesian/bayesian.html#additional-resources",
    "title": "4  Bayesian",
    "section": "4.6 Additional Resources",
    "text": "4.6 Additional Resources\n\n4.6.1 INLA Related (beginner)\n\nBest introduction blog post of INLA with some comparisons to MCMC methods\nA whole book about INLA, introduction level with heavy emphasis on applied examples, and organized very intuitively for applied statisticians.\nMiscellaneous list of INLA tutorials\n\n\n\n4.6.2 Advanced (spatial, stochastic pde) Modeling with INLA\n\nAdvanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA\nGeospatial Health Data: Modeling and Visualization with R-INLA and Shiny\n\nSoftware Comparisons - Comparing NIMBLE, JAGS, Stan - upshot is to learn Stan for best balance of flexibility and speed."
  },
  {
    "objectID": "categorical/categorical.html#overview",
    "href": "categorical/categorical.html#overview",
    "title": "5  Categorical Data Analysis",
    "section": "5.1 Overview",
    "text": "5.1 Overview\nThis is a very broad area, but I feel like it’s also super confusing because it’s all very confouded in terms of all the chisq tests that are scattered throughout the place. I’m trying to organize everything for myself here.\n\nCMH test\nscore test for table\nPearson\nYates (continuity correction of Pearson)\nBarnard test (based on fixing 1 margin)\nfisher exact (based on fixing all margins)\nMcNemar Test\nGeneralized CMH test"
  },
  {
    "objectID": "categorical/categorical.html#x2-sampling-mechanisms",
    "href": "categorical/categorical.html#x2-sampling-mechanisms",
    "title": "5  Categorical Data Analysis",
    "section": "5.2 2x2 sampling mechanisms",
    "text": "5.2 2x2 sampling mechanisms\nThe many different sampling mechanisms arise from different study designs, and are critical to the assumptions of the population you are studying.\nIn an epidemiological context, the data is given as\n\n\n\n\ndisease\nno disease\n\n\n\n\n\nexposure\na\nb\nn1\n\n\nno exposure\nc\nd\nn0\n\n\n\nm1\nm0\nN\n\n\n\n\nPoisson (nothing fixed)\nMultinomial (N) is fixed\n\n“Cross Sectional” studies\n\nTwo-sample Binomial (1 margin fixed)\n\n“Cohort Study” = n1, n0 fixed\n“Case Control Study” = m1, m0 fixed\n\na type of “retrospective” study.\n\n\n5.3 Hypergeometric (2 margins fixed)\n\n\n5.3.1 Poisson (cell means)\n\n\nCode\n# Independent Poisson\n\n#' When specifying the mean, they can be specified cellwise, or by table, grouped by the nrow*ncol, in order of the columns\n#'\n#' @param mean means of the cells. If 1 number, all cells will have same \n#'\n#' @return\n#' @export\n#'\n#' @examples\nrpoisson_table <- function(num_tables = 1, mean = 5, nrow = 2,  ncol = 2) {\n  cells <- rpois(ncol * nrow * num_tables, lambda = mean)\n  vec_table <- split(cells, gl(num_tables, ncol*nrow))\n  vapply(vec_table, matrix, nrow = nrow, ncol = ncol, byrow = FALSE, FUN.VALUE = matrix(1:(ncol*nrow), nrow = nrow))\n}\n\n\n\n\nCode\n# 2x2 examples\npoisson_table_examples <- rpoisson_table(5, mean = 5, nrow = 2, ncol = 2)\napply(poisson_table_examples,\n      MARGIN = 3,\n      FUN = addmargins,\n      simplify = FALSE)\n\n\n$`1`\n          Sum\n     5  4   9\n     5  7  12\nSum 10 11  21\n\n$`2`\n         Sum\n     9 3  12\n     8 6  14\nSum 17 9  26\n\n$`3`\n         Sum\n    3  8  11\n    1  8   9\nSum 4 16  20\n\n$`4`\n        Sum\n    3 3   6\n    4 3   7\nSum 7 6  13\n\n$`5`\n         Sum\n    4  6  10\n    2  4   6\nSum 6 10  16\n\n\n\n\n5.3.2 Multinomial (Grand Total Fixed)\n\n\nCode\n# grand total fixed\nrmultinom_table <- function(num_tables = 1, N = 20, nrow = 2, ncol = 2, p = c(1, 1, 1, 1)) {\n  stopifnot(length(p) == nrow * ncol)\n  # p is internally standardized by rmultinom\n  cells <- rmultinom(num_tables, N, p)\n  dim(cells) <- c(nrow, ncol, num_tables)\n  cells\n}\n\n\n\n\nCode\n# examples\nmultinom_table_examples <- rmultinom_table(5, N = 20, nrow = 2, ncol = 2, p = c(1, 1, 1, 1))\napply(multinom_table_examples,\n      MARGIN = 3,\n      FUN = addmargins,\n      simplify = FALSE)\n\n\n[[1]]\n         Sum\n     4 3   7\n     8 5  13\nSum 12 8  20\n\n[[2]]\n         Sum\n    6  8  14\n    2  4   6\nSum 8 12  20\n\n[[3]]\n         Sum\n     9 7  16\n     3 1   4\nSum 12 8  20\n\n[[4]]\n         Sum\n     6 4  10\n     7 3  10\nSum 13 7  20\n\n[[5]]\n         Sum\n    1  7   8\n    7  5  12\nSum 8 12  20\n\n\n\n\n5.3.3 Binomial Two Sample (One fixed margin)\n\n\nCode\n# Sample a 2x2 table\nrbinom_table <- function(num_tables = 1, row_n = c(10, 10), p = c(.5, .5)) {\n  vapply(1:num_tables, \n       FUN = function(x) {\n         a <- rbinom(length(row_n), row_n, p) # fix margins\n         binom_table <- cbind(a, b = row_n-a) # make other column by subtraction and combine\n         colnames(binom_table) <- NULL\n         binom_table}, \n       FUN.VALUE = matrix(rep(1.1, 4), nrow =2)) # expected 2x2 table\n}\n\n\n\n\nCode\nbinom_table_examples <- rbinom_table(5, row_n = c(10, 10), p = c(.5, .5))\napply(binom_table_examples,\n      MARGIN = 3,\n      FUN = addmargins,\n      simplify = FALSE)\n\n\n[[1]]\n         Sum\n    3  7  10\n    6  4  10\nSum 9 11  20\n\n[[2]]\n         Sum\n     7 3  10\n     5 5  10\nSum 12 8  20\n\n[[3]]\n         Sum\n     6 4  10\n     7 3  10\nSum 13 7  20\n\n[[4]]\n         Sum\n    3  7  10\n    5  5  10\nSum 8 12  20\n\n[[5]]\n         Sum\n     6 4  10\n     6 4  10\nSum 12 8  20\n\n\n\n\n5.3.4 Hypergeometric\n\n\nCode\n# Fixed margins\n# one hypergeometrical deviate will determine the entire table.\n# only doing 2x2 tables with sampling method\n\nrhyper_table <- function(num_tables, row_n = c(10, 10), col_n = c(10, 10))  {\n  a <- rhyper(num_tables, row_n[1], row_n[2], col_n[1])\n  vapply(a, \n       FUN = function(x) {\n         col1 <- c(x, col_n[1] - x)\n         col2 <- row_n - col1\n         contingency_table <- cbind(col1, col2)\n         colnames(contingency_table) <- NULL # get rid of column names\n         contingency_table\n       },\n       FUN.VALUE = matrix(c(0, 0, 0, 1.1), nrow = 2))}\n\n\n\n\nCode\n# examples\nhyper_table_examples <- rhyper_table(num_tables = 5, row_n = c(10, 10), col_n = c(10, 10))\napply(hyper_table_examples,\n      MARGIN = 3,\n      FUN = addmargins,\n      simplify = FALSE)\n\n\n[[1]]\n          Sum\n     4  6  10\n     6  4  10\nSum 10 10  20\n\n[[2]]\n          Sum\n     6  4  10\n     4  6  10\nSum 10 10  20\n\n[[3]]\n          Sum\n     6  4  10\n     4  6  10\nSum 10 10  20\n\n[[4]]\n          Sum\n     6  4  10\n     4  6  10\nSum 10 10  20\n\n[[5]]\n          Sum\n     5  5  10\n     5  5  10\nSum 10 10  20"
  },
  {
    "objectID": "categorical/categorical.html#measures-of-association",
    "href": "categorical/categorical.html#measures-of-association",
    "title": "5  Categorical Data Analysis",
    "section": "5.4 Measures of Association",
    "text": "5.4 Measures of Association\nGiven tables, it’s important to distinguish the properties of how to make comparisons, and summarize the information given. When dealing with percentages and ratios, it’s sometimes difficult to have an intuitive understanding of meaning on the percentage scale. The example Professor Guanhua Chen gives to stimulate you for this thinking is from cartalk:\n\nRAY: Potatoes are 99 percent water and one percent what? Potato. So say you take a bunch of potatoes, like 100 pounds of potatoes and you set them out on your back porch to dry out. TOM: Yeah, when they are dry they should weigh about a pound. RAY: Well, we’re not drying out completely. And as the potatoes dry out the water begins to evaporate. And after a while, enough water has evaporated so that they are now 98 percent water. If you were to weigh those potatoes at that moment… TOM: They’d be lighter. RAY: Yes, how much lighter? That’s the question. Now you can solve this puzzler algebraically, and if you don’t solve it algebraically, you are going to get the wrong answer. TOM: Really? RAY: Really.What’s your answer, off the top of your head? TOM: 99 pounds. RAY: You are wrong. Answer: RAY: Now, unencumbered by the thought process as usual, my brother guessed 99 pounds. TOM: Yeah. RAY: Now, when I guessed, off the top of my head, I guessed about 90 pounds. TOM: ’Cause it just feels right. RAY: But if you do the math, 1 percent of 100 –which is what the potato is– is one pound. As we told you, that’s 1 percent. So 2 percent, when it’s 98 percent water, two percent of the new weight of the mass is still going to be equal to that one pound, and 2 percent of 50 pounds is a pound. So the potato weight is now 50 pounds, not 100.\n\n\nRisk Ratio:\n\nnot symmetric\n\nOdds Ratio\n\nsymmetric, exposure gives information about disease and vice versa"
  },
  {
    "objectID": "categorical/categorical.html#testing",
    "href": "categorical/categorical.html#testing",
    "title": "5  Categorical Data Analysis",
    "section": "5.5 Testing",
    "text": "5.5 Testing\n\n\nCode\ntribble(~test, ~parameter, ~sampling, ~pvalue, ~hypothesis,\n        \"pearson\", \"parameter\",\"two sample z proportion\", \"approximate\", \"association\",\n        \"cochran\", \"cell\", \"unconditional two sample\", \"approximate\", \"association\")\n\n\n# A tibble: 2 × 5\n  test    parameter sampling                 pvalue      hypothesis \n  <chr>   <chr>     <chr>                    <chr>       <chr>      \n1 pearson parameter two sample z proportion  approximate association\n2 cochran cell      unconditional two sample approximate association\n\n\nA list of tests:\n\nCochran summary test\n(Cochran)-Mantel-Haenszel summary test\nGeneralized CMH Test\nCochran’s Q test\nPearson Chi Squared\nCochran-Armitrage Trend Test\nMann-Whitney U test (Wilcoxon Rank Sum)\nWilcoson Signed Rank Test (paired samples)\nKrustkal Wallis\n\nExact tests\n\nFisher Exact Test\nBarnard Exact Test\n\nDependent “Matched pairs” tests\n\nMcNemar Test\n\n\n5.5.1 Pearson\nGiven the following table,\n\n\nCode\ntribble(~\"\", ~\"Disease\", ~\"No Disease\", ~\"\",\n        \"Exposed\", \"a\", \"b\", \"\\\\$n_1\\\\$\",\n        \"Not Exposed\" , \"c\", \"d\", \"\\\\$n_2\\\\$\",\n        \"\", \"\\\\$m_1\\\\$\", \"\\\\$m_2\\\\$\", \"N\") %>% \n  kable(\"html\")\n\n\n\n\n \n  \n     \n    Disease \n    No Disease \n     \n  \n \n\n  \n    Exposed \n    a \n    b \n    \\$n_1\\$ \n  \n  \n    Not Exposed \n    c \n    d \n    \\$n_2\\$ \n  \n  \n     \n    \\$m_1\\$ \n    \\$m_2\\$ \n    N \n  \n\n\n\n\n\nAssume two sample proportional sampling model: A derivation from 2 sample proportion (pooled) z-test:\n\n\\begin{aligned}\nz^2 &= \\frac{(\\hat p_1 - \\hat p_2)^2}{\\hat p(1 - \\hat p)(\\frac{1}{n_1} + \\frac{1}{n_2})}\n&=\n\\end{aligned}\n\n\n\\begin{aligned}\nX^2 = \\sum_i^c \\frac{|O_i - E_i|}{E_i} \\sim \\chi^2_{c-1}\n\\end{aligned}\n\nResources: - Pearson as Score Test - 7 proofs of independence test\n\n\n5.5.2 Barnard Exact Test\nBarnard is considered an “unconditional” approach to exact testing. contrast to Fisher exact test which is a “conditional” approach to testing the p-value\nBarnard is effectively a 2 stage test, given some observed table X, a “p-value” is the probability of observing a table more extreme than the observed.\nThe two stages are thus:\n\ndetermine which tables are more “extreme”\ncalculated probability of those tables\n\nThus, the “exactness” part of the description refers to how a probability is calculated.\nMany methods have been proposed for stage 1\n\nSuissa and Shuster (1985) use pooled and unpooled z statistic for two proportions.\nBooschloo (1970) used the p-value from fisher’s exact test to determine extremeness…\nSantner Snell - difference in proportion\n\n\n\nCode\nX <- matrix(c(3, 0, 0, 3), nrow = 2)\n# row is fixed\nBarnardTest(X, method = \"z-pooled\")\n\n\n\n    Z-pooled Exact Test\n\ndata:  3 out of 3 vs. 0 out of 3\ntest statistic = 2.4495, first sample size = 3, second sample size = 3,\np-value = 0.03125\nalternative hypothesis: true difference in proportion is not equal to 0\nsample estimates:\ndifference in proportion \n                       1 \n\n\nCode\n# z-pooled observed is...\n1 / sqrt(.25 * (1/3 + 1/3)) # observed test statistic\n\n\n[1] 2.44949\n\n\nCode\n# how to calculate p-value from this test statistic, sum of binomial products from extreme tables,\n# the null is that pi_1 = pi_2 = pi, since we don't know the actual value of pi, we take the supremum of these values\n\ntrue_pi <- seq(0, 1, .01)\n\npossible_p <- dbinom(3, 3, prob = true_pi) * dbinom(0, 3, prob = true_pi) + dbinom(0, 3, prob = true_pi) * dbinom(3, 3, prob = true_pi)\n\n\n# maximum occurs at .5\ntrue_pi[which.max(possible_p)]\n\n\n[1] 0.5\n\n\nCode\n# thus, overall p-value is\nmax(possible_p) # .03125\n\n\n[1] 0.03125\n\n\nNow we compare the methods for choosing “extremeness” with the assumed sampling mechanism.\n\n\nCode\nset.seed(1)\nfoo <- rbinom_table(1000, row_n = c(10, 10), p = c(.5, .5)) # null is no association\n\n# z pooled (score)\nbarnard_pooled <- apply(foo, MARGIN = 3,\n                        FUN = function (x) {BarnardTest(x, method = \"z-pooled\")$p.value})\n\nbarnard_unpooled <- apply(foo, MARGIN = 3,\n                        FUN = function (x) {BarnardTest(x, method = \"z-unpooled\")$p.value})\n\nbarnard_boschloo <- apply(foo, \n                          MARGIN = 3,\n                          FUN = function (x) {BarnardTest(x, method = \"boschloo\")$p.value})\n\n\nbarnard_csm <- apply(foo, \n                          MARGIN = 3,\n                          FUN = function (x) {BarnardTest(x, method = \"csm\")$p.value})\n\nbarnard_santner_snell <- apply(foo, \n                               MARGIN = 3,\n                               FUN = function (x) {BarnardTest(x, method = \"santner and snell\")$p.value})\n\n# for reference\nfisher_exact <-  apply(foo, \n                               MARGIN = 3,\n                               FUN = function (x) {fisher.test(x)$p.value})\n\n\n\n# comment out any that you don't want to appear in the plot\nbarnard <- bind_cols(pooled = barnard_pooled,\n                     unpooled = barnard_unpooled,\n                     boschloo = barnard_boschloo,\n                     csm = barnard_csm,\n                     santner_snell = barnard_santner_snell,\n                     fisher_exact = fisher_exact) %>% \n  pivot_longer(everything(), names_to = \"type\", values_to = \"p\")\n\n# there's some overplotting happening, dodge doesn't seem to work well for ecdf's\nbarnard %>% ggplot(aes(x = p, color = type)) +\n  stat_ecdf()  +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) +\n  labs(title = \"Barnard extremeness method comparison\")\n\n\n\n\n\nFor accuracy of the size of the test, it seems that csm is the most accurate, but also the most computationally intensive. We not that all the tests have more approriate “size” than the fisher_exact test.\nResources\n\nExplanation of difference between unconditional and conditional\nExact Test Package Documentation - exact.test function documentation has more information about barnard implmenetation.\n\n\n\n5.5.3 McNemar\nMcNemar tests are used when there is some “dichotomous trait”, for matched pairs. This means that the responses are statistically dependent. This is common for some longitudinal studies in which a single individual is asked two questions, and their answers are coded as locations in the table. Thus, the grand total, should be the number of pairs of data, not the total number of observations.\nFor example, suppose a person is asked if they voted democrat\n\n\nCode\ntribble(~\"\", ~\"2008 democrat\", ~\"2008 republican\",\n        \"2004 democrat\", 175, 16,\n        \"2004 republican\", 54, 188) %>% \n  kbl()\n\n\n\n\n \n  \n     \n    2008 democrat \n    2008 republican \n  \n \n\n  \n    2004 democrat \n    175 \n    16 \n  \n  \n    2004 republican \n    54 \n    188 \n  \n\n\n\n\n\nMcNemar tests “Marginal Homogeneity”, meaning that the probabilities of the margins are the same. p_a + p_b = p_a + p_c and p_c + p_d = p_b + p_d. Thus, this means that we are testing H_0: p_b = p_c, H_A: p_b \\neq p_c. The score statistic is:\n\n\\begin{aligned}\nz_0^2 = \\frac{(b-c)^2}{b+c} \\sim \\chi^2_1\n\\end{aligned}\n\n\nVariance is\n\n\\begin{aligned}\n\\hat\\sigma_0 (d) = \\frac{b + c}{N^2}\n\\end{aligned}\n\n\n\nCode\n# Presidential election, dependent table\npres <- matrix(c(175, 16,\n                      54, 188),\n                    ncol = 2,\n                    byrow = TRUE)\npres_mcnemar <- mcnemar.test(pres, correct = FALSE)\n# (54 - 16)^2 / (54 + 16) # 20.629\npres_mcnemar\n\n\n\n    McNemar's Chi-squared test\n\ndata:  pres\nMcNemar's chi-squared = 20.629, df = 1, p-value = 5.576e-06\n\n\nNotes\n\nonly the off diagonal matters for significance, where as the main diagonal\n\nA good reference is 11.1 in Categorical Data Analysis, 3rd edition by Agresti\n\n\n5.5.4 Breslow-Day Test\na test of homogeneity.\n\n\n5.5.5 CMH testing\nThe CMH testing is technically supposed to be done on tables in strata. the data type is an I X J X K, in which we have K strata and an I X J contingency table in each.\n\nyou are not penalized for adding tables with sparse data with the CMH test statistic\nthis reduces to the N-1 adjusted pearson chisquared statistic for 1 strata\nthe test assumes that there is a common odds ratio to estimate, but in order to test the hypothesis we can use a Breslow-Day Test of Homogeneity\nConditional logistic regression gives a similar answer, clogit in package survival because similar to cox model as well.\n\n\nCMH - McNemar Equivalence\nIf you express the “population averaged” table and run McNemar, you will get the same statistic as expressing the data as a “subject specific” table for an individual per stratum.\n\n\nCode\ntest <- matrix(c(5, 7, 3, 4), ncol = 2)\nchisq.test(test, correct = FALSE)\n\n\nWarning in chisq.test(test, correct = FALSE): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  test\nX-squared = 0.0025703, df = 1, p-value = 0.9596\n\n\nCode\n0.0025703 / 19 * 18 # the \"N-1\" chisq statistic in a 2 x 2\n\n\n[1] 0.002435021\n\n\nCode\nfoo <- array(c(5, 7, 3, 4,\n               0, 0, 1, 1), dim = c(2, 2, 2))\nmantelhaen.test(foo, correct = FALSE)\n\n\n\n    Mantel-Haenszel chi-squared test without continuity correction\n\ndata:  foo\nMantel-Haenszel X-squared = 0.0024351, df = 1, p-value = 0.9606\nalternative hypothesis: true common odds ratio is not equal to 1\n95 percent confidence interval:\n 0.1444191 6.2805369\nsample estimates:\ncommon odds ratio \n         0.952381 \n\n\nCode\nbar <- array(c(5, 7, 3, 4,\n               0, 0, 1, 1,\n               1, 6, 0, 0), dim = c(2, 2, 3))\nmantelhaen.test(bar, correct = FALSE)\n\n\n\n    Mantel-Haenszel chi-squared test without continuity correction\n\ndata:  bar\nMantel-Haenszel X-squared = 0.0024351, df = 1, p-value = 0.9606\nalternative hypothesis: true common odds ratio is not equal to 1\n95 percent confidence interval:\n 0.1444191 6.2805369\nsample estimates:\ncommon odds ratio \n         0.952381 \n\n\nCode\nmantelhaen_2by2 <-  function(x) {\n  mantelhaen.test(array(c(x, 0, 0, 1, 1), dim = c(2, 2, 2)), correct = FALSE)\n}\n\n\n\n\n\n5.5.6 2x2 tables Comparison\n\nPoisson Sampling\n\n\nCode\n# random poisson\nfoo <- rpoisson_table(num_tables = 1000, mean = 5, nrow = 2, ncol = 2)\n\n# pearson uncorrected\npearson_uncorrected_p <- \n  apply(foo, \n      MARGIN = 3,\n      function(x) {\n        suppressWarnings(chisq.test(x, correct = FALSE))$p.value})\n\n# pearson corrected\npearson_corrected_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(chisq.test(x, correct = TRUE))$p.value})\n\nfisher_exact_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(fisher.test(x))$p.value})\n\n# Pearson N-1 correction equivalent\ncmh_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          mantelhaen_2by2(x)$p.value\n        })\n\n# G-test (LRT)\nlrt_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(g.test(x))$p.value\n        })\n\n\ncontingency_p <- bind_cols(uncorrected = pearson_uncorrected_p,\n          corrected = pearson_corrected_p,\n          exact = fisher_exact_p,\n          cmh = cmh_p,\n          lrt = lrt_p)\n\ncontingency_p %>% \n  pivot_longer(everything(), names_to = \"test\", values_to = \"p\") %>% \n  ggplot(aes(p, color = test)) +\n  stat_ecdf(geom = \"step\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) + \n  labs(title = \"Tests of association\") #+\n\n\n\n\n\nCode\n  coord_cartesian(xlim = c(0, .1), ylim = c(0, .1)) # for zooming in\n\n\n<ggproto object: Class CoordCartesian, Coord, gg>\n    aspect: function\n    backtransform_range: function\n    clip: on\n    default: FALSE\n    distance: function\n    expand: TRUE\n    is_free: function\n    is_linear: function\n    labels: function\n    limits: list\n    modify_scales: function\n    range: function\n    render_axis_h: function\n    render_axis_v: function\n    render_bg: function\n    render_fg: function\n    setup_data: function\n    setup_layout: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    setup_params: function\n    train_panel_guides: function\n    transform: function\n    super:  <ggproto object: Class CoordCartesian, Coord, gg>\n\n\n\n\nBinomial Sampling\n\n\nCode\nfoo <- rbinom_table(num_tables = 9000)\n\n# pearson uncorrected\npearson_uncorrected_p <- \n  apply(foo, \n      MARGIN = 3,\n      function(x) {\n        suppressWarnings(chisq.test(x, correct = FALSE))$p.value})\n\n# pearson corrected\npearson_corrected_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(chisq.test(x, correct = TRUE))$p.value})\n\nfisher_exact_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(fisher.test(x))$p.value})\n\n# Pearson N-1 correction equivalent\ncmh_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          mantelhaen_2by2(x)$p.value\n        })\n\n# G-test (LRT)\nlrt_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(g.test(x))$p.value\n        })\n\n\ncontingency_p <- bind_cols(uncorrected = pearson_uncorrected_p,\n          corrected = pearson_corrected_p,\n          exact = fisher_exact_p,\n          cmh = cmh_p,\n          lrt = lrt_p)\n\ncontingency_p %>% \n  pivot_longer(everything(), names_to = \"test\", values_to = \"p\") %>% \n  ggplot(aes(p, color = test)) +\n  stat_ecdf(geom = \"step\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) #+\n\n\n\n\n\nCode\n  coord_cartesian(xlim = c(0, .1), ylim = c(0, .1)) # for zooming in\n\n\n<ggproto object: Class CoordCartesian, Coord, gg>\n    aspect: function\n    backtransform_range: function\n    clip: on\n    default: FALSE\n    distance: function\n    expand: TRUE\n    is_free: function\n    is_linear: function\n    labels: function\n    limits: list\n    modify_scales: function\n    range: function\n    render_axis_h: function\n    render_axis_v: function\n    render_bg: function\n    render_fg: function\n    setup_data: function\n    setup_layout: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    setup_params: function\n    train_panel_guides: function\n    transform: function\n    super:  <ggproto object: Class CoordCartesian, Coord, gg>\n\n\n\n\nMultinomial Sampling\n\n\nCode\nfoo <- rmultinom_table(num_tables = 1000)\n\n# pearson uncorrected\npearson_uncorrected_p <- \n  apply(foo, \n      MARGIN = 3,\n      function(x) {\n        suppressWarnings(chisq.test(x, correct = FALSE))$p.value})\n\n# pearson corrected\npearson_corrected_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(chisq.test(x, correct = TRUE))$p.value})\n\nfisher_exact_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(fisher.test(x))$p.value})\n\n# Pearson N-1 correction equivalent\ncmh_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          mantelhaen_2by2(x)$p.value\n        })\n\n# G-test (LRT)\nlrt_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(g.test(x))$p.value\n        })\n\n\ncontingency_p <- bind_cols(uncorrected = pearson_uncorrected_p,\n          corrected = pearson_corrected_p,\n          exact = fisher_exact_p,\n          cmh = cmh_p,\n          lrt = lrt_p)\n\ncontingency_p %>% \n  pivot_longer(everything(), names_to = \"test\", values_to = \"p\") %>% \n  ggplot(aes(p, color = test)) +\n  stat_ecdf(geom = \"step\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) #+\n\n\n\n\n\nCode\n  coord_cartesian(xlim = c(0, .1), ylim = c(0, .1)) # for zooming in\n\n\n<ggproto object: Class CoordCartesian, Coord, gg>\n    aspect: function\n    backtransform_range: function\n    clip: on\n    default: FALSE\n    distance: function\n    expand: TRUE\n    is_free: function\n    is_linear: function\n    labels: function\n    limits: list\n    modify_scales: function\n    range: function\n    render_axis_h: function\n    render_axis_v: function\n    render_bg: function\n    render_fg: function\n    setup_data: function\n    setup_layout: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    setup_params: function\n    train_panel_guides: function\n    transform: function\n    super:  <ggproto object: Class CoordCartesian, Coord, gg>\n\n\n\n\nHypergeometric Sampling\n\n\nCode\nfoo <- rhyper_table(num_tables = 1000)\n\n# pearson uncorrected\npearson_uncorrected_p <- \n  apply(foo, \n      MARGIN = 3,\n      function(x) {\n        suppressWarnings(chisq.test(x, correct = FALSE))$p.value})\n\n# pearson corrected\npearson_corrected_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(chisq.test(x, correct = TRUE))$p.value})\n\nfisher_exact_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(fisher.test(x))$p.value})\n\n# Pearson N-1 correction equivalent\ncmh_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          mantelhaen_2by2(x)$p.value\n        })\n\n# G-test (LRT)\nlrt_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(g.test(x))$p.value\n        })\n\n\ncontingency_p <- bind_cols(uncorrected = pearson_uncorrected_p,\n          corrected = pearson_corrected_p,\n          exact = fisher_exact_p,\n          cmh = cmh_p,\n          lrt = lrt_p)\n\ncontingency_p %>% \n  pivot_longer(everything(), names_to = \"test\", values_to = \"p\") %>% \n  ggplot(aes(p, color = test)) +\n  stat_ecdf(geom = \"step\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) #+\n\n\n\n\n\nCode\n  coord_cartesian(xlim = c(0, .1), ylim = c(0, .1)) # for zooming in\n\n\n<ggproto object: Class CoordCartesian, Coord, gg>\n    aspect: function\n    backtransform_range: function\n    clip: on\n    default: FALSE\n    distance: function\n    expand: TRUE\n    is_free: function\n    is_linear: function\n    labels: function\n    limits: list\n    modify_scales: function\n    range: function\n    render_axis_h: function\n    render_axis_v: function\n    render_bg: function\n    render_fg: function\n    setup_data: function\n    setup_layout: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    setup_params: function\n    train_panel_guides: function\n    transform: function\n    super:  <ggproto object: Class CoordCartesian, Coord, gg>"
  },
  {
    "objectID": "categorical/categorical.html#more-than-two-categories",
    "href": "categorical/categorical.html#more-than-two-categories",
    "title": "5  Categorical Data Analysis",
    "section": "5.6 More than two categories",
    "text": "5.6 More than two categories\nThis section starts to get into 2 x I tables, and even more dimensions like, I X J X K tables, and how we analyze those tables. We’ll start with an overview of the multinomial theory, which is fundamental in extending the binomial (2 categories) into multiple categories. The binomial is a special case of the multinomial distribution\n\nnnet::multinom\nVGAM::vglm(family = multinom)\nmlogit::mlogit\n\nA common example we see in this exposition is housing data from library MASS\n\n\nCode\nlibrary(MASS)\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nCode\ndata(housing)\n\n# The array version\nhousing_arr <- xtabs(Freq~Sat + Infl + Type + Cont, data = housing)\n\n\n\n5.6.1 Poisson GLM Modeling\n\n\nCode\n# simple glm model (satisfaction independent of Infl, Type, Cont)\nhouse_glm <- glm(Freq ~ Infl*Type*Cont + Sat, data = housing, family = poisson)\nsummary(house_glm) # high residual deviance\n\n\n\nCall:\nglm(formula = Freq ~ Infl * Type * Cont + Sat, family = poisson, \n    data = housing)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4.5551  -1.0612  -0.0593   0.6483   4.1478  \n\nCoefficients:\n                                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                        3.136e+00  1.196e-01  26.225  < 2e-16 ***\nInflMedium                         2.733e-01  1.586e-01   1.723 0.084868 .  \nInflHigh                          -2.054e-01  1.784e-01  -1.152 0.249511    \nTypeApartment                      3.666e-01  1.555e-01   2.357 0.018403 *  \nTypeAtrium                        -7.828e-01  2.134e-01  -3.668 0.000244 ***\nTypeTerrace                       -8.145e-01  2.157e-01  -3.775 0.000160 ***\nContHigh                          -2.200e-16  1.690e-01   0.000 1.000000    \nSat.L                              1.159e-01  4.038e-02   2.871 0.004094 ** \nSat.Q                              2.629e-01  4.515e-02   5.824 5.76e-09 ***\nInflMedium:TypeApartment          -1.177e-01  2.086e-01  -0.564 0.572571    \nInflHigh:TypeApartment             1.753e-01  2.279e-01   0.769 0.441783    \nInflMedium:TypeAtrium             -4.068e-01  3.035e-01  -1.340 0.180118    \nInflHigh:TypeAtrium               -1.692e-01  3.294e-01  -0.514 0.607433    \nInflMedium:TypeTerrace             6.292e-03  2.860e-01   0.022 0.982450    \nInflHigh:TypeTerrace              -9.305e-02  3.280e-01  -0.284 0.776633    \nInflMedium:ContHigh               -1.398e-01  2.279e-01  -0.613 0.539715    \nInflHigh:ContHigh                 -6.091e-01  2.800e-01  -2.176 0.029585 *  \nTypeApartment:ContHigh             5.029e-01  2.109e-01   2.385 0.017083 *  \nTypeAtrium:ContHigh                6.774e-01  2.751e-01   2.462 0.013811 *  \nTypeTerrace:ContHigh               1.099e+00  2.675e-01   4.106 4.02e-05 ***\nInflMedium:TypeApartment:ContHigh  5.359e-02  2.862e-01   0.187 0.851450    \nInflHigh:TypeApartment:ContHigh    1.462e-01  3.380e-01   0.432 0.665390    \nInflMedium:TypeAtrium:ContHigh     1.555e-01  3.907e-01   0.398 0.690597    \nInflHigh:TypeAtrium:ContHigh       4.782e-01  4.441e-01   1.077 0.281619    \nInflMedium:TypeTerrace:ContHigh   -4.980e-01  3.671e-01  -1.357 0.174827    \nInflHigh:TypeTerrace:ContHigh     -4.470e-01  4.545e-01  -0.984 0.325326    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 833.66  on 71  degrees of freedom\nResidual deviance: 217.46  on 46  degrees of freedom\nAIC: 610.43\n\nNumber of Fisher Scoring iterations: 5\n\n\nClearly there’s probably some correlation between satisfaction and the other variables, let’s check them out individually.\n\n\nCode\n# addterm will check each term individually, and test marginality\naddterm(house_glm, ~. + Sat:(Infl+Type+Cont), test = \"Chisq\")\n\n\nSingle term additions\n\nModel:\nFreq ~ Infl * Type * Cont + Sat\n         Df Deviance    AIC     LRT   Pr(Chi)    \n<none>        217.46 610.43                      \nInfl:Sat  4   111.08 512.05 106.371 < 2.2e-16 ***\nType:Sat  6   156.79 561.76  60.669 3.292e-11 ***\nCont:Sat  2   212.33 609.30   5.126   0.07708 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInfluence seems to have the largest impact, so we add this to the model, and type probably has a large\n\n\nCode\nhouse_glm1 <- update(house_glm, .~. + Sat:(Infl + Type + Cont))\nsummary(house_glm1)\n\n\n\nCall:\nglm(formula = Freq ~ Infl + Type + Cont + Sat + Infl:Type + Infl:Cont + \n    Type:Cont + Infl:Sat + Type:Sat + Cont:Sat + Infl:Type:Cont, \n    family = poisson, data = housing)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6022  -0.5282  -0.0641   0.5757   1.9322  \n\nCoefficients:\n                                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                        3.135074   0.120112  26.101  < 2e-16 ***\nInflMedium                         0.248327   0.159979   1.552 0.120602    \nInflHigh                          -0.412645   0.184947  -2.231 0.025671 *  \nTypeApartment                      0.292524   0.157477   1.858 0.063231 .  \nTypeAtrium                        -0.792847   0.214413  -3.698 0.000218 ***\nTypeTerrace                       -1.018074   0.221263  -4.601 4.20e-06 ***\nContHigh                          -0.001407   0.169711  -0.008 0.993385    \nSat.L                             -0.098106   0.112592  -0.871 0.383570    \nSat.Q                              0.285657   0.122283   2.336 0.019489 *  \nInflMedium:TypeApartment          -0.017882   0.210496  -0.085 0.932302    \nInflHigh:TypeApartment             0.386869   0.233297   1.658 0.097263 .  \nInflMedium:TypeAtrium             -0.360311   0.304979  -1.181 0.237432    \nInflHigh:TypeAtrium               -0.036788   0.334793  -0.110 0.912503    \nInflMedium:TypeTerrace             0.185154   0.288892   0.641 0.521580    \nInflHigh:TypeTerrace               0.310749   0.334815   0.928 0.353345    \nInflMedium:ContHigh               -0.200060   0.228748  -0.875 0.381799    \nInflHigh:ContHigh                 -0.725790   0.282352  -2.571 0.010155 *  \nTypeApartment:ContHigh             0.569691   0.212152   2.685 0.007247 ** \nTypeAtrium:ContHigh                0.702115   0.276056   2.543 0.010979 *  \nTypeTerrace:ContHigh               1.215930   0.269968   4.504 6.67e-06 ***\nInflMedium:Sat.L                   0.519627   0.096830   5.366 8.03e-08 ***\nInflHigh:Sat.L                     1.140302   0.118180   9.649  < 2e-16 ***\nInflMedium:Sat.Q                  -0.064474   0.102666  -0.628 0.530004    \nInflHigh:Sat.Q                     0.115436   0.127798   0.903 0.366380    \nTypeApartment:Sat.L               -0.520170   0.109793  -4.738 2.16e-06 ***\nTypeAtrium:Sat.L                  -0.288484   0.149551  -1.929 0.053730 .  \nTypeTerrace:Sat.L                 -0.998666   0.141527  -7.056 1.71e-12 ***\nTypeApartment:Sat.Q                0.055418   0.118515   0.468 0.640068    \nTypeAtrium:Sat.Q                  -0.273820   0.149713  -1.829 0.067405 .  \nTypeTerrace:Sat.Q                 -0.032328   0.149251  -0.217 0.828520    \nContHigh:Sat.L                     0.340703   0.087778   3.881 0.000104 ***\nContHigh:Sat.Q                    -0.097929   0.094068  -1.041 0.297851    \nInflMedium:TypeApartment:ContHigh  0.046900   0.286212   0.164 0.869837    \nInflHigh:TypeApartment:ContHigh    0.126229   0.338208   0.373 0.708979    \nInflMedium:TypeAtrium:ContHigh     0.157239   0.390719   0.402 0.687364    \nInflHigh:TypeAtrium:ContHigh       0.478611   0.444244   1.077 0.281320    \nInflMedium:TypeTerrace:ContHigh   -0.500162   0.367135  -1.362 0.173091    \nInflHigh:TypeTerrace:ContHigh     -0.463099   0.454713  -1.018 0.308467    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 833.657  on 71  degrees of freedom\nResidual deviance:  38.662  on 34  degrees of freedom\nAIC: 455.63\n\nNumber of Fisher Scoring iterations: 4\n\n\nCode\n# sum(residuals(house_glm1, type = \"pearson\")^2) / house_glm1$df.residual # dispersion estimate\nhouse_glm1$df.residual # nrow(housing) - length(coef(house_glm1))\n\n\n[1] 34\n\n\nCode\n# sum(residuals(house_glm1, type = \"deviance\")^2) / house_glm1$df.residual\n# 1 - pchisq(deviance(house_glm1), house_glm1$df.residual) # .267? what's the test here...\n\n\nSee 202 for rescaling the predictions from this model to the probability scale (by the margin of satisfaction)\n\n\nCode\nhnames <- lapply(housing[, -5], levels) # \nhouse_pm <- predict(house_glm1, expand.grid(hnames), type = \"response\") # poisson means exp(\\eta)\nhouse_pm <- matrix(house_pm, ncol = 3, byrow = T, dimnames = list(NULL, hnames[[1]])) # list the predictions into matrix form, columns being satisfaction\ncbind(expand.grid(hnames[-1]), house_pm / rowSums(house_pm)) # normalize by row, and attach the name\n\n\n     Infl      Type Cont       Low    Medium      High\n1     Low     Tower  Low 0.3955687 0.2601077 0.3443236\n2  Medium     Tower  Low 0.2602403 0.2674072 0.4723526\n3    High     Tower  Low 0.1504958 0.1924126 0.6570916\n4     Low Apartment  Low 0.5427582 0.2308450 0.2263968\n5  Medium Apartment  Low 0.3945683 0.2622428 0.3431889\n6    High Apartment  Low 0.2551503 0.2110026 0.5338470\n7     Low    Atrium  Low 0.4294218 0.3220096 0.2485686\n8  Medium    Atrium  Low 0.2959630 0.3468082 0.3572289\n9    High    Atrium  Low 0.1865151 0.2719420 0.5415429\n10    Low   Terrace  Low 0.6453059 0.2178758 0.1368183\n11 Medium   Terrace  Low 0.5076883 0.2678600 0.2244517\n12   High   Terrace  Low 0.3676505 0.2413550 0.3909945\n13    Low     Tower High 0.2982776 0.2813636 0.4203589\n14 Medium     Tower High 0.1847507 0.2723332 0.5429161\n15   High     Tower High 0.1009787 0.1852058 0.7138155\n16    Low Apartment High 0.4375458 0.2669645 0.2954897\n17 Medium Apartment High 0.2974727 0.2836249 0.4189024\n18   High Apartment High 0.1794106 0.2128413 0.6077481\n19    Low    Atrium High 0.3319072 0.3570404 0.3110524\n20 Medium    Atrium High 0.2157414 0.3626615 0.4215970\n21   High    Atrium High 0.1283298 0.2684145 0.6032556\n22    Low   Terrace High 0.5471602 0.2650171 0.1878227\n23 Medium   Terrace High 0.4044226 0.3060991 0.2894783\n24   High   Terrace High 0.2729568 0.2570580 0.4699852\n\n\n\n\n5.6.2 Log Linear Models\nlog linear models with iterative proportional scaling is done with function loglm.\n\n\nCode\nloglm(Freq ~ Infl*Type*Cont + Sat*(Infl + Type + Cont), data = housing)\n\n\nCall:\nloglm(formula = Freq ~ Infl * Type * Cont + Sat * (Infl + Type + \n    Cont), data = housing)\n\nStatistics:\n                      X^2 df  P(> X^2)\nLikelihood Ratio 38.66222 34 0.2671359\nPearson          38.90831 34 0.2582333\n\n\n\n\n5.6.3 Multinomial Models\nThe example data we’ll is use party affiliation:\n\n\nCode\n# array version of data\nparty <- array(c(132, 42, 176, 6, 127, 12,\n                 172, 56, 129, 4, 130, 15), \n               dim = c(2, 3, 2),\n               dimnames = list(race = c(\"white\", \"black\"),\n                               party = c(\"democrat\", \"independent\", \"republican\"),\n                               gender = c(\"male\", \"female\")))\nparty_df <- as.data.frame.table(party) # data frame version of data\n\n# Marginal Tables\nrace_party <- margin.table(party, margin = 1:2)\ngender_party <- margin.table(party, margin = c(3, 2))\nrace_gender <- margin.table(party, margin = c(1, 3))\n\n\n\nnnet\n\n\nCode\nparty_mod <- multinom(party ~ race + gender, weights = Freq, party_df) # democrat is the \"reference\"\n\n\n# weights:  12 (6 variable)\ninitial  value 1099.710901 \niter  10 value 1042.893269\nfinal  value 1042.891187 \nconverged\n\n\nCode\nsummary(party_mod)\n\n\nCall:\nmultinom(formula = party ~ race + gender, data = party_df, weights = Freq)\n\nCoefficients:\n            (Intercept) raceblack genderfemale\nindependent   0.2855582 -2.278140   -0.5727764\nrepublican   -0.0497550 -1.118276   -0.2201972\n\nStd. Errors:\n            (Intercept) raceblack genderfemale\nindependent   0.1125979 0.3427945    0.1575210\nrepublican    0.1198046 0.2335145    0.1582522\n\nResidual Deviance: 2085.782 \nAIC: 2097.782 \n\n\nHypothesis testing with nnet\n\n\nVGAM\n\n\nCode\n# VGAM\n# needs version in which \"stimulus factors\" are separated from \"response\" factors.\nhousing_wide <- housing %>% pivot_wider(names_from = \"Sat\", values_from = \"Freq\")\n\n\nOur saturated dataset is one in which every cell is estimated with a parameter.\n\n\nCode\n# saturated model\nhousing_vglm0 <- vglm(cbind(Low, Medium, High) ~ Infl*Type*Cont, data = housing_wide, family = multinomial)\n\n\nWarning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some\nquantities such as z, residuals, SEs may be inaccurate due to convergence at a\nhalf-step\n\n\nCode\ndeviance(housing_vglm0)\n\n\n[1] -9.792167e-14\n\n\nIn the saturated model, we see that our deviance is equal to 0 because it fits the data perfectly.\n\n\nCode\n# full two way interaction model\nhousing_vglm <- vglm(cbind(Low, Medium, High) ~ (Infl + Type + Cont)^2, data = housing_wide, family = multinomial)\nsummary(housing_vglm)\n\n\n\nCall:\nvglm(formula = cbind(Low, Medium, High) ~ (Infl + Type + Cont)^2, \n    family = multinomial, data = housing_wide)\n\nCoefficients: \n                             Estimate Std. Error z value Pr(>|z|)    \n(Intercept):1              -0.2728053  0.2532115  -1.077  0.28131    \n(Intercept):2              -0.3780064  0.2576046  -1.467  0.14227    \nInflMedium:1                0.1982869  0.3081018   0.644  0.51985    \nInflMedium:2               -0.0083922  0.3171262  -0.026  0.97889    \nInflHigh:1                 -0.9649735  0.3945466  -2.446  0.01445 *  \nInflHigh:2                 -0.8646670  0.3794916  -2.278  0.02270 *  \nTypeApartment:1             1.4992506  0.3153481   4.754 1.99e-06 ***\nTypeApartment:2             0.6555399  0.3307945   1.982  0.04751 *  \nTypeAtrium:1                0.6246612  0.4142176   1.508  0.13154    \nTypeAtrium:2                0.3751106  0.4204712   0.892  0.37233    \nTypeTerrace:1               1.2966645  0.4192741   3.093  0.00198 ** \nTypeTerrace:2               0.4378242  0.4595906   0.953  0.34077    \nContHigh:1                 -0.7397711  0.3116324  -2.374  0.01760 *  \nContHigh:2                 -0.2009809  0.3039959  -0.661  0.50853    \nInflMedium:TypeApartment:1 -1.3982019  0.3535169  -3.955 7.65e-05 ***\nInflMedium:TypeApartment:2 -0.5498060  0.3604313  -1.525  0.12716    \nInflHigh:TypeApartment:1   -0.9211659  0.4576832  -2.013  0.04415 *  \nInflHigh:TypeApartment:2   -0.3138506  0.4358689  -0.720  0.47149    \nInflMedium:TypeAtrium:1    -0.9955454  0.4790778  -2.078  0.03771 *  \nInflMedium:TypeAtrium:2    -0.1868591  0.4525047  -0.413  0.67965    \nInflHigh:TypeAtrium:1       0.1416714  0.5758309   0.246  0.80566    \nInflHigh:TypeAtrium:2       0.2132503  0.5362081   0.398  0.69085    \nInflMedium:TypeTerrace:1   -0.9030966  0.4563464  -1.979  0.04782 *  \nInflMedium:TypeTerrace:2    0.0081197  0.4836471   0.017  0.98661    \nInflHigh:TypeTerrace:1     -0.8443360  0.5889810  -1.434  0.15170    \nInflHigh:TypeTerrace:2     -0.2200667  0.5911664  -0.372  0.70970    \nInflMedium:ContHigh:1       0.0119167  0.2883493   0.041  0.96703    \nInflMedium:ContHigh:2      -0.0735225  0.3046628  -0.241  0.80930    \nInflHigh:ContHigh:1        -0.2258679  0.3504496  -0.645  0.51925    \nInflHigh:ContHigh:2         0.0318502  0.3496997   0.091  0.92743    \nTypeApartment:ContHigh:1    0.1350432  0.3218707   0.420  0.67481    \nTypeApartment:ContHigh:2   -0.0002221  0.3186735  -0.001  0.99944    \nTypeAtrium:ContHigh:1       0.3409407  0.4320950   0.789  0.43009    \nTypeAtrium:ContHigh:2       0.2975587  0.4139167   0.719  0.47221    \nTypeTerrace:ContHigh:1      1.1520381  0.4173381   2.760  0.00577 ** \nTypeTerrace:ContHigh:2      0.6313710  0.4353792   1.450  0.14701    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: log(mu[,1]/mu[,3]), log(mu[,2]/mu[,3])\n\nResidual deviance: 5.9443 on 12 degrees of freedom\n\nLog-likelihood: -102.5404 on 12 degrees of freedom\n\nNumber of Fisher scoring iterations: 3 \n\nNo Hauck-Donner effect found in any of the estimates\n\n\nReference group is level  3  of the response\n\n\nCode\ndeviance(housing_vglm)\n\n\n[1] 5.944319\n\n\nCode\n# Lack of fit tests\n1 - pchisq(deviance(housing_vglm), df.residual(housing_vglm)) # deviance\n\n\n[1] 0.9188628\n\n\nCode\n1 - pchisq(sum(residuals(housing_vglm, type = \"pearson\")^2), df.residual(housing_vglm)) # pearson\n\n\n[1] 0.917741\n\n\nThe null hypothesis here is that the model is specified correctly. High p-values mean we fail to reject that the model is correct. In general, lack of fit tests are pretty bad tests for telling us any information. We would prefer to do some manual model searching.\n\n\nCode\ndrop1(housing_vglm, test = \"LRT\")\n\n\nSingle term deletions\n\nModel:\ncbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n          Df Deviance    AIC     LRT Pr(>Chi)  \n<none>         5.9443 277.08                   \nInfl:Type 12  27.0024 274.14 21.0581  0.04954 *\nInfl:Cont  4   6.8284 269.96  0.8841  0.92684  \nType:Cont  6  15.2341 274.37  9.2898  0.15793  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results here say that we could probably drop Infl:Cont and Type:Cont. In fact, dropping Infl:Cont, we would get the biggest drop in AIC, indicating better model fit for number of parameters we estimate.\n\n\nCode\nhousing_vglm1 <- update(housing_vglm, .~. - Infl:Cont)\n\n\nWarning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some\nquantities such as z, residuals, SEs may be inaccurate due to convergence at a\nhalf-step\n\n\nCode\ndrop1(housing_vglm1, test = \"LRT\")\n\n\nSingle term deletions\n\nModel:\ncbind(Low, Medium, High) ~ Infl + Type + Cont + Infl:Type + Type:Cont\n          Df Deviance    AIC     LRT Pr(>Chi)  \n<none>         6.8284 269.96                   \nInfl:Type 12  28.2559 267.39 21.4275  0.04446 *\nType:Cont  6  16.1072 267.24  9.2788  0.15850  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDoing it again shows we could again get lower AIC by dropping either parameter…. an automated way of doing this can be done through step4vglm\n\n\nCode\n# Forward-Backward Step selection on 2-way interaction model\nhousing_vglm_step <- step4vglm(housing_vglm, direction = \"both\")\n\n\nStart:  AIC=277.08\ncbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n\n            Df Deviance    AIC\n- Infl:Cont  4   6.8284 269.96\n- Infl:Type 12  27.0024 274.14\n- Type:Cont  6  15.2341 274.37\n<none>           5.9443 277.08\n\n\nWarning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some\nquantities such as z, residuals, SEs may be inaccurate due to convergence at a\nhalf-step\n\n\n\nStep:  AIC=269.96\ncbind(Low, Medium, High) ~ Infl + Type + Cont + Infl:Type + Type:Cont\n\n            Df Deviance    AIC\n- Type:Cont  6  16.1072 267.24\n- Infl:Type 12  28.2559 267.39\n<none>           6.8284 269.96\n+ Infl:Cont  4   5.9443 277.08\n\nStep:  AIC=267.24\ncbind(Low, Medium, High) ~ Infl + Type + Cont + Infl:Type\n\n            Df Deviance    AIC\n- Infl:Type 12   38.662 265.80\n<none>           16.107 267.24\n+ Type:Cont  6    6.828 269.96\n+ Infl:Cont  4   15.234 274.37\n- Cont       2   32.871 280.01\n\nStep:  AIC=265.8\ncbind(Low, Medium, High) ~ Infl + Type + Cont\n\n            Df Deviance    AIC\n<none>           38.662 265.80\n+ Infl:Type 12   16.107 267.24\n+ Type:Cont  6   28.256 267.39\n+ Infl:Cont  4   37.472 272.61\n- Cont       2   54.722 277.86\n- Type       6  100.889 316.03\n- Infl       4  147.780 366.92\n\n\nCode\nhousing_vglm_step@post$anova # shows the steps that the algorithm took\n\n\n         Step Df   Deviance Resid. Df Resid. Dev      AIC\n1             NA         NA        12   5.944319 277.0807\n2 - Infl:Cont  4  0.8840624        16   6.828381 269.9648\n3 - Type:Cont  6  9.2787763        22  16.107157 267.2436\n4 - Infl:Type 12 22.5550474        34  38.662205 265.7986\n\n\nThe steps the algorithm is saved in the slot @post$anova. We can see that the additive model was selected, dropping all the interactions.\n\n\nCode\n# additive model\nhousing_vglm2 <- vglm(cbind(Low, Medium, High) ~ Infl + Type + Cont, data = housing_wide, family = multinomial)\n\nanova(housing_vglm2, housing_vglm, type = 1)\n\n\nAnalysis of Deviance Table\n\nModel 1: cbind(Low, Medium, High) ~ Infl + Type + Cont\nModel 2: cbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)  \n1        34     38.662                       \n2        12      5.944 22   32.718  0.06595 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nthere’s weak evidence that those dropped coefficients were not zero… so in favor of the more parsimonious and interpretable model, we choose the additive model. Now we do some diagnostics, show the mosaic plot of the pearson chisq values.\n\n\nCode\nsum(residuals(housing_vglm2, \"pearson\")^2) # asymptotically the same\n\n\n[1] 38.91043\n\n\nCode\ndeviance(housing_vglm2) # pretty darn close\n\n\n[1] 38.6622\n\n\nCode\n# grab standardized residuals... I think theses are on the raw scale, need to derive\nhousing_vglm2_stdres <- housing_wide %>% \n  dplyr::select(Infl,Type, Cont) %>% \n  bind_cols(residuals(housing_vglm2, \"stdres\")) %>% \n  pivot_longer(Low:High, names_to = \"Sat\", values_to = \"stdres\")\n\nfoo <- xtabs(stdres~Sat + Infl + Type, data=housing_vglm2_stdres)\n\nfoo\n\n\n, , Type = Tower\n\n        Infl\nSat             Low     Medium       High\n  High    2.3368820  1.7993578  7.6329517\n  Low    -3.1561606 -1.7459604 -5.4886972\n  Medium  0.7896368 -0.1247062 -2.5826432\n\n, , Type = Apartment\n\n        Infl\nSat             Low     Medium       High\n  High   -8.7451504  1.0581085  7.6844538\n  Low     9.5841342 -1.4240255 -5.7464336\n  Medium -0.5702533  0.3521384 -2.3637276\n\n, , Type = Atrium\n\n        Infl\nSat             Low     Medium       High\n  High   -2.3120155  0.8251904  2.0911215\n  Low     0.4934728 -3.1374902 -2.6638237\n  Medium  2.0341706  2.4451851  0.5348104\n\n, , Type = Terrace\n\n        Infl\nSat             Low     Medium       High\n  High   -7.1933577 -4.3805628  2.2529711\n  Low     8.6764258  2.8198862 -1.6825344\n  Medium -1.3182128  1.8356588 -0.6954066\n\n\nCode\nmosaicplot(foo)\n\n\n\n\n\n\n\nCode\nhousing_wide_matrix <- housing_wide %>% dplyr::select(Low:High) %>% \n  data.matrix()\nsat_margin <- housing_wide_matrix %>% rowSums()\n\nhousing_vglm2_predicted <- fitted(housing_vglm2) * sat_margin\n(housing_wide_matrix - housing_vglm2_predicted) / sqrt(housing_vglm2_predicted)\n\n\n              Low      Medium         High\n [1,] -1.27131702  0.65442726  0.793847557\n [2,]  2.05554037 -0.52448903 -1.131108776\n [3,]  0.48542299  0.00980894 -0.237618822\n [4,]  0.83488118 -0.06530752 -1.226738951\n [5,] -0.52159491  0.72901366 -0.077987991\n [6,]  0.19903520 -0.58897340  0.232680471\n [7,] -0.20002910 -0.40632189  0.725380811\n [8,] -0.09968461 -0.54894903  0.631617806\n [9,]  0.93631742  0.41590003 -0.844215602\n[10,] -0.44816554 -0.29018330  1.339493961\n[11,] -1.27460531  0.60886282  1.251820928\n[12,] -0.50068951 -0.23393207  0.669307443\n[13,] -1.50554299 -0.15670496  1.396421868\n[14,]  0.57743627  0.25994911 -0.520953337\n[15,] -0.07366774 -0.30940893  0.185311590\n[16,]  0.57671852  0.21220772 -0.903490751\n[17,] -0.71913615 -0.80963872  1.272211462\n[18,] -0.77139003  0.70614331  0.001230827\n[19,] -0.19903792  0.10678590  0.091194172\n[20,] -0.59885245  0.37522109  0.080380548\n[21,]  0.96158954 -0.06254569 -0.401788912\n[22,]  0.85710435 -0.33167080 -1.068931434\n[23,]  0.91913589  0.24740415 -1.340806857\n[24,] -0.60596707 -0.06819771  0.512236271\n\n\nCode\nresiduals(housing_vglm2, type = \"response\") + fitted(housing_vglm2)\n\n\n          Low    Medium      High\n1  0.30000000 0.3000000 0.4000000\n2  0.36956522 0.2391304 0.3913043\n3  0.17543860 0.1929825 0.6315789\n4  0.60396040 0.2277228 0.1683168\n5  0.36440678 0.2966102 0.3389831\n6  0.26530612 0.1836735 0.5510204\n7  0.40625000 0.2812500 0.3125000\n8  0.28571429 0.2857143 0.4285714\n9  0.27272727 0.3181818 0.4090909\n10 0.58064516 0.1935484 0.2258065\n11 0.36585366 0.3170732 0.3170732\n12 0.30434783 0.2173913 0.4782609\n13 0.20000000 0.2714286 0.5285714\n14 0.21250000 0.2875000 0.5000000\n15 0.09677419 0.1612903 0.7419355\n16 0.46706587 0.2754491 0.2574850\n17 0.26815642 0.2513966 0.4804469\n18 0.14705882 0.2450980 0.6078431\n19 0.31746032 0.3650794 0.3174603\n20 0.17857143 0.3928571 0.4285714\n21 0.18421053 0.2631579 0.5526316\n22 0.61290323 0.2473118 0.1397849\n23 0.47692308 0.3230769 0.2000000\n24 0.20833333 0.2500000 0.5416667\n\n\nCode\nobs_p <- housing_wide_matrix / sat_margin\nfit_p <- fitted(housing_vglm2)\n\nresiduals(housing_vglm2, type = \"response\")\n\n\n           Low        Medium          High\n1  -0.09556873  0.0398922904  5.567644e-02\n2   0.10932496 -0.0282767182 -8.104824e-02\n3   0.02494279  0.0005699035 -2.551270e-02\n4   0.06120222 -0.0031222147 -5.808001e-02\n5  -0.03016154  0.0343673813 -4.205846e-03\n6   0.01015582 -0.0273291797  1.717336e-02\n7  -0.02317183 -0.0407595729  6.393140e-02\n8  -0.01024868 -0.0610938738  7.134255e-02\n9   0.08621220  0.0462397822 -1.324520e-01\n10 -0.06466070 -0.0243274212  8.898812e-02\n11 -0.14183466  0.0492131823  9.262148e-02\n12 -0.06330269 -0.0239637078  8.726640e-02\n13 -0.09827758 -0.0099349949  1.082126e-01\n14  0.02774930  0.0151667891 -4.291609e-02\n15 -0.00420447 -0.0239154916  2.811996e-02\n16  0.02952007  0.0084845677 -3.800464e-02\n17 -0.02931623 -0.0322282664  6.154450e-02\n18 -0.03235176  0.0322567566  9.500771e-05\n19 -0.01444687  0.0080390048  6.407869e-03\n20 -0.03717000  0.0301956212  6.974382e-03\n21  0.05588069 -0.0052566447 -5.062404e-02\n22  0.06574300 -0.0177052766 -4.803772e-02\n23  0.07250046  0.0169777976 -8.947826e-02\n24 -0.06462348 -0.0070579688  7.168145e-02\n\n\nCode\n# varfun <- object@family@charfun\n# vfun <- varfun(x = NULL, eta = predict(object), \n#                   extra = object@extra, varfun = TRUE)\n#                 ans <- (y - E1)/sqrt(vfun * (1 - c(hatvalues(object))))\n# \n# 1 - hatvalues(housing_vglm2)\n# \n# varfun <- housing_vglm2@family@charfun\n# vfun <- varfun(x = NULL, eta = predict(housing_vglm2), extra = housing_vglm2@extra, varfun = TRUE)\n# \n# housing_vglm2@family@vfamily\n# \n# w <- weights(object, type = \"prior\")\n#                 x <- y * c(w)\n#                 E1 <- E1 * c(w)\n#                 if (any(x < 0) || anyNA(x)) stop(\"all entries of 'x' must be nonnegative and finite\")\n#                 if ((n <- sum(x)) == 0) stop(\"at least one entry of 'x' must be positive\")\n#                 if (length(dim(x)) > 2L) stop(\"invalid 'x'\")\n#                 if (length(x) == 1L) stop(\"'x' must at least have 2 elements\")\n#                 sr <- rowSums(x)\n#                 sc <- colSums(x)\n#                 E <- outer(sr, sc, \"*\")/n\n#                 v <- function(r, c, n) c * r * (n - r) * (n - \n#                   c)/n^3\n#                 V <- outer(sr, sc, v, n)\n#                 dimnames(E) <- dimnames(x)\n#                 ans <- stdres <- (x - E)/sqrt(V)\n# rowSums(fitted)\n# \n# \n# housing_vglm2@y # observed\nplot(housing_vglm2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncoef(housing_vglm2, matrix = TRUE)\n\n\n              log(mu[,1]/mu[,3]) log(mu[,2]/mu[,3])\n(Intercept)            0.1387428         -0.2804860\nInflMedium            -0.7348632         -0.2884673\nInflHigh              -1.6126311         -0.9476957\nTypeApartment          0.7356317          0.2999430\nTypeAtrium             0.4079781          0.5393484\nTypeTerrace            1.4123277          0.7457572\nContHigh              -0.4818270         -0.1209751\n\n\n\n\nCode\n# the predicted probabilities by each of the covariates.\nbind_cols(housing_wide %>% dplyr::select(Infl, Type, Cont),\n          fitted(housing_vglm2))\n\n\n# A tibble: 24 × 6\n   Infl   Type      Cont    Low Medium  High\n   <fct>  <fct>     <fct> <dbl>  <dbl> <dbl>\n 1 Low    Tower     Low   0.396  0.260 0.344\n 2 Medium Tower     Low   0.260  0.267 0.472\n 3 High   Tower     Low   0.150  0.192 0.657\n 4 Low    Apartment Low   0.543  0.231 0.226\n 5 Medium Apartment Low   0.395  0.262 0.343\n 6 High   Apartment Low   0.255  0.211 0.534\n 7 Low    Atrium    Low   0.429  0.322 0.249\n 8 Medium Atrium    Low   0.296  0.347 0.357\n 9 High   Atrium    Low   0.187  0.272 0.542\n10 Low    Terrace   Low   0.645  0.218 0.137\n# … with 14 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\nCode\nanova(housing_vglm1, housing_vglm, type = 1) # score tests not available in VGAM currently\n\n\nAnalysis of Deviance Table\n\nModel 1: cbind(Low, Medium, High) ~ Infl + Type + Cont + Infl:Type + Type:Cont\nModel 2: cbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)\n1        16     6.8284                     \n2        12     5.9443  4  0.88406   0.9268\n\n\nCode\ndeviance(housing_vglm)\n\n\n[1] 5.944319\n\n\nCode\ndropterm(housing_vglm, test = \"Chisq\")\n\n\nSingle term deletions\n\nModel:\ncbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n          Df    AIC     LRT Pr(Chi)  \n<none>       277.08                  \nInfl:Type 12 274.14 21.0581 0.04954 *\nInfl:Cont  4 269.96  0.8841 0.92684  \nType:Cont  6 274.37  9.2898 0.15793  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "matrix/matrix.html#basic-matrices",
    "href": "matrix/matrix.html#basic-matrices",
    "title": "6  Matrix Math",
    "section": "6.1 Basic Matrices",
    "text": "6.1 Basic Matrices\nLet j be the vector of all 1’s, and J be the matrix of all ones.\n\n\\begin{aligned}\njj' &= J \\\\\nj'x  &= \\sum x \\\\\nj'A  &= \\operatorname{colsums}(A) \\\\\nAj &= \\operatorname{rowsums}(A) \\\\\nJx &= \\begin{bmatrix}\n  \\sum x \\\\\n  \\vdots \\\\\n  \\sum x \\\\\n\\end{bmatrix}\n\\end{aligned}"
  },
  {
    "objectID": "matrix/matrix.html#adjacency-matrix",
    "href": "matrix/matrix.html#adjacency-matrix",
    "title": "6  Matrix Math",
    "section": "6.2 Adjacency Matrix",
    "text": "6.2 Adjacency Matrix\n\n6.2.1 Example Spectrums\n\n\n6.2.2 Spectral Radius Bounds\n\n\n\n\n\n\nTheorem 2.3: Adjacency Matrix Spectral Radius Bound (Hong, Shu, and Fang 2001)\n\n\n\n\n\\begin{aligned}\n\\rho(A) \\leq \\frac{d_{min} -1 + \\sqrt{(d_{min} + 1)^2 + 4(2m-nd_{min})}}{2}\n\\end{aligned}\n\n\nd_{min} is minimum degree on the graph\nn,m is number of nodes, edges in the graph\nequality is reached for regular graphs, or bi-degreed graphs of either d_{min} or n-1\n\nMore simply, for a simply connected graph d_{min} = 1 the expression simplifies\n\n\\begin{align*}\n\\rho(A) \\leq \\sqrt{1 + 2m - n}\n\\end{align*}\n\\tag{6.1}\n\nequality reached for complete or star graph\n\n\n\n\n\nCode\nhong_upper_adj_spectral <- function(d_min, n, m, easy = FALSE) {\n  if (easy) return(sqrt(1 + 2*m - n))\n  (d_min - 1 + sqrt((d_min + 1)^2 + 4 * (2 * m - n * d_min))) / 2\n}\n\n\n\n\nCode\ng5_list <- read_file6(\"data/graph5c.g6\", type = \"igraph\")\n\nspectrum(g5_list[[5]], which = list(pos = \"LM\", howmany = 1))$values\n\n\n[1] 2.135779\n\n\nCode\neigen(as_adj(g5_list[[2]]))$values[1]\n\n\n[1] 1.847759\n\n\nCode\ng5_hong_df <- tibble(g = g5_list) |> \n  rownames_to_column(\"g_id\") |> \n  rowwise() |> \n  mutate(min_deg = min(degree(g)),\n         n_edge = ecount(g),\n         n_vertex = vcount(g),\n         spectral_radius = eigen(as_adj(g))$values[1],\n         upper_spectral = hong_upper_adj_spectral(min_deg, n_vertex, n_edge),\n         easy_upper_spectral = hong_upper_adj_spectral(min_deg, n_vertex, n_edge, easy = TRUE)) |> \n  arrange(upper_spectral, easy_upper_spectral, spectral_radius)\n\ng5_hong_df |> ggplot(aes(factor(g_id, unique(g_id)), spectral_radius)) +\n  geom_point(aes(color = \"Spectral Radius\")) +\n  geom_line(aes(y = upper_spectral, group = 1, color = \"Hong Upper Bound\", linetype = \"Hong Upper Bound\")) +\n  geom_line(aes(y = easy_upper_spectral, group = 1, color = \"Hong Easy Upper Bound\", linetype = \"Hong Easy Upper Bound\")) +\n  guides(colour = guide_legend(override.aes = list(shape = c(26, 26, 19), linetype = c(1, 2, 0)))) +\n  scale_color_manual(\"\", values = c(\"red\", \"red\", \"black\")) +\n  scale_shape(guide = \"none\") + \n  scale_linetype(guide = \"none\") +\n  labs(x = \"graph\",\n       y = \"Adj. Spectral Radius\",\n       title = \"Hong Spectral Radius of Adjacency for all connected 5-graphs\")"
  },
  {
    "objectID": "matrix/matrix.html#laplacian",
    "href": "matrix/matrix.html#laplacian",
    "title": "6  Matrix Math",
    "section": "6.3 Laplacian",
    "text": "6.3 Laplacian\n\n6.3.1 Example Spectrums\n\nComponentsComplete GraphStar GraphBipartite GraphPath\n\n\nIf there are 2 connected components, 2 of the eigenvalues will be 0.\n\n\nCode\ng1 <- graph(~1-2-3-1-4-5-6-4)\nL1 <- laplacian_matrix(g1)\nplot(g1)\n\n\n\n\n\nCode\neigen(L1)$values |> zapsmall()\n\n\n[1] 4.561553 3.000000 3.000000 3.000000 0.438447 0.000000\n\n\nCode\ng2 <- graph(~1-2-3-1, 4-5-6-4)\nplot(g2)\n\n\n\n\n\nCode\nL2 <- laplacian_matrix(g2)\neigen(L2)$values |> zapsmall()\n\n\n[1] 3 3 3 3 0 0\n\n\n\n\n\n\nCode\ng <- make_full_graph(5)\nL <- laplacian_matrix(g)\nplot(g)\n\n\n\n\n\nCode\neigen(L)$values |> zapsmall()\n\n\n[1] 5 5 5 5 0\n\n\n\n\n\n\nCode\ng <- make_star(5, mode = \"undirected\")\nL <- laplacian_matrix(g)\nplot(g)\n\n\n\n\n\nCode\neigen(L)$values |> zapsmall()\n\n\n[1] 5 1 1 1 0\n\n\n\n\n\n\nCode\ng <- make_full_bipartite_graph(5, 3)\nL <- laplacian_matrix(g, normalized = F)\nplot(g)\n\n\n\n\n\nCode\neigen(L)$values |> zapsmall()\n\n\n[1] 8 5 5 3 3 3 3 0\n\n\n\n\n\n\nCode\ng <- make_graph(~1-2-3-4-5-6-7-8-9-10)\nL <- laplacian_matrix(g, normalized = F)\nplot(g)\n\n\n\n\n\nCode\neigen(L)$values |> zapsmall()\n\n\n [1] 3.902113 3.618034 3.175571 2.618034 2.000000 1.381966 0.824429 0.381966\n [9] 0.097887 0.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.3.2 Spectral Radius Bounds\n\nSimple\nThere are 3 simple bounds on the Laplacian presented by (Anderson and Morley 1985),\n\nnumber of nodes in graph\n\n \\lambda_1(L) \\leq n\n\nequality reached on complete graph, for example.\n\n\nmax degree\n\n \\lambda_1(L) \\leq 2d_{max}\n\nequality nears on a path graph.\n\n\nmaximal ends of an edge\n\n \\lambda_1(L) \\leq \\max_{i\\sim j} w_i + w_j \n\nwhere maximum is over edges in graph, and w_i + w_j is the sum of weights of that edge’s endpoints.\nI believe this is true over any weighted, connected, undirected graph\nequality is reached when bipartite graph\n\n\n\n\n6.3.3 Laplacian for Distributed Summation\n\n\n\n\n\n\nSource\n\n\n\nThis insight comes from Sivan Toledo, where he has a nice description of the problem.\n\n\nIf the eventual algorithm goal is to have the sum of the graph sitting on every node, the repeated application of a matrix multiplication should converge to the matrix of all ones J. The spectrum of J is simply n and the rest zeros. \\frac{1}{n}J is special because it sets each node to have the mean, and the single non-zero eigenvalue is 1.\n\n\nCode\neigen(matrix(rep(1/4, 16), nrow = 4))$values |> zapsmall()\n\n\n[1] 1 0 0 0\n\n\nSuppose the initial state of the graph y is values at each of the n nodes of the graph.\n\n\\begin{aligned}\nn(I - \\frac{1}{n}L)^ky\n\\end{aligned}\n\nAs k increases, we’d expect that the matrix spectrum converges to match J, since L has a zero eigenvalue, one eigen value limit is 1. And since the Laplacian has spectral radius upper bounded by number of nodes,\n\n\nCode\nset.seed(1)\ng <- sample_gnp(10, .5)\nA <- g %>% as_adj()\nL <- Diagonal(x = rowSums(A)) - A\n\n\n\n\nCode\nJ <- Matrix(1, nrow = 10, ncol = 10)\nJ %*% cbind(1:10)\n\n\n10 x 1 Matrix of class \"dgeMatrix\"\n      [,1]\n [1,]   55\n [2,]   55\n [3,]   55\n [4,]   55\n [5,]   55\n [6,]   55\n [7,]   55\n [8,]   55\n [9,]   55\n[10,]   55\n\n\n\n\nCode\nJe <- eigen(J)\nLn <- laplacian_matrix(g, normalized = TRUE)\neigen(Ln)\n\n\neigen() decomposition\n$values\n [1] 1.647271e+00 1.467555e+00 1.450327e+00 1.166667e+00 1.084726e+00\n [6] 1.000000e+00 9.451136e-01 7.853920e-01 4.529494e-01 2.442491e-15\n\n$vectors\n             [,1]        [,2]         [,3]          [,4]        [,5]\n [1,] -0.16557306 -0.35884141  0.414024644  4.378811e-16 -0.41543869\n [2,]  0.01245928  0.32716246 -0.261033301  7.071068e-01 -0.26421090\n [3,]  0.07815309  0.07003764  0.522362456 -2.958733e-16  0.30735544\n [4,]  0.49476837 -0.29623231 -0.126497326  3.013764e-16 -0.34924549\n [5,] -0.26126348 -0.45016699 -0.226430018  3.655149e-16 -0.17830741\n [6,]  0.01453893 -0.34738481 -0.412927706 -6.383782e-16  0.64394097\n [7,]  0.29116939 -0.06983555  0.410643861 -1.133005e-16  0.11871973\n [8,]  0.39734608  0.39945650 -0.007375584 -5.551115e-17  0.08547476\n [9,]  0.01245928  0.32716246 -0.261033301 -7.071068e-01 -0.26421090\n[10,] -0.64049820  0.27700977  0.113930213 -2.030681e-18  0.05918033\n               [,6]        [,7]       [,8]        [,9]      [,10]\n [1,] -3.131423e-15  0.23529056  0.5901861  0.09578594 -0.2948839\n [2,] -3.906316e-15  0.22258023 -0.1102821  0.25044926 -0.3611576\n [3,] -6.324555e-01 -0.02451212 -0.2642891  0.25347743 -0.2948839\n [4,] -3.162278e-01  0.13949439 -0.2339562 -0.57753213 -0.1474420\n [5,]  6.578071e-15 -0.66595972 -0.2431392  0.18631243 -0.3296902\n [6,] -1.421198e-15  0.30002682  0.2257963 -0.02773400 -0.3900947\n [7,]  7.071068e-01  0.04045948 -0.3410158 -0.03156311 -0.3296902\n [8,]  6.679915e-15 -0.53861397  0.5160385 -0.17841799 -0.2948839\n [9,] -2.959103e-15  0.22258023 -0.1102821  0.25044926 -0.3611576\n[10,] -1.665335e-16  0.01531268 -0.1004178 -0.63187856 -0.2948839\n\n\nCode\n# not correct\n# M <- diag(10) - L / (diag(L) + 1) # direct averaging of over all nodes ***\n# M <- diag(L) - L/2 # average across edges?\n\n# correct\nM <- diag(10) - L / 10\n\nmat_pow <- function(M, t = 10) {\n  if (t == 1) {return(M)}\n  return(M %*% mat_pow(M, t - 1))\n}\n# mat_pow(M, 100)\n\ny <- cbind(1:10)\nfor (i in 1:100) {\n  y <- M %*% y\n}\ny\n\n\n10 x 1 Matrix of class \"dgeMatrix\"\n          [,1]\n [1,] 5.499981\n [2,] 5.499980\n [3,] 5.499978\n [4,] 5.500109\n [5,] 5.499981\n [6,] 5.499988\n [7,] 5.499989\n [8,] 5.499992\n [9,] 5.499980\n[10,] 5.500024\n\n\n\n\n6.3.4 Normalized Laplacian\nThe primary reason for looking at the normalized laplacian is because it removes dependence on the number of nodes in the graph, which would change bounds. Rather, the eigenvalues of a normalized laplacian will range from 0 \\leq 2, reaching 2 for bipartite graphs.\n\n\nCode\nD <- 1 / sqrt(diag(L))\nNL <- Diagonal(x = D) %*% L %*% Diagonal(x = D)\nNL\n\n\n10 x 10 sparse Matrix of class \"dgCMatrix\"\n                                                                            \n [1,]  1.0000000 -0.2041241  .          .    .         -0.1889822  .        \n [2,] -0.2041241  1.0000000 -0.2041241  .   -0.1825742 -0.1543033 -0.1825742\n [3,]  .         -0.2041241  1.0000000  .   -0.2236068 -0.1889822  .        \n [4,]  .          .          .          1.0  .          .          .        \n [5,]  .         -0.1825742 -0.2236068  .    1.0000000  .         -0.2000000\n [6,] -0.1889822 -0.1543033 -0.1889822  .    .          1.0000000 -0.1690309\n [7,]  .         -0.1825742  .          .   -0.2000000 -0.1690309  1.0000000\n [8,] -0.2500000  .          .          .   -0.2236068 -0.1889822  .        \n [9,] -0.2041241 -0.1666667 -0.2041241  .   -0.1825742 -0.1543033 -0.1825742\n[10,]  .          .          .         -0.5  .         -0.1889822 -0.2236068\n                                      \n [1,] -0.2500000 -0.2041241  .        \n [2,]  .         -0.1666667  .        \n [3,]  .         -0.2041241  .        \n [4,]  .          .         -0.5000000\n [5,] -0.2236068 -0.1825742  .        \n [6,] -0.1889822 -0.1543033 -0.1889822\n [7,]  .         -0.1825742 -0.2236068\n [8,]  1.0000000  .         -0.2500000\n [9,]  .          1.0000000  .        \n[10,] -0.2500000  .          1.0000000\n\n\n\nExample Spectrums of Normalized Laplacian\n\nRingPathCompleteBipartite\n\n\n\n\nCode\ng <- make_ring(5)\nplot(g)\n\n\n\n\n\nCode\nN <- laplacian_matrix(g, normalized = TRUE)\n# 1 - cos(2 * pi * 0:4 / 5) |> sort() # exact\neigen(N)$values |> zapsmall()\n\n\n[1] 1.809017 1.809017 0.690983 0.690983 0.000000\n\n\n\n\n\n\nCode\ng <- graph(~1-2-3-4-5)\nN <- laplacian_matrix(g, normalized = TRUE)\n# 1 - cos(pi * 0:4 / 4) |> sort() # exact\neigen(N)$values |> zapsmall()\n\n\n[1] 2.0000000 1.7071068 1.0000000 0.2928932 0.0000000\n\n\n\n\n\n\nCode\ng <- make_full_graph(5)\nN <- laplacian_matrix(g, normalized = TRUE)\neigen(N)$values |> zapsmall() # n / n-1\n\n\n[1] 1.25 1.25 1.25 1.25 0.00\n\n\n\n\n\n\nCode\ng <- make_full_bipartite_graph(6, 4)\nN <- laplacian_matrix(g, normalized = TRUE)\neigen(N)$values |> zapsmall() # 0, 1 (n + m - 2), 2\n\n\n [1] 2 1 1 1 1 1 1 1 1 0\n\n\n\n\n\n\n\n\n6.3.5 Fiedler Bounds (normalized Laplacian)\nFiedler Eigenvalue is the smallest non-zero eigenvalue.\n\nSimple\nFor k-regular graph, and diameter > 4, we have\n\n\\begin{aligned}\n\\limsup \\lambda_{n-1} \\leq 1 - 2\\frac{\\sqrt{k-1}}{k}\n\\end{aligned}\n\n\nequality is reached for ramanujan graphs\nnote that the conditions exclude bipartite graphs\nthe fact that it’s “lim sup” makes this quite useless, because it’s asymptotically toward infinity”. In fact, for some regular 4 graphs, 14 node, diameter 5 graphs, the Fiedler value is still well above this bound.\n\nRather, a more general (and useful upper bound):\n\n\n\n\n\n\nLemma 1.14: Fiedler Upper Bound (dia \\geq 4) (Chung 1997)\n\n\n\nLet G be a graph with diameter D \\geq 4, and let k denote the maximum degree of G. Then,\n\n\\begin{aligned}\n\\lambda_{n-1} \\leq 1 - 2\\frac{\\sqrt{k-1}}{k}\\left(1 - \\frac{2}{D}\\right) + \\frac{2}{D}\n\\end{aligned}\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy contracting weighted graphs, and using Rayleigh quotient to upper bound.\n\n\n\n\n\nNote that the original reference (Nilli 1991) , studies this problem for non-standardized Laplacian.\n\n\nCode\nupper_fiedler_nilli <- function(k, D) {\n  if (D < 4) abort(\"upper bound only valid when diameter greater than 4\")\n  return(1 - 2 * sqrt(k -1) / k * (1 - 2 / D) + 2 / D)\n}\n\n\nLet’s generate all permutations of connected 8 graphs, and pick out those with diameter greater than 4. All graph generation is done with the program geng and filtered with pickg (McKay and Piperno 2013).\n\n\nCode\ng8_dia4plus_list <- read_file6(\"data/graph8cdia4p.g6\", type = \"igraph\")\n\ng8_dia4plus_df <- tibble(g = g8_dia4plus_list) |> \n  rownames_to_column(\"g_id\") |> \n  rowwise() |> \n  mutate(fiedler = net_fiedler(g, normalize = T),\n         k = max(degree(g)),\n         D = diameter(g),\n         upper_fiedler = upper_fiedler_nilli(k, D)) |> \n  arrange(desc(upper_fiedler), desc(fiedler))\n  \ng8_dia4plus_df |> \n  ggplot(aes(x = factor(g_id, levels = unique(g_id)),\n             y = fiedler, color = \"Fiedler Value\")) +\n  geom_point(size = .5, alpha = .7) +\n  geom_line(aes(y = upper_fiedler, group = upper_fiedler, color = \"Upper Bound\")) +\n  labs(color = \"Color\",\n       title = \"Nilli Bound on Fiedler Value for connected-8 Graphs\",\n       x = \"Unique Graph Combinations\",\n       y = \"Fiedler Value\") +\n  scale_color_manual(values = c(\"black\", \"red\")) +\n  scale_x_discrete(breaks = NULL, labels = NULL) + \n  theme_minimal()\n\n\n\n\n\n\n\nCode\n# files with regular, diameter larger than 4\n# These are all 4 regular graphs, connected, 13/14 nodes, d\nreg4_dia4plus_files <- c(\"data/reg4c13dia4.g6\", \"data/reg4c13dia5.g6\", \"data/reg4c14dia4.g6\", \"data/reg4c14dia5.g6\") \nreg4_dia4plus_list <- reg4_dia4plus_files |> \n  map(read_file6, type = \"igraph\")\n\nreg4_dia4plus_df <- tibble(g = reg4_dia4plus_list, path = reg4_dia4plus_files)  |> \n  unnest_longer(g, indices_include = TRUE, indices_to = \"graph_id\") |> \n  rowwise() |> \n  mutate(match = list(str_match(path, \"data/reg4c(\\\\d*)dia(\\\\d*)\")),\n         nodes = match[2],\n         dia = match[3],\n         k = 4,\n         fiedler = net_fiedler(g, normalized = T),\n         upper_fiedler = 1 - 2*sqrt(4-1) / 4) |> \n  select(-c(match, path)) |> arrange(desc(dia), nodes)\n\nreg4_dia4plus_df <- reg4_dia4plus_df |> mutate(dia = as.numeric(dia),\n                           islower = upper_fiedler > fiedler,\n                           other_upper = 1 - 2 * sqrt(4 - 1)/4 * (1 - 2 / dia) + 2/dia,\n                           other_islower = other_upper > fiedler) |> \n  arrange(islower, other_islower, desc(dia))\n\n# among regular graphs, 13-14 nodes, 4+ diameter\nreg4_dia4plus_df\n\n\n# A tibble: 4,271 × 10\n# Rowwise: \n   g        graph_id nodes   dia     k fiedler upper_f…¹ islower other…² other…³\n   <list>      <int> <chr> <dbl> <dbl>   <dbl>     <dbl> <lgl>     <dbl> <lgl>  \n 1 <igraph>        1 13        4     4   0.203     0.134 FALSE      1.07 TRUE   \n 2 <igraph>        2 13        4     4   0.250     0.134 FALSE      1.07 TRUE   \n 3 <igraph>        3 13        4     4   0.207     0.134 FALSE      1.07 TRUE   \n 4 <igraph>        4 13        4     4   0.205     0.134 FALSE      1.07 TRUE   \n 5 <igraph>        5 13        4     4   0.204     0.134 FALSE      1.07 TRUE   \n 6 <igraph>        6 13        4     4   0.257     0.134 FALSE      1.07 TRUE   \n 7 <igraph>        7 13        4     4   0.255     0.134 FALSE      1.07 TRUE   \n 8 <igraph>        8 13        4     4   0.257     0.134 FALSE      1.07 TRUE   \n 9 <igraph>        9 13        4     4   0.188     0.134 FALSE      1.07 TRUE   \n10 <igraph>       10 13        4     4   0.192     0.134 FALSE      1.07 TRUE   \n# … with 4,261 more rows, and abbreviated variable names ¹​upper_fiedler,\n#   ²​other_upper, ³​other_islower\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nreg4_dia4plus_df |> unite(col = \"g_uid\", !!!c(\"nodes\", \"dia\", \"graph_id\"), remove = FALSE) |>\n  ggplot(aes(fct_reorder2(g_uid, fiedler, dia, .desc = T), fiedler)) +\n  geom_point() + \n  geom_point(aes(y = upper_fiedler), color = \"red\") +\n  geom_point(aes(y = other_upper), color = \"red\")\n\n\n\n\n\nCode\n# reg4_dia4plus_df\n# reg4_dia4plus_df # well... they are all lower bounded? if it's an infinite family of regular graphs... it should get closer right? b/c it's lim sup?\n\n\n\n\nBy Volume (global)\nA loose lower bound for the smallest non-trivial eigenvalue (Chung 1997, 7):\n\n\\begin{aligned}\n\\lambda_{n-1} \\geq \\frac{1}{D\\operatorname{vol}(G)}\n\\end{aligned}\n where D is the diameter of the graph, and volume of the graph is the sum of degrees for each node.\n\n\n\n\n\n\nDefinition of Volume\n\n\n\nNote that Chung (1997) uses the definition \\operatorname{vol}(G) = \\sum_{x\\in S}d_x where d_x is the degree of vertex x. Other references use \\operatorname{vol}(G) = |G| which is the number of nodes, or giving each node weight 1.\n\n\nLet’s find the Fiedler value of every graph of size 5 and graph them against the bound,\n\n\nCode\ng5_list <- read_file6(\"data/graph5c.g6\", type = \"igraph\")\n\n# fiedler values\nfiedler5 <- lapply(g5_list, function(g) {\n  vol <- sum(degree(g)) # conservative\n  # vol <- vcount(g)\n  M <- laplacian_matrix(g, normalized = TRUE)\n  dia <- diameter(g, directed = FALSE)\n  list(g = list(g),\n       fiedler = eigen(M)$values[4],\n       vol = vol,\n       dia = dia)\n})\n\ngraph_fiedler <- tibble(gg = fiedler5) |> unnest_wider(gg) |> unnest(cols = c(g)) |> \n  mutate(bound = 1/ (dia * vol),\n         diff = fiedler - bound) |> \n  arrange(diff)\n\n\n# calculate the lower bound.\nfiedler_bound_line <- tibble(dia = rep(list(seq(1, 4, .1)), 3), vol = c(5, 10, 20)) |> rowwise() |> \n  mutate(bound = list(1 / vol / dia)) |> \n  unnest_longer(col = c(dia, bound))\n\n\n# The bound is generally quite bad for graphs \ngraph_fiedler |> ggplot(aes(dia, fiedler, color = vol)) +\n  geom_line(data = fiedler_bound_line, mapping = aes(dia, bound, color = vol, group = vol)) +\n  geom_point() +\n  labs(title = \"Simple Fiedler Bound\")\n\n\n\n\n\nEven using the vertex count definition for volume (less conservative), the bound is quite low for most of the Fiedler values.\n\n\nBy Cheeger’s (Sparsest Cut)\nCheeger’s constant is loosely defined in english, as the minimal ratio, of cost of cutting edges, to the size of sets it cuts off. That is, a “dumbbell” shape graph, where large vertex sets are on both side, and only cutting 1 edge in the middle would have a very very low cheeger constant.\n\n\\begin{aligned}\nh_G &= \\min_S \\frac{|\\delta S|}{\\min \\{|S|,|\\bar S| \\}} \\\\&= \\frac{\\text{cutting edges cost}}{\\text{vertex set volume}}\n\\end{aligned}\n\nCalculating Cheeger’s constant is an NP-Hard problem, meaning that the problem is likely non-polynomial for solution and checking.\nThe bounds on Fiedler’s value, with cheegers constants have the form,\n\n\\begin{aligned}\n\\frac{h_G^2}{2d_{max}} \\leq \\lambda_{n-1}  \\leq 2h_G\n\\end{aligned}\n\n\n\nCode\n# graph_boundary_edges <- function(g, vs) {\n#   if(!inherits(g, \"igraph\")) {\n#     abort(\"`g` must be of class `igraph`\")\n#   }\n#   inc_edges <- igraph::incident_edges(g, vs)\n#   purrr::reduce()\n# }\n\n# graph_interior_edges <- function(g, vs) {\n#   if(!inherits(g, \"igraph\")) {\n#     abort(\"`g` must be of class `igraph`\")\n#   }\n#   inc_edges <- igraph:incident_edges(g, vs)\n# }\n\n\n# forget the functions... just do on matrices\n\n\n\n\nCode\n#TODO: implemented assuming undirected graph, generalize?\ncheeger <- function(A, S) {\n  # calculate vertex boundary by taking neighbors minus initial set\n  adj_vec_set <- A[S,, drop = F] |> colSums()\n  ego1 <- which(adj_vec_set > 0)\n  dS <- setdiff(ego1, S) # vertex boundary\n  \n  # calculate edge boundary by subsetting matrix from S to !S\n  bridge_matrix <- A[S, dS, drop = F]\n  \n  bridge_edges <- bridge_matrix |> \n    as(\"lgCMatrix\") |> # also assumes unweighted here when converting to logical\n    which(arr.ind = T) \n  bridge_edges[,\"row\"] <- S[bridge_edges[,\"row\"]] # convert indices\n  bridge_edges[,\"col\"] <- dS[bridge_edges[,\"col\"]]\n  vol_dS <- sum(bridge_matrix)\n  min_vol_S <- min(length(S), nrow(A) - length(S)) # use length as volume metric\n  return(vol_dS / min_vol_S)\n}\n\n# generate subsets for each graph\nall_comb <- mapply(function(x) combn(5, x, simplify = F), 1:5)\nall_comb_df <- tibble(elem = all_comb) |>\n  unnest_longer(elem)\n\ngraph_df <- g5_list |> lapply(as_adj) |>\n  tibble(A = _) |> rownames_to_column(var = \"id\") |> \n  mutate(id = as.numeric(id))\n\ncheeger_df <- full_join(graph_df, all_comb_df, by = character()) |> rowwise() |> \n  mutate(cheeger_values = cheeger(A, elem))\n\ncheeger_constant_df <- cheeger_df |> filter(cheeger_values > 0) |>  group_by(id) |> arrange(id, cheeger_values) |> slice(1) |> ungroup()\n\n# cheeger_constant_df |> slice(2) |> unlist()\n\nfiedler_value_df <- tibble(gg = fiedler5) |> unnest_wider(gg) |> unnest(cols = c(g)) |> \n  rownames_to_column(var = \"id\") |> \n  rowwise() |> \n  mutate(id = as.numeric(id),\n         max_deg = max(degree(g))) |> \n  select(id, fiedler, dia, vol, max_deg) |> \n  ungroup()\n\n\ncheeger_fiedler_df <- cheeger_constant_df |> select(id, elem, cheeger_values) |> left_join(fiedler_value_df, by = \"id\") |> \n  mutate(upper_cheeger = cheeger_values * 2,\n         lower_cheeger = cheeger_values^2 / 2/ max_deg) # dividing by max deg\n# cheeger_fiedler_df |> mutate(inbound = fiedler > lower_cheeger & fiedler < upper_cheeger)\n\ncheeger_fiedler_df |>\n  ggplot() +\n  geom_point(aes(id, lower_cheeger), color = \"red\") +\n  geom_point(aes(id, fiedler, color = vol)) +\n  geom_point(aes(id, upper_cheeger), color = \"red\") +\n  labs(y = \"Eigenvalue\",\n       x = \"Graph ID\",\n       title = \"\")\n\n\n\n\n\nCode\n# many variations of cheeger's unfortunately\n# seems incorrect for\n\n\n\n\n\n6.3.6 Laplacian decomposition as incidence matrix\nIf we define an incidence matrix as a |V| \\times |E| matrix, in which each column has a 1 and -1 for in positions the edge connects the vertices,\n\n\nCode\n# ve_incidence <- function(g) {\n#   stopifnot(class(g) == \"igraph\")\n#   g %>% get.adjedgelist() %>% map_dbl(~-onehot(.x, n = ecount(g)))\n# }\n\n\nonehot <- function(x, n = max(x)) {\n  y <- vector(mode = \"numeric\",length = n)\n  y[x] <- 1\n  return(y)\n}\n\n\n# edge list\nve_incidence_matrix <- function(g) {\n  stopifnot(class(g) == \"igraph\")\n  onehot_edge <- function(x, n = max(x)) {\n    y <- vector(mode = \"numeric\",length = n)\n    y[x * sign(x)] <- sign(x)\n    return(y)\n  }\n  g_el <- get.edgelist(g) \n  g_el[,2] <- -g_el[,2]\n  g_el %>% apply(1, onehot_edge, n = vcount(g))\n}\n\nset.seed(1)\ng <- sample_gnp(10, .5)\nA <- g %>% as_adj()\nL <- Diagonal(x = rowSums(A)) - A\nB <- ve_incidence_matrix(g) # incidence matrix\n\n\nOur incidence matrix B looks like\n\n\nB = \\begin{bmatrix} 1 &0 &0 &0 &1 &0 &0 &0 &0 &0 &1 &0 &0 &1 &0 &0 &0 &0 &0 &0 &0 &0 &0 \\\\-1 &1 &1 &0 &0 &1 &0 &1 &0 &0 &0 &0 &0 &0 &1 &0 &0 &0 &0 &0 &0 &0 &0 \\\\0 &-1 &0 &1 &0 &0 &1 &0 &0 &0 &0 &0 &0 &0 &0 &1 &0 &0 &0 &0 &0 &0 &0 \\\\0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &1 &0 &0 &0 \\\\0 &0 &-1 &-1 &0 &0 &0 &0 &1 &0 &0 &1 &0 &0 &0 &0 &1 &0 &0 &0 &0 &0 &0 \\\\0 &0 &0 &0 &-1 &-1 &-1 &0 &0 &1 &0 &0 &1 &0 &0 &0 &0 &1 &0 &0 &1 &0 &0 \\\\0 &0 &0 &0 &0 &0 &0 &-1 &-1 &-1 &0 &0 &0 &0 &0 &0 &0 &0 &1 &0 &0 &1 &0 \\\\0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &-1 &-1 &-1 &0 &0 &0 &0 &0 &0 &0 &0 &0 &1 \\\\0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &-1 &-1 &-1 &-1 &-1 &-1 &0 &0 &0 &0 \\\\0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &-1 &-1 &-1 &-1 \\\\ \\end{bmatrix}\n\n\n\n\nL= \\begin{bmatrix} 4 &-1 &0 &0 &0 &-1 &0 &-1 &-1 &0 \\\\-1 &6 &-1 &0 &-1 &-1 &-1 &0 &-1 &0 \\\\0 &-1 &4 &0 &-1 &-1 &0 &0 &-1 &0 \\\\0 &0 &0 &1 &0 &0 &0 &0 &0 &-1 \\\\0 &-1 &-1 &0 &5 &0 &-1 &-1 &-1 &0 \\\\-1 &-1 &-1 &0 &0 &7 &-1 &-1 &-1 &-1 \\\\0 &-1 &0 &0 &-1 &-1 &5 &0 &-1 &-1 \\\\-1 &0 &0 &0 &-1 &-1 &0 &4 &0 &-1 \\\\-1 &-1 &-1 &0 &-1 &-1 &-1 &0 &6 &0 \\\\0 &0 &0 &-1 &0 &-1 &-1 &-1 &0 &4 \\\\ \\end{bmatrix}\n\n\nJust to see the sparsity pattern of B, (we try spam’s display routine)\n\n\n\n\n\n\n\n6.3.7 Laplacian Stochastic Matrix\nI - D^{\\dagger}A"
  },
  {
    "objectID": "matrix/matrix.html#stochastic-matrices",
    "href": "matrix/matrix.html#stochastic-matrices",
    "title": "6  Matrix Math",
    "section": "6.4 Stochastic Matrices",
    "text": "6.4 Stochastic Matrices\nmatrices in which each matrix row sums to 1."
  },
  {
    "objectID": "matrix/matrix.html#matrix-norms",
    "href": "matrix/matrix.html#matrix-norms",
    "title": "6  Matrix Math",
    "section": "6.5 Matrix Norms",
    "text": "6.5 Matrix Norms\nThis section aims to quantify and give intuition behind the following matrix norms.\n\nspectral\nfrobenius\nL^n\n\n\n\n\n\nAnderson, William N., and Thomas D. Morley. 1985. “Eigenvalues of the Laplacian of a Graph.” Linear and Multilinear Algebra 18 (2): 141–45. https://doi.org/10.1080/03081088508817681.\n\n\nChung, Fan R. K. 1997. Spectral Graph Theory. American Mathematical Soc. https://books.google.com?id=YUc38_MCuhAC.\n\n\nHong, Yuan, Jin-Long Shu, and Kunfu Fang. 2001. “A Sharp Upper Bound of the Spectral Radius of Graphs.” Journal of Combinatorial Theory, Series B 81 (2): 177–83. https://doi.org/10.1006/jctb.2000.1997.\n\n\nMcKay, Brendan D., and Adolfo Piperno. 2013. “Practical Graph Isomorphism, II.” arXiv. https://doi.org/10.48550/arXiv.1301.1493.\n\n\nNilli, A. 1991. “On the Second Eigenvalue of a Graph.” Discrete Mathematics 91 (2): 207–10. https://doi.org/10.1016/0012-365X(91)90112-F."
  },
  {
    "objectID": "stochastic/stochastic.html#gamblers-ruin",
    "href": "stochastic/stochastic.html#gamblers-ruin",
    "title": "7  Stochastic",
    "section": "7.1 Gamblers Ruin",
    "text": "7.1 Gamblers Ruin\nThe random walk we’re simulating is a symmetric random walk from some initial location, and calculating the expected hitting time of crossing some upper or lower boundary\n\n\n\n\n\nCode\nset.seed(20)\nk <- 10\nrw <- rw_sim(a, b, p, k, return_path = TRUE)\n\nqplot(0:(length(rw) - 1), rw, geom = \"line\", xlab = \"time\", ylab = \"state\", linetype = \"random walk path\") + \n  geom_hline(yintercept = b, color = \"tomato1\") + \n  geom_hline(yintercept = a, color = \"tomato1\") +\n  annotate(\"point\", x = 0, y = 10, size = 2, color = \"lightblue\") +\n  annotate(\"label\", label = \"k - initial state\", x = 3, y = 9, fill = \"lightblue\", size = 2) +\n  annotate(\"point\", x = 44, y = 0, size = 2, color = \"lightblue\") +\n  annotate(\"label\", label = \"T - hitting time\", x = 41, y = 1, fill = \"lightblue\", size = 2) +\n  scale_linetype_manual(values = c(3)) + \n  labs(linetype = \"\",\n       title = \"Hitting Times of Random Walk\") + \n  lims(y = c(a, b)) + \n  theme_minimal() + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\nBy theory, for the symmetric random walk, (see Durrett)\n\n\\begin{aligned}\nE[T|X_0 = k] = (k - a)(b - k)\n\\end{aligned}\n\n\n\nCode\nhitting_times <- replicate(10000,\n                           rw_sim(a,b,p, k))\nmean(hitting_times)\n\n\n[1] 101.0178"
  },
  {
    "objectID": "stochastic/stochastic.html#stochastic-processes",
    "href": "stochastic/stochastic.html#stochastic-processes",
    "title": "7  Stochastic",
    "section": "7.2 Stochastic Processes",
    "text": "7.2 Stochastic Processes\nA great overview of Differential Equations in R is covered by\n\n“sde” - stochastic differential equations\n“Sim.DiffProc”- simulate diffusion processes\n“ReacTran” - functions for generating finite differences on a grid\n\n\n7.2.1 Wiener Process\nThe most basic wiener process takes the form, which describes Brownian motion. This differential equation models 1 dimensional diffusion, and to see this, we can imagine the probability distribution over time. Each of the sample paths are a random walk with gaussian increment with proportional\n\ndx = dW\n\n\nx is the position, which is a function of time\ndW is the Wiener Noise, gaussian distribution with\n\n\n\nCode\nx0 <- 0 # initial position\nt0 <- 0 # initial time\ndt <- .01\nnt <- 100 # how many time steps to take, \ndx <- rnorm(nt, 0, dt) # sample steps\nx <- cumsum(c(x0, dx)) # sample path\n\nrwiener <- function(x0 = 0, t0 = 0, dt = .01, nt = 100) {\n  dx <- rnorm(nt, 0, sqrt(dt)) # sample steps\n  cumsum(c(x0, dx)) # sample path\n}\n\nset.seed(1)\nwiener_paths <- replicate(2000, rwiener())\nwiener_ts <- ts(wiener_paths, start = t0, deltat = dt)\nts.plot(wiener_ts, col = rgb(0,0,0,alpha = .03), ylim = c(-4, 4), main = \"Wiener Sample Paths\")\n\n\n\n\n\nInstead of the sample path approach, if we instead think about the probability density of our position as a function of time, we can see that the probability function becomes more diffuse over time. Thus, it’s not surprising that we can describe the diffusion with a PDE through the Fokker-Plank Equation.\n\n\nCode\n# code to create animations\nx <- seq(-3, 3, by = .01)\n# animation of density\nsaveGIF({\n    for (i in 2:100)  {\n      hist(wiener_ts[i,], xlim = c(-3, 3), ylim = c(0, 10),\n           freq = F,\n           breaks = 40,\n           main = paste0(\"Density of Sample Paths, Time = \", i*dt),\n           xlab = \"x\")\n      lines(x, dnorm(x, sd = sqrt(dt * i)), col = \"red\")\n      legend(\"topright\", legend = c(\"theoretical density\"), col = 2, lty = 1, bty = \"n\")\n    }\n}, movie.name = \"test.gif\", loop = T, interval = .01)\n\n# along the sample paths\nsaveGIF({\n    for (i in 2:100)  {\n      ts.plot(wiener_ts, col = rgb(0,0,0,alpha = .03), ylim = c(-4, 4), main = \"Wiener Sample Paths\")\n      abline(v = i * dt, col = \"red\")\n    }\n}, movie.name = \"sample.gif\", loop = T, interval = .01)\n\n\n\n\n\n\n\n\n\n\n\n\nThe associated Fokker Plank equation associated with this stochastic differential equation, is simply the heat equation.\n\n\\frac{\\partial}{\\partial t} P(x, t) =\\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}P(x,t)\n\n\nInitial condition: P(x, 0) = \\delta(x)\n\\delta(x) is the delta-dirac function, which has infinite mass at 0.\nBoundary Condition: P(a, t) = P(b, t) = 0\n\nsince our simulation has to occur on some bounded region [a,b], we just set the simulation to have absorbing boundaries.\n\n\nSolving the PDE with initial value conditions analytically, we find that the solution to this eigenvalue problem is\n\n\\begin{aligned}\nP(x, t) = \\frac{1}{\\sqrt{2\\pi t}}\\exp \\left(\\frac{x^2}{2t}\\right)\n\\end{aligned}\n\nWe can also calculate the solution by finite differencing. We can solve the PDE and show that our solutions match the rate given by the SDE formulation of the PDE. We use code from the vignette of the ReacTran R package. ReacTran package uses the method of lines for solving PDE’s, in which we set up a discretized grid, and solve the ODE as a vector\n\n\nCode\n# ReacTran uses the\nN <- 601\nxgrid <- setup.grid.1D(x.up = -3, x.down = 3, N = N) # grid of values\nx <- xgrid$x.mid\nD.coeff <- .5 # diffusion coefficient from solving FP\n\n# defines the diffusion (the derivative with respect to time)\n# Since our function has no time dependence, we only need to calculate the derivatives for the next step\nDiffusion <- function (t, Y, parms){\n  tran <- tran.1D(C = Y, C.up = 0, C.down = 0, # dirchlet boundary conditions, set to 0\n                D = D.coeff, dx = xgrid)\n  list(dY = tran$dC)\n}\n\n# Set initial condition of the differential equation, we approximate the \nYini <- c(rep(0, 300), 100, rep(0, 300)) # very tall initial mass\ntt <- seq(t0, dt * nt, dt) #times to simulate\n\n# solve heat equation\nout <- ode.1D(y = Yini, times = tt, func = Diffusion, parms = NULL, dimens = N)\n\n# library(tidyverse)\ncolorBreaks = 10^(seq(-2, 2, length.out = 255)) # different\nplot(raster(t(out[,-1]), xmn =0, xmx = 1, ymn = -3, ymx = 3),\n     asp = NA,\n     breaks = colorBreaks,\n     col = rev(hcl.colors(length(colorBreaks) - 1L, palette = \"Rocket\")), legend = FALSE,\n     xlab = \"time\", ylab = \"x\", interpolate = TRUE)\n\n\n\n\n\n\n\n7.2.2 Ornstein-Uhlenbeck Process\nAdding a drift term to the stochastic equation, gives the stochastic equation a mean. The negative in front of the drift implies that it will always regress to 0, because it’s a function of x (kind of like a spring constant). The \\theta parameter controls how strong the restoring force is.\n\ndx = -\\theta x\\, dt + \\sigma \\, dW\n\n\nx is position, which is a function of time.\ndW is the wiener process.\n\\theta is the rate of return to the mean (0)\nVariance of\n\nThe following shows the effect of the parameters \\theta = 3 and \\sigma = .5 with initial state x_0 = 5. We also show the process with a different initial state x_0 = -3 to show the restoring effect to the mean.\n\n\nCode\n# OU function\nou_paths <- function(npaths = 1, theta = 3, sigma = .5, x0 = 5, nt = 100, t0 = 0, dt = .01) {\n  sde_path_ou <- function() {\n    x <- vector(mode = \"numeric\", length = nt + 1)\n    x[1] <- x0\n    wiener_noise <- rnorm(nt, sd = sqrt(dt))\n    for (i in 1:nt) {\n      dx <- -theta * x[i] * dt + sigma * wiener_noise[i]\n      x[i+1] <- x[i] + dx\n    }\n    return(x)\n  }\n  ts(replicate(npaths, sde_path_ou()), start = t0, deltat = dt)\n}\n\ntheta <- 3\nsig <- .5\ny0 <- 5\nnt <- 100\ndt <- .01\nt0 <- 0\n\nset.seed(1)\nou_ts <- ou_paths(npaths = 500, theta = theta, sigma = sig, x0 = y0, nt = nt, t0 = t0, dt = dt)\n\nts.plot(ou_ts, col = rgb(0,0,0,alpha = .05)) # plot\n\ntt <- seq(0, 1, .01)\ntheory_mean <- y0*exp(-theta*tt)\ntheory_var <- sig^2 / 2 / theta * (1 - exp(-2 * theta * tt))\n\nlines(tt, theory_mean, col = \"red\")\nlines(tt, theory_mean + 2 * sqrt(theory_var), lty = 2, col = \"red\")\nlines(tt, theory_mean - 2 * sqrt(theory_var), lty = 2, col = \"red\")\nlegend(\"topright\", legend = c(\"mean\", \"\\u00B1 2sd\"), col = 2, lty = 1:2)\n\n\n\n\n\nBecause of this restoring property to the mean, a closely related stochastic process is known as the Vasicek model (commonly used to model interest rates), which adds another parameter to control what the mean is.\n\ndx = \\theta(\\mu - x)\\, dt + \\sigma \\, dW\n\n\nreduces to the 0 mean OU process when \\mu = 0.\nThe long term variance of this model is also\n\nThis process is important because it’s the continuous time analogue of a discrete time AR1 process.\nSimilarly, the associated Fokker-Plank Equation for this SDE is:\n\n\\begin{aligned}\n\\frac{\\partial P(x, t)}{\\partial t} = -\\mu \\frac{\\partial P(x, t)}{\\partial x} + \\frac{\\sigma^2}{2}\\frac{\\partial^2P(x, t)}{\\partial x^2}\n\\end{aligned}\n\n\n\nCode\n# simulation parameters\nt0 <- 0                       # time start\ndt <- .01                     # time step \ntn <- 1                       # time end\nnt <- tn/dt                   # number of time steps\ntgrid <- seq(t0, nt * dt, dt) # time grid\n\nx0 <- 6                       # space start\nxn <- -2                    # space end\nnx <- 800                     # number of grid points\nxgrid <- setup.grid.1D(x.up = x0, x.down = xn, N = nx) # space grid\nx <- xgrid$x.int\n\nsig <- .5            # Parameters from OU simulation\nmu <- -3             # Parameters from OU simulation  \nD_coef <- sig^2 / 2  # Diffusion function\n\ny0 <- c(rep(0, 100), 100, rep(0, 699))                 # initial condition\n\n# advection-diffusion (method of lines)\nadvec <- function(t, Y, parms) {\n  trans <- tran.1D(C = Y, D = D_coef, v = mu*x, C.up = 0, C.down = 0, dx = xgrid)\n  return(list(dY = trans$dC))\n}\n\n# solve advec/diffusion equation\nout <- ode.1D(y = y0, times = tgrid, func = advec, parms = NULL, dimens = nx)\n\n# plot solution\ncolorBreaks = 10^(seq(-3, 3, length.out = 255)) # different to capture more drift in lower parameters\nplot(raster(t(out[,-1]), xmn = 0, xmx = 1, ymn = xn, ymx = x0),\n     asp = NA,\n     breaks = colorBreaks,\n     col = rev(hcl.colors(length(colorBreaks) - 1L, palette = \"Rocket\")), legend = FALSE,\n     xlab = \"time\", ylab = \"x\", interpolate = TRUE)\n\n\n\n\n\nCode\n# rcOU(n=1, Dt=0.1, x0=1, theta=c(0,2,1))\n\n\n\n\n\n\n\n\nODE for Mean/Variance of OU process\n\n\n\nWe can get a First Order ODE characterization of the mean and variance for the Ornstein-Uhlenbeck process.\n\\begin{aligned}\n\\frac{d\\langle x \\rangle}{dt} = -\\theta \\langle x \\rangle \\\\\n\\langle x\\rangle = \\langle x_0 \\rangle e^{-\\theta t}\n\\end{aligned}\n\n\\frac{dV}{dt} = -2 \\theta V + \\sigma^2\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\ndx = -\\theta x\\, dt + \\sigma\\, dW\n First we note the expression for the derivative of variance, and calculate the necessary components.\n\n\\begin{align*}\n\\frac{dV}{dt} &= \\frac{d\\langle x^2\\rangle}{dt} - \\frac{d \\langle x\\rangle^2}{dt} \\\\\n&= \\frac{d\\langle x^2\\rangle}{dt} - 2 \\langle x\\rangle \\frac{d \\langle x\\rangle}{dt}\n\\end{align*}\n\nHence, we need to evaluate the mean differentials to the second order, from\n\n\\begin{align}\nd\\langle x\\rangle &= -\\theta \\langle x\\rangle \\, dt\\\\\ndx^2 &= (x + dx)^2 - x^2 \\\\\n&=2x\\,dx + (dx)^2 \\\\\n&= (-2\\theta x^2 + \\sigma^2)\\,dt + \\sigma \\, dW \\\\\nd\\langle x^2\\rangle &= (-2\\theta \\langle x^2\\rangle + \\sigma^2)\\, dt\n\\end{align}\n\nWe’ve used that the rules of Ito’s calculus, that dt\\,dW = 0, (dW)^2 = dt, (dt)^2 =0. Plugging in the values and simplifying gives us the desired result.\n\n\n\n\n\n\n\nCode\ntheta <- 3\nsig <- .5\n\n# simplest first order ode\nou_mean <- function(t, y, parms) {\n  dy <- -theta * y\n  return(list(dy))\n}\n\n\nx0 <- 5 # initial mean\ntt <- seq(0, 1, by = .01)\nout_mean <- ode(x0, tt, ou_mean, parms = NULL)\n\ncbind(out_mean[,2],\n      5 * exp(-3* tt)) |> head() # matches\n\n\n         [,1]     [,2]\n[1,] 5.000000 5.000000\n[2,] 4.852224 4.852228\n[3,] 4.708817 4.708823\n[4,] 4.569653 4.569656\n[5,] 4.434600 4.434602\n[6,] 4.303538 4.303540\n\n\nCode\n# function coding differential equation \nou_var <- function(t, v, parms) {\n  dv <- -2 * theta * v + sig^2\n  return(list(dv))\n}\n\ny0 <- 0 # initial variance\ntt <- seq(0, 1, by = .01)\n\nout <- ode(y0, tt, ou_var, parms = NULL)\n\ncbind(out[,2],\n      sig^2 / 2 / theta * (1 - exp(-2 * theta * tt))) |> head() # matches!\n\n\n            [,1]        [,2]\n[1,] 0.000000000 0.000000000\n[2,] 0.002426880 0.002426478\n[3,] 0.004712408 0.004711648\n[4,] 0.006864444 0.006863741\n[5,] 0.008891073 0.008890506\n[6,] 0.010799760 0.010799241\n\n\n\n\n7.2.3 General Linear SDE\n\ndx = -\\gamma x \\, dt + g x\\, dW"
  },
  {
    "objectID": "stochastic/stochastic.html#reaction-diffusion-equations",
    "href": "stochastic/stochastic.html#reaction-diffusion-equations",
    "title": "7  Stochastic",
    "section": "7.3 Reaction Diffusion Equations",
    "text": "7.3 Reaction Diffusion Equations\nThese are non-linear differential equations, and a system of them. We can start describing the reactions of SIR model as sets of nonlinear differential equations. There are a number of famous examples of these, we’ll study the Grey-Scott system, then the SIR system, and hopefully we’ll see the reaction diffusion nonlinearity around the boundaries of the different stable sets.\n\n7.3.1 Grey-Scott Model\nA really cool web simulation of the phenomena I want to recreate can be found by Karl Sims, Reaction Diffusion Tutorial. Luckily, someone else has already implemented a version of this, and we’ll just borrow their code (Fronkonstin).\n\nThe Grey-Scott Model describes the following irreversible, reactions of three compound U, V, P and P is an inert product.\n\n\\begin{align*}\nU + 2V &\\rightarrow 3V \\\\\nV &\\rightarrow P\n\\end{align*}\n\nWe will use the simulation parameters from Pearson (1993), in particular, the equations that result from this reaction diffusion is\n\n\\frac{\\partial U}{\\partial t} = D_u \\nabla^2U - UV^2 + F(1 - U) \\\\\n\\frac{\\partial V}{\\partial t} = D_v \\nabla^2V + UV^2 - (F + k)V\n\n\nD_u = 2 \\times 10^{-5}\nD_v = 10^{-5}\nperiodic boundary condition\nF and k are known as the feed and kill rates of the reactants. since concentration ranges between 0 and 1, the reaction term in the first equation is positive, and then F controls how much of reactant U is being introduced.\n\n\n\nCode\ngray_scott <- function(U0 = NULL, V0 = NULL,\n                       feed_rate = 0.0545,\n                       kill_rate = 0.062,\n                       N = 256,\n                       tN = 2000,\n                       D.u = 1,\n                       D.v = .5,\n                       save_frame_freq = 20,\n                       video_file = \"gray_scott.mp4\",\n                       pic_dir = NULL,\n                       init_strategy = c(\"random\"),\n                       seed = 1,\n                       ...) {\n  set.seed(seed)\n  pct <- proc.time()\n  init_strategy <- match.arg(init_strategy)\n  if (missing(U0) | missing(V0)) {\n    #TODO: implement different initialization strategies for different patterns\n    U0 <- matrix(1, nrow = N, ncol = N)\n    V0 <- matrix(0, nrow = N, ncol = N)\n    V0[sample(N^2, ceiling(N^2 / 20))] <- 1 # 10% of cells\n    } else if (!all(c(dim(U0), dim(V0)) == N)) {\n      rlang::abort(\"Initial Matrix must be a grid with dimension N\")\n    }\n  \n  U <- U0\n  V <- V0\n  # 9 point stencil for Laplacian\n  # yuvj420p pix format used: https://superuser.com/questions/1273920/deprecated-pixel-format-used-make-sure-you-did-set-range-correctly\n  L <- matrix(c(0.05, 0.2, 0.05, \n                0.2,  -1, 0.2,\n                0.05, 0.2, 0.05), nrow = 3)\n  \n  \n  if (missing(pic_dir))\n    pic_dir <- tempdir() \n  else \n    pic_dir <- fs::dir_create(fs::path_wd(pic_dir))\n\n  # clean directory  \n  if (length(Sys.glob(fs::path(pic_dir, \"*.jpg\"))) > 0 ) {\n    rlang::abort(sprintf(\"%s not empty, please clean out *.jpg files to prevent overwriting!\", pic_dir))\n  }\n\n  jpeg_file <- fs::path(pic_dir, sprintf(\"plot%06d.jpg\", 0))\n  jpeg(jpeg_file)\n      # setup base plot graphical parameters\n      par(bty = \"n\", oma = rep(0, 4), mar = rep(0, 4))\n      plot(raster(V0),\n           axes = F, bty = \"n\", frame.plot = F,\n           interpolate = F,\n           breaks = hist(V0, breaks = 256, plot = F)$breaks,\n           col = hcl.colors(256,  palette = \"ag_sunset\"),\n           legend = FALSE)\n      \n      dev.off()\n  \n  p <- progressr::progressor(tN / save_frame_freq)\n  for (i in 1:tN) {\n    dU <- D.u * filter2(U, L) - U * V^2 + feed_rate * (1 - U)\n    dV <- D.v * filter2(V, L) + U * V^2 - (feed_rate + kill_rate) * V\n    U <- U + dU\n    V <- V + dV\n    # save frame in temp folder\n    if (i %% save_frame_freq == 0) {\n      p(message = sprintf(\"Timestep: %g\", i))\n      jpeg_file <- fs::path(pic_dir,\n                            sprintf(\"plot%06d.jpg\", i))\n      jpeg(jpeg_file)\n      # setup base plot graphical parameters\n      par(bty = \"n\", oma = rep(0, 4), mar = rep(0, 4))\n      plot(raster(V),\n           axes = F, bty = \"n\", frame.plot = F,\n           interpolate = T,\n           breaks = hist(V, breaks = 256, plot = F)$breaks,\n           col = hcl.colors(256,  palette = \"ag_sunset\"),\n           legend = FALSE)\n      dev.off()\n    }\n  }\n  \n  # create video directory\n  fs::dir_create(fs::path_wd(fs::path_dir(video_file)))\n  \n  #TODO: check if ffmpeg available on system?\n  ffmpeg_cmd <- sprintf('ffmpeg -y -f image2 -pattern_type glob -i \"%s/*.jpg\" -framerate 60 -c:v libx264 -crf 20 -filter:v \"format=yuvj420p\" %s', pic_dir, fs::path_wd(video_file))\n  # run ffmpeg command\n  system(ffmpeg_cmd)\n  cat(paste0(\"Running command: \", ffmpeg_cmd, \"\\n\"))\n  proc.time() - pct # elapsed wall time\n}\n# relative to this script (when running commands in notebook)\nwith_progress(\n  gray_scott(video_file = \"vid/gray_scott.mp4\", pic_dir = \"gray_pic\"), handlers = handlers(\"progress\")) # for text updates\n\n\n\n\n\n\n\nGray Scott Reaction Diffusion, f=.0545, k=.062\n\n\n\n\n7.3.2 SIR Diffusion\n\n\n\nhere’s we’re trying to upsample the values that come out of the\nThe SIR image has a slightly different quality to it in that the lines around the image are thinner\n\n\n\n\n\nSIR Reaction Diffusion, f=.0545, k=.062\n\n\n\n\n7.3.3 SIS system\nI’m curious if the SIS system without vital dynamics will show any interesting patterns. We know there must be an absorbing state, but this system can also reach an endemic state based on the reproduction number. If we add diffusion into the system we should get interesting results, at the very least some cool videos.\n\n\n7.3.4 Brusselator\n\n\n7.3.5 Lorentz Attractor\n\n\n\n\nPearson, John E. 1993. “Complex Patterns in a Simple System.” Science 261 (5118): 189–92. https://doi.org/10.1126/science.261.5118.189."
  },
  {
    "objectID": "meta/meta.html#effect-size",
    "href": "meta/meta.html#effect-size",
    "title": "8  Meta Analysis",
    "section": "8.1 Effect Size",
    "text": "8.1 Effect Size\nIn order to pool studies, the most important item is the effect size. Largely effect size can be categorized into relative or absolute. Another breakdown is whether the effect size is standardized or not.\n\n8.1.1 Mean\n\n\n8.1.2 Median\n\n\n8.1.3 Proportion\n\n\n8.1.4 Logit Proportion\n\n\n8.1.5 Correlation\nStandard error formulas are not trivial. Even something like the correlation coefficients have been glossed over and never revisited. We’ve said “good enough”! The commonly reported values are given here:\n\n(1 - \\rho^2)^2 / n is reported by Hald (2008) pp. 126\n\\frac{(1-\\rho^2)^2}{n-1} is reported by Dingman and Perry (1956)\n\\frac{(1-\\rho^2)^2}{n-2} is reported by\n\\frac{\\rho^2}{n}\\left(\\frac{M_{22}}{\\rho^2\\sigma_x^2 \\sigma_y^2} + \\frac{M_{40}}{4\\sigma_x^4} + \\frac{M_{04}}{4\\sigma_y^4} - \\frac{M_{13}}{\\rho \\sigma_x\\sigma_y^3} - \\frac{M_{31}}{\\rho \\sigma_x^3\\sigma_y1} + \\frac{M_{22}}{\\rho \\sigma_x^2\\sigma_y^2}\\right) is reported by Sheppard and Forsyth (1899) (in the bivariate normal case, this reduces to the first case)\n\n\n\nCode\nsample_cor_norm <- function(mean1, mean2, sd1, sd2, n) {\n  x <- rnorm(n, mean1, sd2)\n  y <- rnorm(n, mean2, sd2)\n  cor(x, y)\n}\nsample_cor_norm_std <- partial(sample_cor_norm, mean1 = 0, mean2=0, sd1=1, sd2=1)\n\n# resampling simulation, (for n = 10, 20, 30, repeat the sampling 10 times.)\ndat_resample_cor_norm_std <- seq(10, 410, 20) |> \n  enframe(name = NULL, value = \"n\") |> \n  rowwise() |> \n  mutate(sample_cor = list(replicate(10, sample_cor_norm_std(n))), # increase replicates to make it closer to truth\n         sample_cor_se = sd(sample_cor))\n\ntheoretical_resample_cor_se <- tibble(x = seq(3, 410),\n                             y = 1/sqrt(x),\n                             y1 = 1/sqrt(x -1),\n                             y2 = 1/sqrt(x-2)) |> \n  pivot_longer(cols = y:y2)\n\ndat_resample_cor_norm_std |> ggplot() +\n  geom_point(aes(n, sample_cor_se)) +\n  geom_line(data = theoretical_resample_cor_se,\n            aes(x, value, linetype = name), color = \"red\")\n\n\n\n\n\nIt doesn’t really matter the denominator, it’s not a big difference. This looks at the actual values of sampling correlation coefficient as n increases.\n\n\nCode\n# Alternative way of looking at the drop off...\ndat_cor_norm_std <- 5:200 |> \n  enframe(name = NULL, value = \"n\") |> \n  rowwise() |> \n  mutate(sample_cor = sample_cor_norm_std(n))\n\ntheoretical_cor_se <- tibble(x = seq(5,200, .5),\n                             y2 = 2/sqrt(x),\n                             y_2 = -2/sqrt(x)) |> \n  pivot_longer(y2:y_2)\ndat_cor_norm_std |> ggplot(aes(n, sample_cor)) +\n    geom_line(data = theoretical_cor_se,\n            mapping = aes(x, value, group = name), linetype = 2, color = \"red\") +\n  geom_line(color = \"grey50\") +\n  theme_minimal() +\n  lims(y = c(-.8, .8)) +\n  labs(y = \"Sample Pearson Correlation\",\n       title = \"2-SE Envelope of Sample Correlation as function of n\")\n\n\nWarning: Removed 6 row(s) containing missing values (geom_path).\n\n\n\n\n\nWe have the luxury of just resampling from our target population, so we don’t really have to bootstrap, but evaluating how good the bootstrap is in various situations is probably worth looking into as well.\nNow we’re curious how bad these distributions can get if the distributions are skewed. Let’s do a correlated example with exponential distributions.\n\n\nCode\n# simulating function for correlated variables\nsample_cor_exp <- function(lambda1, lambda2, n) {\n  x <- rexp(n, lambda1)\n  y <- rexp(n, lambda2)\n  cor(x, x + y) # expected correlation is 1/lambda1 + 1/lambda2\n}\n\ndat_resample_cor_exp <- seq(10, 410, 20) |> \n  enframe(name = NULL, value = \"n\") |> \n  rowwise() |> \n  mutate(sample_cor = list(replicate(50, sample_cor_exp(2, 3, n))), # increase replicates to make it closer to truth\n         sample_cor_se = sd(sample_cor))\n\nrho <- 5/6\ntheoretical_resample_cor_se <- tibble(x = seq(3, 410),\n                             y = (1 - rho^2)/sqrt(x))\n\ntheoretical_resample_cor_se |> ggplot() +\n  geom_line(mapping = aes(x = x, y = y), \n            color = \"red\",\n            linetype = 2) +\n  geom_point(data = dat_resample_cor_exp,\n             mapping = aes(n, sample_cor_se)) +\n  labs(y=\"standard error\",\n       title = \"Standard Errors vs \\u03C1 = 5/6\")\n\n\n\n\n\nOverall, when the distribution is skewed, we’re notably underestimating the standard error almost across the board.\n\n\nCode\n# envelope graph\ndat_sample_cor_exp <- 5:200 |> \n  enframe(name = NULL, value = \"n\") |> \n  rowwise() |> \n  mutate(sample_cor = sample_cor_exp(2, 3, n)) # 5/6 cor theoretical\n\n# theoretical envelop, upper lower 2nd std dev, around (d)\nrho <- 5/6\ntheoretical_cor_se <- tibble(x = seq(5,200, .5),\n                             y_upper = 2 * (1 - rho^2)/sqrt(x) + rho,\n                             y_lower = -2 * (1 - rho^2)/sqrt(x) + rho) |> \n  pivot_longer(starts_with(\"y\"))\n\ndat_sample_cor_exp |> ggplot() +\n  geom_line(aes(x = n, y= sample_cor), color = \"grey50\") +\n  geom_line(data = theoretical_cor_se, \n            mapping = aes(x = x, value, group = name),\n            color = \"red\") +\n  theme_minimal() +\n  labs(title = \"2-SE Envelope of Sample Correlation of Exponential RV when \\u03C1 = 5/6\")\n\n\n\n\n\nWhen the distributions are skewed, it seems they peak the bottom a little more, but this matches the story above in which we are underestimating the standard error a little, because it’s poking out a little more all over.\n\n\nZ transformed Pearson"
  },
  {
    "objectID": "meta/meta.html#diversity-indices",
    "href": "meta/meta.html#diversity-indices",
    "title": "8  Meta Analysis",
    "section": "8.2 Diversity Indices",
    "text": "8.2 Diversity Indices\n\nShannon diversity\nHill Diversity Indices\n\nIt has been noted that there are biases when bootstrapping these diversity indices.1 Furthermore, there have been attempts to quantify the standard error as well.2\n\n8.2.1 Hill/Renyi Diversity\n\n\\begin{align*}\nD_a &= \\left(\\sum_i^S p_i^a\\right)^{\\frac{1}{1-a}} \\\\\nD_0 &= \\sum_i^S p_i^0 = S & \\text{Species Richness}\\\\\nD_1 &= \\exp\\left(-\\sum_i p_i \\log p_i\\right) & \\text{Exp(Shannon-Wiener Entropy Index)} \\\\\nD_2 &= \\frac{1}{\\sum_i^S p_i^2} & \\text{Simpson's Reciprocol Index}\n\\end{align*}\n\n\n\n\nLimit details for D_1\n\nWhen a = 1, the exponent becomes 1/0, so we must use the limiting definition as a\\rightarrow 1 to see how the diversity metric reduces to entropy.\n\n\\begin{align*}\n\\lim_{a\\rightarrow 1} \\left(\\sum_{i}^S p_i^a\\right)^{\\frac{1}{1-a}} &= \\lim_{a\\rightarrow 1} \\exp\\left(\\frac{1}{1-a}\\log\\left(\\sum_{i}^S p_i^a\\right)\\right) \\\\\n&= \\exp\\left(\\lim_{a\\rightarrow 1}\\frac{\\log\\left(\\sum_{i}^S p_i^a\\right)}{1-a}\\right) \\\\\n&= \\exp\\left(\\lim_{a\\rightarrow 1}\\frac{\\sum_i^S p^a_i \\log_i p_i}{-\\sum_i^S p_i^a}\\right) \\\\\n&= \\exp\\left(-\\sum_i^S p \\log_i p_i\\right)\n\\end{align*}\n\nwhere we’ve used L’hopitals rule in step 2, and \\sum_i^S p_i = 1 in step 3.\n\n\n\n\n8.2.2 Species Diversity\nThe most basic metric, basically binarizing the counts, and saying these are all the species that we see.\n\n\n8.2.3 Shannon diversity\nShannon diversity is defined as:\n\n\\begin{align*}\nH = -\\sum p_i \\log p_i\n\\end{align*}\n\n\np_i is the proportion among all the species, so \\sum p_i = 1.\nH is maximized when proportions are the same among proportions\nSpecies with count 0 do not contribute to entropy (they also don’t change the proportion of existing species).\n\nHowever if using Shannon Equitability Index, the total species count does matter.\n\n\nIs Shannon diversity dependent on number of species? Assume uniform distribution.\n\n\nCode\nS <- 2:200\nH <- S |> map_dbl(~entropy(rep(20, .x)))\nH_equitable <- H / log(S)\ntibble(S, H, H_equitable) |> ggplot() +\n  geom_line(aes(S, H, color = \"Shannon Index (H)\")) +\n  geom_line(aes(S, H_equitable, color = \"Equitable Shannon Index\")) +\n  theme_minimal() +\n  labs(y = \"Entropy\",\n       title = \"Uniform distribution of species\")\n\n\n\n\n\nWhat’s the sampling distribution of Shannon’s diversity index?\n\n\nCode\n#' simulate uniform counts\n#'\n#' @param S number of species\n#' @param n sequencing depth\n#'\n#' @return\n#' @export\n#'\n#' @examples\nsim_uniform_species <- function(S, n = S*5){\n  y <- rdunif(n, S)\n  table(y)[as.character(1:S)] %|% 0L |> as.numeric()\n}\n\n# gives sample community matrix, with uniform distribution across both margins.\nsim_uniform_community_matrix <- function(S, sample_n = 10, sample_min = 100, sample_max = 400) {\n  # sequencing effort will vary between samples\n  n <- runif(sample_n, sample_min, sample_max)\n\n  # each line is a count from samples\n  mapply(n, FUN = sim_uniform_species, S = S) |> \n    t()\n}\n\n\n# Bias of entropy estimation as a function of sequencing depth.\nentropy_uniform_se <- expand.grid(n = c(2, 5, 10, 50, 200, 500, 1000),\n                                  S = 2:200) |> rowwise() |> \n  mutate(H = entropy(sim_uniform_species(S, n)))\n\nS <- seq(2:200)\nH_theoretical <- S |> map_dbl(~entropy(rep(10, .x)))\ntheoretical_entropy_uniform <- tibble(S, H_theoretical)\n\nentropy_uniform_se |> ggplot() +\n  geom_line(aes(S, H, color = n, group = n)) +\n  geom_line(data = theoretical_entropy_uniform,\n            mapping = aes(S, H_theoretical)) +\n  scale_color_gradient(trans = \"log\", breaks = c(2, 10, 100, 1000))\n\n\n\n\n\nThe bias it seems can be quite massive if you choose different sequencing efforts. Thus, we must standardize our metric before comparing alpha diversity.\nNormalization Techniques (Slides with more3)\n\nSubsampling to a common depth (rarefaction)\nEqualize depths by scaling OTU counts to common depth (different from rarefaction, scaling vs subsampling)\nTransform counts into relative abundance (denominator is each sample)\nNormalize data based on 16S\n\nNow we’ll try to quantify this bias in the Shannon Diversity Metric, using different estimators of the uniform.\n\n\nCode\n# Chao Shen entropy estimator\nS <- 100\nn <- 50:1000\n\nset.seed(1)\ndat_entropy_bias <- expand_grid(n, S) |> \n  rowwise() |> \n  mutate(y = list(sim_uniform_species(S, n)),\n         entropy_est = list(list(H_ML = entropy(y),\n                            H_MM = entropy(y, method = \"MM\"),\n                            H_Jeffreys = entropy(y, method = \"Jeffreys\"),\n                            H_Laplace = entropy(y, method = \"Laplace\"),\n                            H_minimax = entropy(y, method = \"minimax\"),\n                            H_CS = entropy(y, method = \"CS\"),\n                            H_shrink = entropy(y, method = \"shrink\", verbose = FALSE))),\n         H_Z = Entropy.z(y)) |> \n  unnest_wider(entropy_est) |> \n  pivot_longer(cols = starts_with(\"H\"), names_to = \"H_type\" , values_to = \"H\")\n\ndat_entropy_true <- tibble(n, H_theory = entropy(rep(1, 100)))\n\ng <- dat_entropy_bias |> ggplot() +\n  geom_line(data = dat_entropy_true,\n            mapping = aes(n, H_theory), color = \"red\") + \n  geom_line(aes(n, H, color = H_type), alpha = .5)\n\nconfig(ggplotly(g), modeBarButtonsToRemove = c(\"zoom\", \"pan\", \"select\", \"zoomIn\", \"zoomOut\",\n                                               \"autoScale\", \"select2d\", \"hoverClosestCartesian\", \"hoverCompareCartesian\"), displaylogo = FALSE)\n\n\n\n\n\n\nThe H_ML is the standard plug in estimator of the entropy. We can see that most of them underestimate the true entropy, but the shrinkage estimator does surprisingly well. This is something to look into, I think it shrinks toward the uniform distribution anyway, so it’s not really a fair comparison. overall we note that:\n\nH_CS probably has the highest standard error\n\n\n\n8.2.4 Rarefaction (aside)\nThere is a difference in terminology rarefaction and rarifying. Rarefaction curves are the curves, plotting affect of the alpha diversity metric against the sampling effort. rarifying is the practice of subsampling to a similar depth.\nThe main idea, is that having more sample counts means that more species are likely to be represented. Thus, in order to make samples comparable, we should calculate an “expected species” count with some sort of common effort. Rarefied estimators for species counts are statistically inadmissable (McMurdie and Holmes 2014), due to the simple fact that we’re throwing away data, and thus artificially increasing standard error when we don’t need to. However, it still remains a common technique so understanding why it’s not optimal is still relevant. Since rarefying is basically based on hypergeometric subsampling, exact formulas for standard error and expected value exist.\nIn general, this is what rarefied curves look like:\n\n\nCode\ndata(BCI)\nrarecurve(BCI)\nabline(v=300, col = 2)\n\n\n\n\n\nA single curve is one sample (row), and plots the expected species count as a function of sub-sampling depth.\n\n\nCode\nset.seed(1)\nuniform_cm <- sim_uniform_community_matrix(100)\n# rarefying gives expected species diversity.\nrarefy(uniform_cm, 118, se = T)\n\n\n        [,1]     [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\nS  67.860019 68.13418 68.816150 69.537161 70.031099 69.334350 68.897680\nse  2.417449  2.72915  2.953008  3.075216  2.255168  3.063407  3.047504\n        [,8]      [,9] [,10]\nS  69.540933 69.126184    69\nse  3.020944  2.948892     0\nattr(,\"Subsample\")\n[1] 118\n\n\nCode\n# sum(uniform_cm[10,] != 0) # 69, since this is actual data for 10, the se is zero.\nuniform_drare <- drarefy(uniform_cm, 118) # margin sums are the expected species count, so each cell is the probability of seeing that species \n\n# subsample first one 179 down to 118.\ncbind(uniform_drare[1, ],\n      1 - exp(lchoose(179 - uniform_cm[1,], 118) - lchoose(179, 118))) |> head()\n\n\n      [,1]      [,2]\n 0.8851296 0.8851296\n 0.9617099 0.9617099\n 0.0000000 0.0000000\n 0.0000000 0.0000000\n 0.6592179 0.6592179\n 0.6592179 0.6592179\n\n\ndrarify formula counts subsampling arrangements without replacement. Let’s say sample 1 had 200 counts total, and we wanted to subsample down to 120. Then, we the total number of subsamples is taking those 120 of those 200 counts. We can imagine that we’re pulling from a pool of S_1, S_2, S_2, S_3, for which the sample counts would be c(1, 2, 1).\n\n\\begin{align*}\nP(\\text{at least one }S_1 \\text{ sampled}) =1 - \\frac{\\binom{200 - S_1}{120}}{\\binom{200}{120}}\n\\end{align*}\n\n\n\nCode\nrarecurve(uniform_cm, sample = 118)\n\n\n\n\n\nWhen all the curves are coming from the same distribution, it’s reasonable that the rarefied curves will all look the same."
  },
  {
    "objectID": "meta/meta.html#estimators-for-species-diversity",
    "href": "meta/meta.html#estimators-for-species-diversity",
    "title": "8  Meta Analysis",
    "section": "8.3 Estimators for Species Diversity",
    "text": "8.3 Estimators for Species Diversity\n\nbreakaway - estimates through nonlinear regression of probability ratios, fraction of singleton’s to empty, doubles to singles, etc. Kemp has characterized the distribution of these ratios.\nDivNet Paper\nEntropy Estimators\n\nAsymptotic Normality of Entropy Estimator\nEntropic Representation and Estimation of Diversity Indices"
  },
  {
    "objectID": "meta/meta.html#entropy",
    "href": "meta/meta.html#entropy",
    "title": "8  Meta Analysis",
    "section": "8.4 Entropy",
    "text": "8.4 Entropy\n\n\nCode\n# se calculated by delta method (for binomial at least)\nentropy_delta_se <- function(n, p) {\n  sqrt(p * (1 - p) / n * (1 + log(p))^2)\n}\n\np <- c(.01, .1, .3, .5)\nn <- seq(5, 200, 15)\nsim_df <- expand_grid(n, p)\nentropy_se_df <- sim_df |> rowwise() |> \n  mutate(y = list(rbinom(200, n, p)),\n         p_hat = list(y / n),\n         sample_entropy = list(p_hat * log(p_hat)),\n         se_sample_entropy = sd(sample_entropy, na.rm = TRUE),\n         delta_se = entropy_delta_se(n, p))\n\n# deltamethod(~ 1 / (x1 + x2))\n# deriv(expression(x * log(x)), \"x\", function.arg = TRUE)\n# deriv(~x * log(x), \"x\")\n\n\nentropy_se_df |> ggplot(aes(n, se_sample_entropy, color = factor(p))) +\n  geom_point() +\n  geom_line(mapping = aes(n, delta_se),\n            linetype = 2) +\n  facet_wrap(~p, nrow = 2)"
  },
  {
    "objectID": "meta/meta.html#group-differences",
    "href": "meta/meta.html#group-differences",
    "title": "8  Meta Analysis",
    "section": "8.5 Group Differences",
    "text": "8.5 Group Differences\nThere’s a lot of these, so here’s a quick list\n\n8.5.1 Mean Difference\n\n\n8.5.2 Standardized Mean Difference (Cohen’s d)\nNote that the standard t statistic is defined as \\frac{\\sqrt{n}(\\bar x_2 - \\bar x_1)}{s_p} which differs from d by a factor of \\sqrt{n}.\n\nAdjustments\n\n\nName\n\nEstimator\nSE\n\n\n\n\nMD\n\n\\bar x_2 - \\bar x_1\n\ns_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\n\n\nSMD\nCohen’s d\n\\frac{\\bar x_2 - \\bar x_1}{s_{p}}\n\n\\sqrt{\\frac{n_1 + n_2}{n_1n_2} + \\frac{d^2}{2(n_1 + n_2)}}\n\nSO Derivation of SE\n\n\n\nHedge’s g\n\n\n\n\n(log) Risk Ratio\n\n\\log\\left(\\frac{a/n_{1}}{b/n_2}\\right)\n\n\n\n(log) Odds Ratio\n\n\\log\\left(\\frac{a/b}{c/d}\\right)\n\n\n\n(log) Incidence Risk Ratio\n\n\n\n\n\nHazard Ratio\n\n\n\n\n\n\n\nreliability / test-retest / attenuated/ error in measurement models\n\nThese types of estimators standardize by \\sqrt{r_{xx}}, when available and many variants thereafter\n\nSmall Sample Adjustments\n\nGenerally small samples have quite a large bias"
  },
  {
    "objectID": "meta/meta.html#pooling-effect-size",
    "href": "meta/meta.html#pooling-effect-size",
    "title": "8  Meta Analysis",
    "section": "8.6 Pooling Effect Size",
    "text": "8.6 Pooling Effect Size\nNow that we have an understanding of how effect sizes matter for different variables, how do we actually pool the studies?\n\n\n\n\n\n\nDon’t use lme or lm\n\n\n\nAlthough lme() and lm() have a weights argument, they are NOT (directly) appropriate for meta analyses. The weights that are provided to those arguments are relative, and thus still estimate an additional error component. The (inverse) weights from the meta analyses are different in that they should fully specify the error variance, and they should be treated as fixed variance. This class of models falls under “small area estimation” or Fay-Harriot models.\nViechtbauer (author of metafor R package) has a great post about this topic.\n\n\nWe’ll follow along with the Chapter 4: Doing Meta Analysis in R.\n\n\nCode\n## Fixed Efffect estimating\ndata(SuicidePrevention)\n# Since all raw data, need to calculate standardized effect size metrics\nSP_calc <- esc_mean_sd(grp1m = SuicidePrevention$mean.e,\n                       grp1sd = SuicidePrevention$sd.e,\n                       grp1n = SuicidePrevention$n.e,\n                       grp2m = SuicidePrevention$mean.c,\n                       grp2sd = SuicidePrevention$sd.c,\n                       grp2n = SuicidePrevention$n.c,\n                       study = SuicidePrevention$author,\n                       es.type = \"g\") %>% \n                     as.data.frame()\n\n\nWarning in missing(grp1sd) || is.null(grp1sd) || is.na(grp1sd): 'length(x) = 9 >\n1' in coercion to 'logical(1)'\n\n\nWarning in missing(grp2sd) || is.null(grp2sd) || is.na(grp2sd): 'length(x) = 9 >\n1' in coercion to 'logical(1)'\n\n\nCode\ncbind(SP_calc$weight, \n1/ SP_calc$se^2) # weights are calculated by inverse se^2\n\n\n          [,1]     [,2]\n [1,] 46.09784 46.09784\n [2,] 34.77314 34.77314\n [3,] 14.97625 14.97625\n [4,] 32.18243 32.18243\n [5,] 24.52054 24.52054\n [6,] 54.50431 54.50431\n [7,] 29.99941 29.99941\n [8,] 19.84837 19.84837\n [9,] 26.63768 26.63768\n\n\nCode\nfe_est <- with(SP_calc, sum(es * weight) / sum(weight)) # pooled estimate \n\n\nRandom effect weighting adds and additional\nConsider a Knapp and Hartung (2003) adjustment, changing tests to use t-distribution for the extra parameter estimated in the random effect model. Sidik and Jonkman (2005) evaluates the estimators and finds Knapp Hartung variance estimates to be the best protection against error. The main problem is estimating the weights used in the meta-analyses.\n\n\nCode\n## Random effect\nlibrary(meta)\ndata(ThirdWave)\nm.gen <- metagen(TE = TE,\n                 seTE = seTE,\n                 studlab = Author,\n                 data = ThirdWave,\n                 sm = \"SMD\",\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"REML\",\n                 hakn = TRUE,\n                 title = \"Third Wave Psychotherapies\")\n\nm.gen <- metagen(TE = TE,\n                 seTE = seTE,\n                 studlab = Author,\n                 data = ThirdWave,\n                 sm = \"SMD\",\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"DL\",\n                 hakn = FALSE,\n                 title = \"Third Wave Psychotherapies\")\n\nsummary(m.gen)\n\n\nReview:     Third Wave Psychotherapies\n\n                          SMD            95%-CI %W(random)\nCall et al.            0.7091 [ 0.1979; 1.2203]        5.0\nCavanagh et al.        0.3549 [-0.0300; 0.7397]        6.3\nDanitzOrsillo          1.7912 [ 1.1139; 2.4685]        3.7\nde Vibe et al.         0.1825 [-0.0484; 0.4133]        8.0\nFrazier et al.         0.4219 [ 0.1380; 0.7057]        7.4\nFrogeli et al.         0.6300 [ 0.2458; 1.0142]        6.3\nGallego et al.         0.7249 [ 0.2846; 1.1652]        5.7\nHazlett-Stevens & Oren 0.5287 [ 0.1162; 0.9412]        6.0\nHintz et al.           0.2840 [-0.0453; 0.6133]        6.9\nKang et al.            1.2751 [ 0.6142; 1.9360]        3.8\nKuhlmann et al.        0.1036 [-0.2781; 0.4853]        6.3\nLever Taylor et al.    0.3884 [-0.0639; 0.8407]        5.6\nPhang et al.           0.5407 [ 0.0619; 1.0196]        5.3\nRasanen et al.         0.4262 [-0.0794; 0.9317]        5.1\nRatanasiripong         0.5154 [-0.1731; 1.2039]        3.6\nShapiro et al.         1.4797 [ 0.8618; 2.0977]        4.1\nSong & Lindquist       0.6126 [ 0.1683; 1.0569]        5.7\nWarnecke et al.        0.6000 [ 0.1120; 1.0880]        5.2\n\nNumber of studies combined: k = 18\n\n                        SMD           95%-CI    z  p-value\nRandom effects model 0.5741 [0.4082; 0.7399] 6.78 < 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0752 [0.0357; 0.3046]; tau = 0.2743 [0.1891; 0.5519]\n I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 45.50   17  0.0002\n\nDetails on meta-analytical method:\n- Inverse variance method\n- DerSimonian-Laird estimator for tau^2\n- Jackson method for confidence interval of tau^2 and tau\n\n\n\n\nCode\nweights <- 1/(ThirdWave$seTE^2)\nfe_est <- with(ThirdWave, sum(TE * weights) / sum(weights)) # pooled estimate \nQ <- sum(with(ThirdWave, (TE - fe_est)^2 / seTE^2))\n\n\n\n\nCode\nk <- nrow(ThirdWave)\nmax(0, (Q - (k - 1))/  (sum(weights) - sum(weights^2) / sum(weights))) # DL estimator\n\n\n[1] 0.07523267\n\n\n\n\nCode\nlibrary(esc)\n# Define the data we need to calculate SMD/d\ngrp1m <- 50   # mean of group 1\ngrp2m <- 60   # mean of group 2\ngrp1sd <- 10  # sd of group 1\ngrp2sd <- 10  # sd of group 2\ngrp1n <- 100  # n of group1\ngrp2n <- 100  # n of group2\n\n# Calculate effect size\nesc_mean_sd(grp1m = grp1m, grp2m = grp2m, \n            grp1sd = grp1sd, grp2sd = grp2sd, \n            grp1n = grp1n, grp2n = grp2n)\n\n\n\nEffect Size Calculation for Meta Analysis\n\n     Conversion: mean and sd to effect size d\n    Effect Size:  -1.0000\n Standard Error:   0.1500\n       Variance:   0.0225\n       Lower CI:  -1.2940\n       Upper CI:  -0.7060\n         Weight:  44.4444\n\n\n\n\n\n\nDingman, Harvey F., and Norman C. Perry. 1956. “A Comparison of the Accuracy of the Formula for the Standard Error of Pearson “r” with the Accuracy of Fisher’s z-Transformation.” The Journal of Experimental Education 24 (4): 319–21. https://doi.org/10.1080/00220973.1956.11010555.\n\n\nHald, Anders. 2008. A History of Parametric Statistical Inference from Bernoulli to Fisher, 1713-1935. Springer Science & Business Media.\n\n\nKnapp, Guido, and Joachim Hartung. 2003. “Improved tests for a random effects meta-regression with a single covariate.” Statistics in Medicine 22 (17): 2693–2710. https://doi.org/10.1002/sim.1482.\n\n\nMcMurdie, Paul J., and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible.” PLOS Computational Biology 10 (4): e1003531. https://doi.org/10.1371/journal.pcbi.1003531.\n\n\nSheppard, W. F., and Andrew Russell Forsyth. 1899. “III. On the Application of the Theory of Error to Cases of Normal Distribution and Normal Correlation.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 192 (January): 101–67. https://doi.org/10.1098/rsta.1899.0003.\n\n\nSidik, Kurex, and Jeffrey N. Jonkman. 2005. “A note on variance estimation in random effects meta-regression.” Journal of Biopharmaceutical Statistics 15 (5): 823–38. https://doi.org/10.1081/BIP-200067915."
  },
  {
    "objectID": "exponential/exponential.html#exponential-modeling",
    "href": "exponential/exponential.html#exponential-modeling",
    "title": "9  Exponential Modeling",
    "section": "9.1 Exponential Modeling",
    "text": "9.1 Exponential Modeling\nThere are many ways to fit an exponential model\n\n\nCode\n# Simulate data\nstar_count <- function(d, a = 500, b = .7) {\n  a * exp(-d * b)\n}\n\nset.seed(1)\nx <- 0:19\ny <- rnegbin(20, mu = star_count(0:19), theta = 100000)\n\n# simulate distance position of stars\nbin_offsets <- y |> map(~sort(runif(.x))) |> reduce(c)\nbin_start <- map2(x, y, .f = ~rep(.x, .y)) |> reduce(c)\nstar_pos <- bin_start + bin_offsets\n\ndiff(star_pos)\n\n\n   [1] 9.597652e-05 6.195621e-03 2.907616e-03 1.134377e-04 1.928067e-03\n   [6] 9.663649e-03 5.045898e-03 1.590043e-03 5.900601e-03 2.628127e-04\n  [11] 5.987849e-03 1.689830e-03 3.242628e-03 1.498245e-03 1.694452e-03\n  [16] 7.860756e-04 1.709354e-03 2.041416e-03 9.738570e-04 3.770091e-03\n  [21] 3.279675e-03 1.588414e-03 6.013783e-06 6.490583e-04 4.923800e-04\n  [26] 3.496718e-03 1.586042e-03 1.121345e-03 1.306609e-03 2.815112e-03\n  [31] 1.621292e-03 1.673421e-04 8.728891e-05 5.087562e-03 8.746670e-04\n  [36] 2.197108e-03 8.734883e-03 3.443789e-04 4.675877e-04 4.311248e-03\n  [41] 9.717434e-04 7.322796e-04 2.311110e-03 1.803405e-03 6.249826e-05\n  [46] 3.756178e-03 2.229521e-04 7.169484e-05 1.259641e-03 3.147101e-03\n  [51] 1.834618e-03 1.855225e-03 5.230620e-04 1.181946e-03 2.789363e-03\n  [56] 2.031644e-03 4.604310e-03 8.289795e-04 2.154936e-04 2.044186e-03\n  [61] 2.578669e-05 7.858516e-04 4.618031e-04 1.378378e-04 1.283525e-03\n  [66] 4.516909e-03 2.516079e-03 1.459756e-03 5.625044e-04 1.377929e-04\n  [71] 3.225633e-04 3.695957e-03 5.228072e-04 3.658538e-04 6.620051e-03\n  [76] 1.940574e-05 5.502056e-04 1.312910e-03 1.681824e-03 3.713125e-03\n  [81] 9.763965e-04 1.241096e-03 7.010642e-04 4.262945e-04 3.856981e-03\n  [86] 2.658521e-04 6.668135e-04 1.556502e-03 1.642253e-03 2.102186e-04\n  [91] 1.474215e-03 1.223945e-03 1.013228e-04 7.424993e-04 3.671825e-03\n  [96] 3.019662e-04 6.349548e-05 3.838142e-03 2.356407e-03 1.767251e-03\n [101] 5.862953e-04 1.480192e-03 9.494252e-04 1.100327e-03 3.836851e-03\n [106] 5.545542e-03 4.852858e-03 3.380637e-03 1.773769e-03 5.086160e-04\n [111] 5.437150e-03 2.660643e-03 5.078267e-04 4.567042e-03 2.277347e-03\n [116] 1.808272e-03 1.694734e-03 6.707529e-03 7.607373e-04 4.868580e-04\n [121] 6.953231e-04 7.676261e-04 3.909292e-03 2.238319e-03 4.282964e-03\n [126] 4.357807e-03 8.024173e-04 4.512395e-04 5.446658e-04 2.763811e-03\n [131] 1.544373e-03 1.774939e-03 2.963034e-03 2.961243e-04 3.699216e-04\n [136] 1.072463e-03 3.431877e-04 9.663661e-04 6.589696e-03 8.082450e-04\n [141] 9.784591e-05 2.100968e-04 2.784184e-03 2.133742e-03 3.485444e-04\n [146] 1.557981e-03 8.158242e-03 6.568262e-04 1.246061e-04 2.616439e-04\n [151] 4.660743e-03 7.790717e-03 1.609190e-03 2.398819e-04 3.480044e-03\n [156] 7.172821e-04 4.655015e-03 8.190128e-04 3.528436e-03 6.759362e-04\n [161] 3.184095e-03 1.661575e-03 1.729990e-03 3.360235e-05 4.500665e-03\n [166] 2.428212e-04 1.799954e-03 1.667319e-04 9.578662e-04 4.329509e-04\n [171] 7.370247e-04 2.363870e-03 1.830698e-03 2.677493e-03 2.743284e-03\n [176] 9.937233e-04 7.765112e-05 2.482987e-04 3.023058e-03 1.623863e-03\n [181] 6.869687e-04 2.842671e-03 6.788331e-04 6.291065e-03 2.866806e-03\n [186] 2.625846e-03 1.082918e-03 3.740541e-05 4.728022e-05 2.509642e-03\n [191] 1.966639e-04 1.423493e-03 7.296663e-04 3.542910e-03 5.172847e-04\n [196] 1.217160e-03 1.222466e-03 7.882717e-04 1.799269e-03 3.774597e-03\n [201] 8.266110e-04 1.173834e-03 1.280347e-03 5.788652e-03 6.956942e-04\n [206] 1.258928e-03 2.485511e-04 6.432619e-04 4.553779e-04 1.747316e-03\n [211] 3.492550e-03 7.396960e-05 2.514177e-03 1.806322e-03 3.040127e-03\n [216] 1.923416e-03 2.703762e-04 2.538257e-03 4.653775e-04 1.735589e-05\n [221] 1.876902e-03 2.882716e-03 5.651761e-03 8.740369e-04 8.895900e-05\n [226] 1.760324e-03 2.557158e-03 6.286362e-04 5.815597e-05 1.258761e-03\n [231] 9.547088e-04 4.528033e-03 1.050965e-03 8.235895e-05 3.642856e-03\n [236] 4.902529e-05 6.286483e-03 1.492314e-04 4.113826e-04 7.640438e-04\n [241] 1.378964e-03 4.300958e-03 5.210338e-04 7.987337e-04 6.566611e-04\n [246] 2.410540e-03 3.152735e-03 2.135827e-03 1.427555e-03 1.252233e-03\n [251] 3.034717e-04 3.163718e-03 4.271183e-04 3.629797e-04 1.125691e-03\n [256] 8.043246e-04 4.051539e-03 2.179951e-03 6.401562e-04 1.159424e-03\n [261] 6.447016e-03 7.226078e-04 1.335797e-03 3.471188e-04 5.919330e-04\n [266] 2.274474e-03 6.464787e-04 1.926442e-03 4.095146e-04 1.400328e-03\n [271] 1.088670e-03 3.600011e-03 9.610550e-05 6.062197e-04 2.483384e-03\n [276] 8.344918e-04 2.100854e-04 3.562869e-03 2.064184e-03 2.129900e-04\n [281] 1.566311e-03 5.409940e-03 1.319426e-03 7.222202e-04 1.283053e-03\n [286] 7.381022e-04 1.581724e-03 2.970617e-03 5.263703e-03 4.114357e-03\n [291] 1.511446e-04 5.516163e-03 6.530474e-03 7.235620e-04 2.258916e-03\n [296] 3.741209e-03 1.087345e-03 1.886950e-04 6.061480e-04 1.480907e-03\n [301] 7.906417e-03 2.701939e-03 3.289718e-03 1.920273e-04 3.851326e-03\n [306] 4.829871e-03 1.384049e-03 2.153968e-03 5.138837e-03 7.364594e-04\n [311] 1.643967e-03 6.685448e-04 2.780251e-03 2.046850e-03 2.043338e-04\n [316] 1.140889e-03 3.701563e-04 1.137548e-03 3.307940e-03 4.896037e-03\n [321] 5.765061e-03 1.363246e-03 2.940910e-03 5.699933e-03 1.006106e-03\n [326] 2.073036e-03 4.415459e-03 2.192322e-03 2.187213e-03 5.072337e-04\n [331] 1.311736e-03 2.085343e-04 1.751514e-03 6.119132e-04 2.900271e-03\n [336] 3.769983e-03 1.374485e-03 8.947682e-06 1.429440e-03 1.665754e-03\n [341] 2.294844e-04 8.200554e-04 9.434167e-04 3.189942e-03 3.317576e-04\n [346] 1.275524e-03 3.129378e-03 5.964605e-03 2.451818e-03 5.278464e-03\n [351] 1.345564e-03 5.487159e-03 1.138175e-03 2.144363e-04 6.288742e-04\n [356] 1.670949e-03 3.084574e-03 1.184753e-03 2.627538e-03 3.637975e-03\n [361] 3.162308e-04 3.080263e-03 3.116192e-03 2.146024e-03 6.226453e-04\n [366] 7.708110e-04 2.544373e-05 3.025944e-03 2.672176e-04 7.561950e-04\n [371] 1.345797e-03 1.978712e-03 3.776763e-03 3.519538e-05 7.729460e-04\n [376] 9.063799e-04 2.361732e-03 1.542619e-03 2.061956e-03 4.209329e-04\n [381] 9.255058e-04 4.587450e-03 2.336278e-03 1.436903e-03 5.619621e-03\n [386] 9.367939e-04 7.070558e-03 1.153229e-04 1.823963e-03 4.422406e-04\n [391] 1.015904e-03 5.937547e-04 1.616475e-03 1.660580e-03 1.776679e-04\n [396] 1.071160e-03 7.430942e-03 8.341135e-05 6.583656e-03 1.593979e-03\n [401] 1.070212e-03 1.032650e-03 1.164576e-03 6.692212e-04 1.694893e-03\n [406] 2.994278e-03 5.368516e-03 3.927060e-03 4.727419e-04 4.378880e-03\n [411] 2.703066e-03 6.479471e-03 1.261281e-03 1.735685e-04 1.130116e-03\n [416] 5.199480e-03 2.527630e-03 1.630988e-03 4.924464e-03 1.483294e-03\n [421] 1.232405e-04 2.347411e-03 5.060555e-03 6.842995e-04 9.674975e-04\n [426] 2.122965e-03 1.075219e-04 7.371628e-04 1.238187e-03 3.655849e-03\n [431] 1.636397e-03 1.574944e-03 9.501074e-05 4.449398e-03 1.080854e-03\n [436] 1.049308e-03 7.498902e-03 9.143825e-04 2.626013e-04 5.998003e-03\n [441] 3.344108e-03 2.119552e-03 7.883303e-04 5.205192e-04 1.855234e-03\n [446] 8.857728e-04 2.999660e-04 5.831908e-03 7.807277e-06 8.635352e-03\n [451] 3.513230e-04 2.403066e-03 5.233119e-03 3.233650e-03 9.761548e-04\n [456] 3.078888e-03 1.014754e-02 3.450631e-03 1.059985e-04 1.155465e-03\n [461] 2.072153e-03 8.754598e-04 3.056780e-04 1.319733e-04 6.387201e-04\n [466] 3.318009e-03 3.379006e-04 1.781541e-03 1.021097e-03 2.701278e-03\n [471] 7.229606e-04 1.172006e-03 2.196610e-03 4.517369e-04 7.399952e-04\n [476] 2.776389e-05 3.093001e-03 1.148536e-03 9.718228e-04 4.417274e-03\n [481] 1.929449e-04 1.842540e-04 6.454869e-04 7.686014e-04 5.290303e-04\n [486] 1.574120e-03 3.866318e-04 2.981476e-03 7.919392e-04 1.902080e-03\n [491] 1.005022e-03 4.891767e-03 3.193608e-04 6.340127e-04 6.008516e-03\n [496] 2.383368e-03 2.464022e-03 5.453755e-04 3.531764e-03 1.216365e-03\n [501] 7.901748e-04 2.398069e-03 1.599386e-03 7.394005e-04 8.454411e-04\n [506] 1.065165e-03 1.646525e-04 2.051600e-03 1.118928e-04 8.689074e-03\n [511] 6.993289e-03 4.852476e-03 1.088567e-02 8.950538e-04 2.363638e-03\n [516] 9.566329e-04 2.725557e-03 1.426373e-03 3.383826e-03 3.408159e-03\n [521] 2.982090e-03 5.131956e-03 1.475564e-05 8.094027e-04 5.777355e-03\n [526] 5.593896e-04 3.823247e-04 6.586144e-03 8.346333e-03 3.112576e-03\n [531] 3.080247e-03 6.955682e-03 8.756758e-03 2.656404e-03 8.839782e-03\n [536] 1.363394e-02 1.189240e-03 1.957414e-03 5.135766e-03 5.948856e-03\n [541] 1.974053e-03 7.299694e-03 2.774562e-03 9.192023e-04 5.307096e-03\n [546] 5.431633e-03 6.671372e-03 5.239761e-04 2.300136e-03 9.946494e-03\n [551] 2.879309e-04 1.434801e-02 1.486375e-03 5.658111e-03 2.540834e-03\n [556] 3.247915e-03 9.146738e-04 2.488370e-03 4.011370e-03 7.379058e-03\n [561] 4.606105e-03 5.028942e-04 1.213145e-02 1.472150e-03 3.995990e-03\n [566] 2.236161e-04 1.302564e-02 6.554911e-03 8.834939e-03 1.883143e-03\n [571] 3.421248e-03 6.800720e-03 8.854185e-03 5.363062e-03 4.670727e-04\n [576] 3.531741e-03 4.003172e-03 8.505388e-03 1.676582e-03 8.986728e-03\n [581] 8.740375e-03 5.018260e-03 1.608986e-03 2.510713e-03 3.418626e-03\n [586] 1.785368e-03 3.172664e-04 4.760744e-04 2.052269e-04 1.679534e-03\n [591] 1.041151e-03 1.919076e-03 3.055609e-03 3.057534e-04 4.947025e-03\n [596] 4.040983e-04 4.674926e-03 2.110414e-03 1.799914e-03 9.121995e-04\n [601] 8.076854e-03 1.759557e-03 2.106057e-03 2.361428e-03 1.023955e-03\n [606] 5.242297e-03 1.583172e-03 6.894371e-03 4.947072e-03 1.418282e-02\n [611] 2.621838e-03 9.632604e-04 5.951077e-04 1.944575e-03 2.377280e-03\n [616] 2.980599e-03 1.317150e-03 2.127492e-03 9.353692e-05 3.163238e-03\n [621] 3.852389e-03 6.417679e-03 2.057972e-03 1.356076e-03 4.300221e-03\n [626] 1.398688e-02 2.359303e-03 7.019425e-05 9.449830e-04 5.569984e-04\n [631] 3.403114e-03 2.372583e-03 5.836313e-03 4.290515e-03 5.101484e-03\n [636] 1.621114e-03 5.069605e-03 3.776255e-03 3.119442e-04 1.531231e-03\n [641] 8.151896e-03 5.977638e-04 1.826656e-03 4.297705e-03 4.847919e-03\n [646] 3.728169e-03 2.236242e-03 5.733693e-03 6.677285e-04 2.873982e-03\n [651] 4.001611e-03 1.832793e-03 3.093553e-03 2.211737e-03 8.076471e-04\n [656] 5.026920e-03 5.958707e-03 2.418420e-03 9.117261e-03 1.479609e-03\n [661] 9.643665e-03 1.384607e-04 9.174092e-03 1.154933e-03 5.200273e-03\n [666] 5.865172e-03 1.096706e-02 6.483681e-04 2.359416e-03 1.809626e-03\n [671] 1.409512e-03 8.220951e-03 1.903693e-05 9.436179e-03 3.682431e-03\n [676] 5.375016e-04 3.389996e-03 1.610966e-02 3.357724e-03 4.399797e-03\n [681] 4.563550e-05 7.150120e-04 5.373291e-04 8.001404e-03 4.349861e-03\n [686] 1.027140e-02 6.932766e-03 1.762852e-03 3.156674e-04 3.484508e-03\n [691] 2.404724e-04 2.252918e-03 2.482627e-04 6.009878e-04 1.523639e-03\n [696] 7.399737e-03 2.690446e-03 6.494364e-04 1.972830e-03 1.129596e-03\n [701] 1.203953e-02 1.340942e-04 2.003629e-03 4.711529e-03 1.060523e-03\n [706] 3.397977e-04 2.821826e-04 1.828532e-03 1.380447e-02 1.007376e-03\n [711] 2.080805e-03 6.919630e-03 8.975437e-04 1.387731e-04 3.633380e-04\n [716] 4.553031e-03 5.996801e-03 3.284284e-03 6.435794e-03 7.727278e-03\n [721] 7.709923e-03 5.446568e-03 4.732984e-04 1.083766e-04 7.795024e-03\n [726] 4.593418e-04 1.342768e-03 3.685765e-04 5.272246e-03 7.602199e-03\n [731] 2.482461e-03 5.473610e-03 5.914203e-04 5.833897e-03 1.185916e-03\n [736] 3.346588e-03 4.766458e-04 4.297896e-03 4.302047e-04 2.418478e-03\n [741] 3.317586e-03 2.944138e-03 2.502028e-02 1.298477e-03 2.308900e-03\n [746] 1.030613e-03 1.671933e-03 2.811734e-03 3.723715e-03 3.074885e-03\n [751] 2.472243e-03 5.034593e-03 9.571121e-04 3.453750e-03 9.659801e-04\n [756] 3.065499e-03 1.951412e-03 3.416800e-03 3.897993e-03 6.298528e-04\n [761] 1.705471e-03 2.146515e-03 4.637728e-04 3.930487e-03 5.334727e-03\n [766] 3.682870e-03 1.127676e-03 1.237026e-03 8.118422e-03 5.454158e-04\n [771] 3.795318e-03 8.648862e-04 4.707431e-03 1.574632e-02 3.344304e-03\n [776] 1.859251e-03 8.257135e-03 2.486953e-04 1.519706e-03 1.769524e-03\n [781] 4.848030e-03 3.054096e-03 1.365755e-02 1.526611e-03 1.116674e-03\n [786] 1.872042e-03 3.701174e-03 9.850992e-03 7.163357e-03 2.070127e-03\n [791] 8.157620e-04 3.015835e-03 7.740832e-04 6.411839e-03 3.777907e-03\n [796] 9.186125e-03 7.697935e-03 2.561446e-02 6.464905e-03 2.242665e-02\n [801] 9.446213e-03 3.994043e-03 1.630996e-02 9.783265e-03 4.739526e-04\n [806] 1.987571e-02 9.783439e-03 5.945270e-03 1.994112e-03 9.153957e-03\n [811] 4.721437e-03 4.860295e-04 7.770923e-03 1.070812e-02 2.101569e-03\n [816] 1.728327e-03 2.188436e-03 7.261206e-03 8.984825e-04 1.613982e-02\n [821] 1.351119e-02 9.605088e-03 1.322460e-02 5.407072e-03 1.536859e-03\n [826] 3.540678e-03 2.288414e-02 1.804280e-03 3.727205e-02 1.344934e-02\n [831] 4.704816e-03 5.585833e-03 6.900921e-03 1.381142e-02 3.267545e-06\n [836] 1.243708e-02 1.742443e-03 6.389060e-03 2.613671e-02 8.753235e-04\n [841] 2.861191e-04 2.687410e-03 9.232769e-03 2.051267e-02 5.881631e-02\n [846] 1.103577e-02 1.282095e-03 1.642057e-02 1.434853e-02 6.684200e-03\n [851] 1.116583e-02 2.813887e-02 7.414172e-03 1.403400e-02 2.951729e-03\n [856] 9.188484e-03 1.491828e-02 1.555670e-03 2.988526e-03 8.403806e-04\n [861] 8.176505e-03 2.582449e-03 5.973042e-03 1.056918e-02 5.358207e-03\n [866] 9.938674e-03 1.809272e-02 4.296012e-03 1.388756e-02 1.068674e-02\n [871] 6.146302e-03 2.447065e-03 9.499837e-03 2.879219e-02 1.359182e-02\n [876] 9.469270e-03 1.474041e-03 7.684108e-03 6.608756e-04 5.706212e-03\n [881] 1.237381e-02 9.176153e-03 5.285707e-03 4.478747e-03 1.419948e-02\n [886] 1.706201e-02 9.028147e-03 2.845119e-03 1.109358e-02 5.806999e-03\n [891] 3.274670e-05 1.506753e-02 1.852395e-02 1.484857e-02 2.739300e-02\n [896] 1.842800e-02 3.343552e-02 1.423533e-02 1.142598e-03 4.352645e-04\n [901] 3.560511e-03 1.058936e-02 1.146518e-02 6.927344e-03 7.949512e-03\n [906] 1.325014e-02 3.347640e-03 1.165296e-02 6.262431e-02 3.741877e-04\n [911] 1.622539e-02 1.434850e-02 3.167318e-02 5.512541e-02 1.536230e-02\n [916] 2.798424e-03 3.235764e-02 4.738306e-03 3.918817e-02 2.677679e-02\n [921] 7.152597e-03 7.280458e-03 8.843749e-03 3.026957e-03 8.980569e-03\n [926] 4.444661e-03 3.424853e-02 3.394069e-02 8.370698e-03 1.515591e-02\n [931] 1.849513e-02 3.041593e-02 1.174784e-02 6.741069e-02 1.281456e-02\n [936] 2.430789e-02 1.845993e-02 1.970209e-02 3.776644e-03 3.377786e-02\n [941] 1.878788e-02 4.888204e-02 1.045966e-02 9.685676e-03 2.415992e-02\n [946] 3.954495e-03 1.982421e-03 5.160084e-03 4.610995e-02 1.066270e-01\n [951] 1.032101e-02 2.513031e-02 8.182281e-03 3.416043e-02 4.222371e-02\n [956] 3.624376e-02 2.744754e-02 6.826530e-03 1.397725e-02 5.053447e-02\n [961] 3.494593e-02 2.671826e-02 1.158660e-02 2.067080e-02 1.138164e-02\n [966] 2.184365e-02 4.752632e-02 9.147254e-03 2.281156e-02 3.149502e-02\n [971] 5.923346e-02 1.759670e-01 1.954359e-03 2.336873e-02 3.302932e-02\n [976] 2.844264e-02 1.123319e-01 3.639931e-02 4.422604e-02 2.412996e-03\n [981] 3.348943e-05 1.624749e-01 7.727121e-02 1.285767e-01 2.538879e-02\n [986] 1.952381e-01 1.266535e-01 4.634832e-02 4.599607e-03 1.991575e-02\n [991] 5.231131e-02 2.187517e-01 1.817350e-01 5.985113e-02 4.352494e-02\n [996] 1.133106e-01 1.072945e-01 1.735455e-01 1.274540e-01 3.519266e-01\n[1001] 9.584075e-01 4.892394e-02 9.776872e-01 2.274324e+00\n\n\nCode\nplot(log(star_pos), log(c(0, diff(star_pos)) + 1))\n\n\n\n\n\nCode\nbeeswarm(star_pos, horizontal = TRUE, method = \"center\")\n\n\n\n\n\nCode\nplot(x, y)\n\n\n\n\n\nCode\n# log method\nmod_loglm <- lm(log(y + 1) ~ x)\n\nmod_loglm |> summary()\n\n\n\nCall:\nlm(formula = log(y + 1) ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4665 -0.6116 -0.1001  0.7550  1.5978 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.63861    0.41155  11.271 1.37e-09 ***\nx           -0.31721    0.03703  -8.566 9.14e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.955 on 18 degrees of freedom\nMultiple R-squared:  0.803, Adjusted R-squared:  0.7921 \nF-statistic: 73.37 on 1 and 18 DF,  p-value: 9.142e-08\n\n\nCode\nexp(coef(mod_loglm)[2]) # decay param\n\n\n        x \n0.7281783 \n\n\nCode\n# nls\nmod_nls <- nls(y~a * exp(-b * x), data = tibble(x, y),\n               start = list(a = 300, b = .5))\nsummary(mod_nls)\n\n\n\nFormula: y ~ a * exp(-b * x)\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \na 513.64215    3.86330  132.95   <2e-16 ***\nb   0.70819    0.01038   68.25   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.982 on 18 degrees of freedom\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 4.554e-06\n\n\nCode\n# poisson method\nmod_glm <- glm(y ~ x, data = tibble(x, y), family = poisson(link = \"log\"))\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = poisson(link = \"log\"), data = tibble(x, \n    y))\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.86884  -0.29658  -0.12090   0.06696   1.32656  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  6.24785    0.03845  162.50   <2e-16 ***\nx           -0.72222    0.02328  -31.02   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3319.382  on 19  degrees of freedom\nResidual deviance:    5.173  on 18  degrees of freedom\nAIC: 59.975\n\nNumber of Fisher Scoring iterations: 4\n\n\nCode\n# now if I sample the same stars, but with half the bin width...\nx_half <- seq(0, 19, .5)\ny_half <- table(cut(star_pos,breaks = x_half, include.lowest = T))\n\n\n# nls method, can't adjust for the bin size, and residual errors\nmod_nls_half <- nls(y_half ~ a * exp(-b * x_half[-length(x_half)]),\n                    data = tibble(x_half[-length(x_half)], y_half),\n                    start = list(a = 200, b = .5))\n\nsummary(mod_nls_half)\n\n\n\nFormula: y_half ~ a * exp(-b * x_half[-length(x_half)])\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \na 286.46601    8.35228   34.30   <2e-16 ***\nb   0.65138    0.03134   20.78   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.788 on 36 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 2.446e-06\n\n\nCode\nmod_glm_half <- glm(y_half ~ x_half[-length(x_half)], family = poisson(link = \"log\"), offset = rep(log(1/2), length(y_half)))\n\nsummary(mod_glm_half);sigma(mod_glm_half)\n\n\n\nCall:\nglm(formula = y_half ~ x_half[-length(x_half)], family = poisson(link = \"log\"), \n    offset = rep(log(1/2), length(y_half)))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7798  -0.6302  -0.1626  -0.0380   3.2904  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)              6.39092    0.04117  155.23   <2e-16 ***\nx_half[-length(x_half)] -0.70399    0.02232  -31.54   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3229.433  on 37  degrees of freedom\nResidual deviance:   36.893  on 36  degrees of freedom\nAIC: 123.62\n\nNumber of Fisher Scoring iterations: 4\n\n\n[1] 1.01232\n\n\n\n\nCode\nsummary(mod_glm);sigma(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = poisson(link = \"log\"), data = tibble(x, \n    y))\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.86884  -0.29658  -0.12090   0.06696   1.32656  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  6.24785    0.03845  162.50   <2e-16 ***\nx           -0.72222    0.02328  -31.02   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3319.382  on 19  degrees of freedom\nResidual deviance:    5.173  on 18  degrees of freedom\nAIC: 59.975\n\nNumber of Fisher Scoring iterations: 4\n\n\n[1] 0.5360842\n\n\n\n\nCode\nanova(mod_glm_half)\n\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: y_half\n\nTerms added sequentially (first to last)\n\n                        Df Deviance Resid. Df Resid. Dev\nNULL                                       37     3229.4\nx_half[-length(x_half)]  1   3192.5        36       36.9\n\n\n\n\nCode\nmod_quasi_glm <- glm(y~x, family = quasipoisson(link = \"log\"))\n\nmod_quasi_glm_half <- update(mod_glm_half, family = quasipoisson(link = \"log\"))\nsummary(mod_quasi_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = quasipoisson(link = \"log\"))\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.86884  -0.29658  -0.12090   0.06696   1.32656  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.24785    0.02302  271.39   <2e-16 ***\nx           -0.72222    0.01394  -51.81   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 0.3585152)\n\n    Null deviance: 3319.382  on 19  degrees of freedom\nResidual deviance:    5.173  on 18  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\n\nCode\nsummary(mod_quasi_glm_half)\n\n\n\nCall:\nglm(formula = y_half ~ x_half[-length(x_half)], family = quasipoisson(link = \"log\"), \n    offset = rep(log(1/2), length(y_half)))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7798  -0.6302  -0.1626  -0.0380   3.2904  \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              6.39092    0.04384  145.78   <2e-16 ***\nx_half[-length(x_half)] -0.70399    0.02377  -29.62   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 1.133809)\n\n    Null deviance: 3229.433  on 37  degrees of freedom\nResidual deviance:   36.893  on 36  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nCode\nplot(x, y)\n# lines(x, exp(coef(mod_loglm)[1] + coef(mod_loglm)[2]*x) - 1) # bad model...\nlines(x, predict(mod_glm, type = \"response\"))\nlines(x, predict(mod_nls, type = \"response\"), col = 2)\npoints(x_half[-length(x_half)], y_half, pch = 16)\nlines(x_half[-length(x_half)], predict(mod_glm_half, type = \"response\"), col = 3)\nlines(x_half[-length(x_half)], predict(mod_nls_half, type = \"response\"), col = 2, lty = 2)\n\n\n\n\n\nThis seems like a difficult thing to grapple with.. the rate of decrease is not the same. It seems if you halve the exposure time, you’d expect similar rates of decrease, but you should end up with the same estimate of the mean poisson value.\n\n\nCode\npredict(mod_glm_half) |> as.numeric()\n\n\n [1]  5.69776984  5.34577312  4.99377640  4.64177968  4.28978296  3.93778624\n [7]  3.58578953  3.23379281  2.88179609  2.52979937  2.17780265  1.82580593\n[13]  1.47380921  1.12181249  0.76981577  0.41781905  0.06582234 -0.28617438\n[19] -0.63817110 -0.99016782 -1.34216454 -1.69416126 -2.04615798 -2.39815470\n[25] -2.75015142 -3.10214813 -3.45414485 -3.80614157 -4.15813829 -4.51013501\n[31] -4.86213173 -5.21412845 -5.56612517 -5.91812189 -6.27011861 -6.62211532\n[37] -6.97411204 -7.32610876\n\n\nCode\npredict(mod_glm)[2]\n\n\n       2 \n5.525628 \n\n\nCode\n# 4.994005 vs 5.5\n\n\nThe values are quite different… for the rate parameters, which is a little uncomfortable, and I’m not sure why."
  },
  {
    "objectID": "longitudinal/within_between.html#introduction",
    "href": "longitudinal/within_between.html#introduction",
    "title": "10  The Within-Between Model",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nThis material is largely sourced from Chapter 9 of Longitudinal Data Analysis.\nFor fixed effects models,\n\nX_{ij} is independent of \\varepsilon_{ij} and \\varepsilon_{ij'}.\n\nan observation will not help determine the value of the next\n\nFE models “clear out” any higher level trends, and just focus on the within estimate\nThe \\hat\\beta estimate will be consistent even when the higher level effect is correlated with the covariates."
  },
  {
    "objectID": "longitudinal/within_between.html#the-model",
    "href": "longitudinal/within_between.html#the-model",
    "title": "10  The Within-Between Model",
    "section": "10.2 The Model",
    "text": "10.2 The Model\nTo introduce the model, we will look at a simulated example where there is a higher level effect\n\n\nCode\n# simulated  \ndat_skel <- data.frame(cohort = rep(1:3, each = 16),\n           id = rep(1:12, each = 4),\n           age = c(rep(c(4, 6, 7, 10), 4), # cohort 1\n                   rep(c(4, 5, 8, 9) + 1, 4), # cohort 2 \n                   rep(c(4, 7, 8, 10) + 2, 4))) # cohort 3\n# creating data\nset.seed(1)\nid_int <- rnorm(12, 2) + 10\nid_slope <- rnorm(12, mean = 1, sd = .01) # random slope\ncohort_int <- c(1, 4, 7)\n\ndat <- dat_skel %>% \n  mutate(y = id_int + id_slope * age + cohort_int[cohort] + rnorm(nrow(.), sd = 5),\n         id = factor(id),\n         cohort = factor(cohort))\n\ndat %>% head()\n\n\n  cohort id age        y\n1      1  1   4 19.44783\n2      1  1   6 18.77012\n3      1  1   7 18.46414\n4      1  1  10 17.23703\n5      1  2   4 14.93811\n6      1  2   6 20.32587\n\n\n\n\nCode\ndat %>% ggplot(aes(age, y, color = cohort, group = id)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\nCode\ndat_centered <- dat %>% arrange(id, age) %>%\n  group_by(id) %>% \n  mutate(age_mean = mean(age),\n         y_mean = mean(y),\n         age_centered = age - age_mean,\n         y_centered = y - y_mean,\n         age_baseline = age - first(age),\n         y_baseline = y - first(y))\n\ndat_baseline <- dat_centered %>% \n  filter(age_baseline != 0)\n\n\n\n\nCode\n# Fixed Effect Estimate (within model)\ncentered_lm <- lm(y_centered ~ age_centered, data = dat_centered) # centered regression\n\n# between model\nbaseline_lm <- lm(y_baseline~age_baseline, data = dat_baseline) # baseline adjusted (centered)\nid_lm <- lm(y~age + id, data = dat_centered) # id model (centered)\n\nmean_lm <- lm(y_mean ~ age_mean, data = dat_centered) # mean regression (pooled model)\n\npooled_lm <- lm(y~age, data = dat_centered)\n\ncohort_lm <- lm(y~age + cohort, data = dat_centered)\n\n# mixed model\nmmod <- lmer(y ~ age + (1|id), data = dat)\n\n# within-between model\nwb_mmod <- lmer(y~age_centered + age_mean + (1 | id), data = dat_centered)\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\n\nCode\ncbind(\n  baseline_lm = coef(baseline_lm),\n  mean = coef(mean_lm),\n  cohort = coef(cohort_lm)[1:2],\n  id = coef(id_lm)[1:2],\n  pooled = coef(pooled_lm),\n  centered = coef(centered_lm),\n  mixed = fixef(mmod)) %>% rbind(NA) %>% \n  cbind(within_between = fixef(wb_mmod))\n\n\n             baseline_lm      mean    cohort       id    pooled     centered\n(Intercept)    -0.508907 -3.670061 11.107892 9.596570 10.993983 4.411316e-16\nage_baseline    1.316630  3.640294  1.316031 1.316031  1.768289 1.316031e+00\n                      NA        NA        NA       NA        NA           NA\n                 mixed within_between\n(Intercept)  12.131986      -3.670061\nage_baseline  1.623012       1.316031\n                    NA       3.640294\n\n\n\n\\begin{aligned}\nw = \\frac{(1 - \\rho_y)\\rho_x}{(1 - \\rho_y) + n\\rho_y(1-\\rho_x)}\n\\end{aligned}\n and\n\n\\begin{aligned}\n\\hat \\beta_2^{(RE)} = (1 - w)\\hat\\beta_2^{(FE)} + w\\hat\\beta_2^{(B)}\n\\end{aligned}\n\n\n\nCode\n# help function for weight\nfixed_random_weight <- function(rho_x, rho_y, n) {\n  (1-rho_y) * rho_x / ((1-rho_y) + n * rho_y * (1 - rho_x))\n}\n\n\n\n\nCode\n# response correlation\nrho_y <- VarCorr(mmod) %>% as.data.frame() %>% pull(vcov) %>% \n  as_mapper(~.x[1]/sum(.x))()\n\n# correlation of age\nage_lm <- lm(age~id, data = dat_centered)\nrho_x <- summary(age_lm)$r.squared\n\nn <- 4\nw <- fixed_random_weight(rho_x, rho_y, n)\n(1-w) * fixef(wb_mmod)[2] + w * fixef(wb_mmod)[3]\n\n\nage_centered \n    1.623012 \n\n\n\n\nCode\ndat %>% ggplot(aes(age, y, color = cohort, group = id)) +\n  geom_point(alpha = .2) +\n  geom_line(alpha = .2) +\n  geom_abline(slope = coef(centered_lm)[2], intercept = coef(centered_lm)[1] + 10) + # centered estimate\n  geom_abline(slope = coef(mean_lm)[2], intercept = coef(mean_lm)[1]) + # mean estimate\n  geom_abline(slope = fixef(mmod)[2], intercept = fixef(mmod)[1], linetype = 2) + # mixed effects estimate\n  theme_minimal()"
  },
  {
    "objectID": "longitudinal/within_between.html#fev-example",
    "href": "longitudinal/within_between.html#fev-example",
    "title": "10  The Within-Between Model",
    "section": "10.3 FEV example",
    "text": "10.3 FEV example\n\n\nCode\nlibrary(foreign)\nfev <- read.dta(\"data/fev1.dta\") %>% \n  filter(id != 197) %>%\n  mutate(id = factor(id),\n         y =  logfev1 - 2 * (log(ht)))\n\nfev_centered <- fev %>% group_by(id) %>% \n  mutate(\n    y_mean = mean(y),\n    age_mean = mean(age),\n    age_centered = age - age_mean)\n\n\n\n\nCode\n# fitting all models\nfev_lm <- lm(y ~ age + id, data = fev) # lm\nfev_mer <- lmer(y~age + (1|id), data = fev) # random\n\n# within/between model\nfev_wb <- lmer(y ~ age_centered + age_mean + (1|id),\n               data = fev_centered,\n               REML = FALSE)\n\n# cross sectional\nfev_mean_lm <- lm(y_mean ~ age_mean, data = fev_centered)\n\n\n# age correlation (between/within)\nfev_age_lm <- lm(age ~ id, data = fev)\n\n\n\n\nCode\n# comparing the coefficients\ncbind(fixed = coef(fev_lm)[1:2],\n      random = fixef(fev_mer),\n      mean = coef(fev_mean_lm)) %>% \n  rbind(NA) %>% cbind(within_between = fixef(fev_wb))\n\n\n                  fixed      random        mean within_between\n(Intercept) -0.39873981 -0.35517115 -0.37154929    -0.34833979\nage          0.02982264  0.02980696  0.03109488     0.02982264\n                     NA          NA          NA     0.02923539\n\n\n\n\nCode\nrho_y <- VarCorr(fev_mer) %>% \n  as.data.frame() %>%\n  pull(vcov) %>%\n  as_mapper(~.x[1]/sum(.x))() # .689\n\n# age covariance\n# fev_age_ss <- anova(fev_age_lm)$`Sum Sq`\n# rho_x <- fev_age_ss[1] / sum(fev_age_ss) \nrho_x <- summary(fev_age_lm)$r.squared # .172\n\nnbar <- fev %>% count(id) %>% pull(n) %>% mean()\n\nw <- fixed_random_weight(rho_x, rho_y, nbar)\nw\n\n\n[1] 0.01297669\n\n\nSince this value of w is very low we expect that the mixed estimate is very close to the fixed estimate.\n\n\nCode\n# hausman test manually\n\nhaus_contr <- cbind(c(0, -1, 1))\nhaus_mean <- fixef(fev_wb) %*% haus_contr\nhaus_var <- t(haus_contr) %*% vcov(fev_wb) %*% haus_contr\nhaus_z <- haus_mean / sqrt(haus_var)[1]\npchisq(haus_z^2, 1, lower.tail = FALSE) # p = .84\n\n\n          [,1]\n[1,] 0.8412728\n\n\nCode\n# hausman with plm\n\n# phtest(y~age, data = fev_centered, index = c(\"id\"))\nwi <- plm(y~age, data = fev_centered, index = c(\"id\"), model = \"within\" )\nfe <- plm(y~age, data = fev_centered, index = c(\"id\"), model = \"between\")\nmi <- plm(y~age, data = fev_centered, index = c(\"id\"), model = \"random\")\n\nphtest(wi, mi)\n\n\n\n    Hausman Test\n\ndata:  y ~ age\nchisq = 0.042027, df = 1, p-value = 0.8376\nalternative hypothesis: one model is inconsistent"
  },
  {
    "objectID": "longitudinal/within_between.html#resources",
    "href": "longitudinal/within_between.html#resources",
    "title": "10  The Within-Between Model",
    "section": "10.4 Resources",
    "text": "10.4 Resources\n\nSeparation of individual-level and cluster-level covariate effects in regression analysis of correlated data"
  },
  {
    "objectID": "epidemiology/epidemiology.html#contagion-process-on-time-varying-networks",
    "href": "epidemiology/epidemiology.html#contagion-process-on-time-varying-networks",
    "title": "11  Epidemiology",
    "section": "11.1 Contagion Process on Time-Varying Networks",
    "text": "11.1 Contagion Process on Time-Varying Networks\nThis section will explore some of the results of the paper Temporal Gillespie Algorithm: Fast Simulation of Contagion Processes on Time-Varying Networks.\nThere are some included files for the algorithms described in the paper. There doesn’t seem to be too much special about these programs, but well work the time component into the network. I think the major disadvantages of these data is\n./SIR <data> dt beta mu T_simulation ensembleSize outputTimeResolution\nwhere:\n  <data> - path of text file containing contact data (temporal network);\n  dt - time-resolution of recorded contact data (time-step length);\n  beta - probability per time-step of infection when in contact with an\n    infected node;\n  mu - probability per time-step of recovery for an infected node;\n  T_simulation - length of simulation in number of time-steps;\n  ensembleSize - number of independent realizations of the SIR process;\n  outputTimeResolution - time-resolution of the average number of infected\n    and recovered nodes that the program gives as output."
  },
  {
    "objectID": "network/network_centrality.html#introduction",
    "href": "network/network_centrality.html#introduction",
    "title": "12  Network Centrality",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nSeveral types of measuring centrality\n\nDegree\n\nCounting the number of in/out edges. summing by weight\n\nEigenvalue\n\nCaptures the idea that being connected to someone else important means that you yourself are important.\n\nKatz\nPageRank"
  },
  {
    "objectID": "network/network_centrality.html#network-fixtures",
    "href": "network/network_centrality.html#network-fixtures",
    "title": "12  Network Centrality",
    "section": "12.2 Network Fixtures",
    "text": "12.2 Network Fixtures\n\n\nCode\nset.seed(1)\n# ER random graph, 20 nodes, average 5/20 connections, directed\ng_uni <- sample_gnp(20, 5/20, directed=TRUE)\n\n# Simple 1,1, 2, 3 graph\ng_simple_adj_matrix <- matrix(c(0, 1, 1, 1,\n                     1, 0, 0, 1,\n                     1, 0, 0, 0,\n                     1, 1, 0, 0),\n                     nrow = 4, ncol = 4)\n\ng_simple <- graph_from_adjacency_matrix(g_simple_adj_matrix, mode = \"undirected\")\n# g_simple %>% set_vertex_attr(\"name\", value = letters[1:4]) # label with alphabet\n\n# ER random graph, 20 nodes, average 5/20, undirected\n\ng_uni_bi <- sample_gnp(20, 5/20, directed=TRUE)\n\n\n# Simple Directed graph, acyclic\ng_simple_dir_adj_matrix <- matrix(c(0, 1, 1, 0, 1, 0, 0, 0,\n                                    0, 0, 1, 1, 0, 0, 0, 0,\n                                    0, 0, 0, 0, 0, 1, 0, 0,\n                                    0, 0, 0, 0, 1, 0, 1, 0,\n                                    0, 0, 0, 0, 0, 0, 0, 0,\n                                    0, 0, 0, 0, 0, 0, 0, 0,\n                                    0, 0, 0, 0, 0, 0, 0, 0,\n                                    0, 0, 0, 0, 0, 1, 0, 0),\n                                  nrow = 8,\n                                  ncol = 8,\n                                  byrow = TRUE)\n\ng_simple_dir <- graph_from_adjacency_matrix(g_simple_dir_adj_matrix, mode = \"directed\")"
  },
  {
    "objectID": "network/network_centrality.html#eigenvalue-centrality",
    "href": "network/network_centrality.html#eigenvalue-centrality",
    "title": "12  Network Centrality",
    "section": "12.3 Eigenvalue centrality",
    "text": "12.3 Eigenvalue centrality\n\n\nCode\n# The graph we look at\nggraph(g_simple, layout = \"kk\") +\n  geom_edge_fan() +\n  geom_node_label(aes(label = 1:4))\n\n\n\n\n\n\n\nCode\n# Eigenvector centrality through igraph\ncent_eig_igraph <- eigen_centrality(g_simple)$vector\n# Through eigenvector decomp\ncent_eig_manual <- eigen(g_simple_adj_matrix)$vectors[,1] / eigen(g_simple_adj_matrix)$vectors[1,1]\n\n# They are the same!\ncbind(\n  cent_eig_igraph, # Eigencentrality\n  cent_eig_manual # eig centrality calculation\n)\n\n\n     cent_eig_igraph cent_eig_manual\n[1,]       1.0000000       1.0000000\n[2,]       0.8546377       0.8546377\n[3,]       0.4608111       0.4608111\n[4,]       0.8546377       0.8546377\n\n\n\n\nCode\n# Note how\neigen_centrality\n\n\nfunction (graph, directed = FALSE, scale = TRUE, weights = NULL, \n    options = arpack_defaults) \n{\n    if (!is_igraph(graph)) {\n        stop(\"Not a graph object\")\n    }\n    directed <- as.logical(directed)\n    scale <- as.logical(scale)\n    if (is.null(weights) && \"weight\" %in% edge_attr_names(graph)) {\n        weights <- E(graph)$weight\n    }\n    if (!is.null(weights) && any(!is.na(weights))) {\n        weights <- as.numeric(weights)\n    }\n    else {\n        weights <- NULL\n    }\n    options.tmp <- arpack_defaults\n    options.tmp[names(options)] <- options\n    options <- options.tmp\n    on.exit(.Call(C_R_igraph_finalizer))\n    res <- .Call(C_R_igraph_eigenvector_centrality, graph, directed, \n        scale, weights, options)\n    if (igraph_opt(\"add.vertex.names\") && is_named(graph)) {\n        names(res$vector) <- vertex_attr(graph, \"name\", V(graph))\n    }\n    res\n}\n<bytecode: 0x7ff30c39be38>\n<environment: namespace:igraph>\n\n\nCode\neigen_centrality(g_simple_dir)$vector\n\n\n[1] 0.9767991 1.0000000 0.9081894 0.7377802 0.6537287 0.4051718 0.2812982\n[8] 0.1544825\n\n\nCode\neigen(g_simple_dir_adj_matrix)$vectors[,1] / eigen(g_simple_adj_matrix)$vectors[1,1]\n\n\n[1] 1.63498 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000"
  },
  {
    "objectID": "network/network_centrality.html#katz-centrality",
    "href": "network/network_centrality.html#katz-centrality",
    "title": "12  Network Centrality",
    "section": "12.4 Katz Centrality",
    "text": "12.4 Katz Centrality\n\n\nCode\nlength(V(g_simple_dir))\n\n\n[1] 8\n\n\nCode\nggraph(g_simple_dir, layout = \"kk\") +\n  geom_edge_fan(arrow = arrow(length = unit(4, \"mm\")),\n                end_cap = circle(3, \"mm\")) +\n  geom_node_label(aes(label = 1:length(V(g_simple_dir))))\n\n\n\n\n\n\n\nCode\n# Calculating Katz Centrality with tidygraph (rank ordered?)\n# Only undirected?\nas_tbl_graph(g_simple) %>% \n  mutate(centrality = centrality_katz())\n\n\n# A tbl_graph: 4 nodes and 4 edges\n#\n# An undirected simple graph with 1 component\n#\n# Node Data: 4 × 1 (active)\n  centrality\n       <dbl>\n1      119. \n2      101. \n3       55.1\n4      101. \n#\n# Edge Data: 4 × 2\n   from    to\n  <int> <int>\n1     1     2\n2     1     3\n3     1     4\n# … with 1 more row\n\n\n\n\nCode\nalpha_centrality(g_simple_dir, alpha = 1)\n\n\n[1] 1 2 4 3 5 6 4 1"
  },
  {
    "objectID": "network/network_centrality.html#pagerank",
    "href": "network/network_centrality.html#pagerank",
    "title": "12  Network Centrality",
    "section": "12.5 PageRank",
    "text": "12.5 PageRank\n\n\nCode\npage_rank(g_simple)$vector\n\n\n[1] 0.3667359 0.2459278 0.1414085 0.2459278"
  },
  {
    "objectID": "network/network_gnar.html#tutorial",
    "href": "network/network_gnar.html#tutorial",
    "title": "13  GNAR",
    "section": "13.1 Tutorial",
    "text": "13.1 Tutorial\n\n\n\n\n\n\n\nCode\n# make_graph(~1-2-3-1, 1-4-5) # make igraph\n# fiveVTS\nfive_gnar <- GNARfit(vts = fiveVTS, net = fiveNet, alphaOrder = 2)\n# alphaOrder gives time lag\n\n\n\n\nCode\nplot(fiveVTS[,1], ylab = \"Node A time series\")\nlines(fitted(five_gnar)[,1], col = 2)\n\n\n\n\n\nThe residuals are examined as follows\n\n\nCode\nlayout(matrix(c(1,2), 2, 1))\npar(mar = c(4.1, 4.1, 1, 2.1), cex.axis = 0.9)\nplot(ts(residuals(five_gnar)[,1]), ylab = \"model residuals\")\nhist(residuals(five_gnar)[,1], main = \"\", xlab = \"model residuals\")\n\n\n\n\n\nShow that GNAR does not have a problem with missing data,\n\n\nCode\nfiveVTS0 <- fiveVTS\n\nfiveVTS0[50:150, 3] <- NA\nnafit <- GNARfit(vts = fiveVTS0, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nlayout(matrix(c(1, 2), 2, 1))\npar(mar = c(4.1, 4.1, 0.75, 2.1), cex.axis = 0.75)\nplot(ts(fitted(nafit)[, 3]), ylab = \"Node C fitted values\")\npar(mar = c(4.1, 4.1, 0.75, 2.1), cex.axis = 0.75)\nplot(ts(fitted(nafit)[, 4]), ylab = \"Node D fitted values\")"
  },
  {
    "objectID": "observable/observable.html#observable",
    "href": "observable/observable.html#observable",
    "title": "14  Observable Testing",
    "section": "14.1 Observable",
    "text": "14.1 Observable\n\n\nCode\ndata = FileAttachment(\"penguins.csv\").csv({ typed: true })\nviewof bill_length_min = Inputs.range(\n  [32, 50], \n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"], \n  { value: [\"Torgersen\", \"Biscoe\"], \n    label: \"Islands:\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfiltered = data.filter(function(penguin) {\n  return bill_length_min < penguin.bill_length_mm &&\n         islands.includes(penguin.island);\n})\n\n\n\n\n\n\n\n\n\nCode\nfiltered\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({\n  marks:[\n    Plot.dot(filtered, {x: \"body_mass_g\", y: \"flipper_length_mm\", fill: \"species\"})\n    ],\n  color: {legend: true},\n  marginLeft: 100,\n})"
  },
  {
    "objectID": "spatial/carbayes.html",
    "href": "spatial/carbayes.html",
    "title": "15  CARBayes",
    "section": "",
    "text": "16 A simple example\nWe’ll use the lollipop graph, and suppose that we observe some random process on the graph."
  },
  {
    "objectID": "spatial/carbayes.html#cressie-parameterization",
    "href": "spatial/carbayes.html#cressie-parameterization",
    "title": "15  CARBayes",
    "section": "16.1 Cressie parameterization",
    "text": "16.1 Cressie parameterization\nCressie’s parameterization gives another parameter to control the degree of spatial averaging in the\nThe variable is \\rho, which will take values between 0 < \\rho 1. As \\rho \\rightarrow 0, then the spatial variability disappears.\n\n\\begin{aligned}\n\\phi_i | \\phi_{-i} \\sim N(0, (I-\\rho C)^{-1}D)\n\\end{aligned}\n\nLet\n\n\\begin{aligned}\nY = \\mu + \\phi\n\\end{aligned}\n\n\n\nCode\n# CAR(rho, sigma2)\ncressieCAR <- function(nsamples = 10, W, rho = .5, sigma2=1) {\n  DW <- diag(rowSums(W))\n  Q <- (DW - rho * W) / sigma2\n  print(solve(Q))\n  mvtnorm::rmvnorm(nsamples, sigma = solve(Q))\n}\n\nlibrary(tidyverse)\ncressie_sim <- data.frame(rho = c(0, .1, .4, .8, .9)) %>% \n  rowwise() %>% \n  mutate(x = list(data.frame(cressieCAR(10, W, rho, sigma2 = 2))))\n\n\n     [,1] [,2]      [,3] [,4]\n[1,]    1    0 0.0000000    0\n[2,]    0    1 0.0000000    0\n[3,]    0    0 0.6666667    0\n[4,]    0    0 0.0000000    2\n            [,1]        [,2]       [,3]        [,4]\n[1,] 1.004365710 0.051984758 0.03532945 0.003532945\n[2,] 0.051984758 1.004365710 0.03532945 0.003532945\n[3,] 0.035329447 0.035329447 0.67125949 0.067125949\n[4,] 0.003532945 0.003532945 0.06712595 2.006712595\n           [,1]       [,2]      [,3]       [,4]\n[1,] 1.08901515 0.25568182 0.1893939 0.07575758\n[2,] 0.25568182 1.08901515 0.1893939 0.07575758\n[3,] 0.18939394 0.18939394 0.7575758 0.30303030\n[4,] 0.07575758 0.07575758 0.3030303 2.12121212\n          [,1]      [,2]     [,3]      [,4]\n[1,] 1.8777614 1.1634757 1.030928 0.8247423\n[2,] 1.1634757 1.8777614 1.030928 0.8247423\n[3,] 1.0309278 1.0309278 1.546392 1.2371134\n[4,] 0.8247423 0.8247423 1.237113 2.9896907\n         [,1]     [,2]     [,3]     [,4]\n[1,] 3.120493 2.430838 2.281369 2.053232\n[2,] 2.430838 3.120493 2.281369 2.053232\n[3,] 2.281369 2.281369 2.788340 2.509506\n[4,] 2.053232 2.053232 2.509506 4.258555\n\n\nCode\ncressie_sim  %>% unnest_wider(x) %>% unnest(X1:X4) %>% \n  pivot_longer(X1:X4) %>% \n  ggplot(aes(name, value)) +\n  geom_boxplot()\n\n\n\n\n\nCode\nW <- make_star(5, mode = \"undirected\") %>% as_adj()\n\nQcar(W) # not symmetric when the weight matrix is not symmetric\n\n\n5 x 5 sparse Matrix of class \"dgCMatrix\"\n                   \n[1,]  4 -1 -1 -1 -1\n[2,] -1  1  .  .  .\n[3,] -1  .  1  .  .\n[4,] -1  .  .  1  .\n[5,] -1  .  .  .  1"
  },
  {
    "objectID": "spatial/carbayes.html#conclusions-about-besag-model",
    "href": "spatial/carbayes.html#conclusions-about-besag-model",
    "title": "15  CARBayes",
    "section": "16.2 Conclusions about Besag model",
    "text": "16.2 Conclusions about Besag model\nI don’t think using the graph laplacian for spatial random effects is very powerful, using a laplacian matrix for the normal distribution does not induce a very strong effect, and also presupposes the spatial dependencies in the model. I think that it’s not very economical use of the parameters in the model. There’s not nearly the degree of spatial averaging that you’d expect"
  },
  {
    "objectID": "spatial/carbayes.html#simulations-of-car-models",
    "href": "spatial/carbayes.html#simulations-of-car-models",
    "title": "15  CARBayes",
    "section": "16.3 Simulations of CAR models",
    "text": "16.3 Simulations of CAR models\nHere we use libraries to visualize the process, and thus the richness of our modeling space.\n\n\nCode\nlibrary(mclcar)\n\n\n\n\nCode\nset.seed(33)\nn.torus <- 10\nrho <- 0.2\nsigma <- 1.5\nprec <- 1/sigma\nbeta <- c(1, 1)\nXX <- cbind(rep(1, n.torus^2), sample(log(1:n.torus^2)/5))\nmydata1 <- CAR.simTorus(n1 = n.torus, n2 = n.torus, rho = rho, prec = prec)\n\nCAR.simTorus\n\n\nfunction (n1, n2, rho, prec) \n{\n    x <- list(c(2, n2), rep(1, 2))\n    Wx <- circulant.spam(x, n = n2)\n    Ix <- diag.spam(1, n1)\n    W <- kronecker(Ix, Wx) + kronecker(Wx, Ix)\n    ew <- eigen(W, only.values = TRUE)$values\n    min <- 1/min(ew)\n    max <- 1/max(ew)\n    if (rho < min | rho > max) \n        stop(paste(\"rho should be within\", min, max))\n    I <- diag.spam(1, n1 * n2)\n    Q <- as.spam(prec * (I - rho * W))\n    cholR <- chol.spam(Q, pivot = \"MMD\", memory = list(nnzcolindices = 6.25 * \n        n1 * n2))\n    X <- backsolve(cholR, rnorm(n1 * n2))\n    W <- as.matrix(W)\n    result <- list(W = W, X = X)\n    return(result)\n}\n<bytecode: 0x7fda99e945c0>\n<environment: namespace:mclcar>\n\n\nCode\nWmat <- mydata1$W\n\nmydata2 <- CAR.simWmat(rho = .2, prec = prec, W = Wmat)\n\n\n\n\nCode\ntmp <- graph_from_adjacency_matrix(Wmat, mode = \"undirected\")\nplot(tmp, layout = layout.grid(tmp))"
  },
  {
    "objectID": "spatial/carbayes.html#bym-model",
    "href": "spatial/carbayes.html#bym-model",
    "title": "15  CARBayes",
    "section": "16.4 BYM Model",
    "text": "16.4 BYM Model\nTo better fit the data, we split up the spatial effects into spatial components and independent errors for each of the locations. Now it’s more like a mixed model, where we can add some level of independent noise to each observation"
  },
  {
    "objectID": "bootstrap/bootstrap.html#estimating-bias",
    "href": "bootstrap/bootstrap.html#estimating-bias",
    "title": "16  Bootstrap",
    "section": "16.1 Estimating Bias",
    "text": "16.1 Estimating Bias\n\n\\begin{aligned}\n\\text{Bias}_F(\\hat \\theta, \\theta) = E_F[\\hat \\theta] - \\theta\n\\end{aligned}\n\nWe note that \\hat\\theta = s(\\mathbf{x}) is function of the data \\hat\\theta = s(\\mathbf{x}), whereas \\theta = t(F) is a function of the underlying true distribution. In order to use bootstrap to estimate the bias of an estimator, we use average of bootstrap samples to estimate the expected value, while we use the empirical distribution to estimate the true parameter.\n\n16.1.1 Example (sample standard deviation)\n\n\nCode\nset.seed(1)\nx <- rnorm(30) # \"Empirical Distribution\" from normal distribution\nB <- 1000 # Number of bootstrap samples to take.\ngen_boot <- function(x) {\n  xb <- sample(x, replace = TRUE)\n  sum((xb - mean(xb))^2 / length(xb)) \n}\n\n# bootstrap expected value\nthetahat <- mean(replicate(B, gen_boot(x)))\ntheta <- sum((x - mean(x))^2 / length(x)) # Distributional \"real value\"\n\ncbind(thetahat - theta,\n      1/30) # Pretty close to theoretical \"bias\"\n\n\n           [,1]       [,2]\n[1,] -0.0298494 0.03333333\n\n\nI’m sure if you increase the number of replicates"
  },
  {
    "objectID": "causal/causal.html#propensity-scores",
    "href": "causal/causal.html#propensity-scores",
    "title": "17  Causal Statistics",
    "section": "17.1 Propensity Scores",
    "text": "17.1 Propensity Scores\nWhat are propensity scores? A propensity score is defined as the probability of a treatment assignment conditional on some background observed covariates.\nThis allows one to mimic some of the conditions of a randomized controlled trial. In a randomized control trial, one can be fairly certain that each group is balanced due to the law of large numbers.\n\n\\begin{aligned}\n\\Pr(T = 1 \\, | X=x )\n\\end{aligned}\n For more background information see Matching Methods."
  },
  {
    "objectID": "causal/causal.html#matching",
    "href": "causal/causal.html#matching",
    "title": "17  Causal Statistics",
    "section": "17.2 Matching",
    "text": "17.2 Matching\nThere are many ways to match:"
  },
  {
    "objectID": "causal/causal.html#r-implementation",
    "href": "causal/causal.html#r-implementation",
    "title": "17  Causal Statistics",
    "section": "17.3 R Implementation",
    "text": "17.3 R Implementation\nThe package MatchIt provides the function matchit to attempt matching of covariates automatically. There are many methods to matching covariates. The main page of the package can be found here. The package also uses Zelig for additional features."
  },
  {
    "objectID": "color/color.html#viewing-color-palettes",
    "href": "color/color.html#viewing-color-palettes",
    "title": "Appendix A — Colors in R",
    "section": "A.1 Viewing Color Palettes",
    "text": "A.1 Viewing Color Palettes\n\n\nCode\nhcl.pals(type = \"sequential\")\n\n\n [1] \"Grays\"         \"Light Grays\"   \"Blues 2\"       \"Blues 3\"      \n [5] \"Purples 2\"     \"Purples 3\"     \"Reds 2\"        \"Reds 3\"       \n [9] \"Greens 2\"      \"Greens 3\"      \"Oslo\"          \"Purple-Blue\"  \n[13] \"Red-Purple\"    \"Red-Blue\"      \"Purple-Orange\" \"Purple-Yellow\"\n[17] \"Blue-Yellow\"   \"Green-Yellow\"  \"Red-Yellow\"    \"Heat\"         \n[21] \"Heat 2\"        \"Terrain\"       \"Terrain 2\"     \"Viridis\"      \n[25] \"Plasma\"        \"Inferno\"       \"Rocket\"        \"Mako\"         \n[29] \"Dark Mint\"     \"Mint\"          \"BluGrn\"        \"Teal\"         \n[33] \"TealGrn\"       \"Emrld\"         \"BluYl\"         \"ag_GrnYl\"     \n[37] \"Peach\"         \"PinkYl\"        \"Burg\"          \"BurgYl\"       \n[41] \"RedOr\"         \"OrYel\"         \"Purp\"          \"PurpOr\"       \n[45] \"Sunset\"        \"Magenta\"       \"SunsetDark\"    \"ag_Sunset\"    \n[49] \"BrwnYl\"        \"YlOrRd\"        \"YlOrBr\"        \"OrRd\"         \n[53] \"Oranges\"       \"YlGn\"          \"YlGnBu\"        \"Reds\"         \n[57] \"RdPu\"          \"PuRd\"          \"Purples\"       \"PuBuGn\"       \n[61] \"PuBu\"          \"Greens\"        \"BuGn\"          \"GnBu\"         \n[65] \"BuPu\"          \"Blues\"         \"Lajolla\"       \"Turku\"        \n[69] \"Hawaii\"        \"Batlow\"       \n\n\nCode\nhcl.colors(256, palette = \"ag_sunset\")\n\n\n  [1] \"#4B1D91\" \"#4D1C92\" \"#4F1C92\" \"#511B92\" \"#531B93\" \"#551B93\" \"#571A93\"\n  [8] \"#591A94\" \"#5B1A94\" \"#5C1994\" \"#5E1995\" \"#601895\" \"#611895\" \"#631895\"\n [15] \"#651796\" \"#661796\" \"#681696\" \"#6A1697\" \"#6B1697\" \"#6D1597\" \"#6E1597\"\n [22] \"#701598\" \"#711498\" \"#731498\" \"#741498\" \"#761399\" \"#771399\" \"#791399\"\n [29] \"#7A1299\" \"#7C1299\" \"#7D129A\" \"#7F119A\" \"#80119A\" \"#81119A\" \"#83109A\"\n [36] \"#84109B\" \"#86109B\" \"#87109B\" \"#88109B\" \"#8A109B\" \"#8B0F9B\" \"#8C0F9B\"\n [43] \"#8E0F9C\" \"#8F0F9C\" \"#900F9C\" \"#910F9C\" \"#930F9C\" \"#940F9C\" \"#950F9C\"\n [50] \"#960F9C\" \"#980F9C\" \"#990F9C\" \"#9A109C\" \"#9B109C\" \"#9D109C\" \"#9E109D\"\n [57] \"#9F119D\" \"#A0119D\" \"#A1119D\" \"#A3119D\" \"#A4129D\" \"#A5129D\" \"#A6139D\"\n [64] \"#A7139C\" \"#A9149C\" \"#AA149C\" \"#AB159C\" \"#AC159C\" \"#AD169C\" \"#AE169C\"\n [71] \"#AF179C\" \"#B0179C\" \"#B2189C\" \"#B3199C\" \"#B4199C\" \"#B51A9C\" \"#B61B9B\"\n [78] \"#B71B9B\" \"#B81C9B\" \"#B91D9B\" \"#BA1D9B\" \"#BB1E9B\" \"#BC1F9A\" \"#BD209A\"\n [85] \"#BE209A\" \"#BF219A\" \"#C0229A\" \"#C12399\" \"#C22499\" \"#C32499\" \"#C42599\"\n [92] \"#C52698\" \"#C62798\" \"#C72898\" \"#C82897\" \"#C92997\" \"#CA2A97\" \"#CB2B96\"\n [99] \"#CC2C96\" \"#CD2D96\" \"#CE2E95\" \"#CF2E95\" \"#D02F95\" \"#D13094\" \"#D23194\"\n[106] \"#D23293\" \"#D33393\" \"#D43493\" \"#D53592\" \"#D63692\" \"#D73691\" \"#D83791\"\n[113] \"#D93890\" \"#D93990\" \"#DA3A8F\" \"#DB3B8F\" \"#DC3C8E\" \"#DD3D8E\" \"#DE3E8D\"\n[120] \"#DE3F8D\" \"#DF408C\" \"#E0418B\" \"#E1428B\" \"#E2438A\" \"#E24489\" \"#E34589\"\n[127] \"#E44588\" \"#E54687\" \"#E54787\" \"#E64886\" \"#E74985\" \"#E84A85\" \"#E84B84\"\n[134] \"#E94C83\" \"#EA4D82\" \"#EB4E82\" \"#EB4F81\" \"#EC5080\" \"#EC527F\" \"#EC537E\"\n[141] \"#ED557E\" \"#ED567D\" \"#ED587C\" \"#ED597B\" \"#EE5A7B\" \"#EE5C7A\" \"#EE5D79\"\n[148] \"#EE5E78\" \"#EE6078\" \"#EF6177\" \"#EF6276\" \"#EF6475\" \"#EF6575\" \"#EF6674\"\n[155] \"#F06873\" \"#F06972\" \"#F06A72\" \"#F06B71\" \"#F06D70\" \"#F16E6F\" \"#F16F6F\"\n[162] \"#F1706E\" \"#F1716D\" \"#F1736D\" \"#F1746C\" \"#F1756B\" \"#F2766B\" \"#F2776A\"\n[169] \"#F27969\" \"#F27A69\" \"#F27B68\" \"#F27C68\" \"#F27D67\" \"#F27F66\" \"#F28066\"\n[176] \"#F28165\" \"#F28265\" \"#F38364\" \"#F38464\" \"#F38563\" \"#F38763\" \"#F38862\"\n[183] \"#F38962\" \"#F38A61\" \"#F38B61\" \"#F38C61\" \"#F38D60\" \"#F38E60\" \"#F38F5F\"\n[190] \"#F3915F\" \"#F3925F\" \"#F3935F\" \"#F3945E\" \"#F3955E\" \"#F3965E\" \"#F3975E\"\n[197] \"#F3985E\" \"#F3995E\" \"#F39A5E\" \"#F39B5D\" \"#F39D5D\" \"#F39E5D\" \"#F29F5D\"\n[204] \"#F2A05E\" \"#F2A15E\" \"#F2A25E\" \"#F2A35E\" \"#F2A45E\" \"#F2A55E\" \"#F2A65F\"\n[211] \"#F2A75F\" \"#F2A85F\" \"#F1A960\" \"#F1AA60\" \"#F1AB61\" \"#F1AC61\" \"#F1AD62\"\n[218] \"#F1AE62\" \"#F1AF63\" \"#F1B063\" \"#F0B164\" \"#F0B265\" \"#F0B465\" \"#F0B566\"\n[225] \"#F0B667\" \"#F0B768\" \"#EFB869\" \"#EFB96A\" \"#EFBA6A\" \"#EFBB6B\" \"#EFBC6C\"\n[232] \"#EEBD6E\" \"#EEBE6F\" \"#EEBE70\" \"#EEBF71\" \"#EDC072\" \"#EDC173\" \"#EDC275\"\n[239] \"#EDC376\" \"#EDC477\" \"#ECC579\" \"#ECC67A\" \"#ECC77C\" \"#ECC87D\" \"#EBC97F\"\n[246] \"#EBCA81\" \"#EBCB82\" \"#EACC84\" \"#EACD86\" \"#EACE88\" \"#EACF8A\" \"#E9D08C\"\n[253] \"#E9D18F\" \"#E8D191\" \"#E8D295\" \"#E7D39A\"\n\n\nCode\nplot(0, \n     type = \"n\",\n     axes = F,\n     xlab = \"\", ylab = \"\",\n     xlim = c(0, 1),\n     ylim = c(0, 1),\n     xaxs = \"i\",\n     yaxs = \"i\",\n     omi = rep(0, 4),\n     oma = rep(0, 4),\n     mar = rep(0, 4),\n     frame.plot = F)\nrect(0, .5, .8, 1, col = terrain.colors(256))\n\n\n\n\n\n\n\nCode\ngrid.raster(seq(0, 1, .01))\n\n\n\n\n\n\n\nCode\n# you can color the matrix directly as well, and generate the raster that way\nredGradient <- matrix(hcl(0, 80, seq(50, 80, 10)),\n                      nrow=4, ncol=5)\n\ngrid.raster(redGradient)"
  }
]