[
  {
    "objectID": "index.html#quarto",
    "href": "index.html#quarto",
    "title": "Personal Documentation",
    "section": "0.1 Quarto",
    "text": "0.1 Quarto\nWelcome to my documentation."
  },
  {
    "objectID": "advanced/advanced.html#plotting-in-r",
    "href": "advanced/advanced.html#plotting-in-r",
    "title": "2  Advanced R",
    "section": "2.1 Plotting in R",
    "text": "2.1 Plotting in R\nThe interface of plotting is powered by the package grid\n\n2.1.1 grid package\n\ngrobs : graphical object\nviewports: defining regions where to plot\n\n\n\nCode\ngrid.circle(x=seq(0.1, 0.9, length=100), # position\n            y=0.5 + 0.4*sin(seq(0, 2*pi, length=100)),\n            r=abs(0.1*cos(seq(0, 2*pi, length=100))))\n\n\n\n\n\n\nviewports\nviewports are “drawing contexts”, basically defining regions of plotting.\n\n\nCode\n# An easy first example\ngrid.newpage()\nvp1 <- viewport(x = .5, y = .5,\n                height = .9, width = .9,\n                xscale=c(0,1), yscale=c(0,1))\n\nfor (i in 1:5) {\n pushViewport(vp1)\n grid.rect()\n}\n\n\n\n\n\nCode\n# current.vpTree() # check the drawing context\n\n\nWhat’s going on here?!\nThe viewports are stored in a tree structure, and the active context of the viewport\nExample of recreating a scatter plot\n\n\nCode\nx <- runif(10)\ny <- runif(10)\nplot(x, y)\n\n\n\n\n\nLooking above, we can decompose the plot into 9 sections,\n\n\nCode\n# first layer\nmain_vp <- viewport(layout = grid.layout(3, 3,\n                                         widths = unit(c(5, 1, 2), c(\"lines\", \"null\", \"lines\")),\n                                         heights = unit(c(5, 1, 2), c(\"lines\", \"null\", \"lines\"))),\n                    name = \"main\")\n\n# second layer \nmr_vp <- viewport(layout.pos.row = 2, layout.pos.col = 3, name = \"margin_right\")\nmb_vp <- viewport(layout.pos.row = 3, layout.pos.col = 2, name = \"margin_bottom\")\nmt_vp <- viewport(layout.pos.row = 1, layout.pos.col = 2, name = \"margin_top\")\nml_vp <- viewport(layout.pos.row = 2, layout.pos.col = 3, name = \"margin_left\")\ncenter_vp <- viewport(layout.pos.row = 2, layout.pos.col = 2, name = \"center\",\n                      xscale = extendrange(x),\n                      yscale = extendrange(y))\n\n# create the viewport\nsplot <- vpTree(main_vp, vpList(mr_vp, mb_vp, mt_vp, ml_vp, center_vp))\nsplot # Currently just the description, and not associated with the graphics device\n\n\nviewport[main]->(viewport[margin_right], viewport[margin_bottom], viewport[margin_top], viewport[margin_left], viewport[center]) \n\n\nCode\n# viewport of current device\ncurrent.vpTree() # current vp tree of graphic device\n\n\nviewport[ROOT] \n\n\nCode\ncurrent.viewport() # This is the current viewport LAYER (in the tree) of our graphics device\n\n\nviewport[ROOT] \n\n\nCode\n# In order to make the viewport splot active, we must add it to the vpTree of our device\npushViewport(splot)\ncurrent.vpTree() # notice how our tree has changed\n\n\nviewport[ROOT]->(viewport[main]->(viewport[margin_right], viewport[margin_top], viewport[margin_left], viewport[margin_bottom], viewport[center])) \n\n\nCode\ncurrent.viewport() # our current layer is center\n\n\nviewport[center] \n\n\nCode\ngrid.points(x, y) # drawn to center, for some reason this isn't showing up\ngrid.xaxis()\ngrid.yaxis()\ngrid.rect()\n\n# navigate to bottom margin\nseekViewport(\"margin_bottom\")\ngrid.text(\"Random X\", y=unit(1, \"lines\"))\n\nseekViewport(\"margin_left\")\ngrid.text(\"Random Y\", x=unit(1, \"lines\"), rot = 90)\n\n\n\n\n\nRun all the code all at once, rmarkdown does some weird stuff with viewports\n\n\nCode\npushViewport(viewport())\nupViewport()\npushViewport(viewport(name = \"A\"))\nupViewport()\npushViewport(viewport(name = \"B\"))\nupViewport()\n\n\n\n\n\nCode\ncurrent.vpTree()\n\n\nviewport[ROOT]->(viewport[GRID.VP.2], viewport[A], viewport[B])"
  },
  {
    "objectID": "advanced/advanced.html#gtable",
    "href": "advanced/advanced.html#gtable",
    "title": "2  Advanced R",
    "section": "2.2 gtable",
    "text": "2.2 gtable\nUnofficial Guide\nThe important structures of gtable object are\n\ngrobs:d\n\n\n\nCode\n# creating a gtable\ngtable(unit(1:3, c(\"cm\")),\n       unit(5, \"cm\"))\n\n\nTableGrob (1 x 3) \"layout\": 0 grobs\n\n\nCode\na <- rectGrob(gp = gpar(fill = \"red\"))\n\na <- rectGrob(gp = gpar(fill = \"red\"))\nb <- grobTree(rectGrob(), textGrob(\"new\\ncell\"))\nc <- ggplotGrob(qplot(1:10,1:10))\nd <- linesGrob()\nmat <- matrix(list(a, b, c, d), nrow = 2)\ng <- gtable_matrix(name = \"demo\", grobs = mat, \n                   widths = unit(c(2, 4), \"cm\"), \n                   heights = unit(c(2, 5), c(\"in\", \"lines\")))\ng\n\n\nTableGrob (2 x 2) \"demo\": 4 grobs\n  z     cells name                 grob\n1 1 (1-1,1-1) demo   rect[GRID.rect.14]\n2 2 (2-2,1-1) demo gTree[GRID.gTree.15]\n3 3 (1-1,2-2) demo       gtable[layout]\n4 4 (2-2,2-2) demo lines[GRID.lines.57]"
  },
  {
    "objectID": "advanced/advanced.html#gridextra",
    "href": "advanced/advanced.html#gridextra",
    "title": "2  Advanced R",
    "section": "2.3 gridextra",
    "text": "2.3 gridextra\n\n\nCode\ndummy_grob <- function(id)  {\n  grobTree(rectGrob(gp=gpar(fill=id, alpha=0.5)), textGrob(id))\n}\ngs <- lapply(1:9, dummy_grob)\ngrid.arrange(ncol=4, grobs=gs, \n               top=\"top\\nlabel\", bottom=\"bottom\\nlabel\", \n               left=\"left\\nlabel\", right=\"right\\nlabel\")"
  },
  {
    "objectID": "advanced/advanced.html#r-macos",
    "href": "advanced/advanced.html#r-macos",
    "title": "2  Advanced R",
    "section": "2.4 R MacOS",
    "text": "2.4 R MacOS\n\n2.4.1 Matrix Operations\n\nWhich BLAS library is used by R?\nUsing the faster veclib dynamically linked library\nA tutorial/discussion from 2019 about rebuilding R binary with openBLAS and openMP\n\nThere are two common libraries for matrix operations, LAPACK and BLAS. There are many varieties of these two libraries,\n\n\nCode\nsessionInfo() # Shows library used for matrix products\n\n\nR version 4.2.0 (2022-04-22)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur/Monterey 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] grid      stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] gridExtra_2.3   gtable_0.3.0    forcats_0.5.1   stringr_1.4.0  \n [5] dplyr_1.0.9     purrr_0.3.4     readr_2.1.2     tidyr_1.2.0    \n [9] tibble_3.1.8    ggplot2_3.3.6   tidyverse_1.3.1\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.1.2  xfun_0.31         haven_2.5.0       colorspace_2.0-3 \n [5] vctrs_0.4.1       generics_0.1.3    htmltools_0.5.3   yaml_2.3.5       \n [9] utf8_1.2.2        rlang_1.0.6       pillar_1.8.0      glue_1.6.2       \n[13] withr_2.5.0       DBI_1.1.3         dbplyr_2.2.0      modelr_0.1.8     \n[17] readxl_1.4.0      lifecycle_1.0.1   munsell_0.5.0     cellranger_1.1.0 \n[21] rvest_1.0.2       htmlwidgets_1.5.4 codetools_0.2-18  evaluate_0.15    \n[25] labeling_0.4.2    knitr_1.39        tzdb_0.3.0        fastmap_1.1.0    \n[29] fansi_1.0.3       broom_0.8.0       backports_1.4.1   scales_1.2.0     \n[33] jsonlite_1.8.0    farver_2.1.1      fs_1.5.2          hms_1.1.1        \n[37] digest_0.6.29     stringi_1.7.8     cli_3.4.1         tools_4.2.0      \n[41] magrittr_2.0.3    crayon_1.5.1      pkgconfig_2.0.3   ellipsis_0.3.2   \n[45] xml2_1.3.3        reprex_2.0.1      lubridate_1.8.0   assertthat_0.2.1 \n[49] rmarkdown_2.14    httr_1.4.3        rstudioapi_0.13   R6_2.5.1         \n[53] compiler_4.2.0   \n\n\nOf note, R binary versus the CRAN version of the R binary have different libraries bundled together. For older versions of Rstudio, there is a BLAS library that is provided with Apple that sometimes is faster. Ultimately, R will look for a symlinked library in the lib/ folder.\n\n\nCode\n# Install openblas\nbrew install openblas\n\n# Some potentially useful symlinking commands\nln -sf /usr/local/opt/openblas/lib/libopenblas.dylib /usr/local/Cellar/r/4.1.2/lib/R/lib/libRblas.dylib\nln -sf /usr/local/opt/openblas/lib/libopenblas.dylib /usr/local/Cellar/r/4.1.2/lib/libRblas.dylib"
  },
  {
    "objectID": "advanced/advanced.html#metaprogramming",
    "href": "advanced/advanced.html#metaprogramming",
    "title": "2  Advanced R",
    "section": "2.5 Metaprogramming",
    "text": "2.5 Metaprogramming\n\n2.5.1 Expressions\n\nconstants\nsymbols\n\n\n\nCode\nlibrary(rlang)\n\n\n\nAttaching package: 'rlang'\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int,\n    flatten_lgl, flatten_raw, invoke, splice\n\n\nCode\n# two ways of creating a symbol\nsym(\"x\") # symbol\n\n\nx\n\n\nCode\nclass(expr(x)) # symbol\n\n\n[1] \"name\"\n\n\nCode\nquo(x)\n\n\n<quosure>\nexpr: ^x\nenv:  global\n\n\nCode\n?quo\n\n\nHelp on topic 'quo' was found in the following packages:\n\n  Package               Library\n  tidyselect            /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n  rlang                 /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n  ggplot2               /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n  dplyr                 /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n\nUsing the first match ...\n\n\nCode\nexpr(\"x\") # expression\n\n\n[1] \"x\"\n\n\nCode\nas_string(sym(\"x\")) \n\n\n[1] \"x\"\n\n\nCode\nas_name(sym(\"x\")) # character\n\n\n[1] \"x\"\n\n\nCode\nclass(as_name(\"x\")) # character\n\n\n[1] \"character\"\n\n\n\n\n2.5.2 Quosures\nQuosures are special type of defused expression that also keeps track of the original context the expression was written in.\n\nquo\n`quo is used to capture local expressions, and also track the environment the expression was written.\n\n\nenquo\n\n{% raw %} enquo is used to defuse function arguments, the {{ is short for this common pattern. The arguments need to be defused because otherwise R will try to evaluate the expression in its original environment. {% endraw %}\n\n\nCode\nmy_function <- function(var) {\n  var <- enquo(var)\n  their_function(!!var)\n}\n\n# Equivalently\nmy_function <- function(var) {\n  their_function({{ var }})\n}"
  },
  {
    "objectID": "approximation/approximation.html#edgeworth-expansion",
    "href": "approximation/approximation.html#edgeworth-expansion",
    "title": "3  Approximation",
    "section": "3.1 Edgeworth Expansion",
    "text": "3.1 Edgeworth Expansion\nEdgeworth expansion is an improvement to the normal approximation that includes skewness (degree 2) and kurtosis (degree three). The first degree approximation amounts to the normal approximation.\nSuppose we have a general distribution X_i with mean \\mu and variance \\sigma^2. If we standardize the sum, we have a statistic that is approximately N(0, 1)\n\n\\begin{aligned}\nZ = \\frac{1}{\\sqrt{n}} \\sum_i \\frac{X_i - \\mu}{\\sigma}\n\\end{aligned}\n Edgeworth expansion of the pdf of Z is\n\n\\begin{aligned}\nf_Z(z) = \\underbrace{\\underbrace{\\underbrace{\\phi(z)}_{\\text{degree 1}} + \\frac{\\lambda_3}{6\\sqrt{n}}H_3(z)\\phi(z)}_{\\text{degree 2}} + \\left(\\frac{\\lambda_4}{24n}H_4(z)\\phi(z) + \\frac{\\lambda_3^2}{72n}H_6(z)\\phi(z)\\right)}_{\\text{degree 3}} + o\\left(\\frac{1}{n}\\right)\n\\end{aligned}\n where:\n\n\\lambda_r denotes the cumulant of Z, related to the cumulants of X_i by \\lambda_r = \\kappa_r / \\sigma^r if cumulants of X_i are represented by \\kappa_r\nH_r are hermite polynomials.\n\nWe look at how well this approximation works for the sum of chisq random variables. The cumulants of the chisq variables.\nSuppose X_i \\sim \\chi^2_2\nIf we study the mean, the true distribution is \\bar{X_i} \\sim \\Gamma(\\frac{n * df}{2}, \\frac{n}{2}).\n\n\nCode\nset.seed(1)\nn <- 3\n\n# Sampled distribution\nrand_x <- rchisq(1000, 2) + rchisq(1000, 2) + rchisq(1000, 2)\nrand_x_mean <- rand_x / n\n\n# True distribution\n# Gamma(3, 3/2)\nx <- seq(0, 10, by=.1)\ntrue_y <- dgamma(x, 3, 3/2)\n\n# Normal Approximation\n# N(2, 4/3)\nnorm_approx_y <- dnorm(x, mean = 2, sd = sqrt(4/3))\n\n\nFor the edgeworth expansion, since we’re studying the mean \\bar X, this is a transformation of Z, that is,\n\n\\begin{aligned}\n\\bar X = \\frac{(Z\\sqrt{n\\sigma^2} + n\\mu)}{n}\n\\end{aligned}\n so the inverse formula that we use in the pdf transformation formula,\n\n\\begin{aligned}\nZ = \\frac{n\\bar X - n\\mu}{\\sqrt{n\\sigma^2}}\n\\end{aligned}\n The transformation formula would then say,\n\n\\begin{aligned}\nf_{\\bar X}(x) &= \\underbrace{f_Z\\left(\\frac{nx - n\\mu}{\\sqrt{n\\sigma^2}}\\right)}_{\\text{approximated by Edgeworth}} \\left| \\frac{\\sqrt{n}}{\\sigma}\\right|\n\\end{aligned}\n\n\n\nCode\nherm <- Vectorize(function(x, degree = 1 ) {\n  switch(degree,\n         x,\n         x^2 -1,\n         x^3 - 3*x,\n         x^4 - 6*x^2 + 3,\n         x^5 - 10 * x^3 + 15 * x,\n         x^6 - 15*x^4 + 45*x^2 - 15)\n}, \"x\")\n\n\n# Edgeworth expansion, degree 2\nedgeworth_mean2 <- function(x, n, mu, sigma2, lambda3){\n  z <- sqrt(n) * (x - mu) / sqrt(sigma2)\n  sqrt(n / sigma2) * dnorm(z) * (1 + lambda3 / (6 * sqrt(n)) * herm(z, 3))\n}\n\n# Edgeworth expansion, degree 3\nedgeworth_mean3 <- function(x, n, mu, sigma2, lambda3, lambda4) {\n  z <- sqrt(n) * (x - mu) / sqrt(sigma2) # transformation\n  sqrt(n / sigma2) * # transformation scaling\n    dnorm(z) * (1 +  # degree 1 terms\n      lambda3 / (6 * sqrt(n)) * herm(z, 3) + # degree 2 terms\n      lambda4 / (24 * n) * herm(z, 4) + # degree 3 terms\n      lambda3^2 / (72 * n) * herm(z, 6)) # degree 3 terms\n}\n\n\n\n\nCode\n# Creating unified plot with all the \nhist(rand_x_mean, breaks=50, freq = FALSE, main = \"Mean of chisq (Edgeworth)\", xlab = \"x\")\nlines(x, true_y, lwd = 2)\nlines(x, norm_approx_y, lty = 2, col = 2, lwd = 2)\nlines(x, edgeworth_mean2(x, 3, 2, 4, 2), lty = 2, col = 3, lwd = 2)\nlines(x, edgeworth_mean3(x, 3, 2, 4, 2, 6), lty = 2, col = 4, lwd = 2)\nlegend(\"topright\", \n       legend = c(\"True Distribution\", \n                  \"Normal Approximation\", \n                  \"Edgeworth (2 deg)\", \n                  \"Edgeworth (3 deg)\"),\n       col = c(1, 2, 3, 4), \n       lty = c(1, 2, 2, 2))\n\n\n\n\n\n\n3.1.1 R Packages\n\nEW\n\ndoes finite differencing for the moment estimators\nonly two functions, edgeworth and saddlepoint approximations\n\nEQL (extended quasi likelihood)\n\ndoes extended quasi-likelihood and saddlepoint approximations too\nin my opinion, very well written and clear readable code. Used source code to figure out my own errors.\n\n\n\n\nCode\nlibrary(EQL)\n# Usage for our example\new <- edgeworth(x, n = 3, mu = 2, sigma2 = 4, rho3 = 2, rho4 = 6, type = \"mean\", deg = 3)\n\n# They match with our results\nhist(rand_x_mean, breaks=50, freq = FALSE, main = \"Mean of chisq (Edgeworth)\", xlab = \"x\")\nlines(x, true_y)\nlines(x, edgeworth_mean3(x, 3, 2, 4, 2, 6), col = 2)\nlines(x, ew$approx, lty = 2, lwd = 2, col = 3)\n\n\n\n\n\n\n\n3.1.2 Further Resources\n\nhttp://personal.psu.edu/drh20/asymp/fall2004/lectures/edgeworth.pdf\nWriting Projects"
  },
  {
    "objectID": "approximation/approximation.html#saddlepoint-approximation",
    "href": "approximation/approximation.html#saddlepoint-approximation",
    "title": "3  Approximation",
    "section": "3.2 Saddlepoint Approximation",
    "text": "3.2 Saddlepoint Approximation\n\nhttps://stats.stackexchange.com/questions/191492/how-does-saddlepoint-approximation-work/191781\nPreliminary Asymptotics"
  },
  {
    "objectID": "bayesian/bayesian.html#introduction-to-bayesian-modeling",
    "href": "bayesian/bayesian.html#introduction-to-bayesian-modeling",
    "title": "4  Bayesian",
    "section": "4.1 Introduction to Bayesian Modeling",
    "text": "4.1 Introduction to Bayesian Modeling\n\n4.1.1 Software Ecosystem\nThe software mostly is split up by the methodology that each method uses. Seems like results will vary, and syntax also will vary. Trade-offs on speed and use cases.\n\nR Software\n\nINLA - integrated nested laplace approximations. Uses R formula syntax\n\nnot on CRAN because it uses a C library that is machine dependent\noption to use pardiso optimizer, for which you need a license.\n\nBUGS - MCMC based sampling\n\nhas many different versions, seems easier to install on windows\n\nJAGS\n\nthe original MCMC based solutions, has an R implementation with “rjags”\nStands for “Just Another Gibbs Sampler”\ntext file specification of the model\nreimplementation of “BUGS”\n\nrstan - does some magical Hamiltonion Monte Carlo, which supp\n\nbrms - uses STAN as backend, but allows linear and nonlinear models with R formula syntax. Offers more flexibility than rstanarm\nrstanarm - meant as a drop in replacement using STAN for many base R and lme4 models. The main difference is that these models are precompiled in STAN code, while brms will compile everytime into new STAN code, thus will be slower for basic models. However, what you trade in slowness you get flexibility.\nblavaan - latent variable modeling, like SEM modeling, latent growth curve models, confirmatory factor analysis."
  },
  {
    "objectID": "bayesian/bayesian.html#inla",
    "href": "bayesian/bayesian.html#inla",
    "title": "4  Bayesian",
    "section": "4.2 INLA",
    "text": "4.2 INLA\nBroadly speaking, INLA is for a broad class of models used for Guassian markov random fields (GMRF). INLA tends to be faster than other methods, because the software is based on Laplace Approximation. Because of that, it is also a deterministic method unlike stochastic MCMC based software like STAN or BUGS/JAGS\n\n4.2.1 Installation Issues\nThe standard installation as recommended by the website is:\n\n\nCode\ninstall.packages(\"INLA\",repos=c(getOption(\"repos\"),INLA=\"https://inla.r-inla-download.org/R/testing\"), dep=TRUE) # R > 4.1\n\n\nBut I keep getting an error:\n\nError in inla.call.builtin() : INLA installation error; no such file\n\nso i tried installing directly from github with devtools.\n\n/usr/local/lib/R/4.1/site-library/INLA/bin/mac/64bit/inla.run: line 125: /usr/local/lib/R/4.1/site-library/INLA/bin/mac/64bit/inla: No such file or directory\n\nThe “devel” version unfortunately gives the following error, so trying the stable version.\n\n\nCode\ndevtools::install_github(repo = \"https://github.com/hrue/r-inla\", ref = \"stable\", subdir = \"rinla\", build = FALSE)\n\n\nThe stable version also didn’t work, was giving some dynamic library errors, finally tried installing manually and it seemed to work. INLA Binaries for R 4.1. The thread that gave me idea of installing manually.\nIn order to install it manually,\n\nDownload the precompiled binary here: https://inla.r-inla-download.org/R/testing/bin/macosx/contrib/4.1/\n\n\nback up directories if you need a different binary\n\n\nFrom R, run install.packages(\"pathtobinary.tgz\", repos = NULL)\n\n\nPardiso License\nPardiso is parallel computing library, stands for “Parallel Sparse Direct Solver”, and academics can get a license, just need to set the $PARDISO_LIC_PATH to where you download the license\nThere might be some useful documentation on how R finds its matrix libraries.\nSee the Advanced R sections for more on how R finds its libraries. Ultimnately, after some permissioning issues, we have a working example.\n\n\n\n4.2.2 INLA Basics\ninla() - this is the main function that is used from the package.\n\nformula - formula object\ndata - the dataframe\nfamily - string or vector to\n\n\n\n4.2.3 INLA Examples (simulated)\n\nLinear Regression\nWe first simulate some data, and plot the points that\n\n\nCode\n# create the simulated data\nN <- 100 # 500, 5000, 25000, 100000\nx <- rnorm(N, mean = 6, sd = 2)\ny <- rnorm(N, mean = x, sd = 1)\ndat_simple <- list(x = x, y = y, N = N)\n\n\n\n\nCode\n# Basic fit of model, assuming gaussian link\nmod <- inla(y ~ x,\n  family = \"gaussian\",\n  data = dat_simple,\n  control.predictor = list(link = 1)\n)\n\n\nYou can plot the posterior mean for each of the observations, but this plot is not particularly useful.\n\n\nCode\n# attributes(mod)\n# ?plot.inla\n# methods(\"inla\")\nplot(mod, plot.prior = TRUE, single=TRUE) # Plotting the posterior mean for fitted values, with index on axis.\n\n\nIn order to access the estimated coefficients and standard deviations of each parameter, we can use\n\n\nCode\nmod$summary.fixed\nmod$summary.random\nmod$summary.fitted.values\n\n\n\n\nCode\n# Plot solution from INLA, with prediction bounds\ng_inla_slr <- data.frame(dat_simple) %>% ggplot(aes(x, y)) +\n  geom_point() +\n  geom_ribbon(aes(y = mean, ymin = `0.025quant`, ymax=`0.975quant`),\n              data = mod$summary.fitted.values,\n              alpha = .1) +\n  geom_smooth(aes(y = mean), formula = y~x, method=\"lm\", data = mod$summary.fitted.values) +\n  labs(title = \"INLA\")\n  \n# Plot solution from lm\ng_smooth_slr <- data.frame(dat_simple) %>% ggplot(aes(x, y)) + \n  geom_point() + \n  geom_smooth(formula = y~x, method=\"lm\") +\n  labs(title = \"LM\")\n  \n\ng_inla_slr + g_smooth_slr\n\n\n\n\n\nClearly the INLA solution is very very similar to the one produced by LM as we expected.\n\n\n\n4.2.4 Laplace Approximation\nLet’s see an example with a closed form solution conjugate families. A good example of one is the Poisson-Gamma.\n\n\\begin{aligned}\nY | \\lambda &\\sim Poisson(\\lambda) \\\\\n\\lambda &\\sim \\Gamma(\\alpha, \\beta)\n\\end{aligned}\n Suppose we observe that y = 6, and we set a prior \\alpha=\\beta=1.\nThe update for poisson-gamma upon observing y_1, y_2, y_3, \\dots, y_n, (note this is using the rate parameterization, \\beta^\\alpha in the numerator)\n\n\\begin{aligned}\n\\Gamma(\\alpha, \\beta) \\longrightarrow \\Gamma(\\alpha + \\sum_i y_i \\, , \\beta + n)\n\\end{aligned}\n Hence, the true posterior is \\Gamma(1+6, 1 +1), which has density,\n\n\\begin{aligned}\np(\\lambda | Y=6) = \\frac{2^7}{\\Gamma(2)}\\exp(-2 \\lambda)\\lambda^{7 - 1}\n\\end{aligned}\n If we didn’t know this, then we would be trying to approximate the integral in the denominator of bayes rule, we do this with Laplace’s Approximation,\n\n\\begin{aligned}\np(\\lambda|y) &= \\frac{f(y|\\lambda) \\xi(\\lambda)}{\\int f(y|\\lambda)\\xi(\\lambda)\\, d\\lambda} \\\\\n\\end{aligned}\n to approximate the bottom, we have\n\n\\begin{aligned}\n\\int f(y|\\lambda)\\xi(\\lambda)\\, d\\lambda \\approx f(y|\\lambda_0)\\xi(\\lambda_0)\\sqrt{\\frac{2\\pi}{-h''(\\lambda_0)}}\n\\end{aligned}\n\nwhere h = \\log \\left[f(y|\\lambda) \\xi(\\lambda)\\right], and h''(\\lambda_0) is the second derivative evaluated at \\lambda_0, the maximum of the integrand f(y|\\lambda) \\xi(\\lambda). Finding the maximum can be done with an optimization procedure, optim or it can be done analytically.\nHere’s a dump of the calculations!\n\n\\begin{aligned}\nf(y=6|\\lambda)\\xi(\\lambda) &= \\frac{e^{-\\lambda} \\lambda^6}{6!}\\exp(-\\lambda) \\\\\nh(\\lambda) &= -\\log 6! -2\\lambda + 6 \\log \\lambda \\\\\nh'(\\lambda) &= \\frac{6}{\\lambda} -2\n\\end{aligned}\n\nSetting h'(\\lambda) = 0, we find the maximum of the integrand to be \\lambda_0 = 3\n\n\\begin{aligned}\nh''(\\lambda) &= -\\frac{6}{\\lambda^2} \\\\\n-h''(\\lambda)^{-1} &= \\frac{\\lambda^2}{6}\n\\end{aligned}\n Evaluating the maximum, we get -h''(\\lambda_0)^{-1} = 1.5. Now we’re ready to code it up!\n\n\nCode\n# Poisson-Gamma Update\n# gamma(a, b) -> gamma(a + \\sum_i x_i, b + n)\n\n# Let prior be gamma(1, 1)\n# Observe y = 6\n# True Posterior is gamma(1 + 6, 1 + 1)\n\nlgrid <- seq(0, 10, .01)\n\nplot(lgrid, dgamma(lgrid, 1, 1), type=\"l\", xlab=\"lambda\", ylab=\"density\")\nlines(lgrid,dgamma(lgrid, 7, 2), type=\"l\", col=2)\nlegend(\"topright\",legend =  c(\"Prior\", \"Posterior\"),  col = c(1, 2),lty=1)\n\n\n\n\n\n\n\nCode\nest_mode <- (6) / 2 # (alpha + x) / beta\nest_cov <- 1 / ((1 + 6 - 1) / est_mode^2) # (alpha + x - 1) / mode^2\nest_cov\n\n\n[1] 1.5\n\n\nCode\n# integral approximation is\n# likelihood * prior * constant based on curvurature of function\n# The constant in the denominator of Bayes rule, also the integral approximation\nZ <- dpois(6, lambda = est_mode) * dgamma(est_mode, 1, 1) * sqrt(2 * pi * est_cov)\n\nposterior <- function(lambda) {\n  dpois(6, lambda = lambda) * dgamma(lambda, 1, 1) / Z\n}\n\n\n\n\nCode\nplot(lgrid, dgamma(lgrid, 1, 1), type=\"l\", xlab=\"lambda\", ylab=\"density\",\n     main = \"Poisson-Gamma Example with Laplace Approximation\")\nlines(lgrid,dgamma(lgrid, 7, 2), type=\"l\", col=2, lwd=3)\nlines(lgrid, posterior(lgrid), type = \"l\", col=3)\nlines(lgrid, dnorm(lgrid, mean=est_mode, sd = sqrt(est_cov)), type = \"l\", col=3, lty=2) # The approximating normal distribution used in laplace approximation, centered on posterior mean\nlegend(\"topright\", legend = c(\"Prior\", \"True Posterior\", \"Posterior with Laplace approximated Integral\", \"Laplace Approximation\"), \n       lty = c(1, 1, 1, 2),\n       col = c(1, 2, 3, 3),\n       cex = .8)\n\n\n\n\n\n\n\nCode\n# A different way of visualizing the above information\nplot(dgamma(0:10, 7, 2), posterior(0:10), pch=19, cex=.2, xlab = \"True Posterior density, Gamma(7,2)\",\n     ylab = \"posterior density by laplace approximation\",\n     main = \"Comparison of True Posterior and Laplace Approximation\")\nabline(0,1, col=2)\nlegend(\"bottomright\", col = c(1,2), pch = c(19,-1), lty=c(0, 1), legend = c(\"evaluated densities\", \"line of equality\"))\n\n\nResources used when writing this section - https://www.sumsar.net/blog/2013/11/easy-laplace-approximation/ - https://bookdown.org/rdpeng/advstatcomp/laplace-approximation.html - https://en.wikipedia.org/wiki/Laplace%27s_method\n\n\n4.2.5 Resources for INLA\n\nWorked Examples for Bayesian Analysis with INLA from Faraway - author of the “Extending the Linear Model” book, this site also includes many mixed effects model examples."
  },
  {
    "objectID": "bayesian/bayesian.html#stan",
    "href": "bayesian/bayesian.html#stan",
    "title": "4  Bayesian",
    "section": "4.3 STAN",
    "text": "4.3 STAN\nSTAN is sophisticated because it uses the No U-Turn Sampling (NUTS), and thus has a faster convergence.\n\nthere’s a “warm-up” rather than a “burn-in” for the sampling.\nfaster for a complex model\n\n\n4.3.1 rstanarm\nThe functions offered in rstanarm, and quick explanation\n\nstan_lm - basic linear model\nstan_aov - calls stan_lm in the backend\nstan_lmer - calls\n\n\n\nCode\ndata(\"weightgain\", package = \"HSAUR3\")\n# Standard Frequentist Method\nfmod <- aov(weightgain ~ source * type, data = weightgain)\n\n\n# Bayesian aov\nbmod <- stan_aov(weightgain ~ source * type, data = weightgain,\n         prior = R2(location = .5),\n         adapt_delta = .999,\n         seed = 1234)\n\n\n\nSAMPLING FOR MODEL 'lm' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 5.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.55 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.286155 seconds (Warm-up)\nChain 1:                0.22461 seconds (Sampling)\nChain 1:                0.510765 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'lm' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 9e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.276645 seconds (Warm-up)\nChain 2:                0.263396 seconds (Sampling)\nChain 2:                0.540041 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'lm' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.322498 seconds (Warm-up)\nChain 3:                0.182252 seconds (Sampling)\nChain 3:                0.50475 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'lm' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 9e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.30836 seconds (Warm-up)\nChain 4:                0.199297 seconds (Sampling)\nChain 4:                0.507657 seconds (Total)\nChain 4: \n\n\nCode\nsummary(bmod)\n\n\n\nModel Info:\n function:     stan_aov\n family:       gaussian [identity]\n formula:      weightgain ~ source * type\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 40\n\nEstimates:\n                       mean   sd    10%   50%   90%\n(Intercept)           98.7    4.4  93.1  98.7 104.4\nsourceCereal         -12.7    6.2 -20.6 -12.7  -4.8\ntypeLow              -18.7    6.2 -26.4 -18.8 -10.4\nsourceCereal:typeLow  16.9    8.8   5.4  17.1  27.8\nsigma                 14.8    1.8  12.8  14.7  17.2\nlog-fit_ratio          0.0    0.1  -0.1   0.0   0.2\nR2                     0.2    0.1   0.1   0.2   0.3\n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 87.3    3.4 83.0  87.2  91.6 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                     mcse Rhat n_eff\n(Intercept)          0.1  1.0  1492 \nsourceCereal         0.1  1.0  1853 \ntypeLow              0.2  1.0  1422 \nsourceCereal:typeLow 0.2  1.0  2093 \nsigma                0.0  1.0  2533 \nlog-fit_ratio        0.0  1.0  2240 \nR2                   0.0  1.0  1733 \nmean_PPD             0.1  1.0  4009 \nlog-posterior        0.1  1.0   850 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nCode\n# Bayesian lmer method\n\nbmmod <- stan_lmer(weightgain ~ 1 + (1|source) + (1 | type) + (1 | source:type),\n                   data = weightgain,\n                   prior_intercept = cauchy(),\n                   prior_covariance = decov(shape = 2, scale = 2),\n                   adapt_delta = 0.999, seed = 12345)\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 7.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.78 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 6.87361 seconds (Warm-up)\nChain 1:                5.0979 seconds (Sampling)\nChain 1:                11.9715 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 6.09438 seconds (Warm-up)\nChain 2:                5.36233 seconds (Sampling)\nChain 2:                11.4567 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2.8e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.28 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 7.68205 seconds (Warm-up)\nChain 3:                7.34715 seconds (Sampling)\nChain 3:                15.0292 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 6.01372 seconds (Warm-up)\nChain 4:                7.2315 seconds (Sampling)\nChain 4:                13.2452 seconds (Total)\nChain 4: \n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\n\nCode\ntibble(freq = coef(fmod),\n       bayes = coef(bmod))\n\n\n# A tibble: 4 × 2\n   freq bayes\n  <dbl> <dbl>\n1 100    98.7\n2 -14.1 -12.7\n3 -20.8 -18.8\n4  18.8  17.1\n\n\n\n\n4.3.2 Resources for STAN\n\nFaraway worked examples in STAN - largely examples from his books, and pretty basic analyses."
  },
  {
    "objectID": "bayesian/bayesian.html#jags",
    "href": "bayesian/bayesian.html#jags",
    "title": "4  Bayesian",
    "section": "4.4 JAGS",
    "text": "4.4 JAGS\nJAGS stands for “Just another Gibbs Sampler”, and operates by MCMC.\n\n4.4.1 Worked Examples\n\nLinear Regression (simulated)\nWe’ll run the same model that did for INLA, a SLR.\n\n\nCode\n# https://www.precision-analytics.ca/articles/a-gentle-inla-tutorial/\nN <- 100 # 500, 5000, 25000, 100000\nx <- rnorm(N, mean = 5, sd = 1)\nnu <- rnorm(N, 0, 0.1)\nmu <- exp(1 + 0.5 * x + nu)\ny <- rpois(N, mu)\n\n\n\n\nCode\n# https://www.precision-analytics.ca/articles/a-gentle-inla-tutorial/\n\n# Save BUGS style specification to .txt file\ncat(\"model {\n  for(i in 1:N) {\n    y[i] ~ dnorm(mu[i], tau)\n    mu[i] <- alpha + beta * x[i]\n  }\n  alpha ~ dnorm(0, 0.001)\n  beta  ~ dnorm(0, 0.001)\n  tau   ~ dgamma(0.01, 0.01)\n}\", file=\"jags_slr.txt\")\n\n# Initialize the Model \njags_mod_slr <- jags.model(\n  file = \"jags_slr.txt\",\n  data = dat_simple,\n  n.chains = 3,\n)\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 100\n   Unobserved stochastic nodes: 3\n   Total graph size: 407\n\nInitializing model\n\n\nCode\n# Run Jags and save posterior samples\nparams <- c(\"alpha\", \"beta\", \"tau\")\nsamps <- coda.samples(jags_mod_slr, params, n.iter=1000)\n\nsummary(samps)\n\n\n\nIterations = 1:1000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 1000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean      SD Naive SE Time-series SE\nalpha 0.2382 0.50615 0.009241       0.039769\nbeta  0.9858 0.08229 0.001502       0.006511\ntau   0.9590 0.14776 0.002698       0.004212\n\n2. Quantiles for each variable:\n\n         2.5%      25%    50%    75%  97.5%\nalpha -0.4555 -0.02924 0.2080 0.4334 0.9664\nbeta   0.8656  0.95217 0.9898 1.0312 1.1020\ntau    0.6885  0.86472 0.9578 1.0522 1.2467\n\n\n\n\nCode\nplot(samps)\n\n\n\n\n\nThese are the mixing plots and show posterior sample draws, as well as the density of those draws. We can see that the mixing times, and such stabilize quite quickly. We can see in the trace three dotted lines, because we requested three chains."
  },
  {
    "objectID": "bayesian/bayesian.html#cookbook",
    "href": "bayesian/bayesian.html#cookbook",
    "title": "4  Bayesian",
    "section": "4.5 Cookbook",
    "text": "4.5 Cookbook\n\n4.5.1 Conditional Logistic Regression Model\nConditional Logistic Regression Example with INLA\n\n\n4.5.2 Conditional Sampling\nConditional Sampling from INLA"
  },
  {
    "objectID": "bayesian/bayesian.html#additional-resources",
    "href": "bayesian/bayesian.html#additional-resources",
    "title": "4  Bayesian",
    "section": "4.6 Additional Resources",
    "text": "4.6 Additional Resources\n\n4.6.1 INLA Related (beginner)\n\nBest introduction blog post of INLA with some comparisons to MCMC methods\nA whole book about INLA, introduction level with heavy emphasis on applied examples, and organized very intuitively for applied statisticians.\nMiscellaneous list of INLA tutorials\n\n\n\n4.6.2 Advanced (spatial, stochastic pde) Modeling with INLA\n\nAdvanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA\nGeospatial Health Data: Modeling and Visualization with R-INLA and Shiny\n\nSoftware Comparisons - Comparing NIMBLE, JAGS, Stan - upshot is to learn Stan for best balance of flexibility and speed."
  },
  {
    "objectID": "categorical/categorical.html#overview",
    "href": "categorical/categorical.html#overview",
    "title": "5  Categorical Data Analysis",
    "section": "5.1 Overview",
    "text": "5.1 Overview\nThis is a very broad area, but I feel like it’s also super confusing because it’s all very confouded in terms of all the chisq tests that are scattered throughout the place. I’m trying to organize everything for myself here.\n\nCMH test\nscore test for table\nPearson\nYates (continuity correction of Pearson)\nBarnard test (based on fixing 1 margin)\nfisher exact (based on fixing all margins)\nMcNemar Test\nGeneralized CMH test"
  },
  {
    "objectID": "categorical/categorical.html#x2-sampling-mechanisms",
    "href": "categorical/categorical.html#x2-sampling-mechanisms",
    "title": "5  Categorical Data Analysis",
    "section": "5.2 2x2 sampling mechanisms",
    "text": "5.2 2x2 sampling mechanisms\nThe many different sampling mechanisms arise from different study designs, and are critical to the assumptions of the population you are studying.\nIn an epidemiological context, the data is given as\n\n\n\n\ndisease\nno disease\n\n\n\n\n\nexposure\na\nb\nn1\n\n\nno exposure\nc\nd\nn0\n\n\n\nm1\nm0\nN\n\n\n\n\nPoisson (nothing fixed)\nMultinomial (N) total is fixed\n\n“Cross Sectional” studies\n\nTwo-sample Binomial (1 margin fixed)\n\n“Cohort Study” = n1, n0 fixed\n“Case Control Study” = m1, m0 fixed\n\na type of “retrospective” study.\n\n\nHypergeometric (2 margins fixed)\n\nvery rarely the case in real experiments, but unfortunately many methods are based on this assumption for the 2x2 table.\n\n\n\nPoissonMultinomialBinomialHypergeometric\n\n\n\n\nCode\n# Independent Poisson\n\n#' When specifying the mean, they can be specified cellwise, or by table, grouped by the nrow*ncol, in order of the columns\n#'\n#' @param mean means of the cells. If 1 number, all cells will have same \n#'\n#' @return\n#' @export\n#'\n#' @examples\nrpoisson_table <- function(num_tables = 1, mean = 5, nrow = 2,  ncol = 2) {\n  cells <- rpois(ncol * nrow * num_tables, lambda = mean)\n  vec_table <- split(cells, gl(num_tables, ncol*nrow))\n  vapply(vec_table, matrix, nrow = nrow, ncol = ncol, byrow = FALSE, FUN.VALUE = matrix(1:(ncol*nrow), nrow = nrow))\n}\n\n# 2x2 examples\npoisson_table_examples <- rpoisson_table(5, mean = 5, nrow = 2, ncol = 2)\napply(poisson_table_examples,\n      MARGIN = 3,\n      FUN = addmargins,\n      simplify = FALSE)\n\n\n$`1`\n         Sum\n     8 1   9\n     7 5  12\nSum 15 6  21\n\n$`2`\n         Sum\n    4  6  10\n    2  9  11\nSum 6 15  21\n\n$`3`\n         Sum\n    4  7  11\n    2  4   6\nSum 6 11  17\n\n$`4`\n         Sum\n    2  7   9\n    5  4   9\nSum 7 11  18\n\n$`5`\n        Sum\n    4 4   8\n    0 3   3\nSum 4 7  11\n\n\n\n\n\n\nCode\n# grand total fixed\nrmultinom_table <- function(num_tables = 1, N = 20, nrow = 2, ncol = 2, p = c(1, 1, 1, 1)) {\n  stopifnot(length(p) == nrow * ncol)\n  # p is internally standardized by rmultinom\n  cells <- rmultinom(num_tables, N, p)\n  dim(cells) <- c(nrow, ncol, num_tables)\n  cells\n}\n\n\n\n\nCode\n# examples\nmultinom_table_examples <- rmultinom_table(5, N = 20, nrow = 2, ncol = 2, p = c(1, 1, 1, 1))\napply(multinom_table_examples,\n      MARGIN = 3,\n      FUN = addmargins,\n      simplify = FALSE)\n\n\n[[1]]\n         Sum\n     4 7  11\n     7 2   9\nSum 11 9  20\n\n[[2]]\n         Sum\n    5  8  13\n    3  4   7\nSum 8 12  20\n\n[[3]]\n         Sum\n    1  7   8\n    8  4  12\nSum 9 11  20\n\n[[4]]\n         Sum\n    7  6  13\n    2  5   7\nSum 9 11  20\n\n[[5]]\n         Sum\n    4  5   9\n    4  7  11\nSum 8 12  20\n\n\n\n\n\n\nCode\n# Sample a 2x2 table\nrbinom_table <- function(num_tables = 1, row_n = c(10, 10), p = c(.5, .5)) {\n  vapply(1:num_tables, \n       FUN = function(x) {\n         a <- rbinom(length(row_n), row_n, p) # fix margins\n         binom_table <- cbind(a, b = row_n-a) # make other column by subtraction and combine\n         colnames(binom_table) <- NULL\n         binom_table}, \n       FUN.VALUE = matrix(rep(1.1, 4), nrow =2)) # expected 2x2 table\n}\n\nbinom_table_examples <- rbinom_table(5, row_n = c(10, 10), p = c(.5, .5))\napply(binom_table_examples,\n      MARGIN = 3,\n      FUN = addmargins,\n      simplify = FALSE)\n\n\n[[1]]\n         Sum\n     7 3  10\n     6 4  10\nSum 13 7  20\n\n[[2]]\n          Sum\n     5  5  10\n     5  5  10\nSum 10 10  20\n\n[[3]]\n         Sum\n     4 6  10\n     7 3  10\nSum 11 9  20\n\n[[4]]\n          Sum\n     5  5  10\n     5  5  10\nSum 10 10  20\n\n[[5]]\n         Sum\n     4 6  10\n     7 3  10\nSum 11 9  20\n\n\n\n\n\n\nCode\n# Fixed margins\n# one hypergeometrical deviate will determine the entire table.\n# only doing 2x2 tables with sampling method\n\nrhyper_table <- function(num_tables, row_n = c(10, 10), col_n = c(10, 10))  {\n  a <- rhyper(num_tables, row_n[1], row_n[2], col_n[1])\n  vapply(a, \n       FUN = function(x) {\n         col1 <- c(x, col_n[1] - x)\n         col2 <- row_n - col1\n         contingency_table <- cbind(col1, col2)\n         colnames(contingency_table) <- NULL # get rid of column names\n         contingency_table\n       },\n       FUN.VALUE = matrix(c(0, 0, 0, 1.1), nrow = 2))}\n\n# examples\nhyper_table_examples <- rhyper_table(num_tables = 5, row_n = c(10, 10), col_n = c(10, 10))\napply(hyper_table_examples,\n      MARGIN = 3,\n      FUN = addmargins,\n      simplify = FALSE)\n\n\n[[1]]\n          Sum\n     4  6  10\n     6  4  10\nSum 10 10  20\n\n[[2]]\n          Sum\n     4  6  10\n     6  4  10\nSum 10 10  20\n\n[[3]]\n          Sum\n     3  7  10\n     7  3  10\nSum 10 10  20\n\n[[4]]\n          Sum\n     5  5  10\n     5  5  10\nSum 10 10  20\n\n[[5]]\n          Sum\n     5  5  10\n     5  5  10\nSum 10 10  20"
  },
  {
    "objectID": "categorical/categorical.html#measures-of-association",
    "href": "categorical/categorical.html#measures-of-association",
    "title": "5  Categorical Data Analysis",
    "section": "5.3 Measures of Association",
    "text": "5.3 Measures of Association\nGiven tables, it’s important to distinguish the properties of how to make comparisons, and summarize the information given. When dealing with percentages and ratios, it’s sometimes difficult to have an intuitive understanding of meaning on the percentage scale. The example Professor Guanhua Chen gives to stimulate you for this thinking is from cartalk:\n\nRAY: Potatoes are 99 percent water and one percent what? Potato. So say you take a bunch of potatoes, like 100 pounds of potatoes and you set them out on your back porch to dry out. TOM: Yeah, when they are dry they should weigh about a pound. RAY: Well, we’re not drying out completely. And as the potatoes dry out the water begins to evaporate. And after a while, enough water has evaporated so that they are now 98 percent water. If you were to weigh those potatoes at that moment… TOM: They’d be lighter. RAY: Yes, how much lighter? That’s the question. Now you can solve this puzzler algebraically, and if you don’t solve it algebraically, you are going to get the wrong answer. TOM: Really? RAY: Really.What’s your answer, off the top of your head? TOM: 99 pounds. RAY: You are wrong. Answer: RAY: Now, unencumbered by the thought process as usual, my brother guessed 99 pounds. TOM: Yeah. RAY: Now, when I guessed, off the top of my head, I guessed about 90 pounds. TOM: ’Cause it just feels right. RAY: But if you do the math, 1 percent of 100 –which is what the potato is– is one pound. As we told you, that’s 1 percent. So 2 percent, when it’s 98 percent water, two percent of the new weight of the mass is still going to be equal to that one pound, and 2 percent of 50 pounds is a pound. So the potato weight is now 50 pounds, not 100.\n\n\nRisk Ratio:\n\nnot symmetric\n\nOdds Ratio\n\nsymmetric, exposure gives information about disease and vice versa"
  },
  {
    "objectID": "categorical/categorical.html#testing",
    "href": "categorical/categorical.html#testing",
    "title": "5  Categorical Data Analysis",
    "section": "5.4 Testing",
    "text": "5.4 Testing\n\n\nCode\ntribble(~test, ~parameter, ~sampling, ~pvalue, ~hypothesis,\n        \"pearson\", \"parameter\",\"two sample z proportion\", \"approximate\", \"association\",\n        \"cochran\", \"cell\", \"unconditional two sample\", \"approximate\", \"association\")\n\n\n# A tibble: 2 × 5\n  test    parameter sampling                 pvalue      hypothesis \n  <chr>   <chr>     <chr>                    <chr>       <chr>      \n1 pearson parameter two sample z proportion  approximate association\n2 cochran cell      unconditional two sample approximate association\n\n\nA list of summary\nA list of tests:\n\nCochran summary test\n(Cochran)-Mantel-Haenszel summary test\nGeneralized CMH Test\nCochran’s Q test\nPearson Chi Squared\nCochran-Armitrage Trend Test\nCochran-Q Test\nMann-Whitney U test (Wilcoxon Rank Sum)\nWilcoson Signed Rank Test (paired samples)\nKrustkal Wallis\n\nExact tests\n\nFisher Exact Test\nBarnard Exact Test\n\nDependent “Matched pairs” tests\n\nMcNemar Test\n\n\n5.4.1 Pearson\nGiven the following table,\n\n\nCode\ntribble(~\"\", ~\"Disease\", ~\"No Disease\", ~\"\",\n        \"Exposed\", \"a\", \"b\", \"\\\\$n_1\\\\$\",\n        \"Not Exposed\" , \"c\", \"d\", \"\\\\$n_2\\\\$\",\n        \"\", \"\\\\$m_1\\\\$\", \"\\\\$m_2\\\\$\", \"N\") %>% \n  kable(\"html\")\n\n\n\n\n \n  \n     \n    Disease \n    No Disease \n     \n  \n \n\n  \n    Exposed \n    a \n    b \n    \\$n_1\\$ \n  \n  \n    Not Exposed \n    c \n    d \n    \\$n_2\\$ \n  \n  \n     \n    \\$m_1\\$ \n    \\$m_2\\$ \n    N \n  \n\n\n\n\n\nAssume two sample proportional sampling model: A derivation from 2 sample proportion (pooled) z-test:\n\n\\begin{aligned}\nz^2 &= \\frac{(\\hat p_1 - \\hat p_2)^2}{\\hat p(1 - \\hat p)(\\frac{1}{n_1} + \\frac{1}{n_2})}\n&=\n\\end{aligned}\n\n\n\\begin{aligned}\nX^2 = \\sum_i^c \\frac{|O_i - E_i|}{E_i} \\sim \\chi^2_{c-1}\n\\end{aligned}\n\nResources: - Pearson as Score Test - 7 proofs of independence test\n\n\n5.4.2 Barnard Exact Test\nBarnard is considered an “unconditional” approach to exact testing. contrast to Fisher exact test which is a “conditional” approach to testing the p-value\nBarnard is effectively a 2 stage test, given some observed table X, a “p-value” is the probability of observing a table more extreme than the observed.\nThe two stages are thus:\n\ndetermine which tables are more “extreme”\ncalculated probability of those tables\n\nThus, the “exactness” part of the description refers to how a probability is calculated.\nMany methods have been proposed for stage 1\n\nSuissa and Shuster (1985) use pooled and unpooled z statistic for two proportions.\nBooschloo (1970) used the p-value from fisher’s exact test to determine extremeness…\nSantner Snell - difference in proportion\n\n\n\nCode\nX <- matrix(c(3, 0, 0, 3), nrow = 2)\n# row is fixed\nBarnardTest(X, method = \"z-pooled\")\n\n\n\n    Z-pooled Exact Test\n\ndata:  3 out of 3 vs. 0 out of 3\ntest statistic = 2.4495, first sample size = 3, second sample size = 3,\np-value = 0.03125\nalternative hypothesis: true difference in proportion is not equal to 0\nsample estimates:\ndifference in proportion \n                       1 \n\n\nCode\n# z-pooled observed is...\n1 / sqrt(.25 * (1/3 + 1/3)) # observed test statistic\n\n\n[1] 2.44949\n\n\nCode\n# how to calculate p-value from this test statistic, sum of binomial products from extreme tables,\n# the null is that pi_1 = pi_2 = pi, since we don't know the actual value of pi, we take the supremum of these values\n\ntrue_pi <- seq(0, 1, .01)\n\npossible_p <- dbinom(3, 3, prob = true_pi) * dbinom(0, 3, prob = true_pi) + dbinom(0, 3, prob = true_pi) * dbinom(3, 3, prob = true_pi)\n\n\n# maximum occurs at .5\ntrue_pi[which.max(possible_p)]\n\n\n[1] 0.5\n\n\nCode\n# thus, overall p-value is\nmax(possible_p) # .03125\n\n\n[1] 0.03125\n\n\nNow we compare the methods for choosing “extremeness” with the assumed sampling mechanism.\n\n\nCode\nset.seed(1)\nfoo <- rbinom_table(1000, row_n = c(10, 10), p = c(.5, .5)) # null is no association\n\n# z pooled (score)\nbarnard_pooled <- apply(foo, MARGIN = 3,\n                        FUN = function (x) {BarnardTest(x, method = \"z-pooled\")$p.value})\n\nbarnard_unpooled <- apply(foo, MARGIN = 3,\n                        FUN = function (x) {BarnardTest(x, method = \"z-unpooled\")$p.value})\n\nbarnard_boschloo <- apply(foo, \n                          MARGIN = 3,\n                          FUN = function (x) {BarnardTest(x, method = \"boschloo\")$p.value})\n\n\nbarnard_csm <- apply(foo, \n                          MARGIN = 3,\n                          FUN = function (x) {BarnardTest(x, method = \"csm\")$p.value})\n\nbarnard_santner_snell <- apply(foo, \n                               MARGIN = 3,\n                               FUN = function (x) {BarnardTest(x, method = \"santner and snell\")$p.value})\n\n# for reference\nfisher_exact <-  apply(foo, \n                               MARGIN = 3,\n                               FUN = function (x) {fisher.test(x)$p.value})\n\n\n\n# comment out any that you don't want to appear in the plot\nbarnard <- bind_cols(pooled = barnard_pooled,\n                     unpooled = barnard_unpooled,\n                     boschloo = barnard_boschloo,\n                     csm = barnard_csm,\n                     santner_snell = barnard_santner_snell,\n                     fisher_exact = fisher_exact) %>% \n  pivot_longer(everything(), names_to = \"type\", values_to = \"p\")\n\n# there's some overplotting happening, dodge doesn't seem to work well for ecdf's\nbarnard %>% ggplot(aes(x = p, color = type)) +\n  stat_ecdf()  +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) +\n  labs(title = \"Barnard extremeness method comparison\")\n\n\n\n\n\nFor accuracy of the size of the test, it seems that csm is the most accurate, but also the most computationally intensive. We not that all the tests have more approriate “size” than the fisher_exact test.\nResources\n\nExplanation of difference between unconditional and conditional\nExact Test Package Documentation - exact.test function documentation has more information about barnard implmenetation.\n\n\n\n5.4.3 McNemar\nMcNemar tests are used when there is some “dichotomous trait”, for matched pairs. This means that the responses are statistically dependent. This is common for some longitudinal studies in which a single individual is asked two questions, and their answers are coded as locations in the table. Thus, the grand total, should be the number of pairs of data, not the total number of observations.\nFor example, suppose a person is asked if they voted democrat\n\n\nCode\ntribble(~\"\", ~\"2008 democrat\", ~\"2008 republican\",\n        \"2004 democrat\", 175, 16,\n        \"2004 republican\", 54, 188) %>% \n  kbl()\n\n\n\n\n \n  \n     \n    2008 democrat \n    2008 republican \n  \n \n\n  \n    2004 democrat \n    175 \n    16 \n  \n  \n    2004 republican \n    54 \n    188 \n  \n\n\n\n\n\nMcNemar tests “Marginal Homogeneity”, meaning that the probabilities of the margins are the same. p_a + p_b = p_a + p_c and p_c + p_d = p_b + p_d. Thus, this means that we are testing H_0: p_b = p_c, H_A: p_b \\neq p_c. The score statistic is:\n\n\\begin{aligned}\nz_0^2 = \\frac{(b-c)^2}{b+c} \\sim \\chi^2_1\n\\end{aligned}\n\n\nVariance is\n\n\\begin{aligned}\n\\hat\\sigma_0 (d) = \\frac{b + c}{N^2}\n\\end{aligned}\n\n\n\nCode\n# Presidential election, dependent table\npres <- matrix(c(175, 16,\n                      54, 188),\n                    ncol = 2,\n                    byrow = TRUE)\npres_mcnemar <- mcnemar.test(pres, correct = FALSE)\n# (54 - 16)^2 / (54 + 16) # 20.629\npres_mcnemar\n\n\n\n    McNemar's Chi-squared test\n\ndata:  pres\nMcNemar's chi-squared = 20.629, df = 1, p-value = 5.576e-06\n\n\nNotes\n\nonly the off diagonal matters for significance, where as the main diagonal\n\nA good reference is 11.1 in Categorical Data Analysis, 3rd edition by Agresti\n\n\n5.4.4 Breslow-Day Test\na test of homogeneity.\n\n\n5.4.5 Wilcox Test\nIs a test of location for two samples. This somewhat analogous to a nonparametric t-test.\n\n\n5.4.6 Krustkal Wallis\nAlso a test of location for K samples. This is an extension of the Wilcox test that is also nonparametric. this can be thought of a nonparameteric anova.\n\n\n5.4.7 Cochran-Armitrage Trend Test (ordinal)\nAppropriate for IxJ tables in which both directions are ordinal. This assumes the tables are sampled by binomial fixed proportions for the two groups. We can kind of think of fitting some weighted regression to a surface that is determined by the scores along X and Y.\nIn the 2xJ case, this reduces to a Wilcoxon test, (also known as Mann-Whitney).\nThese tests are more powerful for testing specific hypotheses, like a specific trend. If we don’t know what the trend is, then we may be better off testing with a pearson test which just looks for general association.\n\n\n5.4.8 CMH testing\nThe CMH testing is technically supposed to be done on tables in strata. the data type is an I X J X K, in which we have K strata and an I X J contingency table in each.\n\nyou are not penalized for adding tables with sparse data with the CMH test statistic\nthis reduces to the N-1 adjusted pearson chisquared statistic for 1 strata\nthe test assumes that there is a common odds ratio to estimate, but in order to test the hypothesis we can use a Breslow-Day Test of Homogeneity\nConditional logistic regression gives a similar answer, clogit in package survival because similar to cox model as well.\n\n\nGeneralized CMH Testing\nCMH is extended to IxJxK with possibly ordinal factors for I and J. An implementation of these statistics can be found:\n\ncoin::cmh_test()\nvcdExtra:CHMtest()\n\n\n\nCMH - McNemar Equivalence\nIf you express the “population averaged” table and run McNemar, you will get the same statistic as expressing the data as a “subject specific” table for an individual per stratum.\n\n\nCode\ntest <- matrix(c(5, 7, 3, 4), ncol = 2)\nchisq.test(test, correct = FALSE)\n\n\nWarning in chisq.test(test, correct = FALSE): Chi-squared approximation may be\nincorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  test\nX-squared = 0.0025703, df = 1, p-value = 0.9596\n\n\nCode\n0.0025703 / 19 * 18 # the \"N-1\" chisq statistic in a 2 x 2\n\n\n[1] 0.002435021\n\n\nCode\nfoo <- array(c(5, 7, 3, 4,\n               0, 0, 1, 1), dim = c(2, 2, 2))\nmantelhaen.test(foo, correct = FALSE)\n\n\n\n    Mantel-Haenszel chi-squared test without continuity correction\n\ndata:  foo\nMantel-Haenszel X-squared = 0.0024351, df = 1, p-value = 0.9606\nalternative hypothesis: true common odds ratio is not equal to 1\n95 percent confidence interval:\n 0.1444191 6.2805369\nsample estimates:\ncommon odds ratio \n         0.952381 \n\n\nCode\nbar <- array(c(5, 7, 3, 4,\n               0, 0, 1, 1,\n               1, 6, 0, 0), dim = c(2, 2, 3))\nmantelhaen.test(bar, correct = FALSE)\n\n\n\n    Mantel-Haenszel chi-squared test without continuity correction\n\ndata:  bar\nMantel-Haenszel X-squared = 0.0024351, df = 1, p-value = 0.9606\nalternative hypothesis: true common odds ratio is not equal to 1\n95 percent confidence interval:\n 0.1444191 6.2805369\nsample estimates:\ncommon odds ratio \n         0.952381 \n\n\nCode\nmantelhaen_2by2 <-  function(x) {\n  mantelhaen.test(array(c(x, 0, 0, 1, 1), dim = c(2, 2, 2)), correct = FALSE)\n}\n\n\n\n\n\n5.4.9 2x2 tables Comparison\n\nPoisson Sampling\n\n\nCode\n# random poisson\nfoo <- rpoisson_table(num_tables = 1000, mean = 5, nrow = 2, ncol = 2)\n\n# pearson uncorrected\npearson_uncorrected_p <- \n  apply(foo, \n      MARGIN = 3,\n      function(x) {\n        suppressWarnings(chisq.test(x, correct = FALSE))$p.value})\n\n# pearson corrected\npearson_corrected_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(chisq.test(x, correct = TRUE))$p.value})\n\nfisher_exact_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(fisher.test(x))$p.value})\n\n# Pearson N-1 correction equivalent\ncmh_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          mantelhaen_2by2(x)$p.value\n        })\n\n# G-test (LRT)\nlrt_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(g.test(x))$p.value\n        })\n\n\ncontingency_p <- bind_cols(uncorrected = pearson_uncorrected_p,\n          corrected = pearson_corrected_p,\n          exact = fisher_exact_p,\n          cmh = cmh_p,\n          lrt = lrt_p)\n\ncontingency_p %>% \n  pivot_longer(everything(), names_to = \"test\", values_to = \"p\") %>% \n  ggplot(aes(p, color = test)) +\n  stat_ecdf(geom = \"step\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) + \n  labs(title = \"Tests of association\") #+\n\n\n\n\n\nCode\n  coord_cartesian(xlim = c(0, .1), ylim = c(0, .1)) # for zooming in\n\n\n<ggproto object: Class CoordCartesian, Coord, gg>\n    aspect: function\n    backtransform_range: function\n    clip: on\n    default: FALSE\n    distance: function\n    expand: TRUE\n    is_free: function\n    is_linear: function\n    labels: function\n    limits: list\n    modify_scales: function\n    range: function\n    render_axis_h: function\n    render_axis_v: function\n    render_bg: function\n    render_fg: function\n    setup_data: function\n    setup_layout: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    setup_params: function\n    train_panel_guides: function\n    transform: function\n    super:  <ggproto object: Class CoordCartesian, Coord, gg>\n\n\n\n\nBinomial Sampling\n\n\nCode\nfoo <- rbinom_table(num_tables = 9000)\n\n# pearson uncorrected\npearson_uncorrected_p <- \n  apply(foo, \n      MARGIN = 3,\n      function(x) {\n        suppressWarnings(chisq.test(x, correct = FALSE))$p.value})\n\n# pearson corrected\npearson_corrected_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(chisq.test(x, correct = TRUE))$p.value})\n\nfisher_exact_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(fisher.test(x))$p.value})\n\n# Pearson N-1 correction equivalent\ncmh_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          mantelhaen_2by2(x)$p.value\n        })\n\n# G-test (LRT)\nlrt_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(g.test(x))$p.value\n        })\n\n\ncontingency_p <- bind_cols(uncorrected = pearson_uncorrected_p,\n          corrected = pearson_corrected_p,\n          exact = fisher_exact_p,\n          cmh = cmh_p,\n          lrt = lrt_p)\n\ncontingency_p %>% \n  pivot_longer(everything(), names_to = \"test\", values_to = \"p\") %>% \n  ggplot(aes(p, color = test)) +\n  stat_ecdf(geom = \"step\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) #+\n\n\n\n\n\nCode\n  coord_cartesian(xlim = c(0, .1), ylim = c(0, .1)) # for zooming in\n\n\n<ggproto object: Class CoordCartesian, Coord, gg>\n    aspect: function\n    backtransform_range: function\n    clip: on\n    default: FALSE\n    distance: function\n    expand: TRUE\n    is_free: function\n    is_linear: function\n    labels: function\n    limits: list\n    modify_scales: function\n    range: function\n    render_axis_h: function\n    render_axis_v: function\n    render_bg: function\n    render_fg: function\n    setup_data: function\n    setup_layout: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    setup_params: function\n    train_panel_guides: function\n    transform: function\n    super:  <ggproto object: Class CoordCartesian, Coord, gg>\n\n\n\n\nMultinomial Sampling\n\n\nCode\nfoo <- rmultinom_table(num_tables = 1000)\n\n# pearson uncorrected\npearson_uncorrected_p <- \n  apply(foo, \n      MARGIN = 3,\n      function(x) {\n        suppressWarnings(chisq.test(x, correct = FALSE))$p.value})\n\n# pearson corrected\npearson_corrected_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(chisq.test(x, correct = TRUE))$p.value})\n\nfisher_exact_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(fisher.test(x))$p.value})\n\n# Pearson N-1 correction equivalent\ncmh_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          mantelhaen_2by2(x)$p.value\n        })\n\n# G-test (LRT)\nlrt_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(g.test(x))$p.value\n        })\n\n\ncontingency_p <- bind_cols(uncorrected = pearson_uncorrected_p,\n          corrected = pearson_corrected_p,\n          exact = fisher_exact_p,\n          cmh = cmh_p,\n          lrt = lrt_p)\n\ncontingency_p %>% \n  pivot_longer(everything(), names_to = \"test\", values_to = \"p\") %>% \n  ggplot(aes(p, color = test)) +\n  stat_ecdf(geom = \"step\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) #+\n\n\n\n\n\nCode\n  coord_cartesian(xlim = c(0, .1), ylim = c(0, .1)) # for zooming in\n\n\n<ggproto object: Class CoordCartesian, Coord, gg>\n    aspect: function\n    backtransform_range: function\n    clip: on\n    default: FALSE\n    distance: function\n    expand: TRUE\n    is_free: function\n    is_linear: function\n    labels: function\n    limits: list\n    modify_scales: function\n    range: function\n    render_axis_h: function\n    render_axis_v: function\n    render_bg: function\n    render_fg: function\n    setup_data: function\n    setup_layout: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    setup_params: function\n    train_panel_guides: function\n    transform: function\n    super:  <ggproto object: Class CoordCartesian, Coord, gg>\n\n\n\n\nHypergeometric Sampling\n\n\nCode\nfoo <- rhyper_table(num_tables = 1000)\n\n# pearson uncorrected\npearson_uncorrected_p <- \n  apply(foo, \n      MARGIN = 3,\n      function(x) {\n        suppressWarnings(chisq.test(x, correct = FALSE))$p.value})\n\n# pearson corrected\npearson_corrected_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(chisq.test(x, correct = TRUE))$p.value})\n\nfisher_exact_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(fisher.test(x))$p.value})\n\n# Pearson N-1 correction equivalent\ncmh_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          mantelhaen_2by2(x)$p.value\n        })\n\n# G-test (LRT)\nlrt_p <- \n  apply(foo,\n        MARGIN = 3,\n        function(x) {\n          suppressWarnings(g.test(x))$p.value\n        })\n\n\ncontingency_p <- bind_cols(uncorrected = pearson_uncorrected_p,\n          corrected = pearson_corrected_p,\n          exact = fisher_exact_p,\n          cmh = cmh_p,\n          lrt = lrt_p)\n\ncontingency_p %>% \n  pivot_longer(everything(), names_to = \"test\", values_to = \"p\") %>% \n  ggplot(aes(p, color = test)) +\n  stat_ecdf(geom = \"step\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", linetype = 2, alpha = .4) #+\n\n\n\n\n\nCode\n  coord_cartesian(xlim = c(0, .1), ylim = c(0, .1)) # for zooming in\n\n\n<ggproto object: Class CoordCartesian, Coord, gg>\n    aspect: function\n    backtransform_range: function\n    clip: on\n    default: FALSE\n    distance: function\n    expand: TRUE\n    is_free: function\n    is_linear: function\n    labels: function\n    limits: list\n    modify_scales: function\n    range: function\n    render_axis_h: function\n    render_axis_v: function\n    render_bg: function\n    render_fg: function\n    setup_data: function\n    setup_layout: function\n    setup_panel_guides: function\n    setup_panel_params: function\n    setup_params: function\n    train_panel_guides: function\n    transform: function\n    super:  <ggproto object: Class CoordCartesian, Coord, gg>"
  },
  {
    "objectID": "categorical/categorical.html#more-than-two-categories",
    "href": "categorical/categorical.html#more-than-two-categories",
    "title": "5  Categorical Data Analysis",
    "section": "5.5 More than two categories",
    "text": "5.5 More than two categories\nThis section starts to get into 2 x I tables, and even more dimensions like, I X J X K tables, and how we analyze those tables. We’ll start with an overview of the multinomial theory, which is fundamental in extending the binomial (2 categories) into multiple categories. The binomial is a special case of the multinomial distribution\n\nnnet::multinom\nVGAM::vglm(family = multinom)\nmlogit::mlogit\n\nA common example we see in this exposition is housing data from library MASS\n\n\nCode\nlibrary(MASS)\ndata(housing)\n\n# The array version\nhousing_arr <- xtabs(Freq~Sat + Infl + Type + Cont, data = housing)\n\n\n\n5.5.1 Poisson GLM Modeling\n\n\nCode\n# simple glm model (satisfaction independent of Infl, Type, Cont)\nhouse_glm <- glm(Freq ~ Infl*Type*Cont + Sat, data = housing, family = poisson)\nsummary(house_glm) # high residual deviance\n\n\n\nCall:\nglm(formula = Freq ~ Infl * Type * Cont + Sat, family = poisson, \n    data = housing)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4.5551  -1.0612  -0.0593   0.6483   4.1478  \n\nCoefficients:\n                                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                        3.136e+00  1.196e-01  26.225  < 2e-16 ***\nInflMedium                         2.733e-01  1.586e-01   1.723 0.084868 .  \nInflHigh                          -2.054e-01  1.784e-01  -1.152 0.249511    \nTypeApartment                      3.666e-01  1.555e-01   2.357 0.018403 *  \nTypeAtrium                        -7.828e-01  2.134e-01  -3.668 0.000244 ***\nTypeTerrace                       -8.145e-01  2.157e-01  -3.775 0.000160 ***\nContHigh                          -2.200e-16  1.690e-01   0.000 1.000000    \nSat.L                              1.159e-01  4.038e-02   2.871 0.004094 ** \nSat.Q                              2.629e-01  4.515e-02   5.824 5.76e-09 ***\nInflMedium:TypeApartment          -1.177e-01  2.086e-01  -0.564 0.572571    \nInflHigh:TypeApartment             1.753e-01  2.279e-01   0.769 0.441783    \nInflMedium:TypeAtrium             -4.068e-01  3.035e-01  -1.340 0.180118    \nInflHigh:TypeAtrium               -1.692e-01  3.294e-01  -0.514 0.607433    \nInflMedium:TypeTerrace             6.292e-03  2.860e-01   0.022 0.982450    \nInflHigh:TypeTerrace              -9.305e-02  3.280e-01  -0.284 0.776633    \nInflMedium:ContHigh               -1.398e-01  2.279e-01  -0.613 0.539715    \nInflHigh:ContHigh                 -6.091e-01  2.800e-01  -2.176 0.029585 *  \nTypeApartment:ContHigh             5.029e-01  2.109e-01   2.385 0.017083 *  \nTypeAtrium:ContHigh                6.774e-01  2.751e-01   2.462 0.013811 *  \nTypeTerrace:ContHigh               1.099e+00  2.675e-01   4.106 4.02e-05 ***\nInflMedium:TypeApartment:ContHigh  5.359e-02  2.862e-01   0.187 0.851450    \nInflHigh:TypeApartment:ContHigh    1.462e-01  3.380e-01   0.432 0.665390    \nInflMedium:TypeAtrium:ContHigh     1.555e-01  3.907e-01   0.398 0.690597    \nInflHigh:TypeAtrium:ContHigh       4.782e-01  4.441e-01   1.077 0.281619    \nInflMedium:TypeTerrace:ContHigh   -4.980e-01  3.671e-01  -1.357 0.174827    \nInflHigh:TypeTerrace:ContHigh     -4.470e-01  4.545e-01  -0.984 0.325326    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 833.66  on 71  degrees of freedom\nResidual deviance: 217.46  on 46  degrees of freedom\nAIC: 610.43\n\nNumber of Fisher Scoring iterations: 5\n\n\nClearly there’s probably some correlation between satisfaction and the other variables, let’s check them out individually.\n\n\nCode\n# addterm will check each term individually, and test marginality\naddterm(house_glm, ~. + Sat:(Infl+Type+Cont), test = \"Chisq\")\n\n\nSingle term additions\n\nModel:\nFreq ~ Infl * Type * Cont + Sat\n         Df Deviance    AIC     LRT   Pr(Chi)    \n<none>        217.46 610.43                      \nInfl:Sat  4   111.08 512.05 106.371 < 2.2e-16 ***\nType:Sat  6   156.79 561.76  60.669 3.292e-11 ***\nCont:Sat  2   212.33 609.30   5.126   0.07708 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInfluence seems to have the largest impact, so we add this to the model, and type probably has a large\n\n\nCode\nhouse_glm1 <- update(house_glm, .~. + Sat:(Infl + Type + Cont))\nsummary(house_glm1)\n\n\n\nCall:\nglm(formula = Freq ~ Infl + Type + Cont + Sat + Infl:Type + Infl:Cont + \n    Type:Cont + Infl:Sat + Type:Sat + Cont:Sat + Infl:Type:Cont, \n    family = poisson, data = housing)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.6022  -0.5282  -0.0641   0.5757   1.9322  \n\nCoefficients:\n                                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                        3.135074   0.120112  26.101  < 2e-16 ***\nInflMedium                         0.248327   0.159979   1.552 0.120602    \nInflHigh                          -0.412645   0.184947  -2.231 0.025671 *  \nTypeApartment                      0.292524   0.157477   1.858 0.063231 .  \nTypeAtrium                        -0.792847   0.214413  -3.698 0.000218 ***\nTypeTerrace                       -1.018074   0.221263  -4.601 4.20e-06 ***\nContHigh                          -0.001407   0.169711  -0.008 0.993385    \nSat.L                             -0.098106   0.112592  -0.871 0.383570    \nSat.Q                              0.285657   0.122283   2.336 0.019489 *  \nInflMedium:TypeApartment          -0.017882   0.210496  -0.085 0.932302    \nInflHigh:TypeApartment             0.386869   0.233297   1.658 0.097263 .  \nInflMedium:TypeAtrium             -0.360311   0.304979  -1.181 0.237432    \nInflHigh:TypeAtrium               -0.036788   0.334793  -0.110 0.912503    \nInflMedium:TypeTerrace             0.185154   0.288892   0.641 0.521580    \nInflHigh:TypeTerrace               0.310749   0.334815   0.928 0.353345    \nInflMedium:ContHigh               -0.200060   0.228748  -0.875 0.381799    \nInflHigh:ContHigh                 -0.725790   0.282352  -2.571 0.010155 *  \nTypeApartment:ContHigh             0.569691   0.212152   2.685 0.007247 ** \nTypeAtrium:ContHigh                0.702115   0.276056   2.543 0.010979 *  \nTypeTerrace:ContHigh               1.215930   0.269968   4.504 6.67e-06 ***\nInflMedium:Sat.L                   0.519627   0.096830   5.366 8.03e-08 ***\nInflHigh:Sat.L                     1.140302   0.118180   9.649  < 2e-16 ***\nInflMedium:Sat.Q                  -0.064474   0.102666  -0.628 0.530004    \nInflHigh:Sat.Q                     0.115436   0.127798   0.903 0.366380    \nTypeApartment:Sat.L               -0.520170   0.109793  -4.738 2.16e-06 ***\nTypeAtrium:Sat.L                  -0.288484   0.149551  -1.929 0.053730 .  \nTypeTerrace:Sat.L                 -0.998666   0.141527  -7.056 1.71e-12 ***\nTypeApartment:Sat.Q                0.055418   0.118515   0.468 0.640068    \nTypeAtrium:Sat.Q                  -0.273820   0.149713  -1.829 0.067405 .  \nTypeTerrace:Sat.Q                 -0.032328   0.149251  -0.217 0.828520    \nContHigh:Sat.L                     0.340703   0.087778   3.881 0.000104 ***\nContHigh:Sat.Q                    -0.097929   0.094068  -1.041 0.297851    \nInflMedium:TypeApartment:ContHigh  0.046900   0.286212   0.164 0.869837    \nInflHigh:TypeApartment:ContHigh    0.126229   0.338208   0.373 0.708979    \nInflMedium:TypeAtrium:ContHigh     0.157239   0.390719   0.402 0.687364    \nInflHigh:TypeAtrium:ContHigh       0.478611   0.444244   1.077 0.281320    \nInflMedium:TypeTerrace:ContHigh   -0.500162   0.367135  -1.362 0.173091    \nInflHigh:TypeTerrace:ContHigh     -0.463099   0.454713  -1.018 0.308467    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 833.657  on 71  degrees of freedom\nResidual deviance:  38.662  on 34  degrees of freedom\nAIC: 455.63\n\nNumber of Fisher Scoring iterations: 4\n\n\nCode\n# sum(residuals(house_glm1, type = \"pearson\")^2) / house_glm1$df.residual # dispersion estimate\nhouse_glm1$df.residual # nrow(housing) - length(coef(house_glm1))\n\n\n[1] 34\n\n\nCode\n# sum(residuals(house_glm1, type = \"deviance\")^2) / house_glm1$df.residual\n# 1 - pchisq(deviance(house_glm1), house_glm1$df.residual) # .267? what's the test here...\n\n\nSee 202 for rescaling the predictions from this model to the probability scale (by the margin of satisfaction)\n\n\nCode\nhnames <- lapply(housing[, -5], levels) # \nhouse_pm <- predict(house_glm1, expand.grid(hnames), type = \"response\") # poisson means exp(\\eta)\nhouse_pm <- matrix(house_pm, ncol = 3, byrow = T, dimnames = list(NULL, hnames[[1]])) # list the predictions into matrix form, columns being satisfaction\ncbind(expand.grid(hnames[-1]), house_pm / rowSums(house_pm)) # normalize by row, and attach the name\n\n\n     Infl      Type Cont       Low    Medium      High\n1     Low     Tower  Low 0.3955687 0.2601077 0.3443236\n2  Medium     Tower  Low 0.2602403 0.2674072 0.4723526\n3    High     Tower  Low 0.1504958 0.1924126 0.6570916\n4     Low Apartment  Low 0.5427582 0.2308450 0.2263968\n5  Medium Apartment  Low 0.3945683 0.2622428 0.3431889\n6    High Apartment  Low 0.2551503 0.2110026 0.5338470\n7     Low    Atrium  Low 0.4294218 0.3220096 0.2485686\n8  Medium    Atrium  Low 0.2959630 0.3468082 0.3572289\n9    High    Atrium  Low 0.1865151 0.2719420 0.5415429\n10    Low   Terrace  Low 0.6453059 0.2178758 0.1368183\n11 Medium   Terrace  Low 0.5076883 0.2678600 0.2244517\n12   High   Terrace  Low 0.3676505 0.2413550 0.3909945\n13    Low     Tower High 0.2982776 0.2813636 0.4203589\n14 Medium     Tower High 0.1847507 0.2723332 0.5429161\n15   High     Tower High 0.1009787 0.1852058 0.7138155\n16    Low Apartment High 0.4375458 0.2669645 0.2954897\n17 Medium Apartment High 0.2974727 0.2836249 0.4189024\n18   High Apartment High 0.1794106 0.2128413 0.6077481\n19    Low    Atrium High 0.3319072 0.3570404 0.3110524\n20 Medium    Atrium High 0.2157414 0.3626615 0.4215970\n21   High    Atrium High 0.1283298 0.2684145 0.6032556\n22    Low   Terrace High 0.5471602 0.2650171 0.1878227\n23 Medium   Terrace High 0.4044226 0.3060991 0.2894783\n24   High   Terrace High 0.2729568 0.2570580 0.4699852\n\n\n\n\n5.5.2 Log Linear Models\nlog linear models with iterative proportional scaling is done with function loglm.\n\n\nCode\nloglm(Freq ~ Infl*Type*Cont + Sat*(Infl + Type + Cont), data = housing)\n\n\nCall:\nloglm(formula = Freq ~ Infl * Type * Cont + Sat * (Infl + Type + \n    Cont), data = housing)\n\nStatistics:\n                      X^2 df  P(> X^2)\nLikelihood Ratio 38.66222 34 0.2671359\nPearson          38.90831 34 0.2582333\n\n\n\n\n5.5.3 Multinomial Models\nThe example data we’ll is use party affiliation:\n\n\nCode\n# array version of data\nparty <- array(c(132, 42, 176, 6, 127, 12,\n                 172, 56, 129, 4, 130, 15), \n               dim = c(2, 3, 2),\n               dimnames = list(race = c(\"white\", \"black\"),\n                               party = c(\"democrat\", \"independent\", \"republican\"),\n                               gender = c(\"male\", \"female\")))\nparty_df <- as.data.frame.table(party) # data frame version of data\n\n# Marginal Tables\nrace_party <- margin.table(party, margin = 1:2)\ngender_party <- margin.table(party, margin = c(3, 2))\nrace_gender <- margin.table(party, margin = c(1, 3))\n\n\n\nnnet\n\n\nCode\nparty_mod <- multinom(party ~ race + gender, weights = Freq, party_df) # democrat is the \"reference\"\n\n\n# weights:  12 (6 variable)\ninitial  value 1099.710901 \niter  10 value 1042.893269\nfinal  value 1042.891187 \nconverged\n\n\nCode\nsummary(party_mod)\n\n\nCall:\nmultinom(formula = party ~ race + gender, data = party_df, weights = Freq)\n\nCoefficients:\n            (Intercept) raceblack genderfemale\nindependent   0.2855582 -2.278140   -0.5727764\nrepublican   -0.0497550 -1.118276   -0.2201972\n\nStd. Errors:\n            (Intercept) raceblack genderfemale\nindependent   0.1125979 0.3427945    0.1575210\nrepublican    0.1198046 0.2335145    0.1582522\n\nResidual Deviance: 2085.782 \nAIC: 2097.782 \n\n\nHypothesis testing with nnet\n\n\nVGAM\n\n\nCode\n# VGAM\n# needs version in which \"stimulus factors\" are separated from \"response\" factors.\nhousing_wide <- housing %>% pivot_wider(names_from = \"Sat\", values_from = \"Freq\")\n\n\nOur saturated dataset is one in which every cell is estimated with a parameter.\n\n\nCode\n# saturated model\nhousing_vglm0 <- vglm(cbind(Low, Medium, High) ~ Infl*Type*Cont, data = housing_wide, family = multinomial)\n\n\nWarning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some\nquantities such as z, residuals, SEs may be inaccurate due to convergence at a\nhalf-step\n\n\nCode\ndeviance(housing_vglm0)\n\n\n[1] -9.792167e-14\n\n\nIn the saturated model, we see that our deviance is equal to 0 because it fits the data perfectly.\n\n\nCode\n# full two way interaction model\nhousing_vglm <- vglm(cbind(Low, Medium, High) ~ (Infl + Type + Cont)^2, data = housing_wide, family = multinomial)\nsummary(housing_vglm)\n\n\n\nCall:\nvglm(formula = cbind(Low, Medium, High) ~ (Infl + Type + Cont)^2, \n    family = multinomial, data = housing_wide)\n\nCoefficients: \n                             Estimate Std. Error z value Pr(>|z|)    \n(Intercept):1              -0.2728053  0.2532115  -1.077  0.28131    \n(Intercept):2              -0.3780064  0.2576046  -1.467  0.14227    \nInflMedium:1                0.1982869  0.3081018   0.644  0.51985    \nInflMedium:2               -0.0083922  0.3171262  -0.026  0.97889    \nInflHigh:1                 -0.9649735  0.3945466  -2.446  0.01445 *  \nInflHigh:2                 -0.8646670  0.3794916  -2.278  0.02270 *  \nTypeApartment:1             1.4992506  0.3153481   4.754 1.99e-06 ***\nTypeApartment:2             0.6555399  0.3307945   1.982  0.04751 *  \nTypeAtrium:1                0.6246612  0.4142176   1.508  0.13154    \nTypeAtrium:2                0.3751106  0.4204712   0.892  0.37233    \nTypeTerrace:1               1.2966645  0.4192741   3.093  0.00198 ** \nTypeTerrace:2               0.4378242  0.4595906   0.953  0.34077    \nContHigh:1                 -0.7397711  0.3116324  -2.374  0.01760 *  \nContHigh:2                 -0.2009809  0.3039959  -0.661  0.50853    \nInflMedium:TypeApartment:1 -1.3982019  0.3535169  -3.955 7.65e-05 ***\nInflMedium:TypeApartment:2 -0.5498060  0.3604313  -1.525  0.12716    \nInflHigh:TypeApartment:1   -0.9211659  0.4576832  -2.013  0.04415 *  \nInflHigh:TypeApartment:2   -0.3138506  0.4358689  -0.720  0.47149    \nInflMedium:TypeAtrium:1    -0.9955454  0.4790778  -2.078  0.03771 *  \nInflMedium:TypeAtrium:2    -0.1868591  0.4525047  -0.413  0.67965    \nInflHigh:TypeAtrium:1       0.1416714  0.5758309   0.246  0.80566    \nInflHigh:TypeAtrium:2       0.2132503  0.5362081   0.398  0.69085    \nInflMedium:TypeTerrace:1   -0.9030966  0.4563464  -1.979  0.04782 *  \nInflMedium:TypeTerrace:2    0.0081197  0.4836471   0.017  0.98661    \nInflHigh:TypeTerrace:1     -0.8443360  0.5889810  -1.434  0.15170    \nInflHigh:TypeTerrace:2     -0.2200667  0.5911664  -0.372  0.70970    \nInflMedium:ContHigh:1       0.0119167  0.2883493   0.041  0.96703    \nInflMedium:ContHigh:2      -0.0735225  0.3046628  -0.241  0.80930    \nInflHigh:ContHigh:1        -0.2258679  0.3504496  -0.645  0.51925    \nInflHigh:ContHigh:2         0.0318502  0.3496997   0.091  0.92743    \nTypeApartment:ContHigh:1    0.1350432  0.3218707   0.420  0.67481    \nTypeApartment:ContHigh:2   -0.0002221  0.3186735  -0.001  0.99944    \nTypeAtrium:ContHigh:1       0.3409407  0.4320950   0.789  0.43009    \nTypeAtrium:ContHigh:2       0.2975587  0.4139167   0.719  0.47221    \nTypeTerrace:ContHigh:1      1.1520381  0.4173381   2.760  0.00577 ** \nTypeTerrace:ContHigh:2      0.6313710  0.4353792   1.450  0.14701    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: log(mu[,1]/mu[,3]), log(mu[,2]/mu[,3])\n\nResidual deviance: 5.9443 on 12 degrees of freedom\n\nLog-likelihood: -102.5404 on 12 degrees of freedom\n\nNumber of Fisher scoring iterations: 3 \n\nNo Hauck-Donner effect found in any of the estimates\n\n\nReference group is level  3  of the response\n\n\nCode\ndeviance(housing_vglm)\n\n\n[1] 5.944319\n\n\nCode\n# Lack of fit tests\n1 - pchisq(deviance(housing_vglm), df.residual(housing_vglm)) # deviance\n\n\n[1] 0.9188628\n\n\nCode\n1 - pchisq(sum(residuals(housing_vglm, type = \"pearson\")^2), df.residual(housing_vglm)) # pearson\n\n\n[1] 0.917741\n\n\nThe null hypothesis here is that the model is specified correctly. High p-values mean we fail to reject that the model is correct. In general, lack of fit tests are pretty bad tests for telling us any information. We would prefer to do some manual model searching.\n\n\nCode\ndrop1(housing_vglm, test = \"LRT\")\n\n\nSingle term deletions\n\nModel:\ncbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n          Df Deviance    AIC     LRT Pr(>Chi)  \n<none>         5.9443 277.08                   \nInfl:Type 12  27.0024 274.14 21.0581  0.04954 *\nInfl:Cont  4   6.8284 269.96  0.8841  0.92684  \nType:Cont  6  15.2341 274.37  9.2898  0.15793  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results here say that we could probably drop Infl:Cont and Type:Cont. In fact, dropping Infl:Cont, we would get the biggest drop in AIC, indicating better model fit for number of parameters we estimate.\n\n\nCode\nhousing_vglm1 <- update(housing_vglm, .~. - Infl:Cont)\n\n\nWarning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some\nquantities such as z, residuals, SEs may be inaccurate due to convergence at a\nhalf-step\n\n\nCode\ndrop1(housing_vglm1, test = \"LRT\")\n\n\nSingle term deletions\n\nModel:\ncbind(Low, Medium, High) ~ Infl + Type + Cont + Infl:Type + Type:Cont\n          Df Deviance    AIC     LRT Pr(>Chi)  \n<none>         6.8284 269.96                   \nInfl:Type 12  28.2559 267.39 21.4275  0.04446 *\nType:Cont  6  16.1072 267.24  9.2788  0.15850  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDoing it again shows we could again get lower AIC by dropping either parameter…. an automated way of doing this can be done through step4vglm\n\n\nCode\n# Forward-Backward Step selection on 2-way interaction model\nhousing_vglm_step <- step4vglm(housing_vglm, direction = \"both\")\n\n\nStart:  AIC=277.08\ncbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n\n            Df Deviance    AIC\n- Infl:Cont  4   6.8284 269.96\n- Infl:Type 12  27.0024 274.14\n- Type:Cont  6  15.2341 274.37\n<none>           5.9443 277.08\n\n\nWarning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some\nquantities such as z, residuals, SEs may be inaccurate due to convergence at a\nhalf-step\n\n\n\nStep:  AIC=269.96\ncbind(Low, Medium, High) ~ Infl + Type + Cont + Infl:Type + Type:Cont\n\n            Df Deviance    AIC\n- Type:Cont  6  16.1072 267.24\n- Infl:Type 12  28.2559 267.39\n<none>           6.8284 269.96\n+ Infl:Cont  4   5.9443 277.08\n\nStep:  AIC=267.24\ncbind(Low, Medium, High) ~ Infl + Type + Cont + Infl:Type\n\n            Df Deviance    AIC\n- Infl:Type 12   38.662 265.80\n<none>           16.107 267.24\n+ Type:Cont  6    6.828 269.96\n+ Infl:Cont  4   15.234 274.37\n- Cont       2   32.871 280.01\n\nStep:  AIC=265.8\ncbind(Low, Medium, High) ~ Infl + Type + Cont\n\n            Df Deviance    AIC\n<none>           38.662 265.80\n+ Infl:Type 12   16.107 267.24\n+ Type:Cont  6   28.256 267.39\n+ Infl:Cont  4   37.472 272.61\n- Cont       2   54.722 277.86\n- Type       6  100.889 316.03\n- Infl       4  147.780 366.92\n\n\nCode\nhousing_vglm_step@post$anova # shows the steps that the algorithm took\n\n\n         Step Df   Deviance Resid. Df Resid. Dev      AIC\n1             NA         NA        12   5.944319 277.0807\n2 - Infl:Cont  4  0.8840624        16   6.828381 269.9648\n3 - Type:Cont  6  9.2787763        22  16.107157 267.2436\n4 - Infl:Type 12 22.5550474        34  38.662205 265.7986\n\n\nThe steps the algorithm is saved in the slot @post$anova. We can see that the additive model was selected, dropping all the interactions.\n\n\nCode\n# additive model\nhousing_vglm2 <- vglm(cbind(Low, Medium, High) ~ Infl + Type + Cont, data = housing_wide, family = multinomial)\n\nanova(housing_vglm2, housing_vglm, type = 1)\n\n\nAnalysis of Deviance Table\n\nModel 1: cbind(Low, Medium, High) ~ Infl + Type + Cont\nModel 2: cbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)  \n1        34     38.662                       \n2        12      5.944 22   32.718  0.06595 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nthere’s weak evidence that those dropped coefficients were not zero… so in favor of the more parsimonious and interpretable model, we choose the additive model. Now we do some diagnostics, show the mosaic plot of the pearson chisq values.\n\n\nCode\nsum(residuals(housing_vglm2, \"pearson\")^2) # asymptotically the same\n\n\n[1] 38.91043\n\n\nCode\ndeviance(housing_vglm2) # pretty darn close\n\n\n[1] 38.6622\n\n\nCode\n# grab standardized residuals... I think theses are on the raw scale, need to derive\nhousing_vglm2_stdres <- housing_wide %>% \n  dplyr::select(Infl,Type, Cont) %>% \n  bind_cols(residuals(housing_vglm2, \"stdres\")) %>% \n  pivot_longer(Low:High, names_to = \"Sat\", values_to = \"stdres\")\n\nfoo <- xtabs(stdres~Sat + Infl + Type, data=housing_vglm2_stdres)\n\nfoo\n\n\n, , Type = Tower\n\n        Infl\nSat             Low     Medium       High\n  High    2.3368820  1.7993578  7.6329517\n  Low    -3.1561606 -1.7459604 -5.4886972\n  Medium  0.7896368 -0.1247062 -2.5826432\n\n, , Type = Apartment\n\n        Infl\nSat             Low     Medium       High\n  High   -8.7451504  1.0581085  7.6844538\n  Low     9.5841342 -1.4240255 -5.7464336\n  Medium -0.5702533  0.3521384 -2.3637276\n\n, , Type = Atrium\n\n        Infl\nSat             Low     Medium       High\n  High   -2.3120155  0.8251904  2.0911215\n  Low     0.4934728 -3.1374902 -2.6638237\n  Medium  2.0341706  2.4451851  0.5348104\n\n, , Type = Terrace\n\n        Infl\nSat             Low     Medium       High\n  High   -7.1933577 -4.3805628  2.2529711\n  Low     8.6764258  2.8198862 -1.6825344\n  Medium -1.3182128  1.8356588 -0.6954066\n\n\nCode\nmosaicplot(foo)\n\n\n\n\n\n\n\nCode\nhousing_wide_matrix <- housing_wide %>% dplyr::select(Low:High) %>% \n  data.matrix()\nsat_margin <- housing_wide_matrix %>% rowSums()\n\nhousing_vglm2_predicted <- fitted(housing_vglm2) * sat_margin\n(housing_wide_matrix - housing_vglm2_predicted) / sqrt(housing_vglm2_predicted)\n\n\n              Low      Medium         High\n [1,] -1.27131702  0.65442726  0.793847557\n [2,]  2.05554037 -0.52448903 -1.131108776\n [3,]  0.48542299  0.00980894 -0.237618822\n [4,]  0.83488118 -0.06530752 -1.226738951\n [5,] -0.52159491  0.72901366 -0.077987991\n [6,]  0.19903520 -0.58897340  0.232680471\n [7,] -0.20002910 -0.40632189  0.725380811\n [8,] -0.09968461 -0.54894903  0.631617806\n [9,]  0.93631742  0.41590003 -0.844215602\n[10,] -0.44816554 -0.29018330  1.339493961\n[11,] -1.27460531  0.60886282  1.251820928\n[12,] -0.50068951 -0.23393207  0.669307443\n[13,] -1.50554299 -0.15670496  1.396421868\n[14,]  0.57743627  0.25994911 -0.520953337\n[15,] -0.07366774 -0.30940893  0.185311590\n[16,]  0.57671852  0.21220772 -0.903490751\n[17,] -0.71913615 -0.80963872  1.272211462\n[18,] -0.77139003  0.70614331  0.001230827\n[19,] -0.19903792  0.10678590  0.091194172\n[20,] -0.59885245  0.37522109  0.080380548\n[21,]  0.96158954 -0.06254569 -0.401788912\n[22,]  0.85710435 -0.33167080 -1.068931434\n[23,]  0.91913589  0.24740415 -1.340806857\n[24,] -0.60596707 -0.06819771  0.512236271\n\n\nCode\nresiduals(housing_vglm2, type = \"response\") + fitted(housing_vglm2)\n\n\n          Low    Medium      High\n1  0.30000000 0.3000000 0.4000000\n2  0.36956522 0.2391304 0.3913043\n3  0.17543860 0.1929825 0.6315789\n4  0.60396040 0.2277228 0.1683168\n5  0.36440678 0.2966102 0.3389831\n6  0.26530612 0.1836735 0.5510204\n7  0.40625000 0.2812500 0.3125000\n8  0.28571429 0.2857143 0.4285714\n9  0.27272727 0.3181818 0.4090909\n10 0.58064516 0.1935484 0.2258065\n11 0.36585366 0.3170732 0.3170732\n12 0.30434783 0.2173913 0.4782609\n13 0.20000000 0.2714286 0.5285714\n14 0.21250000 0.2875000 0.5000000\n15 0.09677419 0.1612903 0.7419355\n16 0.46706587 0.2754491 0.2574850\n17 0.26815642 0.2513966 0.4804469\n18 0.14705882 0.2450980 0.6078431\n19 0.31746032 0.3650794 0.3174603\n20 0.17857143 0.3928571 0.4285714\n21 0.18421053 0.2631579 0.5526316\n22 0.61290323 0.2473118 0.1397849\n23 0.47692308 0.3230769 0.2000000\n24 0.20833333 0.2500000 0.5416667\n\n\nCode\nobs_p <- housing_wide_matrix / sat_margin\nfit_p <- fitted(housing_vglm2)\n\nresiduals(housing_vglm2, type = \"response\")\n\n\n           Low        Medium          High\n1  -0.09556873  0.0398922904  5.567644e-02\n2   0.10932496 -0.0282767182 -8.104824e-02\n3   0.02494279  0.0005699035 -2.551270e-02\n4   0.06120222 -0.0031222147 -5.808001e-02\n5  -0.03016154  0.0343673813 -4.205846e-03\n6   0.01015582 -0.0273291797  1.717336e-02\n7  -0.02317183 -0.0407595729  6.393140e-02\n8  -0.01024868 -0.0610938738  7.134255e-02\n9   0.08621220  0.0462397822 -1.324520e-01\n10 -0.06466070 -0.0243274212  8.898812e-02\n11 -0.14183466  0.0492131823  9.262148e-02\n12 -0.06330269 -0.0239637078  8.726640e-02\n13 -0.09827758 -0.0099349949  1.082126e-01\n14  0.02774930  0.0151667891 -4.291609e-02\n15 -0.00420447 -0.0239154916  2.811996e-02\n16  0.02952007  0.0084845677 -3.800464e-02\n17 -0.02931623 -0.0322282664  6.154450e-02\n18 -0.03235176  0.0322567566  9.500771e-05\n19 -0.01444687  0.0080390048  6.407869e-03\n20 -0.03717000  0.0301956212  6.974382e-03\n21  0.05588069 -0.0052566447 -5.062404e-02\n22  0.06574300 -0.0177052766 -4.803772e-02\n23  0.07250046  0.0169777976 -8.947826e-02\n24 -0.06462348 -0.0070579688  7.168145e-02\n\n\nCode\n# varfun <- object@family@charfun\n# vfun <- varfun(x = NULL, eta = predict(object), \n#                   extra = object@extra, varfun = TRUE)\n#                 ans <- (y - E1)/sqrt(vfun * (1 - c(hatvalues(object))))\n# \n# 1 - hatvalues(housing_vglm2)\n# \n# varfun <- housing_vglm2@family@charfun\n# vfun <- varfun(x = NULL, eta = predict(housing_vglm2), extra = housing_vglm2@extra, varfun = TRUE)\n# \n# housing_vglm2@family@vfamily\n# \n# w <- weights(object, type = \"prior\")\n#                 x <- y * c(w)\n#                 E1 <- E1 * c(w)\n#                 if (any(x < 0) || anyNA(x)) stop(\"all entries of 'x' must be nonnegative and finite\")\n#                 if ((n <- sum(x)) == 0) stop(\"at least one entry of 'x' must be positive\")\n#                 if (length(dim(x)) > 2L) stop(\"invalid 'x'\")\n#                 if (length(x) == 1L) stop(\"'x' must at least have 2 elements\")\n#                 sr <- rowSums(x)\n#                 sc <- colSums(x)\n#                 E <- outer(sr, sc, \"*\")/n\n#                 v <- function(r, c, n) c * r * (n - r) * (n - \n#                   c)/n^3\n#                 V <- outer(sr, sc, v, n)\n#                 dimnames(E) <- dimnames(x)\n#                 ans <- stdres <- (x - E)/sqrt(V)\n# rowSums(fitted)\n# \n# \n# housing_vglm2@y # observed\nplot(housing_vglm2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncoef(housing_vglm2, matrix = TRUE)\n\n\n              log(mu[,1]/mu[,3]) log(mu[,2]/mu[,3])\n(Intercept)            0.1387428         -0.2804860\nInflMedium            -0.7348632         -0.2884673\nInflHigh              -1.6126311         -0.9476957\nTypeApartment          0.7356317          0.2999430\nTypeAtrium             0.4079781          0.5393484\nTypeTerrace            1.4123277          0.7457572\nContHigh              -0.4818270         -0.1209751\n\n\n\n\nCode\n# the predicted probabilities by each of the covariates.\nbind_cols(housing_wide %>% dplyr::select(Infl, Type, Cont),\n          fitted(housing_vglm2))\n\n\n# A tibble: 24 × 6\n   Infl   Type      Cont    Low Medium  High\n   <fct>  <fct>     <fct> <dbl>  <dbl> <dbl>\n 1 Low    Tower     Low   0.396  0.260 0.344\n 2 Medium Tower     Low   0.260  0.267 0.472\n 3 High   Tower     Low   0.150  0.192 0.657\n 4 Low    Apartment Low   0.543  0.231 0.226\n 5 Medium Apartment Low   0.395  0.262 0.343\n 6 High   Apartment Low   0.255  0.211 0.534\n 7 Low    Atrium    Low   0.429  0.322 0.249\n 8 Medium Atrium    Low   0.296  0.347 0.357\n 9 High   Atrium    Low   0.187  0.272 0.542\n10 Low    Terrace   Low   0.645  0.218 0.137\n# … with 14 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\nCode\nanova(housing_vglm1, housing_vglm, type = 1) # score tests not available in VGAM currently\n\n\nAnalysis of Deviance Table\n\nModel 1: cbind(Low, Medium, High) ~ Infl + Type + Cont + Infl:Type + Type:Cont\nModel 2: cbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)\n1        16     6.8284                     \n2        12     5.9443  4  0.88406   0.9268\n\n\nCode\ndeviance(housing_vglm)\n\n\n[1] 5.944319\n\n\nCode\ndropterm(housing_vglm, test = \"Chisq\")\n\n\nSingle term deletions\n\nModel:\ncbind(Low, Medium, High) ~ (Infl + Type + Cont)^2\n          Df    AIC     LRT Pr(Chi)  \n<none>       277.08                  \nInfl:Type 12 274.14 21.0581 0.04954 *\nInfl:Cont  4 269.96  0.8841 0.92684  \nType:Cont  6 274.37  9.2898 0.15793  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "changepoint/changepoint.html#simulate-the-data",
    "href": "changepoint/changepoint.html#simulate-the-data",
    "title": "6  Changepoint Analysis",
    "section": "6.1 Simulate the data",
    "text": "6.1 Simulate the data\n\n\nCode\ndays <- 1:30\nbp <- 10 # Breakpoint\nn <- 4 # number of measurements\nbeta <- 3 # estimated slope of change at bp\nsigma <- 3 # estimated residual error\n\n# True model\ntm <- Vectorize(function(days, bp, beta, n, sigma) {\n  if (days < 10) {\n    return(rnorm(n, mean = 0, sd = sigma))\n  }\n  else if (days >= 10) {\n    return(rnorm(n, mean = (days - bp)*beta, sd = sigma))\n  }\n}, \"days\")\n\ndat_sim <- tm(1:30, bp, beta, n, sigma)\n\n\n\n\nCode\ndat <- data.frame(days = rep(days, each = n),\n                  y = c(dat_sim)) # stack all columns of matrix into vector\n\ndat %>% ggplot(aes(days, y)) +\n  geom_point()\n\n\n\n\n\n\n\nCode\nmod <- nls(y ~ ifelse(days < 10, constant, (days - 10) *beta + constant),\n    data = dat,\n    start = c(constant = 0, beta = 3))\n\nsummary\n\n\nfunction (object, ...) \nUseMethod(\"summary\")\n<bytecode: 0x7fa874db5110>\n<environment: namespace:base>\n\n\nCode\nplot(dat$days, dat$y)\ncurve(predict(mod, newdata = data.frame(days = x)), add = TRUE)"
  },
  {
    "objectID": "crossover/crossover.html#introduction",
    "href": "crossover/crossover.html#introduction",
    "title": "7  Crossover Designs",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\n\nuniform in sequence\nuniform in period\nbalanced\n\ncarry over effects are all balanced\n\nstrongly balanced\nincluding following self\n\nEffects of interest\n\n“period effect”\n“treatment effect”\n“carryover effect”\n“treatment x period”\n“subject x treatment”\n“subject x period”\n\n\n\nCode\ntribble(~Advantages, ~Disadvantages,\n        \"fewer samples\", \"\",\n        \"more precision per unit\", \"\")\n\n\n# A tibble: 2 × 2\n  Advantages              Disadvantages\n  <chr>                   <chr>        \n1 fewer samples           \"\"           \n2 more precision per unit \"\""
  },
  {
    "objectID": "crossover/crossover.html#x2-design",
    "href": "crossover/crossover.html#x2-design",
    "title": "7  Crossover Designs",
    "section": "7.2 2x2 Design",
    "text": "7.2 2x2 Design\n\n7.2.1 Example: PEF\n\nThe data in Table 3.1 are taken from those which were reported in Senn and Auclair 1990, p. 1290). They are measurements of peak expiratory flow PEF), a measure of lung function National Asthma Education Program, 1991, pp. 6±9), made on 13 children aged 7 to 14 with moderate or severe asthma in a two-treatment two-period cross-over comparing the effects of a single inhaled dose of 200g salbutamol, a well-established bronchodilator, and 12g formoterol, a more recently developed bronchodilator Faulds et al., 1991). The children were randomized to one of two sequence groups. In one group they were given a dose of formoterol in the morning and observed for 8 h in the clinic. They then travelled home where they or their parents took further measurements 10, 11 and 12 h after treatment. PEF is a measurement which patients may record themselves using a simple device.) On a subsequent occasion after a wash-out of at least one day they presented at the clinic again and were given a single dose of salbutamol. Measurements in the clinic followed as before and were again succeeded by measurements at home. For the second sequence group, the procedure was as for the first except that they received salbutamol on the first visit to the clinic and formoterol on the second visit to the clinic.\n\nPeak Exploratory Flow\n\nid - patient id\ntrt - two levels, formoterol (for) and salbutamol (sal)\npef - peak exploratory flow\nperiod - period of experiment\n\n\n\nCode\npef <- read.csv(\"data/pef.csv\") %>% \n  # create variable for treatment sequence\n  mutate(seq = ifelse((trt == \"for\" & period == 1) |\n                        (trt == \"sal\" & period == 2),\n                      \"FS\",\n                      \"SF\"),\n         trt = dplyr::recode(trt,\n                             \"for\" = \"formo\",\n                             \"sal\" = \"salbu\")) %>% \n  rename(\"trt_seq\" = \"seq\")\n\npef %>% ggplot(aes(id, pef, color = trt)) +\n  geom_point() +\n  facet_wrap(~trt_seq)\n\n\n\n\n\n\n\n7.2.2 Analysis\nThe easiest analysis is probably the paired t-test. This throws away information about the periods in the study.\n\n\nCode\npef_wide <- pef %>% pivot_wider(id_cols = c(id, trt_seq), names_from = trt, values_from = \"pef\") %>% \n  mutate(diff = formo - salbu)\npef_wide$diff %>% mean() # 45.385\n\n\n[1] 45.38462\n\n\nCode\npef_wide$diff %>% sd() # 40.593\n\n\n[1] 40.59257\n\n\nCode\npef_t <- t.test(pef_wide$formo, pef_wide$salbu, paired = TRUE)\n\n45.384462 / 4.0312 # the estimated standard error\n\n\n[1] 11.2583\n\n\nCode\npef_t\n\n\n\n    Paired t-test\n\ndata:  pef_wide$formo and pef_wide$salbu\nt = 4.0312, df = 12, p-value = 0.001666\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 20.85477 69.91446\nsample estimates:\nmean difference \n       45.38462 \n\n\nThe next analysis is to adjust for the period effect with just to do a sample t test of the differences by period.\n\n\nCode\npef_FS <- pef_wide %>% filter(trt_seq == \"FS\") %>% pull(diff)\npef_SF <- pef_wide %>% filter(trt_seq == \"SF\") %>% pull(diff)\n\npef_FS_SS <- sum((pef_FS - mean(pef_FS))^2) # FS sum squares\npef_SF_SS <- sum((pef_SF - mean(pef_SF))^2) # SF sum squares\n\n((length(pef_FS) - 1) * pef_FS_SS + (length(pef_SF) -1) * pef_SF_SS) / (length(pef_FS) + length(pef_SF) - 2)\n\n\n[1] 8096.916\n\n\nCode\n(pef_FS_SS + pef_SF_SS) / (length(pef_FS) + length(pef_SF) - 2) # 1500.81\n\n\n[1] 1500.812\n\n\nCode\nsd_pooled(pef_FS, pef_SF)\n\n\n[1] 38.74031\n\n\nCode\n# pooled estimate of variance\n\nt.test(scale(pef_FS, scale = F), scale(pef_SF, scale = F), var.equal = FALSE)\n\n\n\n    Welch Two Sample t-test\n\ndata:  scale(pef_FS, scale = F) and scale(pef_SF, scale = F)\nt = -9.1837e-17, df = 9.1017, p-value = 1\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -49.89732  49.89732\nsample estimates:\n    mean of x     mean of y \n-2.029131e-15  0.000000e+00"
  },
  {
    "objectID": "diffeq/diffeq.html#software",
    "href": "diffeq/diffeq.html#software",
    "title": "8  Differential Equations",
    "section": "8.1 Software",
    "text": "8.1 Software\nIn R, there are a number of packages that deal with differential equations. CRAN View\n\ndeSolve - the main integrator\nReacTran - simulating reaction diffusion equations\nFMA - “flexible modeling\nbvpSolve - boundary value problems\nsimecol - interactive enviromnent for implementing models\n\nStochastic Differential Equations have a different set\n\nsde - simulatio n and inference of stochastic differential equations\nGillespieSSA\nadaptivetau - are for quick simulation\n\n\n8.1.1 deSolve Package\n\n\n8.1.2 FME Package\nStands for flexible modeling pacakge, allows one to define some cost function\n\n\n8.1.3 rootsolver"
  },
  {
    "objectID": "diffeq/diffeq.html#ode",
    "href": "diffeq/diffeq.html#ode",
    "title": "8  Differential Equations",
    "section": "8.2 ODE",
    "text": "8.2 ODE\nUsing ode from package deSolve\n\n8.2.1 Example: Prey and Predator\n\n\nCode\nLVmod <- function(Time, State, Pars) {\n  with(as.list(c(State, Pars)), {\n    Ingestion    <- rIng  * Prey * Predator #  \\beta x y\n    GrowthPrey   <- rGrow * Prey * (1 - Prey/K) # \\alpha x\n    MortPredator <- rMort * Predator\n\n    dPrey        <- GrowthPrey - Ingestion\n    dPredator    <- Ingestion * assEff - MortPredator\n\n    return(list(c(dPrey, dPredator)))\n  })\n}\n\npars  <- c(rIng   = 0.2,    # /day, rate of ingestion\n           rGrow  = 1.0,    # /day, growth rate of prey\n           rMort  = 0.2 ,   # /day, mortality rate of predator\n           assEff = 0.5,    # -, assimilation efficiency\n           K      = 10)     # mmol/m3, carrying capacity\n\nyini  <- c(Prey = 1, Predator = 2) # initial states of the system\ntimes <- seq(0, 200, by = 1) # initial time\nout   <- ode(yini, times, LVmod, pars)\nsummary(out)\n\n\n               Prey    Predator\nMin.      1.0000000   1.8632829\n1st Qu.   1.9999571   3.9999115\nMedian    2.0000000   4.0000000\nMean      2.0317905   3.9606228\n3rd Qu.   2.0000751   4.0000418\nMax.      4.2001812   4.9787222\nN       201.0000000 201.0000000\nsd        0.3138139   0.3489079\n\n\n\n\nCode\n## Default plot method\nplot(out)\n\n\n\n\n\n\n\nCode\n## User specified plotting\nmatplot(out[ , 1], out[ , 2:3], type = \"l\", xlab = \"time\", ylab = \"Conc\",\n        main = \"Lotka-Volterra\", lwd = 2)\nlegend(\"topright\", c(\"prey\", \"predator\"), col = 1:2, lty = 1:2)\n\n\n\n\n\n\n\n8.2.2 Example: Substrate Producer/Consumer\n\n\nCode\nSPCmod <- function(t, x, parms, input)  {\n  with(as.list(c(parms, x)), {\n    import <- input(t)\n    dS <- import - b*S*P + g*C    # substrate\n    dP <- c*S*P  - d*C*P          # producer\n    dC <- e*P*C  - f*C            # consumer\n    res <- c(dS, dP, dC)\n    list(res)\n  })\n}\n\n## The parameters \nparms <- c(b = 0.001, c = 0.1, d = 0.1, e = 0.1, f = 0.1, g = 0.0)\n\n## vector of timesteps\ntimes <- seq(0, 200, length = 101)\n\n## external signal with rectangle impulse\nsignal <- data.frame(times = times,\n                     import = rep(0, length(times)))\n\nsignal$import[signal$times >= 10 & signal$times <= 11] <- 0.2\n\nsigimp <- approxfun(signal$times, signal$import, rule = 2)\n\n## Start values for steady state\nxstart <- c(S = 1, P = 1, C = 1)\n\n## Solve model\nout <- ode(y = xstart, times = times,\n           func = SPCmod, parms = parms, input = sigimp)\n\n## Default plot method\nplot(out)\n\n## User specified plotting\nmf <- par(mfrow = c(1, 2))\n\n\n\n\n\nCode\nmatplot(out[,1], out[,2:4], type = \"l\", xlab = \"time\", ylab = \"state\")\nlegend(\"topright\", col = 1:3, lty = 1:3, legend = c(\"S\", \"P\", \"C\"))\nplot(out[,\"P\"], out[,\"C\"], type = \"l\", lwd = 2, xlab = \"producer\",\n  ylab = \"consumer\")\n\n\n\n\n\nCode\npar(mfrow = mf)\n\n\n\n\n8.2.3 Example: SIR Modeling\nA simple example is provided by https://www.r-bloggers.com/2017/01/sir-model-with-desolve-ggplot2/\n\n\nCode\n# definition of the differential equation\nsir_diffeq <- function(Time, State, Pars) {\n  with(as.list(c(State, Pars)), {\n    dSusceptible <- -beta * Infected * Susceptible\n    dInfected <- beta * Infected * Susceptible - gamma * Infected\n    dRecovered <- gamma * Infected\n    return(list(c(dSusceptible, dInfected, dRecovered)))\n  })\n}\n\n# solver for differential equation\nsir_ode <- function(beta = 1, gamma = .2,\n                initial_state = c(Susceptible = 1-1e-6, # initial state defined\n                                  Infected = 1e-6,\n                                  Recovered = 0),\n                times = seq(0, 100, by = 1)) {\n  pars <- c(beta = beta, gamma = gamma)\n  \n  # check that initial state has the correctly named variables\n  ode(initial_state, times, sir_diffeq, pars)\n}\n\n\n\n\nCode\n# Parameter  values\nbeta <- 1\ngamma <- .2\n\nout <- sir_ode(beta = beta, gamma = gamma)\n\nout %>% as.data.frame() %>%\n  pivot_longer(Susceptible:Recovered, names_to = \"state\", values_to = \"prop\") %>% \n  ggplot(aes(time, prop)) + \n  geom_line(aes(color = state)) +\n  theme_minimal() + \n  labs(title = sprintf(\"SIR, beta = %g, gamma = %g\", beta, gamma))\n\n\n\n\n\n\n\n\n\n\n8.2.4 Example: Rossler attractor\nThis example is a non-linear ordinary differential equation system with chaotic dynamics\n\n\\begin{aligned}\n\\frac{dx}{dt} &= -y - z \\\\\n\\frac{dy}{dt} &= x + ay \\\\\n\\frac{dz}{dt} &= b + z(x -c) \\\\\n\\end{aligned}\n\nequations x and y are linear which gives the attractor some regularity\n\n\nCode\nrosslerode <- function(t, state, parms) {\n  with(as.list(state), {\n    dx <- -y - z\n    dy <- x + .2 * y\n    dz <- .2 + z * (x - 5)\n    return(list(c(dx, dy, dz)))\n  })\n}\n\nyini <- c(x=1, y = 0, z = .9)\ntimes <- seq(0, 100, .05)\n\nout <- ode(times = times, y = yini, func = rosslerode, parms = NULL)\nscatterplot3d::scatterplot3d(out[,\"x\"], out[,\"y\"], out[,\"z\"], pch = 19, cex.symbols = .5)\n\n\n\n\n\n\n\nCode\nrossler_plotly <- plot_ly(out %>% as.data.frame(), x = ~x, y=~y, z= ~z,\n                          marker = list(color= ~time, colorscale = c(\"#FFFFFF\", \"#683531\")))\nrossler_plotly\n\n\nNo trace type specified:\n  Based on info supplied, a 'scatter3d' trace seems appropriate.\n  Read more about this trace type -> https://plotly.com/r/reference/#scatter3d\n\n\nNo scatter3d mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -> https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\n\n8.2.5 Example: Van der Pol Equation\nThis is a good example of a system that can be either stiff or non-stiff.\n\nFor large values of \\mu, it’s stiff.\nFor small values of \\mu, it’s non stiff\n\nThe 2nd order ODE can be transformed into a 2 first order equations:\n\n\\begin{aligned}\ny'' - \\mu(1 - y^2)y' + y &= 0 \\\\\\\\\nx' &= y\\\\\ny' &= \\mu(1 - x^2)y - x\n\\end{aligned}\n\n\n\nCode\nvdpol <- function(t, state, mu) {\n  with(as.list(state), {\n    list(c(y, mu * (1 - x^2) *y - x))\n  })\n}\n\nyini <- c(x = 2, y = 0)\nvdpol_stiff <- ode(yini, times = 0:3000, func = vdpol, parms = 1000)\nvdpol_nonstiff <- ode(yini, times = seq(0, 30, .01), func = vdpol, parms = 1)\n\nplot(vdpol_stiff, type = \"l\", main = \"IVP ODE, stiff\", xlim = c(0, 3000), which = \"x\")\n\n\n\n\n\nCode\nplot( vdpol_nonstiff,type = \"l\", main = \"IVP ODE, nonstiff\", xlim = c(0, 30), which = \"x\")"
  },
  {
    "objectID": "epidemiology/epidemiology.html#contagion-process-on-time-varying-networks",
    "href": "epidemiology/epidemiology.html#contagion-process-on-time-varying-networks",
    "title": "10  Epidemiology",
    "section": "10.1 Contagion Process on Time-Varying Networks",
    "text": "10.1 Contagion Process on Time-Varying Networks\nThis section will explore some of the results of the paper Temporal Gillespie Algorithm: Fast Simulation of Contagion Processes on Time-Varying Networks.\nThere are some included files for the algorithms described in the paper. There doesn’t seem to be too much special about these programs, but well work the time component into the network. I think the major disadvantages of these data is\n./SIR <data> dt beta mu T_simulation ensembleSize outputTimeResolution\nwhere:\n  <data> - path of text file containing contact data (temporal network);\n  dt - time-resolution of recorded contact data (time-step length);\n  beta - probability per time-step of infection when in contact with an\n    infected node;\n  mu - probability per time-step of recovery for an infected node;\n  T_simulation - length of simulation in number of time-steps;\n  ensembleSize - number of independent realizations of the SIR process;\n  outputTimeResolution - time-resolution of the average number of infected\n    and recovered nodes that the program gives as output."
  },
  {
    "objectID": "experimentaldesign/experimentaldesign.html",
    "href": "experimentaldesign/experimentaldesign.html",
    "title": "10  Experimental Design",
    "section": "",
    "text": "11 Introduction\n\\begin{aligned}\nY_{ijk} = \\alpha_i + \\beta_j + \\varepsilon_{ijk}\n\\end{aligned}\nAt its core, one should understand the features of a two way anova in order to understand what blocking does to the covariate.\nThe classic split plot design presented in most textbooks is one in which the whole plot experimental units (wpeu) treated by factor A (with a levels) have r replications (either in the form of block replicates or identical replicates), and the sub-plot experimental units (speu) is completely randomized within the whole plot.\nThe defining feature of split plot designs is that there are multiple sizes of experimental units.\nWhen working with a client, it seems that the labeling of nested variables is quite the headache, so this section is to work out the differences in the estimation in linear models. and where the differences in ANOVA appear.\nThe numbering in this dataset is such that the block numbering goes across the environments. consider the following two models and their anova tables\nSubstituting environ:block123 with block when the numbering is the same gives the same ANOVA table.\nThe difference between these two models is that the nesting is more explicitly stated when the blocks are labeled within each environment and that there is no “aliasing” of effects either. It might not be clear from the second model that even though “block” has 6 levels, why the df = 4 in the anova table when it should intuitively be 5. It’s because there is aliasing with the environment variable, so environment takes 1 df away, and thus one of the estimates of the block is singular. This can be seen in the output of summary(mod):\nShort aside - R knows the difference between nested and crossed data based on whether the main effect is included or not. Notice the difference between these examples, even though environ:block123 is included in both models, it doesn’t have the same degrees of freedom."
  },
  {
    "objectID": "experimentaldesign/experimentaldesign.html#definitions",
    "href": "experimentaldesign/experimentaldesign.html#definitions",
    "title": "10  Experimental Design",
    "section": "11.1 Definitions",
    "text": "11.1 Definitions\nTypes of units in Design and Analysis of Experiments (DOE)\n\nExperimental Unit (EU) - the smallest unit to which a treatment is applied. Experiments can have more than one experimental unit when studying more than one treatment factor. This results in split-plot designs.\nSampling Unit - A unit that makes up the population to be modeled. Sampling units are used to make observations.\nObservational Unit - The unit from which an observation is made.\n\nTypes of factors in DOE\n\nTreatment - normally a factor of interest to the experimentor.\nBlocking - also regarded as a factor, but a known source of variation. This differs from treatment factor in that it CHANGES THE RANDOMIZATION of the experiement.\n\nA note on the word replication, I try to avoid this word, as it is heavily abused and vague. Sometimes clients mean subsamples, sometimes they mean a blocking factor, sometimes they mean experimental units. “I replicated this experiment 3 times”. If they replicated it over years, this would imply blocking in time. “I have 3 replicates of data”, implies they may have 3 data points from the same experimental unit.\nAnother note on “blocking”: Blocking is a design mechanism of efficiency that stems from the REAL WORLD, not mathematics. Clients often come with “blocked” experiments because they believe that just including block will make the design more efficient. “Why did you block on this variable?” “blocking is more efficient and I had extra samples”. If you don’t block on anything useful, it will actually make the design less efficient, and you’re better off with a CRD.\n\n11.1.1 Examples for definitions\n\nStudents in classroom\n\nSuppose we’re studying methods of teaching (treatment factor) on student achievement. The experimental unit is a classroom, because the teaching method is applied to the whole classroom. A sampling unit is a student from that classroom, and the observational unit is some test score of the student, or some measured response from the student.\n\nPlants in a field\n\nWhen we study a fertilizer (treatment factor) on the health of a plant, we are likely not just planting 1 plant, but many in a row for which fertilizer is applied. The row is the experimental unit, a plant is the sampling unit, and the observational unit is making some measurement like root length on that plant."
  },
  {
    "objectID": "experimentaldesign/experimentaldesign.html#fischer-principles-of-design",
    "href": "experimentaldesign/experimentaldesign.html#fischer-principles-of-design",
    "title": "10  Experimental Design",
    "section": "11.2 Fischer Principles of Design",
    "text": "11.2 Fischer Principles of Design\n\nRandomization - necessary for validity of error variance estimates.\nReplication - allows for an estimate of variance.\nBlocking (stratification) - allows for efficient experimentation"
  },
  {
    "objectID": "experimentaldesign/experimentaldesign.html#fitted-values-and-group-means",
    "href": "experimentaldesign/experimentaldesign.html#fitted-values-and-group-means",
    "title": "10  Experimental Design",
    "section": "13.1 Fitted values and Group Means",
    "text": "13.1 Fitted values and Group Means\nThe purpose of this section was an experiement following doing a Finley-Wilkenson regression for a client, in which I was considering calculating multiple ways of calculating the “environmental index”. The point here is a little subtle: The fitted values from a two-way anova model are not the same as the raw calculated grouped means. The upshot of this statement is that (ignoring the replication) \\hat\\alpha_1 + \\hat\\beta_1 = \\bar{y}_{1.} + \\bar{y}_{.1} \\neq y_{11}. (The statement is somewhat adhoc, without defining contrasts estimate, but this is the cell means encoding).\nAfter looking at this, it’s obvious, but the fact that in simple linear regression models, the fitted line goes through the mean, and even in one way anova models, the fitted values are directly the cell means. I just forgot the fact that this changes with two way models.\nImagine the following ways we could slice this problem\n\ncalculating the raw means by group.\ncalculating the fitted values by two way anova w/o interaction\ncalculating the fitted values by one way anova w/ interaction\nfitting several one-way linear models subsetting the data into groups over the other level\n\n\n\nCode\ndata(\"oats\", package = \"MASS\")\n# calculate raw means\nraw_mean <- oats %>% group_by(V, N) %>% \n  summarize(mean_Y = mean(Y)) %>% arrange(V, N)\n\n# additive\nmod <- lm(Y ~ V + N, data = oats)\nemm <- emmeans(mod, specs = c(\"V\", \"N\"))\nadditive_mean <- emm %>% \n  as.data.frame() %>% \n  select(V, N, emmean) %>% \n  arrange(V, N)\n\n# additive with interaction\nmod2 <- lm(Y ~ V + N + V:N, data = oats)\nemm2 <- emmeans(mod, specs = c(\"V\", \"N\"))\n\ninter_mean <- emm2 %>% as.data.frame() %>% select(V,N,emmean) %>% arrange(V,N)\n\n# several one-way linear models\nmod_list <- lmList(Y~ N | V, data = oats)\nnew_dat <- oats %>% distinct(V, N) %>% arrange(V, N)\nlist_mean <- new_dat %>% \n  add_column(fitted = predict(mod_list, newdata = new_dat))\n\n# Manual construction of fitted from marginal means\nN_means <- oats %>% group_by(N) %>% summarize(N_mean = mean(Y)) %>% as.data.frame()\nV_means <- oats %>% group_by(V) %>% summarize(V_mean = mean(Y)) %>% as.data.frame()\noverall_mean <- oats %>% pull(Y) %>% mean()\n# calculate y_i. + y_.j - y_.. for all ij\nmanual_mean <- N_means %>% \n  crossing(V_means) %>% \n  mutate(fitted = N_mean + V_mean - overall_mean) %>% \n  arrange(V, N)\n\nbind_cols(new_dat,\n          tibble(raw = raw_mean$mean_Y,\n                 additive = additive_mean$emmean,\n                 interaction = inter_mean$emmean,\n                 subset_oneway = list_mean$fitted,\n                 manual_fitted = manual_mean$fitted))\n\n\n             V      N       raw  additive interaction subset_oneway\n1  Golden.rain 0.0cwt  80.00000  79.91667    79.91667      80.00000\n2  Golden.rain 0.2cwt  98.50000  99.41667    99.41667      98.50000\n3  Golden.rain 0.4cwt 114.66667 114.75000   114.75000     114.66667\n4  Golden.rain 0.6cwt 124.83333 123.91667   123.91667     124.83333\n5   Marvellous 0.0cwt  86.66667  85.20833    85.20833      86.66667\n6   Marvellous 0.2cwt 108.50000 104.70833   104.70833     108.50000\n7   Marvellous 0.4cwt 117.16667 120.04167   120.04167     117.16667\n8   Marvellous 0.6cwt 126.83333 129.20833   129.20833     126.83333\n9      Victory 0.0cwt  71.50000  73.04167    73.04167      71.50000\n10     Victory 0.2cwt  89.66667  92.54167    92.54167      89.66667\n11     Victory 0.4cwt 110.83333 107.87500   107.87500     110.83333\n12     Victory 0.6cwt 118.50000 117.04167   117.04167     118.50000\n   manual_fitted\n1       79.91667\n2       99.41667\n3      114.75000\n4      123.91667\n5       85.20833\n6      104.70833\n7      120.04167\n8      129.20833\n9       73.04167\n10      92.54167\n11     107.87500\n12     117.04167\n\n\nLooking at the output, additive and interaction models don’t change the estimated marginal means. The one-way with subset matches the raw mean values by group because those one way models will go through the mean, and thus, fitting them seperately will those results. The Manual way of fitting calculated marginal means by Variety and Nitrogen separately, and then adds them in an “outer” type fashion."
  },
  {
    "objectID": "experimentaldesign/experimentaldesign.html#randomized-complete-block-design-rcbd",
    "href": "experimentaldesign/experimentaldesign.html#randomized-complete-block-design-rcbd",
    "title": "10  Experimental Design",
    "section": "14.1 Randomized Complete Block Design (RCBD)",
    "text": "14.1 Randomized Complete Block Design (RCBD)\nSummary\n\nRCBD’s assume that the blocking factor is additive. That might be a wrong assumption.\n\n\n\\begin{aligned}\nY_{ij} = \\alpha_i + b_j + \\varepsilon_{i(j)}\n\\end{aligned}\n\n\nY_ij - is observation at ith treatment level\n\\alpha_i - is treatment effect for level i\nb_j - is block effect of j\n\\varepsilon_{i(j)} - is surrogate error term, assuming \\varepsilon_{i(j)} \\sim N(0, \\sigma^2)"
  },
  {
    "objectID": "experimentaldesign/experimentaldesign.html#generalized-randomized-block-design-grbd",
    "href": "experimentaldesign/experimentaldesign.html#generalized-randomized-block-design-grbd",
    "title": "10  Experimental Design",
    "section": "14.2 Generalized Randomized Block Design (GRBD)",
    "text": "14.2 Generalized Randomized Block Design (GRBD)\n\n\\begin{aligned}\nY_{ij} = \\alpha_i + b_j + \\alpha b_{ij} + \\varepsilon_{k(ij)}\n\\end{aligned}\n\n\nY_ij - is observation at ith treatment level,\n\\alpha_i - is treatment effect for level i, i = 1 \\dots a\nb_j - is block effect of j, j = 1 \\dots b\n\\alpha b_{ij} - is interaction effect of treatment and block.\n\\varepsilon_{k(ij)} - is error term, assuming \\varepsilon_{k(ij)} \\sim N(0, \\sigma^2)\n\n\n \n\n\n\n\nWithin a single block, there is a replicate of the treatment. As opposed to a RCBD in which each level of the treatment has 1 experimental unit."
  },
  {
    "objectID": "experimentaldesign/experimentaldesign.html#blocking-resources",
    "href": "experimentaldesign/experimentaldesign.html#blocking-resources",
    "title": "10  Experimental Design",
    "section": "14.3 Blocking Resources",
    "text": "14.3 Blocking Resources\n\nGary Lecture Notes"
  },
  {
    "objectID": "experimentaldesign/experimentaldesign.html#oats-example",
    "href": "experimentaldesign/experimentaldesign.html#oats-example",
    "title": "10  Experimental Design",
    "section": "15.1 Oats Example",
    "text": "15.1 Oats Example\nThis is normally analyzed as 1 factor RCBD in whole plot, 1 factor RCD in subplot. We will use this example in two ways, the simple way in which we ignore the blocking factor, and the natural way with the blocking factor.\n\nB - Blocks (6 whole plots)\nV - Varieties (3 levels, whole plot factor)\nN - Nitrogen (4 levels, sub plot factor)\nY - Yields in 1/4 acre (from subplot)\n\n\n\n\nFigure: This image shows 2 out of the 6 blocks (as rows) of the experiment.\n\n\n\n\n\n\n\n\n   yield     variety nitrogen block field123 field\n1    111     Victory   0.0cwt     I        1     1\n2    130     Victory   0.2cwt     I        1     1\n3    157     Victory   0.4cwt     I        1     1\n4    174     Victory   0.6cwt     I        1     1\n5    117 Golden.rain   0.0cwt     I        2     2\n6    114 Golden.rain   0.2cwt     I        2     2\n7    161 Golden.rain   0.4cwt     I        2     2\n8    141 Golden.rain   0.6cwt     I        2     2\n9    105  Marvellous   0.0cwt     I        3     3\n10   140  Marvellous   0.2cwt     I        3     3\n11   118  Marvellous   0.4cwt     I        3     3\n12   156  Marvellous   0.6cwt     I        3     3\n13    61     Victory   0.0cwt    II        1     4\n14    91     Victory   0.2cwt    II        1     4\n15    97     Victory   0.4cwt    II        1     4\n16   100     Victory   0.6cwt    II        1     4\n17    70 Golden.rain   0.0cwt    II        2     5\n18   108 Golden.rain   0.2cwt    II        2     5\n19   126 Golden.rain   0.4cwt    II        2     5\n20   149 Golden.rain   0.6cwt    II        2     5\n21    96  Marvellous   0.0cwt    II        3     6\n22   124  Marvellous   0.2cwt    II        3     6\n23   121  Marvellous   0.4cwt    II        3     6\n24   144  Marvellous   0.6cwt    II        3     6\n25    68     Victory   0.0cwt   III        1     7\n26    64     Victory   0.2cwt   III        1     7\n27   112     Victory   0.4cwt   III        1     7\n28    86     Victory   0.6cwt   III        1     7\n29    60 Golden.rain   0.0cwt   III        2     8\n30   102 Golden.rain   0.2cwt   III        2     8\n31    89 Golden.rain   0.4cwt   III        2     8\n32    96 Golden.rain   0.6cwt   III        2     8\n33    89  Marvellous   0.0cwt   III        3     9\n34   129  Marvellous   0.2cwt   III        3     9\n35   132  Marvellous   0.4cwt   III        3     9\n36   124  Marvellous   0.6cwt   III        3     9\n37    74     Victory   0.0cwt    IV        1    10\n38    89     Victory   0.2cwt    IV        1    10\n39    81     Victory   0.4cwt    IV        1    10\n40   122     Victory   0.6cwt    IV        1    10\n41    64 Golden.rain   0.0cwt    IV        2    11\n42   103 Golden.rain   0.2cwt    IV        2    11\n43   132 Golden.rain   0.4cwt    IV        2    11\n44   133 Golden.rain   0.6cwt    IV        2    11\n45    70  Marvellous   0.0cwt    IV        3    12\n46    89  Marvellous   0.2cwt    IV        3    12\n47   104  Marvellous   0.4cwt    IV        3    12\n48   117  Marvellous   0.6cwt    IV        3    12\n49    62     Victory   0.0cwt     V        1    13\n50    90     Victory   0.2cwt     V        1    13\n51   100     Victory   0.4cwt     V        1    13\n52   116     Victory   0.6cwt     V        1    13\n53    80 Golden.rain   0.0cwt     V        2    14\n54    82 Golden.rain   0.2cwt     V        2    14\n55    94 Golden.rain   0.4cwt     V        2    14\n56   126 Golden.rain   0.6cwt     V        2    14\n57    63  Marvellous   0.0cwt     V        3    15\n58    70  Marvellous   0.2cwt     V        3    15\n59   109  Marvellous   0.4cwt     V        3    15\n60    99  Marvellous   0.6cwt     V        3    15\n61    53     Victory   0.0cwt    VI        1    16\n62    74     Victory   0.2cwt    VI        1    16\n63   118     Victory   0.4cwt    VI        1    16\n64   113     Victory   0.6cwt    VI        1    16\n65    89 Golden.rain   0.0cwt    VI        2    17\n66    82 Golden.rain   0.2cwt    VI        2    17\n67    86 Golden.rain   0.4cwt    VI        2    17\n68   104 Golden.rain   0.6cwt    VI        2    17\n69    97  Marvellous   0.0cwt    VI        3    18\n70    99  Marvellous   0.2cwt    VI        3    18\n71   119  Marvellous   0.4cwt    VI        3    18\n72   121  Marvellous   0.6cwt    VI        3    18\n\n\n\n\n\n\n\n\n\n\n\n\n15.1.1 Without blocking\nHere we ignore the blocking factor. Suppose we just replicated the whole plot experimental unit a few more times. This means that we have 6 replication units for whole plot treatments, and theoretically get a better estimate for “Variety”\nProfessor Ane Factorial and Split Plot\nThe model for the simplest split plot design is:\n\n\\begin{aligned}\nY_{ijk} = \\alpha_i &+ \\eta_{k(i)} + \\\\\n&\\beta_j + (\\alpha\\beta)_{ij} +  \\varepsilon_{ijk}\n\\end{aligned}\n where:\n\n\\alpha is whole plot factor (variety)\n\\eta_{k(i)} is the whole plot error k replicates nested inside the whole plot factor\n\\beta_j - subplot factor (nitrogen)\n(\\alpha\\beta)_{ij} - interaction of whole plot factor and subplot factor\n\\varepsilon_{ijk} - split-plot level random error\n\n\nlm (wrong)\n\n\nCode\n# anova table for factors for sum squares. These give identical tables.\n# oats_lm <- lm(yield~nitrogen*variety + block:variety, data = oats) # same table, different way of specifying whole plot labels\noats_lm <- lm(yield ~ nitrogen*variety + field, data = oats)\nanova(oats_lm)\n\n\nAnalysis of Variance Table\n\nResponse: yield\n                 Df  Sum Sq Mean Sq F value    Pr(>F)    \nnitrogen          3 20020.5  6673.5 37.6856 2.458e-12 ***\nvariety           2  1786.4   893.2  5.0438   0.01056 *  \nfield            15 21888.6  1459.2  8.2404 1.609e-08 ***\nnitrogen:variety  6   321.7    53.6  0.3028   0.93220    \nResiduals        45  7968.7   177.1                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote in this table, variety is tested over the wrong denominator. It should be tested wrt to the whole plot error term.\n\n\naov\n\n\nCode\n# aov specifying whole plot error term\n# oats_aov <- aov(yield~ nitrogen*variety + Error(block:variety), data = oats) # gives warning, but correct\noats_aov <- aov(yield ~ nitrogen*variety + Error(field), data = oats)\nsummary(oats_aov)\n\n\n\nError: field\n          Df Sum Sq Mean Sq F value Pr(>F)\nvariety    2   1786   893.2   0.612  0.555\nResiduals 15  21889  1459.2               \n\nError: Within\n                 Df Sum Sq Mean Sq F value   Pr(>F)    \nnitrogen          3  20020    6673  37.686 2.46e-12 ***\nnitrogen:variety  6    322      54   0.303    0.932    \nResiduals        45   7969     177                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice that variety has a very different p-value here when tested over the correct denominator, in contrast to the earlier lm table.\n\n\naov (wrong)\n\n\nCode\n# Incorrect labeling of field\noats_aov <- aov(yield~nitrogen*variety + Error(field123), data = oats)\nsummary(oats_aov)\n\n\n\nError: field123\n        Df Sum Sq Mean Sq\nvariety  2   1786   893.2\n\nError: Within\n                 Df Sum Sq Mean Sq F value   Pr(>F)    \nnitrogen          3  20020    6673  13.411 8.37e-07 ***\nnitrogen:variety  6    322      54   0.108    0.995    \nResiduals        60  29857     498                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis above table is incorrect, since we’re not labeling our fields differently. We don’t have appropriate replicates for variety, and thus we can’t estimate the error term as we have specified.\n\n\nlmer\n\n\nCode\n# lmer way for split plot\n# oats_lmer <- lmer(yield ~ nitrogen*variety + (1|block:variety))\n# oats_lmer <- lmer(yield ~ nitrogen*variety + (1|block:field123))\noats_lmer <- lmer(yield ~ nitrogen*variety + (1|field), data = oats)\n\nsummary(oats_lmer)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: yield ~ nitrogen * variety + (1 | field)\n   Data: oats\n\nREML criterion at convergence: 538.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.83746 -0.64204  0.01107  0.61254  1.61722 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n field    (Intercept) 320.5    17.90   \n Residual             177.1    13.31   \nNumber of obs: 72, groups:  field, 18\n\nFixed effects:\n                             Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)                  104.5000     7.7975  15.0000  13.402 9.42e-10 ***\nnitrogen.L                    33.6901     5.4327  45.0000   6.201 1.57e-07 ***\nnitrogen.Q                    -4.1667     5.4327  45.0000  -0.767    0.447    \nnitrogen.C                    -0.8199     5.4327  45.0000  -0.151    0.881    \nvarietyMarvellous              5.2917    11.0274  15.0000   0.480    0.638    \nvarietyVictory                -6.8750    11.0274  15.0000  -0.623    0.542    \nnitrogen.L:varietyMarvellous  -4.8075     7.6830  45.0000  -0.626    0.535    \nnitrogen.Q:varietyMarvellous  -1.9167     7.6830  45.0000  -0.249    0.804    \nnitrogen.C:varietyMarvellous   3.9877     7.6830  45.0000   0.519    0.606    \nnitrogen.L:varietyVictory      2.5715     7.6830  45.0000   0.335    0.739    \nnitrogen.Q:varietyVictory     -1.0833     7.6830  45.0000  -0.141    0.888    \nnitrogen.C:varietyVictory     -2.8696     7.6830  45.0000  -0.374    0.711    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) ntrg.L ntrg.Q ntrg.C vrtyMr vrtyVc nt.L:M nt.Q:M nt.C:M\nnitrogen.L   0.000                                                        \nnitrogen.Q   0.000  0.000                                                 \nnitrogen.C   0.000  0.000  0.000                                          \nvartyMrvlls -0.707  0.000  0.000  0.000                                   \nvarityVctry -0.707  0.000  0.000  0.000  0.500                            \nntrgn.L:vrM  0.000 -0.707  0.000  0.000  0.000  0.000                     \nntrgn.Q:vrM  0.000  0.000 -0.707  0.000  0.000  0.000  0.000              \nntrgn.C:vrM  0.000  0.000  0.000 -0.707  0.000  0.000  0.000  0.000       \nntrgn.L:vrV  0.000 -0.707  0.000  0.000  0.000  0.000  0.500  0.000  0.000\nntrgn.Q:vrV  0.000  0.000 -0.707  0.000  0.000  0.000  0.000  0.500  0.000\nntrgn.C:vrV  0.000  0.000  0.000 -0.707  0.000  0.000  0.000  0.000  0.500\n            nt.L:V nt.Q:V\nnitrogen.L               \nnitrogen.Q               \nnitrogen.C               \nvartyMrvlls              \nvarityVctry              \nntrgn.L:vrM              \nntrgn.Q:vrM              \nntrgn.C:vrM              \nntrgn.L:vrV              \nntrgn.Q:vrV  0.000       \nntrgn.C:vrV  0.000  0.000\n\n\n\n\nCode\n# Type III tests, here it's an exact test, so no ambiguity\n# anova(oats_lmer, ddf = \"Kenward-Roger\")\n# Anova(oats_lmer, type = \"III\", test.statistic = \"F\")\nanova(oats_lmer, ddf = \"Satterthwaite\")\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n                  Sum Sq Mean Sq NumDF DenDF F value    Pr(>F)    \nnitrogen         20020.5  6673.5     3    45 37.6856 2.458e-12 ***\nvariety            216.8   108.4     2    15  0.6121    0.5552    \nnitrogen:variety   321.8    53.6     6    45  0.3028    0.9322    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe note that this gives identical results as aov.\n\n\n\n\nSince the interaction doesn’t seem to be significant, we’d probably drop that term from the model and we would have the additive model as our final model for this data.\n\n\nCode\noats_lmer <- lmer(yield ~ nitrogen + variety + (1|field), data = oats)\n# summary(oats_lmer)\n\n\n\n\n15.1.2 With blocking\nNow that we consider blocking, the whole plot experimental unit no longer has replication. Thus, we cannot test the interaction of block and variety. Instead, we use the interaction term as the surrotate error term for whole plots. You might ask, what if I wanted to analyze the whole plot experimental error?\nThe Hasse diagram with blocking looks like\n\n\n\n\n\nThe model for this is\n\n\\begin{aligned}\nY_{ijkl} = \\alpha_i + b_{k} &+ (\\alpha b)_{l(ik)}\\\\\n&\\beta_j + \\alpha\\beta_{ij} +  \\varepsilon_{l(ijk)}\n\\end{aligned}\n where:\n\n\\alpha is whole plot factor (variety) (i = 1 \\dots 3)\nb_k is blocking (k = 1 \\dots 6)\n(\\alpha b)_{k(i)} is the whole plot error k replicates nested inside the whole plot factor\n\\beta_j - subplot factor (Nitrogen) (j = 1 \\dots 4)\n(\\alpha\\beta)_{ij} - interaction of whole plot factor and subplot factor\n\\varepsilon_{ijk} - split-plot level random error\n\n\nlm (wrong)\n\n\nCode\n# Sum of Squres and MSE\noats_lm <- lm(yield~ block + variety*nitrogen + block:variety, data = oats)\nanova(oats_lm)\n\n\nAnalysis of Variance Table\n\nResponse: yield\n                 Df  Sum Sq Mean Sq F value    Pr(>F)    \nblock             5 15875.3  3175.1 17.9297 9.525e-10 ***\nvariety           2  1786.4   893.2  5.0438  0.010557 *  \nnitrogen          3 20020.5  6673.5 37.6856 2.458e-12 ***\nvariety:nitrogen  6   321.8    53.6  0.3028  0.932199    \nblock:variety    10  6013.3   601.3  3.3957  0.002251 ** \nResiduals        45  7968.8   177.1                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAgain, the analysis here ignores that there is a different replication for the whole plots, and so block and variety are tested with the wrong denominator. Nonetheless, it is still useful to have the mean squared errors. We drop the interaction\n\n\naov\n\n\nCode\n# Fixed block effect\n# oats_aov_block_fixed <- aov(yield~ block + variety*nitrogen + Error(block:variety), data = oats) # singular error, but correct\n# oats_aov_block_fixed <- aov(yield~ block + variety*nitrogen + Error(block:field123), data = oats) # singular error, but correct\noats_aov_block_fixed <- aov(yield~ block + variety*nitrogen + Error(field), data = oats) # need to number each whole plot separately\nsummary(oats_aov_block_fixed)\n\n\n\nError: field\n          Df Sum Sq Mean Sq F value Pr(>F)  \nblock      5  15875    3175   5.280 0.0124 *\nvariety    2   1786     893   1.485 0.2724  \nResiduals 10   6013     601                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n                 Df Sum Sq Mean Sq F value   Pr(>F)    \nnitrogen          3  20020    6673  37.686 2.46e-12 ***\nvariety:nitrogen  6    322      54   0.303    0.932    \nResiduals        45   7969     177                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\n# Random block effects\n# oats_aov_block_random <- aov(yield~ block + variety*nitrogen + Error(block)+ Error(block:variety), data = oats) # the below is technically short for this, but aov only allows 1 `Error` term, so you must specify like that.\n# oats_aov_block_random <- aov(yield~ block + variety*nitrogen + Error(block/field), data = oats)  # gives singular error, but correct.\n# oats_aov_block_random <- aov(yield~ block + variety*nitrogen + Error(block/field123), data = oats)  # aov prefers numbered 123 across the blocks\noats_aov_block_random <- aov(yield~ block + variety*nitrogen + Error(block/variety), data = oats)\nsummary(oats_aov_block_random)\n\n\n\nError: block\n      Df Sum Sq Mean Sq\nblock  5  15875    3175\n\nError: block:variety\n          Df Sum Sq Mean Sq F value Pr(>F)\nvariety    2   1786   893.2   1.485  0.272\nResiduals 10   6013   601.3               \n\nError: Within\n                 Df Sum Sq Mean Sq F value   Pr(>F)    \nnitrogen          3  20020    6673  37.686 2.46e-12 ***\nvariety:nitrogen  6    322      54   0.303    0.932    \nResiduals        45   7969     177                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that we don’t get a p-value for the random block now, since block gets its own error term. we also get the same results from when we treated block as fixed.\nSince there’s no interaction between variety and nitrogen, we can drop that from the model and get this as the final model.\n\n\nCode\n# No interaction\noats_aov_block_fixed_additive <- aov(yield~ block + nitrogen + variety + Error(block/variety), data = oats)\nsummary(oats_aov_block_fixed_additive)\n\n\n\nError: block\n      Df Sum Sq Mean Sq\nblock  5  15875    3175\n\nError: block:variety\n          Df Sum Sq Mean Sq F value Pr(>F)\nvariety    2   1786   893.2   1.485  0.272\nResiduals 10   6013   601.3               \n\nError: Within\n          Df Sum Sq Mean Sq F value   Pr(>F)    \nnitrogen   3  20020    6673   41.05 1.23e-13 ***\nResiduals 51   8290     163                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nlmer\n\n\nCode\n# Mixed model approach, fixed block \n# oats_lmer_block_fixed <- lmer(yield~ block + variety*nitrogen + (1 | block:variety), data = oats)\noats_lmer_block_fixed <- lmer(yield~ block + variety*nitrogen + (1 | field), data = oats)\nanova(oats_lmer_block_fixed)\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n                  Sum Sq Mean Sq NumDF DenDF F value    Pr(>F)    \nblock             4675.0   935.0     5    10  5.2800   0.01244 *  \nvariety            526.1   263.0     2    10  1.4853   0.27239    \nnitrogen         20020.5  6673.5     3    45 37.6856 2.458e-12 ***\nvariety:nitrogen   321.8    53.6     6    45  0.3028   0.93220    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# block random\n# oats_lmer_block_random <- lmer(yield~ variety*nitrogen + (1 | block) + (1|block:variety), data = oats)\n# oats_lmer_block_random <- lmer(yield~ variety*nitrogen + (1 | block/field123), data = oats) # shorthand\n# oats_lmer_block_random <- lmer(yield~ variety*nitrogen + (1 | block/field), data = oats) # as long as block:field reduces to a unique 18 terms\n# oats_lmer_block_random <- lmer(yield~ variety*nitrogen + (1 | block) + (1|field), data = oats) # as long as block:field reduces to a unique 18 terms, we can specify random error however.\noats_lmer_block_random <- lmer(yield~ variety*nitrogen + (1 | block/variety), data = oats) # shorthand\nanova(oats_lmer_block_random)\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n                  Sum Sq Mean Sq NumDF DenDF F value    Pr(>F)    \nvariety            526.1   263.0     2    10  1.4853    0.2724    \nnitrogen         20020.5  6673.5     3    45 37.6857 2.458e-12 ***\nvariety:nitrogen   321.7    53.6     6    45  0.3028    0.9322    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSince interaction is non significant, and treating block as random doesn’t change the analysis, we drop interaction from model for the final model.\n\n\nCode\n# dropping non-significant interaction term, fixed block\noats_lmer_block_fixed_additive <- lmer(yield~ block + variety + nitrogen + (1 | field), data = oats) # shorthand\n\n# dropping non-significant interaction term, random block\noats_lmer_block_random_additive <- lmer(yield~ variety + nitrogen + (1 | block/variety), data = oats) # shorthand\n\n\n\n\nCode\nsummary(oats_lmer_block_fixed_additive)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: yield ~ block + variety + nitrogen + (1 | field)\n   Data: oats\n\nREML criterion at convergence: 525.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.96728 -0.65956 -0.02455  0.61656  1.69168 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n field    (Intercept) 109.7    10.47   \n Residual             162.6    12.75   \nNumber of obs: 72, groups:  field, 18\n\nFixed effects:\n                  Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)       135.8611     8.1740  10.0000  16.621 1.30e-08 ***\nblockII           -28.0833    10.0111  10.0000  -2.805  0.01863 *  \nblockIII          -39.4167    10.0111  10.0000  -3.937  0.00279 ** \nblockIV           -37.1667    10.0111  10.0000  -3.713  0.00402 ** \nblockV            -44.4167    10.0111  10.0000  -4.437  0.00126 ** \nblockVI           -39.0833    10.0111  10.0000  -3.904  0.00294 ** \nvarietyMarvellous   5.2917     7.0789  10.0000   0.748  0.47196    \nvarietyVictory     -6.8750     7.0789  10.0000  -0.971  0.35436    \nnitrogen.L         32.9447     3.0052  51.0000  10.963 5.12e-15 ***\nnitrogen.Q         -5.1667     3.0052  51.0000  -1.719  0.09163 .  \nnitrogen.C         -0.4472     3.0052  51.0000  -0.149  0.88229    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) blckII blcIII blckIV blockV blckVI vrtyMr vrtyVc ntrg.L\nblockII     -0.612                                                        \nblockIII    -0.612  0.500                                                 \nblockIV     -0.612  0.500  0.500                                          \nblockV      -0.612  0.500  0.500  0.500                                   \nblockVI     -0.612  0.500  0.500  0.500  0.500                            \nvartyMrvlls -0.433  0.000  0.000  0.000  0.000  0.000                     \nvarityVctry -0.433  0.000  0.000  0.000  0.000  0.000  0.500              \nnitrogen.L   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000       \nnitrogen.Q   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\nnitrogen.C   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n            ntrg.Q\nblockII           \nblockIII          \nblockIV           \nblockV            \nblockVI           \nvartyMrvlls       \nvarityVctry       \nnitrogen.L        \nnitrogen.Q        \nnitrogen.C   0.000\n\n\n\n\nemmeans (manual calculation)\nIf we want to average across the random whole plots, and have an estimate for the variability of nitrogen, we are now considering two sources of variability, the subplot error (residual) and whole plot error. Thus, this is loosely a linear combination of chi-square distributions, thus we need a df approximation.\n\n\nCode\n# the manual calculation of the satterthwaite approximation\noats_lmer_block_fixed_additive_emm <- emmeans(oats_lmer_block_fixed_additive, specs = \"nitrogen\", lmer.df = \"satterthwaite\")\noats_lmer_block_fixed_additive_emm\n\n\n nitrogen emmean   SE   df lower.CL upper.CL\n 0.0cwt     79.4 3.89 29.1     71.4     87.3\n 0.2cwt     98.9 3.89 29.1     90.9    106.8\n 0.4cwt    114.2 3.89 29.1    106.3    122.2\n 0.6cwt    123.4 3.89 29.1    115.4    131.3\n\nResults are averaged over the levels of: block, variety \nDegrees-of-freedom method: satterthwaite \nConfidence level used: 0.95 \n\n\nWhen we treat block as random, we have even more uncertainty over the average levels because we’re incorporating more variability from block.\n\n\nCode\noats_lmer_block_random_additive_emm <- emmeans(oats_lmer_block_random_additive, specs = \"nitrogen\", lmer.df = \"satterthwaite\")\noats_lmer_block_random_additive_emm\n\n\n nitrogen emmean   SE   df lower.CL upper.CL\n 0.0cwt     79.4 7.13 6.64     62.3     96.4\n 0.2cwt     98.9 7.13 6.64     81.8    115.9\n 0.4cwt    114.2 7.13 6.64     97.2    131.3\n 0.6cwt    123.4 7.13 6.64    106.3    140.4\n\nResults are averaged over the levels of: variety \nDegrees-of-freedom method: satterthwaite \nConfidence level used: 0.95 \n\n\nIn the output, we note a partial degree of freedom. How do we use satterthwaite formula to get this df approximation? (and subsequently, the standard error used to calculate the confidence limits). Nitrogen is indexed by j, so we are ultimately interested in \\overline{Y_{\\cdot j \\cdot}}, this is the “emmean” estimate for average nitrogen, in which we average over variety and block.\n\n\\begin{aligned}\nSE(\\overline{Y_{\\cdot j \\cdot}}) &= \\sqrt{\\hat{\\operatorname{Var}}(\\overline{Y_{\\cdot j \\cdot}})} \\\\\n\\operatorname{Var}(\\overline{Y_{\\cdot j \\cdot}}) &= \\operatorname{Var}\\left(\\frac{\\sum_{i=1}^3\\sum_{k=1}^6Y_{ijk}}{(3)(6)}\\right) \\\\\n&= \\frac{1}{18} (\\sigma_W^2 + \\sigma^2)\n\\end{aligned}\n\nFrom the expected mean squares, a moment estimator of the variances are\n\n\\begin{aligned}\n\\hat\\sigma^2 &= SPMSE \\\\\n\\hat \\sigma^2_W &= \\frac{1}{4}(WPMSE - SPMSE)\n\\end{aligned}\n \n\\begin{aligned}\n\\hat{\\operatorname{Var}}(\\overline{Y_{\\cdot j \\cdot}}) &= \\frac{1}{72}WPMSE + \\frac{3}{72}SPMSE\n\\end{aligned}\n\nNow we also know that the mean squared errors have chi-squared distributions (because the are quadratic forms). This is where satterwaithe formula comes in. Now we just need to plug in the squared errors\n\n\\begin{aligned}\n\\nu \\approx \\frac{(\\sum k_is_i^2)^2}{\\sum_i\\frac{(k_is^2_i)^2}{\\nu_i}}\n\\end{aligned}\n\n\n\nCode\noats_lm_block_additive <- lm(yield~ block + variety + nitrogen + block:variety, data = oats)\nanova(oats_lm_block_additive)\n\n\nAnalysis of Variance Table\n\nResponse: yield\n              Df  Sum Sq Mean Sq F value    Pr(>F)    \nblock          5 15875.3  3175.1 19.5317 8.101e-11 ***\nvariety        2  1786.4   893.2  5.4945 0.0069026 ** \nnitrogen       3 20020.5  6673.5 41.0528 1.228e-13 ***\nblock:variety 10  6013.3   601.3  3.6992 0.0009032 ***\nResiduals     51  8290.5   162.6                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\n# with variance estimates\nsperr <- 162.6 # subplot error\nwperr <- (601.3 - 162.6) / 4 # whole plot variance estimate\navg_nitro_se <- sqrt(1/18*(wperr + sperr))\navg_nitro_se\n\n\n[1] 3.889266\n\n\n\n\nCode\n# with mean squared values\nwpmse <- 601.3 # Whole plot Mean Square\nwpdf <- 10 # Whole plot degrees of freedom\nspmse <- 162.6 # sub plot mean squared\nspdf <- 51 # sub plot degrees of freedom\n\n# satterwaithe approximation\napprox_df <- (1/72*wpmse + 3/72*spmse)^2 / ((1/72*wpmse)^2 / wpdf + (3/72*spmse)^2 / spdf)\napprox_df\n\n\n[1] 29.05648\n\n\n\n\nCode\n# lower confidence limit\nnitro1_est <- oats %>% filter(nitrogen == \"0.0cwt\") %>% \n  pull(yield) %>% mean()\n\nnitro1_est + avg_nitro_se * qt(.025, approx_df) # Lower\n\n\n[1] 71.43512\n\n\nCode\nnitro1_est + avg_nitro_se * qt(.975, approx_df) # Upper\n\n\n[1] 87.34266\n\n\nWe can see now all the values match up with emmeans!\n\n\nemmeans plot (misleading)\nThis is a criticism from Should blocks be random or fixed? for when we treat blocks as random. He concludes by stating his preference for fixed blocks.\nThe following graphic is a useful summarization for showing levels of nitrogen change, and often would like to show error bars around those mean estimates. However, when we treat blocks as random (field), we are adding an additional source of variation to the standard error estimates of the mean. When we plot it, the error bars thus look huge, compared to the fixed effects case. Whereas the significance for the difference between the two groups are the same. Even though they overlap, the difference can still be significant.\n\n\nCode\npairs(oats_lmer_block_random_additive_emm)\n\n\n contrast        estimate   SE df t.ratio p.value\n 0.0cwt - 0.2cwt   -19.50 4.25 51  -4.588  0.0002\n 0.0cwt - 0.4cwt   -34.83 4.25 51  -8.196  <.0001\n 0.0cwt - 0.6cwt   -44.00 4.25 51 -10.353  <.0001\n 0.2cwt - 0.4cwt   -15.33 4.25 51  -3.608  0.0038\n 0.2cwt - 0.6cwt   -24.50 4.25 51  -5.765  <.0001\n 0.4cwt - 0.6cwt    -9.17 4.25 51  -2.157  0.1493\n\nResults are averaged over the levels of: variety \nDegrees-of-freedom method: satterthwaite \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\n\n\nCode\nplot(oats_lmer_block_random_additive_emm)\n\n\n\n\n\n\n\nCode\npairs(oats_lmer_block_fixed_additive_emm)\n\n\n contrast        estimate   SE df t.ratio p.value\n 0.0cwt - 0.2cwt   -19.50 4.25 51  -4.588  0.0002\n 0.0cwt - 0.4cwt   -34.83 4.25 51  -8.196  <.0001\n 0.0cwt - 0.6cwt   -44.00 4.25 51 -10.353  <.0001\n 0.2cwt - 0.4cwt   -15.33 4.25 51  -3.608  0.0038\n 0.2cwt - 0.6cwt   -24.50 4.25 51  -5.765  <.0001\n 0.4cwt - 0.6cwt    -9.17 4.25 51  -2.157  0.1493\n\nResults are averaged over the levels of: block, variety \nDegrees-of-freedom method: satterthwaite \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\n\n\nCode\nplot(oats_lmer_block_fixed_additive_emm)"
  },
  {
    "objectID": "experimentaldesign/experimentaldesign.html#steel-example",
    "href": "experimentaldesign/experimentaldesign.html#steel-example",
    "title": "10  Experimental Design",
    "section": "15.2 Steel Example",
    "text": "15.2 Steel Example\nBased on (Box et. al 2005) Experiment designed to study the corrosion resistance of steel bars treated with 4 coatings, C_1, C_2, C_3, C_4 at three furnace temperatures 360^{\\circ}C, 370^{\\circ}C,380^{\\circ}C. Furnace temperature is considered a hard to change factor because of the time it takes to reset the furnace and reach equilibriu temperature. Once the equilibrium temperature is reached, 4 steel bars with randomly assigned coating are randomly positioned in the furnace and heated. The first three temperatures were run on day 1, and the second three temperatures were run on day 2.\n\n\n\n\n\n     y coating temp day\n1   73       2  360   1\n2   83       3  360   1\n3   67       1  360   1\n4   89       4  360   1\n5   65       1  370   1\n6   87       3  370   1\n7   86       4  370   1\n8   91       2  370   1\n9  147       3  380   1\n10 155       1  380   1\n11 127       2  380   1\n12 212       4  380   1\n13 153       4  380   2\n14  90       3  380   2\n15 100       2  380   2\n16 108       1  380   2\n17 150       4  370   2\n18 140       1  370   2\n19 121       3  370   2\n20 142       2  370   2\n21  33       1  360   2\n22  54       4  360   2\n23   8       2  360   2\n24  46       3  360   2\n\n\n\n\n\n\n\nIf we pretend that the randomization was not done with day as a blocking factor, then we have a split plot with CRD in whole plots and CRD in subplots.\n\n15.2.1 with day block\n\n\nCode\n# just get df table, and mean squares\n# wrong denominator for temp effect\nsteel_lm_block <- lm(y ~ day + coating*temp + day:temp, data = steel)\nanova(steel_lm_block)\n\n\nAnalysis of Variance Table\n\nResponse: y\n             Df  Sum Sq Mean Sq  F value    Pr(>F)    \nday           1   782.0   782.0   6.2794  0.033538 *  \ncoating       3  4289.1  1429.7  11.4798  0.001977 ** \ntemp          2 26519.2 13259.6 106.4674 5.446e-07 ***\ncoating:temp  6  3269.7   545.0   4.3757  0.024066 *  \nday:temp      2 13657.6  6828.8  54.8314 9.113e-06 ***\nResiduals     9  1120.9   124.5                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# correct split plot analysis\nsteel_aov_block <- aov(y ~ day + coating*temp + Error(day:temp), data = steel)\n\n\nWarning in aov(y ~ day + coating * temp + Error(day:temp), data = steel):\nError() model is singular\n\n\nCode\nsummary(steel_aov_block)\n\n\n\nError: day:temp\n          Df Sum Sq Mean Sq F value Pr(>F)\nday        1    782     782   0.115  0.767\ntemp       2  26519   13260   1.942  0.340\nResiduals  2  13658    6829               \n\nError: Within\n             Df Sum Sq Mean Sq F value  Pr(>F)   \ncoating       3   4289  1429.7  11.480 0.00198 **\ncoating:temp  6   3270   545.0   4.376 0.02407 * \nResiduals     9   1121   124.5                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# correct split plot analysis\nsteel_lmer_block <- lmer(y~ day + coating*temp + (1|temp:day), data = steel)\nanova(steel_lmer_block)\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n             Sum Sq Mean Sq NumDF DenDF F value   Pr(>F)   \nday            14.3   14.26     1     2  0.1145 0.767278   \ncoating      4289.1 1429.71     3     9 11.4798 0.001977 **\ntemp          483.7  241.83     2     2  1.9417 0.339937   \ncoating:temp 3269.7  544.96     6     9  4.3757 0.024066 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n15.2.2 without day block\n\n\nCode\n# without blocking\nsteel_lmer <- lmer(y~coating*temp + (1|temp:day), data = steel)\nanova(steel_lmer)\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n             Sum Sq Mean Sq NumDF DenDF F value   Pr(>F)   \ncoating      4289.1 1429.71     3     9 11.4798 0.001977 **\ntemp          686.2  343.09     2     3  2.7548 0.209321   \ncoating:temp 3269.7  544.96     6     9  4.3757 0.024066 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\n15.2.3 with block:treatment\nWe just check the model seperating the block x sp-treatment out from the residual term.\n\n\nCode\nsteel_lmer_block_interact <- lmer(y ~ day + coating*temp + day:coating + (1|temp:day), data = steel)\nanova(steel_lmer_block_interact)\n\n\nType III Analysis of Variance Table with Satterthwaite's method\n             Sum Sq Mean Sq NumDF  DenDF F value   Pr(>F)   \nday            16.5   16.54     1 2.0001  0.1145 0.767273   \ncoating      4289.1 1429.71     3 6.0000  9.8969 0.009714 **\ntemp          561.0  280.51     2 2.0001  1.9418 0.339922   \ncoating:temp 3269.8  544.96     6 6.0000  3.7724 0.065509 . \nday:coating   254.1   84.71     3 6.0000  0.5864 0.645799   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n15.2.4 fake data\nHere I try to simulate data\n\n\nCode\nfake_y <- function(day, coating, temp){\n  temp <- scale(as.numeric(temp), scale = F)\n  day <- as.numeric(day)\n  coating <- as.numeric(coating)\n  n <- length(day)\n  2 * day + 2 * temp + (day*temp)*rnorm(n, mean = 2) + .2*coating + 20*day*coating + rnorm(n)\n}\n\nfake_steel <- steel %>% add_column(fake_y = fake_y(steel$day, steel$coating, steel$temp)) \n\n\n\n\n\n\n\n\n\nCode\nfake_steel_aov <- aov(fake_y ~ (day + coating + temp)^2 - day:temp + Error(day:temp), data = fake_steel)\n\n\nWarning in aov(fake_y ~ (day + coating + temp)^2 - day:temp + Error(day:temp), :\nError() model is singular\n\n\nCode\nfake_steel_aov\n\n\n\nCall:\naov(formula = fake_y ~ (day + coating + temp)^2 - day:temp + \n    Error(day:temp), data = fake_steel)\n\nGrand Mean: 86.29519\n\nStratum 1: day:temp\n\nTerms:\n                     day     temp Residuals\nSum of Squares  21602.92 41460.73   1605.22\nDeg. of Freedom        1        2         2\n\nResidual standard error: 28.33036\n9 out of 12 effects not estimable\nEstimated effects may be unbalanced\n\nStratum 2: Within\n\nTerms:\n                  coating day:coating coating:temp Residuals\nSum of Squares  25013.316    2433.340     1007.761  1326.953\nDeg. of Freedom         3           3            6         6\n\nResidual standard error: 14.87141\nEstimated effects may be unbalanced\n\n\nCode\nsummary(fake_steel_aov)\n\n\n\nError: day:temp\n          Df Sum Sq Mean Sq F value Pr(>F)  \nday        1  21603   21603   26.92 0.0352 *\ntemp       2  41461   20730   25.83 0.0373 *\nResiduals  2   1605     803                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nError: Within\n             Df Sum Sq Mean Sq F value   Pr(>F)    \ncoating       3  25013    8338  37.700 0.000274 ***\nday:coating   3   2433     811   3.668 0.082345 .  \ncoating:temp  6   1008     168   0.759 0.626583    \nResiduals     6   1327     221                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "exponential/exponential.html#exponential-modeling",
    "href": "exponential/exponential.html#exponential-modeling",
    "title": "11  Exponential Modeling",
    "section": "11.1 Exponential Modeling",
    "text": "11.1 Exponential Modeling\nThere are many ways to fit an exponential model\n\n\nCode\n# Simulate data\nstar_count <- function(d, a = 500, b = .7) {\n  a * exp(-d * b)\n}\n\nset.seed(1)\nx <- 0:19\ny <- rnegbin(20, mu = star_count(0:19), theta = 100000)\n\n# simulate distance position of stars\nbin_offsets <- y |> map(~sort(runif(.x))) |> reduce(c)\nbin_start <- map2(x, y, .f = ~rep(.x, .y)) |> reduce(c)\nstar_pos <- bin_start + bin_offsets\n\ndiff(star_pos)\n\n\n   [1] 9.597652e-05 6.195621e-03 2.907616e-03 1.134377e-04 1.928067e-03\n   [6] 9.663649e-03 5.045898e-03 1.590043e-03 5.900601e-03 2.628127e-04\n  [11] 5.987849e-03 1.689830e-03 3.242628e-03 1.498245e-03 1.694452e-03\n  [16] 7.860756e-04 1.709354e-03 2.041416e-03 9.738570e-04 3.770091e-03\n  [21] 3.279675e-03 1.588414e-03 6.013783e-06 6.490583e-04 4.923800e-04\n  [26] 3.496718e-03 1.586042e-03 1.121345e-03 1.306609e-03 2.815112e-03\n  [31] 1.621292e-03 1.673421e-04 8.728891e-05 5.087562e-03 8.746670e-04\n  [36] 2.197108e-03 8.734883e-03 3.443789e-04 4.675877e-04 4.311248e-03\n  [41] 9.717434e-04 7.322796e-04 2.311110e-03 1.803405e-03 6.249826e-05\n  [46] 3.756178e-03 2.229521e-04 7.169484e-05 1.259641e-03 3.147101e-03\n  [51] 1.834618e-03 1.855225e-03 5.230620e-04 1.181946e-03 2.789363e-03\n  [56] 2.031644e-03 4.604310e-03 8.289795e-04 2.154936e-04 2.044186e-03\n  [61] 2.578669e-05 7.858516e-04 4.618031e-04 1.378378e-04 1.283525e-03\n  [66] 4.516909e-03 2.516079e-03 1.459756e-03 5.625044e-04 1.377929e-04\n  [71] 3.225633e-04 3.695957e-03 5.228072e-04 3.658538e-04 6.620051e-03\n  [76] 1.940574e-05 5.502056e-04 1.312910e-03 1.681824e-03 3.713125e-03\n  [81] 9.763965e-04 1.241096e-03 7.010642e-04 4.262945e-04 3.856981e-03\n  [86] 2.658521e-04 6.668135e-04 1.556502e-03 1.642253e-03 2.102186e-04\n  [91] 1.474215e-03 1.223945e-03 1.013228e-04 7.424993e-04 3.671825e-03\n  [96] 3.019662e-04 6.349548e-05 3.838142e-03 2.356407e-03 1.767251e-03\n [101] 5.862953e-04 1.480192e-03 9.494252e-04 1.100327e-03 3.836851e-03\n [106] 5.545542e-03 4.852858e-03 3.380637e-03 1.773769e-03 5.086160e-04\n [111] 5.437150e-03 2.660643e-03 5.078267e-04 4.567042e-03 2.277347e-03\n [116] 1.808272e-03 1.694734e-03 6.707529e-03 7.607373e-04 4.868580e-04\n [121] 6.953231e-04 7.676261e-04 3.909292e-03 2.238319e-03 4.282964e-03\n [126] 4.357807e-03 8.024173e-04 4.512395e-04 5.446658e-04 2.763811e-03\n [131] 1.544373e-03 1.774939e-03 2.963034e-03 2.961243e-04 3.699216e-04\n [136] 1.072463e-03 3.431877e-04 9.663661e-04 6.589696e-03 8.082450e-04\n [141] 9.784591e-05 2.100968e-04 2.784184e-03 2.133742e-03 3.485444e-04\n [146] 1.557981e-03 8.158242e-03 6.568262e-04 1.246061e-04 2.616439e-04\n [151] 4.660743e-03 7.790717e-03 1.609190e-03 2.398819e-04 3.480044e-03\n [156] 7.172821e-04 4.655015e-03 8.190128e-04 3.528436e-03 6.759362e-04\n [161] 3.184095e-03 1.661575e-03 1.729990e-03 3.360235e-05 4.500665e-03\n [166] 2.428212e-04 1.799954e-03 1.667319e-04 9.578662e-04 4.329509e-04\n [171] 7.370247e-04 2.363870e-03 1.830698e-03 2.677493e-03 2.743284e-03\n [176] 9.937233e-04 7.765112e-05 2.482987e-04 3.023058e-03 1.623863e-03\n [181] 6.869687e-04 2.842671e-03 6.788331e-04 6.291065e-03 2.866806e-03\n [186] 2.625846e-03 1.082918e-03 3.740541e-05 4.728022e-05 2.509642e-03\n [191] 1.966639e-04 1.423493e-03 7.296663e-04 3.542910e-03 5.172847e-04\n [196] 1.217160e-03 1.222466e-03 7.882717e-04 1.799269e-03 3.774597e-03\n [201] 8.266110e-04 1.173834e-03 1.280347e-03 5.788652e-03 6.956942e-04\n [206] 1.258928e-03 2.485511e-04 6.432619e-04 4.553779e-04 1.747316e-03\n [211] 3.492550e-03 7.396960e-05 2.514177e-03 1.806322e-03 3.040127e-03\n [216] 1.923416e-03 2.703762e-04 2.538257e-03 4.653775e-04 1.735589e-05\n [221] 1.876902e-03 2.882716e-03 5.651761e-03 8.740369e-04 8.895900e-05\n [226] 1.760324e-03 2.557158e-03 6.286362e-04 5.815597e-05 1.258761e-03\n [231] 9.547088e-04 4.528033e-03 1.050965e-03 8.235895e-05 3.642856e-03\n [236] 4.902529e-05 6.286483e-03 1.492314e-04 4.113826e-04 7.640438e-04\n [241] 1.378964e-03 4.300958e-03 5.210338e-04 7.987337e-04 6.566611e-04\n [246] 2.410540e-03 3.152735e-03 2.135827e-03 1.427555e-03 1.252233e-03\n [251] 3.034717e-04 3.163718e-03 4.271183e-04 3.629797e-04 1.125691e-03\n [256] 8.043246e-04 4.051539e-03 2.179951e-03 6.401562e-04 1.159424e-03\n [261] 6.447016e-03 7.226078e-04 1.335797e-03 3.471188e-04 5.919330e-04\n [266] 2.274474e-03 6.464787e-04 1.926442e-03 4.095146e-04 1.400328e-03\n [271] 1.088670e-03 3.600011e-03 9.610550e-05 6.062197e-04 2.483384e-03\n [276] 8.344918e-04 2.100854e-04 3.562869e-03 2.064184e-03 2.129900e-04\n [281] 1.566311e-03 5.409940e-03 1.319426e-03 7.222202e-04 1.283053e-03\n [286] 7.381022e-04 1.581724e-03 2.970617e-03 5.263703e-03 4.114357e-03\n [291] 1.511446e-04 5.516163e-03 6.530474e-03 7.235620e-04 2.258916e-03\n [296] 3.741209e-03 1.087345e-03 1.886950e-04 6.061480e-04 1.480907e-03\n [301] 7.906417e-03 2.701939e-03 3.289718e-03 1.920273e-04 3.851326e-03\n [306] 4.829871e-03 1.384049e-03 2.153968e-03 5.138837e-03 7.364594e-04\n [311] 1.643967e-03 6.685448e-04 2.780251e-03 2.046850e-03 2.043338e-04\n [316] 1.140889e-03 3.701563e-04 1.137548e-03 3.307940e-03 4.896037e-03\n [321] 5.765061e-03 1.363246e-03 2.940910e-03 5.699933e-03 1.006106e-03\n [326] 2.073036e-03 4.415459e-03 2.192322e-03 2.187213e-03 5.072337e-04\n [331] 1.311736e-03 2.085343e-04 1.751514e-03 6.119132e-04 2.900271e-03\n [336] 3.769983e-03 1.374485e-03 8.947682e-06 1.429440e-03 1.665754e-03\n [341] 2.294844e-04 8.200554e-04 9.434167e-04 3.189942e-03 3.317576e-04\n [346] 1.275524e-03 3.129378e-03 5.964605e-03 2.451818e-03 5.278464e-03\n [351] 1.345564e-03 5.487159e-03 1.138175e-03 2.144363e-04 6.288742e-04\n [356] 1.670949e-03 3.084574e-03 1.184753e-03 2.627538e-03 3.637975e-03\n [361] 3.162308e-04 3.080263e-03 3.116192e-03 2.146024e-03 6.226453e-04\n [366] 7.708110e-04 2.544373e-05 3.025944e-03 2.672176e-04 7.561950e-04\n [371] 1.345797e-03 1.978712e-03 3.776763e-03 3.519538e-05 7.729460e-04\n [376] 9.063799e-04 2.361732e-03 1.542619e-03 2.061956e-03 4.209329e-04\n [381] 9.255058e-04 4.587450e-03 2.336278e-03 1.436903e-03 5.619621e-03\n [386] 9.367939e-04 7.070558e-03 1.153229e-04 1.823963e-03 4.422406e-04\n [391] 1.015904e-03 5.937547e-04 1.616475e-03 1.660580e-03 1.776679e-04\n [396] 1.071160e-03 7.430942e-03 8.341135e-05 6.583656e-03 1.593979e-03\n [401] 1.070212e-03 1.032650e-03 1.164576e-03 6.692212e-04 1.694893e-03\n [406] 2.994278e-03 5.368516e-03 3.927060e-03 4.727419e-04 4.378880e-03\n [411] 2.703066e-03 6.479471e-03 1.261281e-03 1.735685e-04 1.130116e-03\n [416] 5.199480e-03 2.527630e-03 1.630988e-03 4.924464e-03 1.483294e-03\n [421] 1.232405e-04 2.347411e-03 5.060555e-03 6.842995e-04 9.674975e-04\n [426] 2.122965e-03 1.075219e-04 7.371628e-04 1.238187e-03 3.655849e-03\n [431] 1.636397e-03 1.574944e-03 9.501074e-05 4.449398e-03 1.080854e-03\n [436] 1.049308e-03 7.498902e-03 9.143825e-04 2.626013e-04 5.998003e-03\n [441] 3.344108e-03 2.119552e-03 7.883303e-04 5.205192e-04 1.855234e-03\n [446] 8.857728e-04 2.999660e-04 5.831908e-03 7.807277e-06 8.635352e-03\n [451] 3.513230e-04 2.403066e-03 5.233119e-03 3.233650e-03 9.761548e-04\n [456] 3.078888e-03 1.014754e-02 3.450631e-03 1.059985e-04 1.155465e-03\n [461] 2.072153e-03 8.754598e-04 3.056780e-04 1.319733e-04 6.387201e-04\n [466] 3.318009e-03 3.379006e-04 1.781541e-03 1.021097e-03 2.701278e-03\n [471] 7.229606e-04 1.172006e-03 2.196610e-03 4.517369e-04 7.399952e-04\n [476] 2.776389e-05 3.093001e-03 1.148536e-03 9.718228e-04 4.417274e-03\n [481] 1.929449e-04 1.842540e-04 6.454869e-04 7.686014e-04 5.290303e-04\n [486] 1.574120e-03 3.866318e-04 2.981476e-03 7.919392e-04 1.902080e-03\n [491] 1.005022e-03 4.891767e-03 3.193608e-04 6.340127e-04 6.008516e-03\n [496] 2.383368e-03 2.464022e-03 5.453755e-04 3.531764e-03 1.216365e-03\n [501] 7.901748e-04 2.398069e-03 1.599386e-03 7.394005e-04 8.454411e-04\n [506] 1.065165e-03 1.646525e-04 2.051600e-03 1.118928e-04 8.689074e-03\n [511] 6.993289e-03 4.852476e-03 1.088567e-02 8.950538e-04 2.363638e-03\n [516] 9.566329e-04 2.725557e-03 1.426373e-03 3.383826e-03 3.408159e-03\n [521] 2.982090e-03 5.131956e-03 1.475564e-05 8.094027e-04 5.777355e-03\n [526] 5.593896e-04 3.823247e-04 6.586144e-03 8.346333e-03 3.112576e-03\n [531] 3.080247e-03 6.955682e-03 8.756758e-03 2.656404e-03 8.839782e-03\n [536] 1.363394e-02 1.189240e-03 1.957414e-03 5.135766e-03 5.948856e-03\n [541] 1.974053e-03 7.299694e-03 2.774562e-03 9.192023e-04 5.307096e-03\n [546] 5.431633e-03 6.671372e-03 5.239761e-04 2.300136e-03 9.946494e-03\n [551] 2.879309e-04 1.434801e-02 1.486375e-03 5.658111e-03 2.540834e-03\n [556] 3.247915e-03 9.146738e-04 2.488370e-03 4.011370e-03 7.379058e-03\n [561] 4.606105e-03 5.028942e-04 1.213145e-02 1.472150e-03 3.995990e-03\n [566] 2.236161e-04 1.302564e-02 6.554911e-03 8.834939e-03 1.883143e-03\n [571] 3.421248e-03 6.800720e-03 8.854185e-03 5.363062e-03 4.670727e-04\n [576] 3.531741e-03 4.003172e-03 8.505388e-03 1.676582e-03 8.986728e-03\n [581] 8.740375e-03 5.018260e-03 1.608986e-03 2.510713e-03 3.418626e-03\n [586] 1.785368e-03 3.172664e-04 4.760744e-04 2.052269e-04 1.679534e-03\n [591] 1.041151e-03 1.919076e-03 3.055609e-03 3.057534e-04 4.947025e-03\n [596] 4.040983e-04 4.674926e-03 2.110414e-03 1.799914e-03 9.121995e-04\n [601] 8.076854e-03 1.759557e-03 2.106057e-03 2.361428e-03 1.023955e-03\n [606] 5.242297e-03 1.583172e-03 6.894371e-03 4.947072e-03 1.418282e-02\n [611] 2.621838e-03 9.632604e-04 5.951077e-04 1.944575e-03 2.377280e-03\n [616] 2.980599e-03 1.317150e-03 2.127492e-03 9.353692e-05 3.163238e-03\n [621] 3.852389e-03 6.417679e-03 2.057972e-03 1.356076e-03 4.300221e-03\n [626] 1.398688e-02 2.359303e-03 7.019425e-05 9.449830e-04 5.569984e-04\n [631] 3.403114e-03 2.372583e-03 5.836313e-03 4.290515e-03 5.101484e-03\n [636] 1.621114e-03 5.069605e-03 3.776255e-03 3.119442e-04 1.531231e-03\n [641] 8.151896e-03 5.977638e-04 1.826656e-03 4.297705e-03 4.847919e-03\n [646] 3.728169e-03 2.236242e-03 5.733693e-03 6.677285e-04 2.873982e-03\n [651] 4.001611e-03 1.832793e-03 3.093553e-03 2.211737e-03 8.076471e-04\n [656] 5.026920e-03 5.958707e-03 2.418420e-03 9.117261e-03 1.479609e-03\n [661] 9.643665e-03 1.384607e-04 9.174092e-03 1.154933e-03 5.200273e-03\n [666] 5.865172e-03 1.096706e-02 6.483681e-04 2.359416e-03 1.809626e-03\n [671] 1.409512e-03 8.220951e-03 1.903693e-05 9.436179e-03 3.682431e-03\n [676] 5.375016e-04 3.389996e-03 1.610966e-02 3.357724e-03 4.399797e-03\n [681] 4.563550e-05 7.150120e-04 5.373291e-04 8.001404e-03 4.349861e-03\n [686] 1.027140e-02 6.932766e-03 1.762852e-03 3.156674e-04 3.484508e-03\n [691] 2.404724e-04 2.252918e-03 2.482627e-04 6.009878e-04 1.523639e-03\n [696] 7.399737e-03 2.690446e-03 6.494364e-04 1.972830e-03 1.129596e-03\n [701] 1.203953e-02 1.340942e-04 2.003629e-03 4.711529e-03 1.060523e-03\n [706] 3.397977e-04 2.821826e-04 1.828532e-03 1.380447e-02 1.007376e-03\n [711] 2.080805e-03 6.919630e-03 8.975437e-04 1.387731e-04 3.633380e-04\n [716] 4.553031e-03 5.996801e-03 3.284284e-03 6.435794e-03 7.727278e-03\n [721] 7.709923e-03 5.446568e-03 4.732984e-04 1.083766e-04 7.795024e-03\n [726] 4.593418e-04 1.342768e-03 3.685765e-04 5.272246e-03 7.602199e-03\n [731] 2.482461e-03 5.473610e-03 5.914203e-04 5.833897e-03 1.185916e-03\n [736] 3.346588e-03 4.766458e-04 4.297896e-03 4.302047e-04 2.418478e-03\n [741] 3.317586e-03 2.944138e-03 2.502028e-02 1.298477e-03 2.308900e-03\n [746] 1.030613e-03 1.671933e-03 2.811734e-03 3.723715e-03 3.074885e-03\n [751] 2.472243e-03 5.034593e-03 9.571121e-04 3.453750e-03 9.659801e-04\n [756] 3.065499e-03 1.951412e-03 3.416800e-03 3.897993e-03 6.298528e-04\n [761] 1.705471e-03 2.146515e-03 4.637728e-04 3.930487e-03 5.334727e-03\n [766] 3.682870e-03 1.127676e-03 1.237026e-03 8.118422e-03 5.454158e-04\n [771] 3.795318e-03 8.648862e-04 4.707431e-03 1.574632e-02 3.344304e-03\n [776] 1.859251e-03 8.257135e-03 2.486953e-04 1.519706e-03 1.769524e-03\n [781] 4.848030e-03 3.054096e-03 1.365755e-02 1.526611e-03 1.116674e-03\n [786] 1.872042e-03 3.701174e-03 9.850992e-03 7.163357e-03 2.070127e-03\n [791] 8.157620e-04 3.015835e-03 7.740832e-04 6.411839e-03 3.777907e-03\n [796] 9.186125e-03 7.697935e-03 2.561446e-02 6.464905e-03 2.242665e-02\n [801] 9.446213e-03 3.994043e-03 1.630996e-02 9.783265e-03 4.739526e-04\n [806] 1.987571e-02 9.783439e-03 5.945270e-03 1.994112e-03 9.153957e-03\n [811] 4.721437e-03 4.860295e-04 7.770923e-03 1.070812e-02 2.101569e-03\n [816] 1.728327e-03 2.188436e-03 7.261206e-03 8.984825e-04 1.613982e-02\n [821] 1.351119e-02 9.605088e-03 1.322460e-02 5.407072e-03 1.536859e-03\n [826] 3.540678e-03 2.288414e-02 1.804280e-03 3.727205e-02 1.344934e-02\n [831] 4.704816e-03 5.585833e-03 6.900921e-03 1.381142e-02 3.267545e-06\n [836] 1.243708e-02 1.742443e-03 6.389060e-03 2.613671e-02 8.753235e-04\n [841] 2.861191e-04 2.687410e-03 9.232769e-03 2.051267e-02 5.881631e-02\n [846] 1.103577e-02 1.282095e-03 1.642057e-02 1.434853e-02 6.684200e-03\n [851] 1.116583e-02 2.813887e-02 7.414172e-03 1.403400e-02 2.951729e-03\n [856] 9.188484e-03 1.491828e-02 1.555670e-03 2.988526e-03 8.403806e-04\n [861] 8.176505e-03 2.582449e-03 5.973042e-03 1.056918e-02 5.358207e-03\n [866] 9.938674e-03 1.809272e-02 4.296012e-03 1.388756e-02 1.068674e-02\n [871] 6.146302e-03 2.447065e-03 9.499837e-03 2.879219e-02 1.359182e-02\n [876] 9.469270e-03 1.474041e-03 7.684108e-03 6.608756e-04 5.706212e-03\n [881] 1.237381e-02 9.176153e-03 5.285707e-03 4.478747e-03 1.419948e-02\n [886] 1.706201e-02 9.028147e-03 2.845119e-03 1.109358e-02 5.806999e-03\n [891] 3.274670e-05 1.506753e-02 1.852395e-02 1.484857e-02 2.739300e-02\n [896] 1.842800e-02 3.343552e-02 1.423533e-02 1.142598e-03 4.352645e-04\n [901] 3.560511e-03 1.058936e-02 1.146518e-02 6.927344e-03 7.949512e-03\n [906] 1.325014e-02 3.347640e-03 1.165296e-02 6.262431e-02 3.741877e-04\n [911] 1.622539e-02 1.434850e-02 3.167318e-02 5.512541e-02 1.536230e-02\n [916] 2.798424e-03 3.235764e-02 4.738306e-03 3.918817e-02 2.677679e-02\n [921] 7.152597e-03 7.280458e-03 8.843749e-03 3.026957e-03 8.980569e-03\n [926] 4.444661e-03 3.424853e-02 3.394069e-02 8.370698e-03 1.515591e-02\n [931] 1.849513e-02 3.041593e-02 1.174784e-02 6.741069e-02 1.281456e-02\n [936] 2.430789e-02 1.845993e-02 1.970209e-02 3.776644e-03 3.377786e-02\n [941] 1.878788e-02 4.888204e-02 1.045966e-02 9.685676e-03 2.415992e-02\n [946] 3.954495e-03 1.982421e-03 5.160084e-03 4.610995e-02 1.066270e-01\n [951] 1.032101e-02 2.513031e-02 8.182281e-03 3.416043e-02 4.222371e-02\n [956] 3.624376e-02 2.744754e-02 6.826530e-03 1.397725e-02 5.053447e-02\n [961] 3.494593e-02 2.671826e-02 1.158660e-02 2.067080e-02 1.138164e-02\n [966] 2.184365e-02 4.752632e-02 9.147254e-03 2.281156e-02 3.149502e-02\n [971] 5.923346e-02 1.759670e-01 1.954359e-03 2.336873e-02 3.302932e-02\n [976] 2.844264e-02 1.123319e-01 3.639931e-02 4.422604e-02 2.412996e-03\n [981] 3.348943e-05 1.624749e-01 7.727121e-02 1.285767e-01 2.538879e-02\n [986] 1.952381e-01 1.266535e-01 4.634832e-02 4.599607e-03 1.991575e-02\n [991] 5.231131e-02 2.187517e-01 1.817350e-01 5.985113e-02 4.352494e-02\n [996] 1.133106e-01 1.072945e-01 1.735455e-01 1.274540e-01 3.519266e-01\n[1001] 9.584075e-01 4.892394e-02 9.776872e-01 2.274324e+00\n\n\nCode\nplot(log(star_pos), log(c(0, diff(star_pos)) + 1))\n\n\n\n\n\nCode\nbeeswarm(star_pos, horizontal = TRUE, method = \"center\")\n\n\n\n\n\nCode\nplot(x, y)\n\n\n\n\n\nCode\n# log method\nmod_loglm <- lm(log(y + 1) ~ x)\n\nmod_loglm |> summary()\n\n\n\nCall:\nlm(formula = log(y + 1) ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.4665 -0.6116 -0.1001  0.7550  1.5978 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.63861    0.41155  11.271 1.37e-09 ***\nx           -0.31721    0.03703  -8.566 9.14e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.955 on 18 degrees of freedom\nMultiple R-squared:  0.803, Adjusted R-squared:  0.7921 \nF-statistic: 73.37 on 1 and 18 DF,  p-value: 9.142e-08\n\n\nCode\nexp(coef(mod_loglm)[2]) # decay param\n\n\n        x \n0.7281783 \n\n\nCode\n# nls\nmod_nls <- nls(y~a * exp(-b * x), data = tibble(x, y),\n               start = list(a = 300, b = .5))\nsummary(mod_nls)\n\n\n\nFormula: y ~ a * exp(-b * x)\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \na 513.64215    3.86330  132.95   <2e-16 ***\nb   0.70819    0.01038   68.25   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.982 on 18 degrees of freedom\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 4.554e-06\n\n\nCode\n# poisson method\nmod_glm <- glm(y ~ x, data = tibble(x, y), family = poisson(link = \"log\"))\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = poisson(link = \"log\"), data = tibble(x, \n    y))\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.86884  -0.29658  -0.12090   0.06696   1.32656  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  6.24785    0.03845  162.50   <2e-16 ***\nx           -0.72222    0.02328  -31.02   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3319.382  on 19  degrees of freedom\nResidual deviance:    5.173  on 18  degrees of freedom\nAIC: 59.975\n\nNumber of Fisher Scoring iterations: 4\n\n\nCode\n# now if I sample the same stars, but with half the bin width...\nx_half <- seq(0, 19, .5)\ny_half <- table(cut(star_pos,breaks = x_half, include.lowest = T))\n\n\n# nls method, can't adjust for the bin size, and residual errors\nmod_nls_half <- nls(y_half ~ a * exp(-b * x_half[-length(x_half)]),\n                    data = tibble(x_half[-length(x_half)], y_half),\n                    start = list(a = 200, b = .5))\n\nsummary(mod_nls_half)\n\n\n\nFormula: y_half ~ a * exp(-b * x_half[-length(x_half)])\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \na 286.46601    8.35228   34.30   <2e-16 ***\nb   0.65138    0.03134   20.78   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.788 on 36 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 2.446e-06\n\n\nCode\nmod_glm_half <- glm(y_half ~ x_half[-length(x_half)], family = poisson(link = \"log\"), offset = rep(log(1/2), length(y_half)))\n\nsummary(mod_glm_half);sigma(mod_glm_half)\n\n\n\nCall:\nglm(formula = y_half ~ x_half[-length(x_half)], family = poisson(link = \"log\"), \n    offset = rep(log(1/2), length(y_half)))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7798  -0.6302  -0.1626  -0.0380   3.2904  \n\nCoefficients:\n                        Estimate Std. Error z value Pr(>|z|)    \n(Intercept)              6.39092    0.04117  155.23   <2e-16 ***\nx_half[-length(x_half)] -0.70399    0.02232  -31.54   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3229.433  on 37  degrees of freedom\nResidual deviance:   36.893  on 36  degrees of freedom\nAIC: 123.62\n\nNumber of Fisher Scoring iterations: 4\n\n\n[1] 1.01232\n\n\n\n\nCode\nsummary(mod_glm);sigma(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = poisson(link = \"log\"), data = tibble(x, \n    y))\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.86884  -0.29658  -0.12090   0.06696   1.32656  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  6.24785    0.03845  162.50   <2e-16 ***\nx           -0.72222    0.02328  -31.02   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3319.382  on 19  degrees of freedom\nResidual deviance:    5.173  on 18  degrees of freedom\nAIC: 59.975\n\nNumber of Fisher Scoring iterations: 4\n\n\n[1] 0.5360842\n\n\n\n\nCode\nanova(mod_glm_half)\n\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: y_half\n\nTerms added sequentially (first to last)\n\n                        Df Deviance Resid. Df Resid. Dev\nNULL                                       37     3229.4\nx_half[-length(x_half)]  1   3192.5        36       36.9\n\n\n\n\nCode\nmod_quasi_glm <- glm(y~x, family = quasipoisson(link = \"log\"))\n\nmod_quasi_glm_half <- update(mod_glm_half, family = quasipoisson(link = \"log\"))\nsummary(mod_quasi_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = quasipoisson(link = \"log\"))\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.86884  -0.29658  -0.12090   0.06696   1.32656  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.24785    0.02302  271.39   <2e-16 ***\nx           -0.72222    0.01394  -51.81   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 0.3585152)\n\n    Null deviance: 3319.382  on 19  degrees of freedom\nResidual deviance:    5.173  on 18  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\n\nCode\nsummary(mod_quasi_glm_half)\n\n\n\nCall:\nglm(formula = y_half ~ x_half[-length(x_half)], family = quasipoisson(link = \"log\"), \n    offset = rep(log(1/2), length(y_half)))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.7798  -0.6302  -0.1626  -0.0380   3.2904  \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              6.39092    0.04384  145.78   <2e-16 ***\nx_half[-length(x_half)] -0.70399    0.02377  -29.62   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 1.133809)\n\n    Null deviance: 3229.433  on 37  degrees of freedom\nResidual deviance:   36.893  on 36  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nCode\nplot(x, y)\n# lines(x, exp(coef(mod_loglm)[1] + coef(mod_loglm)[2]*x) - 1) # bad model...\nlines(x, predict(mod_glm, type = \"response\"))\nlines(x, predict(mod_nls, type = \"response\"), col = 2)\npoints(x_half[-length(x_half)], y_half, pch = 16)\nlines(x_half[-length(x_half)], predict(mod_glm_half, type = \"response\"), col = 3)\nlines(x_half[-length(x_half)], predict(mod_nls_half, type = \"response\"), col = 2, lty = 2)\n\n\n\n\n\nThis seems like a difficult thing to grapple with.. the rate of decrease is not the same. It seems if you halve the exposure time, you’d expect similar rates of decrease, but you should end up with the same estimate of the mean poisson value.\n\n\nCode\npredict(mod_glm_half) |> as.numeric()\n\n\n [1]  5.69776984  5.34577312  4.99377640  4.64177968  4.28978296  3.93778624\n [7]  3.58578953  3.23379281  2.88179609  2.52979937  2.17780265  1.82580593\n[13]  1.47380921  1.12181249  0.76981577  0.41781905  0.06582234 -0.28617438\n[19] -0.63817110 -0.99016782 -1.34216454 -1.69416126 -2.04615798 -2.39815470\n[25] -2.75015142 -3.10214813 -3.45414485 -3.80614157 -4.15813829 -4.51013501\n[31] -4.86213173 -5.21412845 -5.56612517 -5.91812189 -6.27011861 -6.62211532\n[37] -6.97411204 -7.32610876\n\n\nCode\npredict(mod_glm)[2]\n\n\n       2 \n5.525628 \n\n\nCode\n# 4.994005 vs 5.5\n\n\nThe values are quite different… for the rate parameters, which is a little uncomfortable, and I’m not sure why."
  },
  {
    "objectID": "gam/gam.html#r-packages",
    "href": "gam/gam.html#r-packages",
    "title": "12  Generalized Additive Models",
    "section": "12.1 R Packages",
    "text": "12.1 R Packages\n\n“mgcv” - Mixed GAM Computational Vehicle\n\ngamm - for mixed models, uses nlme to fit\ngam - can be used more general than exponential\njagam - interface to JAGS for bayesian estiamtion\n\n“gamm4” - like gamm, but with lme4 in backend instead\n\nSmooth functions in “mgcv”:\n\ns()- for univariate smooths, isotropic smooths, and random effects\nte() - tensor product smooths constructed from singly pernalized marginal smooths\nti() - tensor product interactions with marginal smooth and lower order… for smooth anova models\nt2() alternative tensor product smooth construction,useful with gamm4 package. `\n\nSmooth terms in GAMS, bs = \"\" - tp - low rank, thin plate splines, isotropic smoothers. Isotropic means that rotation of the covariate will not change the result of smoothing. thin plates don’t have “knots”, a truncated eigen-decomposition is used for rank reduction. low rank means fewer coefficients than data to smooth. - ds - duchon splines - cr - cubic regression spline - cs - shrinkage version of cubic splines - cc - cyclic cubic regression splines - sos - splines on the sphere - ps - psplines, b-spline basis and perform well generally - cp - cyclic version of p-spline"
  },
  {
    "objectID": "gam/gam.html#univariate-smoothing",
    "href": "gam/gam.html#univariate-smoothing",
    "title": "12  Generalized Additive Models",
    "section": "12.2 Univariate Smoothing",
    "text": "12.2 Univariate Smoothing\n4.2 of Wood.\nExample on Engine data. Hypothesis is that larger the engine copacity, the faster the engine will wear out.\n\n\nCode\n# read engine data. Hypothesis is that\ndata(engine)\nwith(engine, plot(size, wear, xlab = \"Engine capacity\", ylab=\"Wear Index\"))\n\n\n\n\n\n\nxj are the knots\nx is the data.\n\n\n\nCode\n# function for generating tent functions, the basis function\ntf <- function(x, xj, j) {\n  dj <- xj*0\n  dj[j] <- 1\n  approx(xj, dj, x)$y\n}\ntf.X <- function(x,xj) {\n  # tent function basis matrix given data x, and knot sequence xj\n  nk <- length(xj); n <- length(x)\n  X <- matrix(NA, n, nk)\n  for (j in 1:nk) {\n    X[,j] <- tf(x,xj,j)\n  }\n  X\n}\n\n\n\n\nCode\nsj <- seq(min(engine$size), max(engine$size), length=6)\nX <- tf.X(engine$size,sj)\nb <- lm(wear~X-1, data=engine)\ns <- seq(min(engine$size), max(engine$size), length = 200) # prediction data\nXp <- tf.X(s,sj) # prediction matrix\nplot(engine$size, engine$wear, main = \"piecewise linear estimate without smoothness\")\nlines(s, Xp %*% coef(b))\n\n\n\n\n\ncool, looks reasonable, but should introduce a penalty for the smoothness of the function\nWe wish to penalize the parameters with some function\n\n\\begin{aligned}\n|| y-X\\beta ||^2 + \\lambda \\beta'D'D\\beta  \\\\\n= \\bigg\\| \\begin{bmatrix} y \\\\ 0 \\end{bmatrix} - \\begin{bmatrix}X \\\\ \\sqrt{\\lambda}D\\end{bmatrix}\\bigg\\|^2\n\\end{aligned}\n For example, the penalty can be expressed in the matrix form\n\n\\begin{aligned}\n\\lambda \\sum_{j=2}^{k-1}f(x^*_{j-1} - 2f(x^*_j) + f(x^*_{j+1}))^2\n\\end{aligned}\n\n\n\nCode\ndiff(diag(5), differences=2)\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1   -2    1    0    0\n[2,]    0    1   -2    1    0\n[3,]    0    0    1   -2    1\n\n\nCode\n?diff\n\n\n\n\nCode\n#' penalized regression fit\n#'\n#' @param y response variable\n#' @param x covariate\n#' @param xj knot locations\n#' @param sp smoothing parameter\n#'\n#' @return\n#' @export\n#'\n#' @examples\nprs.fit <- function(y,x, xj, sp) {\n  X <- tf.X(x, xj) ## model matrix\n  D <- diff(diag(length(xj)), differences=2) ## sqrt penalty, diff applied columnwise\n  X <- rbind(X, sqrt(sp)*D) # augmented model matrix\n  y <- c(y, rep(0, nrow(D))) # augmented data\n  lm(y~X-1)\n}\n\n# fit the model\nsj <- seq(min(engine$size), max(engine$size), length = 20) ## knots\nb <- prs.fit(engine$wear, engine$size, sj, 2) # smoothing parameter = 2\n\n# prediction data, with same knot locations\ns <- seq(min(engine$size), max(engine$size), length = 200) # prediction data\nXp <- tf.X(s,sj)\n\nwith(engine, plot(size, wear, main = \"smoothed estimate with lambda = 2\"))\nlines(s, Xp %*% coef(b))\n\n\n\n\n\n\n12.2.1 Examples\n\nBrain Imaging\n\n\nCode\n# specifying your own smoother\nlibrary(mgcv); library(MASS) ## load for mcycle data\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nCode\nsm <- smoothCon(s(times, k=10), data = mcycle, knots=NULL)[[1]]\nbeta <- coef(lm(mcycle$accel~sm$X-1))\n# create predictions\ntimes <- seq(0, 60, length = 200)\nXp <- PredictMat(sm, data.frame(times = times)) # get matrix mapping beta to spline predictions at times\nwith(mcycle, plot(times, accel))\nlines(times, Xp %*% beta)\n\n\n\n\n\n\n\nCode\ndata(brain)\nbrain <- brain[brain$medFPQ > 5e-3,] # exclude 2 outliers\nm0 <- gam(medFPQ ~ s(Y,X,k=100), data=brain)\ngam.check(m0) # general diagnostics\n\n\n\n\n\n\nMethod: GCV   Optimizer: magic\nSmoothing parameter selection converged after 6 iterations.\nThe RMS GCV score gradient at convergence was 6.236018e-05 .\nThe Hessian was positive definite.\nModel rank =  100 / 100 \n\nBasis dimension (k) checking results. Low p-value (k-index<1) may\nindicate that k is too low, especially if edf is close to k'.\n\n         k'  edf k-index p-value    \ns(Y,X) 99.0 86.8    0.86  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nk' gives the maximum possible EDF for smooth df.\n\nResidual plots look terrible, variance obviously increasing with mean. Assuming that there’s a power relationship with mean-variance,\n\n\\begin{aligned}\nvar(y_i) \\propto \\mu_i^\\beta\n\\end{aligned}\n\n\n\nCode\ne <- residuals(m0); fv <- fitted(m0)\nlm(log(e^2) ~ log(fv)) # log \\sigma^2 ~ log mean\n\n\n\nCall:\nlm(formula = log(e^2) ~ log(fv))\n\nCoefficients:\n(Intercept)      log(fv)  \n     -1.961        1.912  \n\n\nmeans that \\beta \\approx 2, implying that variance increases with square of the mean, so this is the gamma distribution. We could use the log link for the generalized linear family\n\n\nCode\nqplot(brain$X, brain$Y, color = brain$medFPQ) + theme_test()\n\n\n\n\n\n\n\nCode\n# 4th root transformation for stabilization\nm1 <- gam(medFPQ^.25 ~ s(Y,X, k=100), data=brain)\nm2 <- gam(medFPQ ~ s(Y,X, k=100), family = Gamma(link=log), data=brain)\ngam.check(m1)\n\n\n\n\n\n\nMethod: GCV   Optimizer: magic\nSmoothing parameter selection converged after 4 iterations.\nThe RMS GCV score gradient at convergence was 4.811308e-06 .\nThe Hessian was positive definite.\nModel rank =  100 / 100 \n\nBasis dimension (k) checking results. Low p-value (k-index<1) may\nindicate that k is too low, especially if edf is close to k'.\n\n         k'  edf k-index p-value    \ns(Y,X) 99.0 64.5    0.92  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# gam.check(m2) # similar, residuals look better\n\n\n\n\nCode\nmean(fitted(m1)^4); mean(fitted(m2)); mean(brain$medFPQ)\n\n\n[1] 0.9855539\n\n\n[1] 1.211483\n\n\n[1] 1.250302\n\n\nCode\nmean(fitted(m1)); mean(brain$medFPQ^.25)\n\n\n[1] 0.9832189\n\n\n[1] 0.9832189\n\n\nThe last value is the actual mean, so it makes sense that the link mean is the most accurate\n\n\nCode\n# m1 unbiased on 4th root scale vs unbiased on response scale in m2... \nvis.gam(m2, plot.type=\"contour\", too.far=.03, color = \"gray\", n.grid=60, zlim=c(-1, 2))\n\n\n\n\n\nCode\nm2\n\n\n\nFamily: Gamma \nLink function: log \n\nFormula:\nmedFPQ ~ s(Y, X, k = 100)\n\nEstimated degrees of freedom:\n60.6  total = 61.61 \n\nGCV score: 0.6216871     \n\n\n\n\nCode\n# try the additive model\nm3 <- gam(medFPQ ~ s(Y, k=30) + s(X, k=30), data = brain, family = Gamma(link=log))\nm3\n\n\n\nFamily: Gamma \nLink function: log \n\nFormula:\nmedFPQ ~ s(Y, k = 30) + s(X, k = 30)\n\nEstimated degrees of freedom:\n 9.58 20.20  total = 30.77 \n\nGCV score: 0.6453502     \n\n\nGCV score is higher, so it’s probably not better…and the aic comparison confirms this\n\n\nCode\nAIC(m2, m3) # m2 is lower\n\n\n         df      AIC\nm2 62.61062 3322.826\nm3 31.77467 3409.491\n\n\nIt makes sense that the additive model is not selected, because there’s no evidence of a longitudinal/latitude effect in strips. What about isotropic or tensor product smooths?\n\n\nCode\ntm <- gam(medFPQ ~ te(Y,X,k=10), data = brain, family = Gamma(link=log))\ntm1 <- gam(medFPQ ~ s(Y, k=10, bs=\"cr\") + s(X, bs=\"cr\", k=10) + ti(X,Y, k=10), data=brain, family = Gamma(link=log))\nAIC(m2, tm, tm1) # selects the isotropic spline. \n\n\n          df      AIC\nm2  62.61062 3322.826\ntm  60.34768 3336.805\ntm1 57.08366 3333.671\n\n\nIn summary, the models we consider are\n\nm1\n\n\n\\begin{aligned}\n\\log(\\mu_i) &= f_1(Y_i, X_i), \\qquad medFPQ \\sim Gamma\n\\end{aligned}\n\n\n\nCode\n# is there an interaction now?\nanova(tm1) # the p-value for the interaction term is significant.\n\n\n\nFamily: Gamma \nLink function: log \n\nFormula:\nmedFPQ ~ s(Y, k = 10, bs = \"cr\") + s(X, bs = \"cr\", k = 10) + \n    ti(X, Y, k = 10)\n\nApproximate significance of smooth terms:\n           edf Ref.df      F  p-value\ns(Y)     8.258  8.750 14.526  < 2e-16\ns(X)     7.494  8.314  6.959  < 2e-16\nti(X,Y) 39.332 49.891  2.291 1.34e-06\n\n\n\n\nCode\n# Symmetry with \"by\"\nbrain$Xc <- abs(brain$X - 64.5)\nbrain$right <- as.numeric(brain$X < 64.5)\nm.sy <- gam(medFPQ ~ s(Y, Xc, k=100), family = Gamma(link = log), data=brain) # symmetric\nm.as <- gam(medFPQ ~ s(Y, Xc, k=100) + s(Y, Xc, k=100, by = right), family = Gamma(link = log), data=brain) # asymmetry model\n\n\n\n\nCode\nm.sy\n\n\n\nFamily: Gamma \nLink function: log \n\nFormula:\nmedFPQ ~ s(Y, Xc, k = 100)\n\nEstimated degrees of freedom:\n51.4  total = 52.44 \n\nGCV score: 0.6489799     \n\n\nCode\nm.as # has a better GCV scroe, indicating that this model is better and by AIC it says the same thing.\n\n\n\nFamily: Gamma \nLink function: log \n\nFormula:\nmedFPQ ~ s(Y, Xc, k = 100) + s(Y, Xc, k = 100, by = right)\n\nEstimated degrees of freedom:\n50.5 44.7  total = 96.2 \n\nGCV score: 0.6176281     \n\n\nCode\nanova(m.as) # p-value for the smooth \"right\" term seems to be far from 0.\n\n\n\nFamily: Gamma \nLink function: log \n\nFormula:\nmedFPQ ~ s(Y, Xc, k = 100) + s(Y, Xc, k = 100, by = right)\n\nApproximate significance of smooth terms:\n                edf Ref.df     F p-value\ns(Y,Xc)       50.48  65.99 4.344  <2e-16\ns(Y,Xc):right 44.72  59.21 2.457  <2e-16"
  },
  {
    "objectID": "gam/gam.html#air-pollution-in-chicago",
    "href": "gam/gam.html#air-pollution-in-chicago",
    "title": "12  Generalized Additive Models",
    "section": "12.3 Air Pollution in Chicago",
    "text": "12.3 Air Pollution in Chicago\n7.4 in Simon Wood book\n\n\nCode\ndata(chicago)\nap0 <- gam(death~s(time, bs=\"cr\", k=200) + pm10median + so2median + o3median + tmpd, data = chicago, family = poisson)\ngam.check(ap0)\n\n\n\n\n\n\nMethod: UBRE   Optimizer: outer newton\nfull convergence after 3 iterations.\nGradient range [3.514596e-08,3.514596e-08]\n(score 0.2546689 & scale 1).\nHessian positive definite, eigenvalue range [0.004247567,0.004247567].\nModel rank =  204 / 204 \n\nBasis dimension (k) checking results. Low p-value (k-index<1) may\nindicate that k is too low, especially if edf is close to k'.\n\n         k' edf k-index p-value    \ns(time) 199 169    0.92  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nqqplot is obviously quite proble\n\n\nCode\npar(mfrow = c(2,1))\nplot(ap0, n = 1000) # plot the smooth functions that make it up\n# n - needs to be several times larger than the effective degrees of freedom\nplot(ap0, residuals = TRUE, n = 1000)\n\n\n\n\n\n\n\nCode\nap1 <- gam(death ~ s(time, bs = \"cr\", k=200) + s(pm10median, bs = \"cr\") + s(so2median, bs = \"cr\") + s(o3median, bs = \"cr\") + s(tmpd, bs = \"cr\"), data = chicago, family = poisson)\ngam.check(ap1) # honestly not that much better\n\n\n\n\n\n\nMethod: UBRE   Optimizer: outer newton\nfull convergence after 8 iterations.\nGradient range [-6.661707e-07,4.670663e-08]\n(score 0.2410737 & scale 1).\nHessian positive definite, eigenvalue range [5.217842e-05,0.004273638].\nModel rank =  236 / 236 \n\nBasis dimension (k) checking results. Low p-value (k-index<1) may\nindicate that k is too low, especially if edf is close to k'.\n\n                  k'    edf k-index p-value    \ns(time)       199.00 167.93    0.94  <2e-16 ***\ns(pm10median)   9.00   6.86    1.02    0.90    \ns(so2median)    9.00   7.38    0.99    0.17    \ns(o3median)     9.00   1.58    1.00    0.38    \ns(tmpd)         9.00   8.27    1.02    0.96    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\n# confirming that I know where poisson residuals are coming from\nresiduals(ap1, type = \"pearson\")[1:10]\n\n\n [1]  0.7543849 -1.8266695  1.1255760  0.7299294  0.6915362 -1.1494885\n [7]  0.3019724  0.3816246 -0.8354141 -1.6209453\n\n\nCode\n# chicago$death - predict(ap1)\n\npredict(ap1) %>% length()\n\n\n[1] 4841\n\n\nCode\ny <- chicago %>% filter(!is.na(pm10median), !is.na(death), !is.na(so2median), !is.na(tmpd)) %>% \n  pull(death)\n\nyhat <- predict(ap1, type = \"response\")\n\ncbind(((y - yhat) / sqrt(yhat))[1:10],\n      residuals(ap1, type = \"pearson\")[1:10])\n\n\n         [,1]       [,2]\n1   0.7543849  0.7543849\n3  -1.8266695 -1.8266695\n4   1.1255760  1.1255760\n6   0.7299294  0.7299294\n7   0.6915362  0.6915362\n8  -1.1494885 -1.1494885\n9   0.3019724  0.3019724\n11  0.3816246  0.3816246\n12 -0.8354141 -0.8354141\n13 -1.6209453 -1.6209453\n\n\n\n\nCode\n# Consider the single index model\n# ?single.index\n# because we're studying air pollution, there's probably some lag involved, or weights thereof\nlagard <- function(x, n.lag = 6) {\n  n <- length(x); X <- matrix(NA, n, n.lag)\n  for (i in 1:n.lag) X[i:n, i] <- x[i:n-i+1] # set the next column, starting at i.\n  X\n}\n\nlagard(1:5, n.lag = 3) # probably most clear just to look at the model matrix here.\n\n\n     [,1] [,2] [,3]\n[1,]    1   NA   NA\n[2,]    2    1   NA\n[3,]    3    2    1\n[4,]    4    3    2\n[5,]    5    4    3\n\n\n\n\nCode\ndat <- list(lag = matrix(0:5, nrow(chicago), 6, byrow = TRUE))\ndat$death <- chicago$death\ndat$time <- chicago$time\ndat$pm10 <- lagard(chicago$pm10median)\ndat$o3 <- lagard(chicago$o3)\ndat$tmp <- lagard(chicago$tmp)\n\n\nsi <- function(theta, dat, opt = TRUE) {\n  alpha <- c(1, theta) ## alpha defined via unconstrained theta\n  kk <- sqrt(sum(alpha^2)); alpha <- alpha/kk ## ||alpha || = 1\n  o3 <- dat$o3 %*% alpha;\n  tmp <- dat$tmp %*% alpha\n  pm10 <- dat$pm10 %*% alpha ## re-weight laggard covariates\n  b <- bam(dat$death ~ s(dat$time, k = 200, bs=\"cr\") + s(pm10, bs=\"cr\") + te(o3, tmp, k=8), family=poisson) ## fit the model\n  cat(\".\") # give user something to watch\n  if (opt) return(b$gcv.ubre) else {\n    b$alpha <- alpha ## add alpha to model object\n    b$J <- outer(alpha, -theta/kk^2) ## get dalpha_i/dtheta_j\n    for (j in 1:length(theta)) b$J[j+1,j] <- b$J[j+1, j] + 1 /kk\n    return(b)\n  }\n}\n\n# f1 <- optim(rep(1, 5), si, method = \"BFGS\", hessian=TRUE, dat = dat)\n# save(f1, file = \"f1.RData\") # The results from f1 took forever to run, almost an hour, so save the results and just load it.\nload(\"f1.RData\")\napsi <- si(f1$par, dat, opt = FALSE)\n\n\n.\n\n\nCode\napsi$alpha\n\n\n[1]  0.03419156  0.67333186  0.63917561  0.25879701  0.24601994 -0.09699472"
  },
  {
    "objectID": "longitudinal/longitudinal.html",
    "href": "longitudinal/longitudinal.html",
    "title": "13  Longitudinal Data Analysis",
    "section": "",
    "text": "14 Why Longitudinal Data Analysis\nMore information about the change, at the trade-off of needing to take repeated measurments.\nThe short hand notation for the observations of a single individual are:\n\\begin{aligned}\nY_{i} = X_{i}\\beta + \\varepsilon_{i}\n\\end{aligned}\n with \\varepsilon_i \\sim N(0, \\Sigma), which expanded would look like,\n\\begin{aligned}\n\\begin{bmatrix}\ny_{i1} \\\\\ny_{i2} \\\\\n\\vdots \\\\\ny_{in_{i}} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nX_{i11} & X_{i12} & \\dots  &X_{i1p}  \\\\\nX_{i21} &  X_{i22} & \\dots  &X_{i2p}  \\\\\n\\vdots &  \\vdots & \\ddots  & \\vdots  \\\\\nX_{in_{i}1} &  X_{in_{i}2} & \\dots  & X_{in_{i}p}  \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\varepsilon_{i1} \\\\\n\\varepsilon_{i2} \\\\\n\\vdots \\\\\n\\varepsilon_{in_{i}}\n\\end{bmatrix}\n\\end{aligned}\nIn total, we would stack them, so we have\n\\begin{aligned}\n\\begin{bmatrix}\nY_1 \\\\\nY_2 \\\\\n\\vdots \\\\\nY_N \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\nX_1 \\\\\nX_2 \\\\\n\\vdots \\\\\nX_N \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_1 \\\\\n\\beta_2 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix} +\n\\begin{bmatrix}\n\\mathbf{\\varepsilon_{1}} \\\\\n\\mathbf{\\varepsilon_{2}} \\\\\n\\vdots \\\\\n\\mathbf{\\varepsilon_{N}}\n\\end{bmatrix}\n\\end{aligned}\n Thus, \\varepsilon has a block diagonal structure with each subject having the same covariance matrix.\nThis section follows 5.5 in Fitzmaurice, Laird, and Ware (2011).\nThe main questions to ask in this model are\nThere are primarily two ways of studying the longitudinal responses\nThere’s - strucchange::gefp - structural change testing.\nWe should note that there are econometric approaches, such as sandwich estimators, and mixed model approaches that are all competing in the space of modeling the variance. Broadly there are three approaches for variance modeling:\nPackages with this approach from the sandwich vignette\nThis is a method of adjusting for differing slopes and effects.\nThere are 4 ways that we can discuss for baseline adjustment from Fitzmaurice, Laird, and Ware (2011) chapter 5.7\nIn summary, Fitzmaurice shows and recommends:\nDepending on the strategy that you choose, the interpretation of the coefficients will differ, so beware, although some cases reduce to linear combinations of the coefficients.\nWe examine the differences of these 4 approaches on the TLC dataset.\nResiduals from a longitudinal model will be correlated. Fitzmaurice, Laird, and Ware (2011) recommends that we “decorrelate” them with a cholesky decomposition of the estimated covariance of the errors. It can get confusing with the estimators and terminology here…\n\\begin{aligned}\n\\varepsilon &= Y_i - X_i\\beta \\\\\n\\hat\\varepsilon &= Y_i - X_i \\hat\\beta \\\\\n\\mathrm{Cov}(\\varepsilon) &= \\Sigma \\\\\n\\widehat{\\mathrm{Cov}}(\\varepsilon) &= \\hat\\Sigma \\\\\n\\mathrm{Cov}(\\hat\\varepsilon) &= ??\n\\end{aligned}\nWe assume that \\mathrm{Cov}(\\hat \\varepsilon) \\approx \\mathrm{Cov}(\\varepsilon).\nThe Econometrics methods that deal with longitudinal data is a little different than statistics. We’ll explore the terminology and methods here.\nProvides functions for panel data from “econometricians” point of view.\nProvides a few functions\nSee Wikipedia, the article is fairly comprehensive.\nCommon forms of the gls command are as follows (from Pinheiro, Pinheiro, and Bates (2000)):\nWe can play around with the structure of covariance matrices with:"
  },
  {
    "objectID": "longitudinal/longitudinal.html#specifics-of-lda",
    "href": "longitudinal/longitudinal.html#specifics-of-lda",
    "title": "13  Longitudinal Data Analysis",
    "section": "14.1 Specifics of LDA",
    "text": "14.1 Specifics of LDA\n\nReplication is a series of observations (each subject) and not individual measurements. Thus the EU is the subject."
  },
  {
    "objectID": "longitudinal/longitudinal.html#approaches-in-lda",
    "href": "longitudinal/longitudinal.html#approaches-in-lda",
    "title": "13  Longitudinal Data Analysis",
    "section": "14.2 Approaches in LDA",
    "text": "14.2 Approaches in LDA\n\nMarginal (averages over population. state employee perspective)\n(Random effects) Mixed Models (doctor perspective, subject specifics)\nTransition Model (Ware et al, 1988)"
  },
  {
    "objectID": "longitudinal/longitudinal.html#historical-methods",
    "href": "longitudinal/longitudinal.html#historical-methods",
    "title": "13  Longitudinal Data Analysis",
    "section": "14.3 Historical Methods",
    "text": "14.3 Historical Methods\n\n14.3.1 Split plot in time\n\nInduces a compound symmetric structure with observations\n\n\n\n14.3.2 Repeated Measures ANOVA\nI think this is the same as split plot in time… The model is\n\nY_{ij} = X_{ij}'\\beta + b_i + e_{ij}\n The covariance structure is compound symmetry, meaning\n\n\\begin{aligned}\n\\text{Cov}(Y_{i}) =\n\\begin{bmatrix}\n\\sigma_b + \\sigma_e & \\sigma_e & \\dots &\\sigma_e \\\\\n\\sigma_e & \\sigma_b + \\sigma_e & \\dots &\\sigma_e \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_e &  \\sigma_e & \\dots & \\sigma_b + \\sigma_e \\\\\n\\end{bmatrix}\n\\end{aligned}\n\n\n\n14.3.3 MANOVA\nSpecial case of the so called profile analysis. The main idea for MANOVA is to make some trasnformations and make some derived response varaibles to analyze. The multiple comes from having multiple responses. One is to make the sum of all the responses, to examine average over time. You could also create a variable for linear change across time, or quadratic change within subject.\nThere are some disadvantages though,\n\nIf design is unbalanced across time, MANOVA can’t be used.\nAlso if there are any missing data, the entire case must be thrown out.\n\n\n\n14.3.4 Summary values\nReduce the sequence of each individual to a small set of summary values. Then, you can use the classic t-test or ANOVA, univariate tests.\n\nArea under curve (AUC) - Can only be used when the people have the same time measurements.\n\nDrawbacks\n\nforces data analyst to think about one aspect of the repeated measures.\nMight have the same summary measure but different response profile.\nMethod can’t be applied if one of the covariates is time varying. b/c variance will not be constant from summary to summary."
  },
  {
    "objectID": "longitudinal/longitudinal.html#inference-on-parameters",
    "href": "longitudinal/longitudinal.html#inference-on-parameters",
    "title": "13  Longitudinal Data Analysis",
    "section": "14.4 Inference on parameters",
    "text": "14.4 Inference on parameters\nLikelihood test requires an additional fit on the null hypothesis, but better properties. Recommend Likelihood ratio based tests and CI. Note the RMLE is a correction for the data estimating both the mean and covariance."
  },
  {
    "objectID": "longitudinal/longitudinal.html#example-tlc",
    "href": "longitudinal/longitudinal.html#example-tlc",
    "title": "13  Longitudinal Data Analysis",
    "section": "16.1 Example TLC",
    "text": "16.1 Example TLC\nThis is an exploratory plot summarizing the eventual statements we’d like to say about the model.\n\n\nCode\ntlc_raw <- read.csv(\"data/tlc.csv\", header=FALSE)\nnames(tlc_raw) <- c(\"id\", \"trt\",\"w0\", \"w1\", \"w4\",\"w6\")\n\ntlc <- tlc_raw %>% gather(\"week\", \"lead\", 3:6)\n\n# numeric version of week\ntlc$week_int <- tlc$week %>% gsub(\".*([0-9]+).*\", \"\\\\1\", .) %>% as.numeric()\n\n# timepoint (1:4)\ntlc$time <- c(1:4)[as.factor(tlc$week)]\n\n# change from baseline\ntlc_diff <- tlc %>% group_by(id) %>% \n  arrange(trt, id, time) %>% \n  mutate(lead_diff = lead - lead[1],\n         lead_base = lead[1],\n         trt = factor(trt, levels = c(\"P\", \"A\"))) %>% \n  arrange(desc(trt), id, time) %>% \n  filter(week != \"w0\")\n\n\n\n\nCode\ntlc %>% ggplot(aes(x = week, y = lead, group = id)) +\n  geom_line(alpha = .2) +\n  facet_wrap(~trt)\n\n\n\n\n\n\n\nCode\n# Mean Response Profile by Trt\ntlc_mrp <- tlc %>% \n  group_by(trt, week) %>% \n  summarise(mlead = mean(lead), \n            .groups = \"drop_last\")\n\ntlc_mrp %>% \n  ggplot(aes(x = week, y=mlead, group=trt)) +\n  geom_line(aes(linetype=trt)) +\n  geom_point() +\n  ggtitle(\"Lead over time\") +\n  theme_classic()\n\n\n\n\n\n\n16.1.1 SAS\n\n\nCode\n* TLC data analysis example, from chapter 5;\n* https://content.sph.harvard.edu/fitzmaur/ala2e/;\n\nDATA tlc_long;\n  INFILE \"~/lda/tlc-data.txt\" DLM=\" \";\n  input id group $ lead0 lead1 lead4 lead6;\n  * create 4 observations from each row;\n  y=lead0; time=0; output;\n  y=lead1; time=1; output;\n  y=lead4; time=4; output;\n  y=lead6; time=6; output;\n  drop lead0 lead1 lead4 lead6; * drop original \"wide\" data columns;\nrun;\n\n/* set reference level */\n/* http://support.sas.com/kb/37/108.html */\nproc mixed noclprint=10 data=tlc_long\nclass id group(ref=\"P\") time(ref=\"0\");\nmodel y = group time group*time / s chisq;\nrepeated time / type=un subject=id r;\nlsmeans group / cl diff;\nrun;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.1.2 R: gls\nWe fit an unstructured covariance, with fully crossed time and treatment factors. In order to get the unstructured covariance matrix, we must use gls, for generalized least squares. This is basically\n\n\nCode\ntlc$trt <- factor(tlc$trt, levels = c(\"P\", \"A\")) # use P as the reference level\n\n# Generalized Least Squares, defaults to REML\ntlc_gls <- gls(lead ~ trt*week,\n               corr=corSymm(form = ~ time | id),\n               weights = varIdent(form = ~ 1 | week),\n               data=tlc)\n\n# Table 5.5 in Fitzmaurice\nsummary(tlc_gls)\n\n\nGeneralized least squares fit by REML\n  Model: lead ~ trt * week \n  Data: tlc \n       AIC      BIC    logLik\n  2452.076 2523.559 -1208.038\n\nCorrelation Structure: General\n Formula: ~time | id \n Parameter estimate(s):\n Correlation: \n  1     2     3    \n2 0.571            \n3 0.570 0.775      \n4 0.577 0.582 0.581\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | week \n Parameter estimates:\n      w0       w1       w4       w6 \n1.000000 1.325880 1.370442 1.524813 \n\nCoefficients:\n              Value Std.Error   t-value p-value\n(Intercept)  26.272 0.7102929  36.98756  0.0000\ntrtA          0.268 1.0045059   0.26680  0.7898\nweekw1       -1.612 0.7919199  -2.03556  0.0425\nweekw4       -2.202 0.8149021  -2.70217  0.0072\nweekw6       -2.626 0.8885253  -2.95546  0.0033\ntrtA:weekw1 -11.406 1.1199438 -10.18444  0.0000\ntrtA:weekw4  -8.824 1.1524456  -7.65676  0.0000\ntrtA:weekw6  -3.152 1.2565645  -2.50843  0.0125\n\n Correlation: \n            (Intr) trtA   weekw1 weekw4 weekw6 trtA:1 trtA:4\ntrtA        -0.707                                          \nweekw1      -0.218  0.154                                   \nweekw4      -0.191  0.135  0.680                            \nweekw6      -0.096  0.068  0.386  0.385                     \ntrtA:weekw1  0.154 -0.218 -0.707 -0.481 -0.273              \ntrtA:weekw4  0.135 -0.191 -0.481 -0.707 -0.272  0.680       \ntrtA:weekw6  0.068 -0.096 -0.273 -0.272 -0.707  0.386  0.385\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-2.1756456 -0.6849980 -0.1515547  0.5294176  5.6327571 \n\nResidual standard error: 5.02253 \nDegrees of freedom: 400 total; 392 residual\n\n\nThe estimated (unstructured), marginal covariance structure can be extracted by getVarCov.\n\n\nCode\ngetVarCov(tlc_gls) # covariance matrix\n\n\nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 25.226 19.107 19.700 22.202\n[2,] 19.107 44.346 35.535 29.675\n[3,] 19.700 35.535 47.377 30.620\n[4,] 22.202 29.675 30.620 58.651\n  Standard Deviations: 5.0225 6.6593 6.8831 7.6584 \n\n\nGetting the type III fixed effect tests, similar to SAS, is a little more work. There must be an easier way, but this shows how to do it manually. Here P0 denotes the mean of placebo at time 0.\n\n\n\nConditional Mean (\\mu)\nCoef\n\n\n\n\nP0\n\\beta_1\n\n\nP1\n\\beta_1 + \\beta_3\n\n\nP4\n\\beta_1 + \\beta_4\n\n\nP6\n\\beta_1 + \\beta_5\n\n\nS0\n\\beta_1 + \\beta_2\n\n\nS1\n\\beta_1 + \\beta_2 + \\beta_3 + \\beta_6\n\n\nS4\n\\beta_1 + \\beta_2 + \\beta_4 + \\beta_7\n\n\nS7\n\\beta_1 + \\beta_2 + \\beta_5 + \\beta_8\n\n\n\nFurthermore, the Wald test statistic is\n\n\\begin{aligned}\nW^2 = (L\\hat\\beta)'\\{L\\mathrm{Cov}(\\hat\\beta)L'\\}^{-1}L\\hat\\beta\n\\end{aligned}\n Compare the following table to the SAS Type 3 tests for Fixed Effects\n\n\nCode\n# variance of estimated coef, beta hat\ncovbeta <- tlc_gls$varBeta\nbeta <- coef(tlc_gls)\n\n# testing interactions together, manually\ntrt_week_coef <- beta[6:8]\ntrt_week_cov <- covbeta[6:8, 6:8]\n# t(trt_week_coef) %*% solve(trt_week_cov) %*% trt_week_coef\n\n# avg treatment - avg control, H0: S = P\ntrt_L <- c(4, 4, 1, 1, 1, 1, 1, 1) - c(4, 0, 1, 1, 1, 0, 0, 0) %>% matrix() %>% t()# c(0,4,0,0,0,1,1,1)\n# manually, its a scalar\n# trt_L[c(2, 6:8)] %*% beta[c(2, 6:8)] * \n#   solve(trt_L[,c(2, 6:8)] %*% covbeta[c(2, 6:8), c(2, 6:8)] %*% cbind(trt_L[,c(2, 6:8)])) * \n#   trt_L[c(2, 6:8)] %*% beta[c(2, 6:8)]\n\n# average by week, H0: w0 = w1 = w4 = w6\nw0_L <- c(2, 1, 0, 0, 0, 0, 0, 0)\nw1_L <- c(2, 1, 2, 0, 0, 1, 0, 0)\nw4_L <- c(2, 1, 0, 2, 0, 0, 1, 0)\nw6_L <- c(2, 1, 0, 0, 2, 0, 0, 1)\nweek_L <- matrix(c(w1_L - w0_L,\n                   w4_L - w0_L,\n                   w6_L - w0_L), nrow = 3, byrow=T)\n\n# Combine the code above\nrbind(anova(tlc_gls, L = trt_L), # Wald F value = 25.43\n      anova(tlc_gls, L = week_L), # Wald F value = 61.49\n      anova(tlc_gls, Terms = 4)) %>%  # Wald F value = 35.9\n  as.data.frame() %>% \n  dplyr::mutate(Chisq = numDF * `F-value`,\n                .after = `F-value`) %>% \n  add_column(source = c(\"Trt\", \"Week\", \"Trt x Week\"), .before = 1)\n\n\n\n\n16.1.3 R: anova.gls\nI’m not entirely sure what’s going on in the anova function in nlme. There is a value for intercept and treatment, and I’m not sure how to interpret that.\nIt seems the week and interaction are being calculated correctly, not sure why the treatment is different from the manual calculations above. In the presence of an interaction effect however, it doesn’t really matter.\nAlso notice that Chisq statistic is just the F statistic * ndf. Assuming chisq gives more liberal estimates, effectively infinite residual degrees of freedom.\nThe following are two anova tables of the same model,\n\n\nCode\n# these results match\nanova(tlc_gls, type = \"marginal\") # nlme\n\n\nDenom. DF: 392 \n            numDF   F-value p-value\n(Intercept)     1 1368.0793  <.0001\ntrt             1    0.0712  0.7898\nweek            3    3.8731  0.0095\ntrt:week        3   35.9293  <.0001\n\n\nCode\nAnova(tlc_gls, \"III\", test.statistic = \"F\", error.df = tlc_gls$dims$N - tlc_gls$dims$p) # car w/ 392 error df\n\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: lead\n             Df         F  Pr(>F)    \n(Intercept)   1 1368.0793 < 2e-16 ***\ntrt           1    0.0712 0.78977    \nweek          3    3.8731 0.00946 ** \ntrt:week      3   35.9293 < 2e-16 ***\nResiduals   392                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\n# more liberal\nAnova(tlc_gls, \"III\", test.statistic = \"Chisq\")\n\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: lead\n            Df     Chisq Pr(>Chisq)    \n(Intercept)  1 1368.0793  < 2.2e-16 ***\ntrt          1    0.0712   0.789625    \nweek         3   11.6192   0.008808 ** \ntrt:week     3  107.7880  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\n# not sure what's going on here\nanova(tlc_gls, type = \"sequential\")\n\n\n\n\n16.1.4 R: lm\nFor comparison, we can see that gls gives the same estimates as the gls model, for profile curves, it basically goes through the mean of each group. lm is NOT the right way to do this analysis, but we want to see what it shows for comparison.\n\n\nCode\ntlc_lm <- lm(lead ~ trt*week, data = tlc)\ncbind(\"gls\" = coef(tlc_gls), \"lm\" = coef(tlc_lm))\n\n\n                gls      lm\n(Intercept)  26.272  26.272\ntrtA          0.268   0.268\nweekw1       -1.612  -1.612\nweekw4       -2.202  -2.202\nweekw6       -2.626  -2.626\ntrtA:weekw1 -11.406 -11.406\ntrtA:weekw4  -8.824  -8.824\ntrtA:weekw6  -3.152  -3.152\n\n\ncompare the standard errors of the estimates though\n\n\nCode\ncbind(\"gls\" = sqrt(diag(as.matrix(tlc_gls$varBeta))),\n      \"lm\" = tlc_lm %>% tidy() %>% pull(std.error))\n\n\n                  gls        lm\n(Intercept) 0.7102929 0.9370175\ntrtA        1.0045059 1.3251428\nweekw1      0.7919199 1.3251428\nweekw4      0.8149021 1.3251428\nweekw6      0.8885253 1.3251428\ntrtA:weekw1 1.1199438 1.8740349\ntrtA:weekw4 1.1524456 1.8740349\ntrtA:weekw6 1.2565645 1.8740349\n\n\nnot quite sure this is the fairest comparison, but the heterogeneity in the estimated variance seems to have stabilized the standardized residuals. Need to double check if this is the right stabilization.\n\n\nCode\nplot(tlc_gls, resid(., type = \"pearson\") ~ week_int) # show against time (instead of default fitted)\n\n\n\n\n\nCode\nplot(tlc$week_int, rstudent(tlc_lm))\n\n\n\n\n\n\n\n16.1.5 R: AUC\nA third strategy is to do summary statistics of the curves. The section in the book refers to creating 1 df tests, which are more powerful than the overall F test of trt x time interaction. Often times, if you measure many many time points, the overall f test becomes diluted and it becomes more difficult to detect differences in the two curves, so these are designed to have more power than those tests. should only be specified prior to analysis though, to keep with proper significance control\nWe consider two tests,\n\naverage difference minus baseline.\n\n\nthe contrast will take the form (where n is the number of occassions)\n\n\n\\begin{aligned}\nL &= (L_1, -L_1) \\\\\nL_1 &= \\left(-1, \\frac{1}{n-1}, \\frac{1}{n-1}, \\dots, \\frac{1}{n-1}\\right)\n\\end{aligned}\n\n\nArea under curve (AUC) minus baseline\n\n\napproximated by trapezoids\n\nboth of these can be formulated as a contrast, we’ll use emmeans for convenience here.\n\n\nCode\nemm_tlc_gls <- emmeans(tlc_gls, specs=c(\"week\", \"trt\"))\n\n\nAnalytical Satterthwaite method not available; using appx-satterthwaite\n\n\nCode\nemm_tlc_gls\n\n\n week trt emmean    SE   df lower.CL upper.CL\n w0   P     26.3 0.710 97.9     24.9     27.7\n w1   P     24.7 0.942 98.1     22.8     26.5\n w4   P     24.1 0.973 98.1     22.1     26.0\n w6   P     23.6 1.083 98.1     21.5     25.8\n w0   A     26.5 0.710 97.9     25.1     27.9\n w1   A     13.5 0.942 98.1     11.7     15.4\n w4   A     15.5 0.973 98.1     13.6     17.4\n w6   A     20.8 1.083 98.1     18.6     22.9\n\nDegrees-of-freedom method: appx-satterthwaite \nConfidence level used: 0.95 \n\n\nCode\navg_minus_baseline_L <- c(-1, 1/3, 1/3, 1/3, 1, -1/3, -1/3, -1/3)\nauc_minus_baseline_L <- c(5.5, -2, -2.5, -1, -5.5, 2, 2.5, 1)\ncontrast(emm_tlc_gls, list(\"avg minus baseline\" = avg_minus_baseline_L,\n                           \"AUC minus baseline\" = auc_minus_baseline_L))\n\n\n contrast           estimate   SE   df t.ratio p.value\n avg minus baseline     7.79 0.95 98.1   8.205  <.0001\n AUC minus baseline   -48.02 5.35 97.9  -8.973  <.0001\n\nDegrees-of-freedom method: appx-satterthwaite \n\n\nCode\nanova(tlc_gls, L = avg_minus_baseline_L)\n\n\nDenom. DF: 392 \n F-test for linear combination(s)\n(Intercept)        trtA      weekw1      weekw4      weekw6 trtA:weekw1 \n -1.0000000   0.3333333   0.3333333   0.3333333   1.0000000  -0.3333333 \ntrtA:weekw4 trtA:weekw6 \n -0.3333333  -0.3333333 \n  numDF F-value p-value\n1     1 91.2127  <.0001"
  },
  {
    "objectID": "longitudinal/longitudinal.html#example-ratdrink",
    "href": "longitudinal/longitudinal.html#example-ratdrink",
    "title": "13  Longitudinal Data Analysis",
    "section": "16.2 Example: Ratdrink",
    "text": "16.2 Example: Ratdrink\nThe following example is from @faraway_extending_2016\n\n\nCode\ndata(ratdrink)\nratdrink %>% ggplot(aes(weeks, wt, group = subject)) +\n  geom_line() +\n  facet_wrap(~treat)\n\n\n\n\n\n\n16.2.1 different intercepts\n\n\nCode\n# full lm\nrat_lm <- lm(wt ~ weeks*treat + subject, data = ratdrink) # fixed \"block\"\nrat_mmer <- lmer(wt ~ weeks*treat + (1|subject), data = ratdrink) # random \"block\"\n\n# plotting the predictions\nrat_preds <- ratdrink %>% add_column(lm_yhat = predict(rat_lm),\n                                     mmer_yhat = predict(rat_mmer))\nrat_preds %>% pivot_longer(c(wt, lm_yhat, mmer_yhat), names_to = \"response\", values_to = \"y\") %>% \n  ggplot() +\n  geom_line(aes(weeks, y, groups = subject)) +\n  facet_grid(response~treat)\n\n\n\n\n\n\n\n16.2.2 different slopes\nThe bottom row is the raw data while, the second row is the predictions from a random subject, and finally the top row is the predictions from the fully fixed model.\n\n\nCode\nrat_lm_diffslope <- lm(wt~weeks*treat + subject + subject:weeks, data = ratdrink)\n# rat_mmer_interaction <- lmer(wt ~ weeks*treat + (1|weeks:subject), data = ratdrink) # weeks is continuous! doesn't work.... too many random effect intercepts\nrat_mmer_interaction <- lmer(wt ~ weeks*treat + (1|subject) + subject:weeks, data = ratdrink) # weird model....b/c subject:weeks is fixed not sure when \nrat_mmer_random_slope <- lmer(wt ~ weeks*treat + (weeks|subject), data = ratdrink)\nrat_mmer_random_slope_nocor <- lmer(wt ~ weeks*treat + (1 | subject) + (0 + weeks || subject), data = ratdrink) # without correlation between intercept and slope\n\nrat_preds_slope <- ratdrink %>% add_column(lm_diffslope_yhat = predict(rat_lm_diffslope),\n                                     mmer_interaction_yhat = predict(rat_mmer_interaction),\n                                     mmer_random_slope_yhat = predict(rat_mmer_random_slope),\n                                     mmer_random_slope_nocor_yhat = predict(rat_mmer_random_slope_nocor))\n\nrat_preds_slope %>% \n  pivot_longer(c(wt, \n                 lm_diffslope_yhat, \n                 mmer_interaction_yhat, \n                 mmer_random_slope_yhat, \n                 mmer_random_slope_nocor_yhat), names_to = \"response\", values_to = \"y\") %>% \n  ggplot() +\n  geom_line(aes(weeks, y, group = subject)) +\n  facet_grid(response~treat)\n\n\n\n\n\n\n\nCode\n# profile conf intervals\nconfint(rat_mmer_random_slope)\n\n\nComputing profile confidence intervals ...\n\n\n                            2.5 %     97.5 %\n.sig01                  3.4506344  7.6654748\n.sig02                 -0.5261030  0.3794203\n.sig03                  2.6064121  4.8687653\n.sigma                  3.7555591  5.1142342\n(Intercept)            48.8692859 56.8907140\nweeks                  24.0547424 28.9052555\ntreatthiouracil        -0.8920062 10.4520061\ntreatthyroxine         -7.0445321  5.4559606\nweeks:treatthiouracil -12.7998322 -5.9401707\nweeks:treatthyroxine   -3.1166339  4.4423449"
  },
  {
    "objectID": "longitudinal/longitudinal.html#example-body-fat-and-menarche",
    "href": "longitudinal/longitudinal.html#example-body-fat-and-menarche",
    "title": "13  Longitudinal Data Analysis",
    "section": "16.3 Example: Body Fat and Menarche",
    "text": "16.3 Example: Body Fat and Menarche\nWe use the MIT growth study menarche example for this section, from Fitzmaurice website\nCovariates/Response:\n\nid : girl id\nage : age at observation\nagemen : age of menarche for girl\ntime : age - agemen, time relative to menarche.\npbf : percentage body fat\n\n\n\nCode\nfat <- read.dta(\"data/fat.dta\")\n\n# select 20 random girls and show response curve\nfat %>% \n  group_nest(id) %>% \n  slice_sample(n=20) %>% \n  unnest(data) %>% \n  ggplot(aes(time, pbf)) + \n  geom_point() +\n  geom_line() +\n  facet_wrap(~id) +\n  geom_vline(xintercept = 0, color = \"red\")\n\n\n\n\n\n\n16.3.1 Modeling\nFitzmaurice, Laird, and Ware (2011) in chapter 8.8 analyzes this as a piecewise random effects model.\n\n\nCode\n# to fit piecewise function, need variable with after menarche time\nfat_post <- fat %>% mutate(timepost = time * (time > 0)) # create variable for post menarche time\nfat_lme <- lme(pbf ~ time + timepost,\n    random = ~time + timepost | id,\n    data = fat_post)\n\n\nThe fixed effects of the model:\n\n\nCode\n# Fixed effects\nfat_lme %>% tidy() %>% filter(effect == \"fixed\") %>% \n  dplyr::select(term, estimate, std.error, statistic, df, p.value)\n\n\n# A tibble: 3 × 6\n  term        estimate std.error statistic    df   p.value\n  <chr>          <dbl>     <dbl>     <dbl> <dbl>     <dbl>\n1 (Intercept)   21.4       0.565     37.8    885 3.95e-187\n2 time           0.417     0.157      2.65   885 8.09e-  3\n3 timepost       2.05      0.228      8.98   885 1.60e- 18\n\n\nThe random effects and standard errors calculated from inverse expected fisher information (with package lmeInfo)\n\n\nCode\n# https://stats.oarc.ucla.edu/r/faq/how-can-i-calculate-standard-errors-for-variance-components-from-mixed-models/\n# Random effects and standard errors\nfat_varcomp <- getVarCov(fat_lme)[upper.tri(getVarCov(fat_lme), diag = TRUE)]\ncbind(estimate = fat_varcomp,\n      `std. err` = sqrt(diag(varcomp_vcov(fat_lme)))[c(1, 2, 3, 4, 5, 6)])\n\n\n                                  estimate  std. err\nTau.id.var((Intercept))          45.939153 5.7556122\nTau.id.cov(time,(Intercept))      2.526047 1.1984610\nTau.id.var(time)                  1.630971 0.4023225\nTau.id.cov(timepost,(Intercept)) -6.109078 1.8486864\nTau.id.cov(timepost,time)        -1.750363 0.5746242\nTau.id.var(timepost)              2.749384 0.9273656"
  },
  {
    "objectID": "longitudinal/longitudinal.html#resources",
    "href": "longitudinal/longitudinal.html#resources",
    "title": "13  Longitudinal Data Analysis",
    "section": "17.1 Resources",
    "text": "17.1 Resources\n\nPractical Econometrics Book"
  },
  {
    "objectID": "longitudinal/longitudinal.html#examples",
    "href": "longitudinal/longitudinal.html#examples",
    "title": "13  Longitudinal Data Analysis",
    "section": "18.1 Examples",
    "text": "18.1 Examples\n\n18.1.1 Example: Spruce Trees\n\ny: response (log of product of tree size and diameter squared)\ntx: treatment ( 2 levels )\n\n0: control condition (25 trees per time point)\n1: ozone exposure at 70 ppb (54 trees per time point)\n\nday: time (days since 1988 Jan 1)\nchamber: block (ozone controlled chamber)\n\n\n\nCode\n# str(spruce88)\n# xtabs(~day + tx, data=spruce88)\n# xtabs(~tx,data = spruce88)\n\n# Example of selecting out some groups of elements for longitudinal analysis. Used above for highlighting\nspruce88 %>% filter(id %in% sample(levels(spruce88[,\"id\"])))\n\n\n[1] y       day     tx      chamber id     \n<0 rows> (or 0-length row.names)\n\n\nCode\nsid <-spruce88 %>% group_by(chamber) %>% sample_n(5)\nspruce_highlight <- spruce88 %>% filter(id %in% sid[[\"id\"]])\n\n# Emphasizing lines in the plot with background lighter\ng_base <- ggplot(data = spruce88, mapping = aes(x=day, y=y, color = factor(tx), group=factor(id))) +\n  facet_wrap(~chamber)\n(g_base +\n   geom_point(alpha=.2)+ geom_line(alpha=.2) + geom_line(data=spruce_highlight, aes(x=day, y=y, group=factor(id))))\n\n\n\n\n\nCode\n# (g_base + geom_point(alpha=.2) + geom_smooth(span=.5, aes(group=\"abc\")))\n# Note: using a constant for the group, will override the previous grouping.\n# Also, the smoothing will come up with \"singularities\" since there are only 5 distinct x data points, and lowess will have trouble.\n\n\n\n\n18.1.2 Exploring correlation from week to week, and getting correlation structure.\nSuggestion if the data is taken at many different points, just round it to the nearest year and we can still get some sense of the correlation over time. In the case everything is discrete, we can just use the data categories as is.\nWe should remove the effect of the means from each week. The suggestion in the book is to remove the covariate effect by fitting a regression on the data.\n\n\nCode\n# remove the effect of means from each week\n# residuals(lm(y~day + tx + chamber, data=spruce88))\n\nstr(spruce88)\n\n\n'data.frame':   395 obs. of  5 variables:\n $ y      : num  4.51 4.98 5.41 5.9 6.14 ...\n $ day    : num  -49 -27 0 26 57 -49 -27 0 26 57 ...\n $ tx     : num  1 1 1 1 1 1 1 1 1 1 ...\n $ chamber: num  1 1 1 1 1 1 1 1 1 1 ...\n $ id     : int  1 1 1 1 1 2 2 2 2 2 ...\n\n\nCode\nspruce_resid <- spruce88 %>% mutate(resid = residuals(lm(y~day + tx + chamber)))\n\n\n\n\n18.1.3 Example: TLC\nFor exploratory analysis, we should show the unstructured estimated covariance matrix from the model, shown for an individual i,\n\n\\begin{aligned}\nY_i = X_i\\beta + \\varepsilon_i\n\\end{aligned}\n\nassuming that \\varepsilon_i \\sim N(0,\\Sigma). We show the estimate \\hat \\Sigma as unstructured covariance matrix:\nThis is table 5.3 in Fitzmaurice, Laird, and Ware (2011).\n\n\nCode\n# direct group means covariance\ntlc_gls_un_cov <- gls(lead ~ trt*week,\n                      correlation=corSymm(form = ~ time | id), \n                      weights = varIdent(form = ~ 1 | week), \n                      data=tlc)\n\ngetVarCov(tlc_gls_un_cov)\n\n\nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 25.226 19.107 19.700 22.202\n[2,] 19.107 44.346 35.535 29.675\n[3,] 19.700 35.535 47.377 30.620\n[4,] 22.202 29.675 30.620 58.651\n  Standard Deviations: 5.0225 6.6593 6.8831 7.6584 \n\n\nWe can also look at the correlation with a scatter plot.\n\n\nCode\ntlc[\"week\"] <- as.factor(tlc$week)\n\n# Split into the exposed and placebo groups and make plots from the placebo group\ntlc_exposed <- tlc %>% filter(trt == \"A\") %>% spread(week, lead)\ntlc_placebo <- tlc %>% filter(trt == \"P\") %>% spread(week, lead)\npairs(tlc_placebo[3:6])\n\n\n\n\n\nCode\ncor(tlc_placebo[3:6])\n\n\n          week_int      time w0 w1\nweek_int 1.0000000 0.9844952 NA NA\ntime     0.9844952 1.0000000 NA NA\nw0              NA        NA  1 NA\nw1              NA        NA NA  1\n\n\n\n\n18.1.4 Example: Body Fat and Menarche\nThe variance covariance matrix can be extracted in two ways:\n\n\nCode\n# using reStruct from model is scale version of covariance matrix\nlist(as.matrix(fat_lme$modelStruct$reStruct[[1]]) * fat_lme$sigma^2,\n     getVarCov(fat_lme))\n\n\n[[1]]\n            (Intercept)      time  timepost\n(Intercept)   45.939153  2.526047 -6.109078\ntime           2.526047  1.630971 -1.750363\ntimepost      -6.109078 -1.750363  2.749384\n\n[[2]]\nRandom effects variance covariance matrix\n            (Intercept)    time timepost\n(Intercept)     45.9390  2.5260  -6.1091\ntime             2.5260  1.6310  -1.7504\ntimepost        -6.1091 -1.7504   2.7494\n  Standard Deviations: 6.7778 1.2771 1.6581 \n\n\nWe can also model the within correlation differently with correlation\ncorCAR1 - models the correlation by phi = .2 (default), has autocorrelation function h(\\cdot), with parameters s distance, and \\phi correlation\n\n\\begin{aligned}\nh(s, \\phi) = \\phi^s \\quad s \\geq 0, \\phi \\geq 0\n\\end{aligned}\n\nWe can manually create the correlation matrix using this function and pairwise distances, or use the corStruct class in nlme.\n\n\nCode\nlist(manual = .2^dist(fat_post$time[fat_post$id == 1]),\n     lme = corMatrix(Initialize(corCAR1(form = ~time | id), data = fat_post))[[1]]) # phi = .2 default\n\n\n$manual\n             1            2            3            4            5\n2 0.1968068917                                                    \n3 0.0454964703 0.2311731563                                       \n4 0.0098617995 0.0501090149 0.2167596607                          \n5 0.0018198587 0.0092469255 0.0399999969 0.1845361667             \n6 0.0003639718 0.0018493853 0.0080000000 0.0369072362 0.2000000156\n\n$lme\n             [,1]        [,2]       [,3]        [,4]        [,5]         [,6]\n[1,] 1.0000000000 0.196806892 0.04549647 0.009861799 0.001819859 0.0003639718\n[2,] 0.1968068917 1.000000000 0.23117316 0.050109015 0.009246926 0.0018493853\n[3,] 0.0454964703 0.231173156 1.00000000 0.216759661 0.039999997 0.0080000000\n[4,] 0.0098617995 0.050109015 0.21675966 1.000000000 0.184536167 0.0369072362\n[5,] 0.0018198587 0.009246926 0.04000000 0.184536167 1.000000000 0.2000000156\n[6,] 0.0003639718 0.001849385 0.00800000 0.036907236 0.200000016 1.0000000000\n\n\n\n\nCode\nfat_car1 <- lme(pbf~time + timepost,\n    random = ~ 1 | id,\n    corr=corCAR1(,form= ~ time | id),\n    data = fat_post)\n\nfat_car1 %>% getVarCov(type = \"marginal\")\n\n\nid 1 \nMarginal variance covariance matrix\n       1      2      3      4      5      6\n1 48.672 39.818 35.798 33.626 32.441 31.897\n2 39.818 48.672 40.439 35.991 33.563 32.448\n3 35.798 40.439 48.672 40.185 35.554 33.427\n4 33.626 35.991 40.185 48.672 39.581 35.408\n5 32.441 33.563 35.554 39.581 48.672 39.878\n6 31.897 32.448 33.427 35.408 39.878 48.672\n  Standard Deviations: 6.9765 6.9765 6.9765 6.9765 6.9765 6.9765 \n\n\nCode\nfat_lme %>% getVarCov(type = \"marginal\")\n\n\nid 1 \nMarginal variance covariance matrix\n       1      2      3      4      5      6\n1 60.288 46.991 43.546 39.949 36.007 32.886\n2 46.991 54.304 42.885 40.853 38.553 35.311\n3 43.546 42.885 51.763 41.668 40.846 37.496\n4 39.949 40.853 41.668 51.991 43.240 39.776\n5 36.007 38.553 40.846 43.240 55.056 42.044\n6 32.886 35.311 37.496 39.776 42.044 48.858\n  Standard Deviations: 7.7645 7.3691 7.1946 7.2105 7.42 6.9898 \n\n\n\n\nCode\ngetVarCov(fat_car1, type = \"marginal\")\n\n\nid 1 \nMarginal variance covariance matrix\n       1      2      3      4      5      6\n1 48.672 39.818 35.798 33.626 32.441 31.897\n2 39.818 48.672 40.439 35.991 33.563 32.448\n3 35.798 40.439 48.672 40.185 35.554 33.427\n4 33.626 35.991 40.185 48.672 39.581 35.408\n5 32.441 33.563 35.554 39.581 48.672 39.878\n6 31.897 32.448 33.427 35.408 39.878 48.672\n  Standard Deviations: 6.9765 6.9765 6.9765 6.9765 6.9765 6.9765 \n\n\nCode\nVarCorr(fat_car1)\n\n\nid = pdLogChol(1) \n            Variance StdDev  \n(Intercept) 31.37028 5.600917\nResidual    17.30157 4.159516\n\n\n\n\n18.1.5 Example: Dental\nThis section mostly uses gls with a variety of different covariance matrices.\nThis dataset shows measuruments of pituitary gland to pteryomaxillary fissure. It has 11 girls and 16 boys at ages 8, 10, 12, 14.\n\nid : patient id\ngender: male/female\ndistance: the response variable\n\n\n\nCode\ndental_wide <- read.table(\"data/dental.txt\",\n                          header = FALSE,\n                          col.names = c(\"id\", \"gender\", \"y1\", \"y2\", \"y3\", \"y4\"))\n\n# long data format\ndental <- dental_wide %>% \n  pivot_longer(cols = y1:y4, \n               names_to = \"age\",\n               values_to = \"distance\") %>% \n  dplyr::mutate(\n    id = factor(id),\n    age = recode(age,\n                 y1 = 8,\n                 y2 = 10,\n                 y3 = 12,\n                 y4 = 14),\n    agef = factor(age),\n    .after = \"age\")\n\ndental_mean <- dental %>%\n  group_by(age, gender) %>% \n  dplyr::summarize(avg_distance = mean(distance),\n                   .groups = \"drop_last\")\n\ndental %>% ggplot(aes(age, distance, group = id)) +\n  geom_point(alpha = .3) +\n  geom_line(alpha = .3) +\n  geom_line(data = dental_mean, mapping = aes(age, avg_distance, group = NULL), color = \"red\") + \n  facet_wrap(~gender)\n\n\n\n\n\n\n\nCode\n# all fixed with id\ndental_fixed <- lm(distance ~ age*gender + id, data = dental)\ndental_lm <- lm(distance ~ age*gender, data = dental)\n\n\n\n\nCode\n# modeling covariance\ndental_ident <- gls(distance~age*gender, data = dental) # same as lm\n\n## heterogenous \ndental_het <- gls(distance~age*gender,\n    data = dental,\n    weights = varIdent(form = ~ 1 | age))\n\n## unstructured\ndental_un <- gls(distance ~ age*gender, \n    data = dental,\n    correlation = corSymm(form = ~1 | id))\n\n## heterogenous unstructured\ndental_hun <- gls(distance ~ age*gender, \n    data = dental,\n    correlation = corSymm(form = ~1 | id),\n    weights = varIdent(form = ~1 | age))\n\n## compound symmetry\ndental_cs <- gls(distance ~ age*gender, \n    data = dental,\n    correlation = corCompSymm(form = ~1 | id))\n\n## Heterogenous CS\ndental_csh <- gls(distance ~ age*gender,\n    data = dental,\n    correlation = corCompSymm(form = ~1 | id),\n    weights = varIdent(form = ~1 | age))\n\n## Autoregressive\ndental_ar <- gls(distance ~ age*gender,\n    data = dental,\n    correlation = corAR1(form = ~1 | id),\n    weights = varIdent(form = ~1 | age))\n\n## Autoregressive Heterogeneous\ndental_arh <- gls(distance ~ age*gender,\n    data = dental,\n    correlation = corAR1(form = ~1 | id),\n    weights = varIdent(form = ~1 | age))\n\n\n\n\nCode\ngetVarCov(dental_un)\ngetVarCov(dental_hun)\ngetVarCov(dental_cs)\ngetVarCov(dental_csh)\ngetVarCov(dental_ar)\ngetVarCov(dental_arh)\n\n\nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 5.2300 3.0094 3.3345 2.6923\n[2,] 3.0094 5.2300 3.0035 3.9166\n[3,] 3.3345 3.0035 5.2300 3.7687\n[4,] 2.6923 3.9166 3.7687 5.2300\n  Standard Deviations: 2.2869 2.2869 2.2869 2.2869 \nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 5.4252 2.7092 3.8411 2.7152\n[2,] 2.7092 4.1906 2.9745 3.3137\n[3,] 3.8411 2.9745 6.2632 4.1333\n[4,] 2.7152 3.3137 4.1333 4.9862\n  Standard Deviations: 2.3292 2.0471 2.5026 2.233 \nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 5.2207 3.2986 3.2986 3.2986\n[2,] 3.2986 5.2207 3.2986 3.2986\n[3,] 3.2986 3.2986 5.2207 3.2986\n[4,] 3.2986 3.2986 3.2986 5.2207\n  Standard Deviations: 2.2849 2.2849 2.2849 2.2849 \nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 5.6967 3.1209 3.7419 3.3309\n[2,] 3.1209 4.2365 3.2269 2.8724\n[3,] 3.7419 3.2269 6.0901 3.4440\n[4,] 3.3309 2.8724 3.4440 4.8256\n  Standard Deviations: 2.3868 2.0583 2.4678 2.1967 \nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 5.8149 3.2683 2.3721 1.3044\n[2,] 3.2683 4.5807 3.3246 1.8281\n[3,] 2.3721 3.3246 6.0172 3.3087\n[4,] 1.3044 1.8281 3.3087 4.5369\n  Standard Deviations: 2.4114 2.1403 2.453 2.13 \nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]\n[1,] 5.8149 3.2683 2.3721 1.3044\n[2,] 3.2683 4.5807 3.3246 1.8281\n[3,] 2.3721 3.3246 6.0172 3.3087\n[4,] 1.3044 1.8281 3.3087 4.5369\n  Standard Deviations: 2.4114 2.1403 2.453 2.13 \n\n\n\n\nCode\n# Modelings including random effects\ndental_lme <- lme(distance ~ age*gender, \n    random = ~1 | id,\n    data = dental)\ndental_lmer <- lmer(distance~age*gender + (1 | id), data = dental)\n\n# same model in lmer and lme, G unstructured\ndental_lmer_age <- lmer(distance ~ age*gender + (age | id), data = dental)\ndental_lme_age <- lme(distance ~ age*gender, \n    random = ~ age | id,\n    data = dental)\n\n# uncorrelated age and id in random effect, G diag\ndental_lme_age_diag <- lme(distance ~ age*gender,\n                       data = dental,\n                       random = list(id = pdDiag(form = ~age))) # heterogenous, but uncorrelated age and id (age || id) in lmer\ndental_lmer_age_diag <- lmer(distance~age*gender + (age || id), data = dental)\n\n\n\n\nCode\ngetVarCov(dental_lme)\ngetVarCov(dental_lme_age)\ngetVarCov(dental_lme_age_diag)\n\ngetVarCov(dental_lme, type = \"marginal\") # same as CS variance\ngetVarCov(dental_lme_age, type = \"marginal\")\ngetVarCov(dental_lme_age_diag, type = \"marginal\")\n\n\nRandom effects variance covariance matrix\n            (Intercept)\n(Intercept)      3.2986\n  Standard Deviations: 1.8162 \nRandom effects variance covariance matrix\n            (Intercept)       age\n(Intercept)     5.78640 -0.289630\nage            -0.28963  0.032524\n  Standard Deviations: 2.4055 0.18035 \nRandom effects variance covariance matrix\n            (Intercept)       age\n(Intercept)      2.4168 0.0000000\nage              0.0000 0.0077469\n  Standard Deviations: 1.5546 0.088017 \nid 1 \nMarginal variance covariance matrix\n       1      2      3      4\n1 5.2207 3.2986 3.2986 3.2986\n2 3.2986 5.2207 3.2986 3.2986\n3 3.2986 3.2986 5.2207 3.2986\n4 3.2986 3.2986 3.2986 5.2207\n  Standard Deviations: 2.2849 2.2849 2.2849 2.2849 \nid 1 \nMarginal variance covariance matrix\n       1      2      3      4\n1 4.9502 3.1751 3.1162 3.0574\n2 3.1751 4.9625 3.3176 3.3888\n3 3.1162 3.3176 5.2351 3.7202\n4 3.0574 3.3888 3.7202 5.7679\n  Standard Deviations: 2.2249 2.2277 2.288 2.4016 \nid 1 \nMarginal variance covariance matrix\n       1      2      3      4\n1 4.7772 3.0366 3.1605 3.2845\n2 3.0366 5.0561 3.3464 3.5014\n3 3.1605 3.3464 5.3970 3.7183\n4 3.2845 3.5014 3.7183 5.7998\n  Standard Deviations: 2.1857 2.2486 2.3231 2.4083 \n\n\nThe covariance stuff is harder to calculate from lmer, I only know how to get from manual components, there may be something better that I don’t know about.\n\n\nCode\n# G\nVarCorr(dental_lmer_age)[[1]]\n\n\n            (Intercept)        age\n(Intercept)   5.7744874 -0.2886962\nage          -0.2886962  0.0324516\nattr(,\"stddev\")\n(Intercept)         age \n  2.4030163   0.1801433 \nattr(,\"correlation\")\n            (Intercept)        age\n(Intercept)   1.0000000 -0.6669087\nage          -0.6669087  1.0000000\n\n\nCode\n# Manual calculation of G = Sigma^2 L'L\nL <- getME(dental_lmer_age, \"Lambda\") # Marginal covariance\nsig <- getME(dental_lmer_age, \"sigma\")\n(crossprod(L) * sig^2)[1:2, 1:2]\n\n\n2 x 2 sparse Matrix of class \"dsCMatrix\"\n                           \n[1,]  5.7889208 -0.01612650\n[2,] -0.0161265  0.01801819\n\n\nCode\n# ZGZ' + R\nZ <- getME(dental_lmer_age, \"Z\")\nG <- Matrix(diag(27)) %x% VarCorr(dental_lmer_age)[[1]] # hadamard\nR <- Matrix(diag(rep(sigma(dental_lmer_age)^2, 108))) # \nvarY <- Z %*% G %*% t(Z) + R # Var(Y)\nvarY[1:4, 1:4]\n\n\n4 x 4 sparse Matrix of class \"dgCMatrix\"\n         1        2        3        4\n1 4.948875 3.174083 3.115916 3.057749\n2 3.174083 4.962347 3.317362 3.389001\n3 3.115916 3.317362 5.235433 3.720254\n4 3.057749 3.389001 3.720254 5.768131\n\n\n\n\nCode\n# both criteria choose the cs matrix, the covariance is quite similar\nAIC(dental_lm,\n  dental_het,\n    dental_un,\n    dental_hun,\n    dental_cs, \n    dental_csh,\n    dental_ar,\n    dental_arh,\n    dental_lme) %>% \n  add_column(BIC = BIC(\n    dental_lm,\n    dental_het,\n    dental_un,\n    dental_hun,\n    dental_cs,\n    dental_csh,\n    dental_ar,\n    dental_arh,\n    dental_lme)$BIC)\n\n\nWarning in AIC.default(dental_lm, dental_het, dental_un, dental_hun,\ndental_cs, : models are not all fitted to the same number of observations\n\n\nWarning in BIC.default(dental_lm, dental_het, dental_un, dental_hun,\ndental_cs, : models are not all fitted to the same number of observations\n\n\n           df      AIC      BIC\ndental_lm   5 488.2418 501.6524\ndental_het  8 498.4114 519.5666\ndental_un  11 448.1706 477.2589\ndental_hun 14 452.5468 489.5683\ndental_cs   6 445.7572 461.6236\ndental_csh  9 449.9724 473.7719\ndental_ar   9 460.7962 484.5957\ndental_arh  9 460.7962 484.5957\ndental_lme  6 445.7572 461.6236\n\n\nI was also curious how this compares to just calculating the sample covariance. It seems not all that different from the unstructured covariance matrix. They are different, but it seems the residuals being calculated are different. I thought ML estimator should be pretty close to the unstructured estimate.\n\n\nCode\n# sample covariance with group centering\ndental %>% group_by(gender, age) %>% \n  mutate(cdist = distance - mean(distance, na.rm = TRUE)) %>%  # group center\n  pivot_wider(id_cols = c(age), names_from = id, values_from = cdist) %>% # matrix format for cov \n  ungroup() %>% select(-age) %>% \n  data.matrix() %>% \n  t() %>% cov()\n\n\n         [,1]     [,2]     [,3]     [,4]\n[1,] 5.207168 2.612325 3.759834 2.605988\n[2,] 2.612325 4.023820 2.814576 3.189576\n[3,] 3.759834 2.814576 6.207441 3.971864\n[4,] 2.605988 3.189576 3.971864 4.793979\n\n\n\n\nCode\n# predictions from each of the models\ndental_yhat <- dental %>% add_column(un = predict(dental_un),\n                                     hun = predict(dental_hun),\n                                     cs = predict(dental_cs),\n                                     csh = predict(dental_csh),\n                                     ar = predict(dental_ar),\n                                     arh = predict(dental_arh),\n                                     het = predict(dental_het),\n                                     lme = predict(dental_lme),\n                                     fixed = predict(dental_fixed),\n                                     lm = predict(dental_lm),\n                                     lme_age_diag = predict(dental_lme_age_diag),\n                                     lme_age = predict(dental_lme_age))\n\ndental_yhat %>% filter(id == 2) %>% \n  pivot_longer(cols = distance:lme_age, names_to = \"type\",\n               values_to = \"response\") %>% \n  ggplot(aes(age, response, color = type)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\nHere we examine what all these models actually predicted. For the marginal model, most of them are being predicted together, which makes sense. The “distance” are the raw values displayed for one individual. We can see that the fixed model (separate fixed effect for id) and total mean model (lm, also called pooled) runs with the other coefficients. As far as estimates go, there are 3 separate groups, and probably some are due to rounding error. I think theoretically, the gls beta estimates should all be the same?\nNow we should also be concered about the effects of vcov of the different models\nThe sandwich estimators here are also worth comparing. All estimators have the form X'\\hat\\Sigma X with different “meat” for \\hat \\Sigma.\n\nHC0: Original white estimator\n\n\\hat\\Sigma = X'\\{\\hat\\varepsilon_i^2\\}_dX\n\nHC1: adjust degrees of freedom of HC0\n\n\\hat\\Sigma = \\frac{N}{N-(k+1)}X'\\{\\hat\\varepsilon_i^2\\}_dX\n\nHC2: incorporate leverage\n\n\\hat\\Sigma = X'\\left\\{\\frac{\\hat\\varepsilon_i^2}{1- h_{ii}}\\right\\}_dX\n\nHC3: incorporate leverage with differnt weights\n\n\\hat\\Sigma = X'\\left\\{\\frac{\\hat\\varepsilon_i^2}{(1- h_{ii})^2}\\right\\}_dX\n\nHC4: incorporate leverage with differnt weights\n\n\\hat\\Sigma = X'\\left\\{\\frac{\\hat\\varepsilon_i^2}{(1- h_{ii})^{\\delta_{i}}}\\right\\}_dX\n\\delta_i = \\min \\{4, \\frac{Nh_{ii}}{k + 1}\\}\n\n\nFor Heteroscedastic errors, there are HAC and Feasible Generalized Least Squares:\n\n\n\n\n\nCode\n# autocorrelated\nNeweyWest(dental_lm) # Bartlett kernel weights\n\n\n            (Intercept)         age    genderM age:genderM\n(Intercept)   1.5805513 -0.11935718 -1.5695856  0.11933808\nage          -0.1193572  0.01170302  0.1289546 -0.01245172\ngenderM      -1.5695856  0.12895463  3.9660625 -0.33731829\nage:genderM   0.1193381 -0.01245172 -0.3373183  0.03234847\n\n\nCode\nNeweyWest(dental_lm, lag = 0) #??\n\n\n            (Intercept)         age    genderM age:genderM\n(Intercept)   2.3956703 -0.20743588 -2.4248178  0.20948506\nage          -0.2074359  0.02111911  0.2205834 -0.02204344\ngenderM      -2.4248178  0.22058338  5.1489658 -0.46060717\nage:genderM   0.2094851 -0.02204344 -0.4606072  0.04503617\n\n\nCode\nNeweyWest(dental_lm, lag = 1) # Bartlett kernel weights\n\n\n            (Intercept)         age    genderM age:genderM\n(Intercept)   2.1562577 -0.17466452 -2.1221197  0.17190552\nage          -0.1746645  0.01688017  0.1804904 -0.01725257\ngenderM      -2.1221197  0.18049043  4.8322769 -0.41689414\nage:genderM   0.1719055 -0.01725257 -0.4168941  0.03939289\n\n\nCode\nvcovHAC(dental_lm)\n\n\n            (Intercept)          age     genderM  age:genderM\n(Intercept)  0.98793825 -0.076828444 -0.85989312  0.061591713\nage         -0.07682844  0.008502217  0.07210153 -0.007497458\ngenderM     -0.85989312  0.072101528  2.07233695 -0.160873344\nage:genderM  0.06159171 -0.007497458 -0.16087334  0.015550636\n\n\nCode\nvcovHAC(dental_lm, weights = weightsAndrews)\n\n\n            (Intercept)          age     genderM  age:genderM\n(Intercept)  0.98793825 -0.076828444 -0.85989312  0.061591713\nage         -0.07682844  0.008502217  0.07210153 -0.007497458\ngenderM     -0.85989312  0.072101528  2.07233695 -0.160873344\nage:genderM  0.06159171 -0.007497458 -0.16087334  0.015550636\n\n\nCode\nvcovHAC(dental_lm, weights = weightsLumley)\n\n\n            (Intercept)         age    genderM age:genderM\n(Intercept)   2.3180590 -0.19769599 -2.6024990  0.21980484\nage          -0.1976960  0.01957897  0.2314731 -0.02220841\ngenderM      -2.6024990  0.23147308  5.2298357 -0.44622075\nage:genderM   0.2198048 -0.02220841 -0.4462207  0.04203649\n\n\n\n\nCode\n# vcovPL is most appropriate I think, bu\n\nvcovPL(dental_lm, cluster = ~age , kernel = \"Bartlett\")\n\n\n            (Intercept)         age     genderM age:genderM\n(Intercept)  0.36436517 -0.03429299 -0.37025009  0.03404624\nage         -0.03429299  0.00491551  0.03440182 -0.00480140\ngenderM     -0.37025009  0.03440182  1.50047722 -0.12229004\nage:genderM  0.03404624 -0.00480140 -0.12229004  0.01217204\n\n\nCode\nvcovPL(dental_lm, cluster = dental$agef) # on the order\n\n\n            (Intercept)         age     genderM age:genderM\n(Intercept)  0.36436517 -0.03429299 -0.37025009  0.03404624\nage         -0.03429299  0.00491551  0.03440182 -0.00480140\ngenderM     -0.37025009  0.03440182  1.50047722 -0.12229004\nage:genderM  0.03404624 -0.00480140 -0.12229004  0.01217204\n\n\nCode\nvcovCL(dental_lm, cluster = ~id)\n\n\n            (Intercept)          age     genderM  age:genderM\n(Intercept)  0.56190635 -0.031179559 -0.56190635  0.031179559\nage         -0.03117956  0.004258417  0.03117956 -0.004258417\ngenderM     -0.56190635  0.031179559  2.02816740 -0.145146882\nage:genderM  0.03117956 -0.004258417 -0.14514688  0.014592405\n\n\nCode\nvcov(dental_cs)\n\n\n            (Intercept)          age    genderM  age:genderM\n(Intercept)   1.4006890 -0.096102802 -1.4006890  0.096102802\nage          -0.0961028  0.008736618  0.0961028 -0.008736618\ngenderM      -1.4006890  0.096102802  2.3636627 -0.162173479\nage:genderM   0.0961028 -0.008736618 -0.1621735  0.014743044\n\n\nCode\nvcov(dental_lme_age) # is cluster supposed to be age?\n\n\n            (Intercept)        age    genderM age:genderM\n(Intercept)   1.5089562 -0.1121399 -1.5089562  0.11213994\nage          -0.1121399  0.0107577  0.1121399 -0.01075770\ngenderM      -1.5089562  0.1121399  2.5463635 -0.18923615\nage:genderM   0.1121399 -0.0107577 -0.1892361  0.01815361\n\n\n\n\nCode\n# standard errors are wildly different, why and how? Need to test reliability.\ntidy(dental_csh)\n\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   17.4      1.18      14.7   3.08e-27\n2 age            0.479    0.0929     5.15  1.22e- 6\n3 genderM       -1.27     1.53      -0.831 4.08e- 1\n4 age:genderM    0.316    0.121      2.62  1.02e- 2\n\n\nCode\ncoeftest(dental_lm, vcov = vcovCL, cluster = ~id)\n\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(>|t|)    \n(Intercept) 17.372727   0.749604 23.1759 < 2.2e-16 ***\nage          0.479545   0.065257  7.3486 4.712e-11 ***\ngenderM     -1.032102   1.424137 -0.7247   0.47025    \nage:genderM  0.304830   0.120799  2.5234   0.01313 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncoeftest(dental_lm, vcov = vcovPL, cluster = ~id) # almost 10x lower standard error?\n\n\n\nt test of coefficients:\n\n             Estimate Std. Error  t value  Pr(>|t|)    \n(Intercept) 17.372727   0.056518 307.3812 < 2.2e-16 ***\nage          0.479546   0.004701 102.0090 < 2.2e-16 ***\ngenderM     -1.032102   0.527300  -1.9573   0.05299 .  \nage:genderM  0.304829   0.043791   6.9610 3.128e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncoeftest(dental_lm, vcov = vcovHAC)\n\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(>|t|)    \n(Intercept) 17.372727   0.993951 17.4785 < 2.2e-16 ***\nage          0.479545   0.092207  5.2007 9.998e-07 ***\ngenderM     -1.032102   1.439561 -0.7170   0.47501    \nage:genderM  0.304830   0.124702  2.4445   0.01619 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIntroducing this many different ways for estimating “robust” standard errors is quite confusing."
  },
  {
    "objectID": "longitudinal/longitudinal.html#example-autism",
    "href": "longitudinal/longitudinal.html#example-autism",
    "title": "13  Longitudinal Data Analysis",
    "section": "19.1 Example: Autism",
    "text": "19.1 Example: Autism\n\n\nCode\nautism <- read.csv(\"data/autism.csv\") %>% \n  mutate(sicdegp = factor(sicdegp),\n         childid = factor(childid)) \n\nautism_copy <- autism\n\nautism %>% ggplot(aes(age, vsae, group = childid, color = sicdegp)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~sicdegp)\n\n\nWarning: Removed 2 rows containing missing values (geom_point).\n\n\nWarning: Removed 1 row(s) containing missing values (geom_path).\n\n\n\n\n\n\n\nCode\n# how many distinct in each\nautism %>% count(sicdegp)\n\n\n  sicdegp   n\n1       1 192\n2       2 255\n3       3 165\n\n\nCode\nautism %>% count(sicdegp, childid) %>%\n  pivot_wider(names_from = sicdegp, values_from = n)\n\n\n# A tibble: 158 × 4\n   childid   `1`   `2`   `3`\n   <fct>   <int> <int> <int>\n 1 2           5    NA    NA\n 2 6           4    NA    NA\n 3 8           3    NA    NA\n 4 10          4    NA    NA\n 5 13          3    NA    NA\n 6 22          3    NA    NA\n 7 31          4    NA    NA\n 8 32          3    NA    NA\n 9 38          5    NA    NA\n10 41          3    NA    NA\n# … with 148 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\n# 2 missing vsae values\nautism %>% lapply(rlang::as_function(~which(is.na(.x)))) # columnwise which missing\n\n\n$age\ninteger(0)\n\n$vsae\n[1] 507 553\n\n$sicdegp\ninteger(0)\n\n$childid\ninteger(0)\n\n\n\n\nCode\n# without child id\nautism_lm <- lm(vsae ~ age*sicdegp, data = autism) # autism\n\n# with child id\nautism_lm_id <- lm(vsae ~ age*sicdegp + childid, data = autism)\n\n# with id varying slopes\nautism_lm_id_slope <- lm(vsae ~ age*sicdegp + childid + childid:age + childid:sicdegp, data = autism)\n\n\n# predictions, plotted raw\nautism_predict <- autism %>% filter(complete.cases(.)) %>% \n  add_column(\n  yhat_lm = predict(autism_lm),\n  yhat_lm_id = predict(autism_lm_id),\n  yhat_lm_id_slope = predict(autism_lm_id_slope))\n\nautism_predict %>% pivot_longer(cols = c(vsae, starts_with(\"yhat\")),\n                                names_to = \"type\",\n                                values_to = \"y\") %>% \n  ggplot(aes(age, y, group = childid, color = type)) +\n  geom_line(alpha = .2) +\n  facet_grid(type~sicdegp)\n\n\n\n\n\n\n\nCode\n# fully fixed\nc(slope1 = coef(autism_lm)[2],\n  slope2 = sum(coef(autism_lm)[c(2,5)]),\n  slope3 = sum(coef(autism_lm)[c(2,6)]))\n\n\nslope1.age     slope2     slope3 \n  2.615931   4.233238   6.980310 \n\n\nCode\n# with id\nc(slope1 = coef(autism_lm_id)[\"age\"],\n  slope2 = sum(coef(autism_lm_id)[c(\"age\", \"age:sicdegp2\")]),\n  slope3 = sum(coef(autism_lm_id)[c(\"age\", \"age:sicdegp3\")]))\n\n\nslope1.age     slope2     slope3 \n  2.598931   4.072511   7.075103 \n\n\n\n\nCode\n# TODO: with id_slope\n# average coefficients for each fitted child? seems okay way to summarize this fixed effects model.\nrg_lm_id_slope <- ref_grid(autism_lm_id_slope, at = list(\"age\" = c(2, 3), \"sicdegp\")) # nesting\n\n\nNOTE: A nesting structure was detected in the fitted model:\n    childid %in% sicdegp\n\n\nCode\nemmp_lm_id_slope <- pairs(emmeans(rg_lm_id_slope, specs = c(\"age\", \"sicdegp\"), by = \"childid\")) %>% as.data.frame()\nemmp_lm_id_slope %>% group_by(sicdegp) %>% \n  summarize(mean_slope = -mean(estimate, na.rm = TRUE)) %>% pull(mean_slope)\n\n\n[1] 3.004215 3.631826 7.258098\n\n\n\n\nCode\n# random models\nautism_lme_id <- lme(vsae~age*sicdegp,\n                  random = ~ 1 | childid,\n                  data = autism[complete.cases(autism),])\n\nautism_lmer_id <- lmer(vsae~ age * sicdegp + (1| childid),\n                       data = autism)\n\nautism_lmer_id_slope <- lmer(vsae~ age * sicdegp + ( age | childid),\n                             data = autism)\n\n\nboundary (singular) fit: see help('isSingular')\n\n\nCode\nautism_lmer_id_slope_split <- lmer(vsae~ age * sicdegp + (1 | childid) + ( 0 + age | childid),\n                             data = autism)\n\n\nboundary (singular) fit: see help('isSingular')\n\n\nCode\n# uncorrelated intercept and age\nautism_lmer_id_slope_uncor <- lmer(vsae ~ age * sicdegp + (age || childid),\n                             data = autism)\n\n\nboundary (singular) fit: see help('isSingular')\n\n\nCode\n# unstructured covariance matrix\n# gls(vsae~age*sicdegp,\n#     random = ~1 | childid,\n#     correlation = corSymm(form = ~1|sicdegp),\n#     weights = varIdent(form = ~ age)) \n\n\nautism_lme_id_slope <- lme(vsae~age*sicdegp,\n                           random = ~ age | childid,\n                           data = autism[complete.cases(autism),],\n                           control = lmeControl(opt = \"optim\"))\n\nsummary(autism_lme_id_slope)\n\n\nLinear mixed-effects model fit by REML\n  Data: autism[complete.cases(autism), ] \n      AIC      BIC    logLik\n  4716.13 4760.166 -2348.065\n\nRandom effects:\n Formula: ~age | childid\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev   Corr  \n(Intercept) 8.602952 (Intr)\nage         4.019888 -0.998\nResidual    7.769026       \n\nFixed effects:  vsae ~ age * sicdegp \n                 Value Std.Error  DF   t-value p-value\n(Intercept)   1.843174 1.6899783 449  1.090650  0.2760\nage           2.971975 0.6268443 449  4.741170  0.0000\nsicdegp2     -0.324782 2.2318720 155 -0.145520  0.8845\nsicdegp3     -3.858107 2.4775418 155 -1.557232  0.1215\nage:sicdegp2  0.715160 0.8290366 449  0.862640  0.3888\nage:sicdegp3  4.334815 0.9181896 449  4.721046  0.0000\n Correlation: \n             (Intr) age    scdgp2 scdgp3 ag:sc2\nage          -0.891                            \nsicdegp2     -0.757  0.675                     \nsicdegp3     -0.682  0.608  0.517              \nage:sicdegp2  0.674 -0.756 -0.891 -0.460       \nage:sicdegp3  0.609 -0.683 -0.461 -0.888  0.516\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.87638128 -0.34680674 -0.02510791  0.31120771  5.28293064 \n\nNumber of Observations: 610\nNumber of Groups: 158 \n\n\nCode\n# lmer_id\nc(fixef(autism_lmer_id)[\"age\"],\n  sum(fixef(autism_lmer_id)[c(\"age\", \"age:sicdegp2\")]),\n  sum(fixef(autism_lmer_id)[c(\"age\", \"age:sicdegp3\")]))\n\n\n     age                   \n2.618120 4.147079 7.041573 \n\n\nCode\n# lme_id_slope\nc(fixef(autism_lme_id_slope)[\"age\"],\n  sum(fixef(autism_lme_id_slope)[c(\"age\", \"age:sicdegp2\")]),\n  sum(fixef(autism_lme_id_slope)[c(\"age\", \"age:sicdegp3\")]))\n\n\n     age                   \n2.971975 3.687136 7.306791 \n\n\n\n\nCode\n# random models"
  },
  {
    "objectID": "longitudinal/longitudinal.html#method-implementations",
    "href": "longitudinal/longitudinal.html#method-implementations",
    "title": "13  Longitudinal Data Analysis",
    "section": "20.1 Method Implementations",
    "text": "20.1 Method Implementations\n\n20.1.1 1. raw\n\n\n\n\n\n\n\nThis method is just the standard response profile analysis, which we’ve shown above.\n\n\nCode\ntlc_baseline_1 <- tlc_gls\n\n\n\n\n20.1.2 2. constrain baseline\n\n\n\n\n\n\n\nIn order to force baseline to be modeled the same, we need to remove the main effect for treament, and the interaction term involving the baseline. Unfortunately we can’t remove a single level of the interaction if we specify week*trt, nor does gls accept a manually made model matrix, so we must construct the variables through the formula interface.\n\n\nCode\n# keep in outcome, assume similar intercept \n# remove main effect, and week0 interactions.\n# gls doesn't take model.matrix, so manual specification through formula is the way to do it\n# update also acts kinda weird\n# table 5.7  in Fitzmaurice\n# https://content.sph.harvard.edu/fitzmaur/ala2e/\n\ntlc_baseline_2 <- gls(lead ~ week + \n                        I(week == \"w1\" & trt == \"A\") +\n                        I(week == \"w4\" & trt == \"A\") +\n                        I(week == \"w6\" & trt == \"A\"),\n                      data = tlc,\n                      correlation = corSymm(form = ~ time | id),\n                      weights = varIdent(form = ~ 1 | week))\n\n# type III fixed effect test for interaction hypothesis\nanova(tlc_baseline_2, Terms = 3:5) # Chisq = F * ndf = 111.94\n\n\nDenom. DF: 393 \n F-test for: I(week == \"w1\" & trt == \"A\"), I(week == \"w4\" & trt == \"A\"), I(week == \"w6\" & trt == \"A\") \n  numDF  F-value p-value\n1     3 37.31422  <.0001\n\n\n\n\n20.1.3 3. change from baseline\n\n\n\n\n\n\n\nThis analysis is just a reframed version of the first analysis, at least in terms of estimates.\n\n\nCode\n# modeling the difference\n# time - 1, because gls throws error. corSymm needs consecutive integers starting at 1.\ntlc_baseline_3 <- gls(lead_diff ~ trt*week,\n                      correlation = corSymm(form = ~ time-1 | id), # unstructured\n                      weights = varIdent(form = ~ 1 | week),  # heterogenous\n                      data = tlc_diff %>% mutate(trt = factor(trt, levels = c(\"P\", \"A\"))))\n\nanova(tlc_baseline_3, Terms = c(2, 4)) # trt x week interaction test is now test on combined c(trt, trt:week) of diff\n\n\nDenom. DF: 294 \n F-test for: trt, trt:week \n  numDF  F-value p-value\n1     3 35.92872  <.0001\n\n\n\n\n\n\n\nBaseline Change Coefficients\n \n  \n    term \n    change \n  \n \n\n  \n    (Intercept) \n    -1.612 \n  \n  \n    trtA \n    -11.406 \n  \n  \n    weekw4 \n    -0.590 \n  \n  \n    weekw6 \n    -1.014 \n  \n  \n    trtA:weekw4 \n    2.582 \n  \n  \n    trtA:weekw6 \n    8.254 \n  \n\n\n\n\n\n\n\nRaw Coefficients\n \n  \n    term \n    raw \n  \n \n\n  \n    (Intercept) \n    26.272 \n  \n  \n    trtA \n    0.268 \n  \n  \n    weekw1 \n    -1.612 \n  \n  \n    weekw4 \n    -2.202 \n  \n  \n    weekw6 \n    -2.626 \n  \n  \n    trtA:weekw1 \n    -11.406 \n  \n  \n    trtA:weekw4 \n    -8.824 \n  \n  \n    trtA:weekw6 \n    -3.152 \n  \n\n\n\n\n\n\nTo see that they are the same, notice that the baseline change model term estimates are simply linear combinations of the raw model.\n\n\nCode\ntribble(~`change model`, ~`raw model terms`, ~`value` , ~`interpretation`,\n        \"(Intercept)\", \"weekw1\", \"-1.612\",  \"in placebo, diff between w1 and baseline\",\n        \"trtA\", \"trtA:weekw1\", \"-11.406\",  \"difference in trt slopes from baseline to w1\",\n        \"weekw4\", \"weekw4 - weekw1\",\"-0.59\", \"in placebo, diff between w4 and w1\",\n        \"trtA:weekw4\", \"trtA:weekw4 - trtA:weekw1\",\"2.582\", \"diff in trt slope from w4 to w1\") %>% kbl() %>% kable_styling(full_width = FALSE)\n\n\n\n\n \n  \n    change model \n    raw model terms \n    value \n    interpretation \n  \n \n\n  \n    (Intercept) \n    weekw1 \n    -1.612 \n    in placebo, diff between w1 and baseline \n  \n  \n    trtA \n    trtA:weekw1 \n    -11.406 \n    difference in trt slopes from baseline to w1 \n  \n  \n    weekw4 \n    weekw4 - weekw1 \n    -0.59 \n    in placebo, diff between w4 and w1 \n  \n  \n    trtA:weekw4 \n    trtA:weekw4 - trtA:weekw1 \n    2.582 \n    diff in trt slope from w4 to w1 \n  \n\n\n\n\n\nGiven that they are the same (or linear combinations), what about the standard errors of the estimates? Let’s check for “weekw4”\n\n\nCode\ncbind(c(-1, 1) %*% tlc_gls$varBeta[3:4, 3:4] %*% c(-1, 1), # old model std err\n      tlc_baseline_3$varBeta[\"weekw4\", \"weekw4\"]) # baseline change model std err\n\n\n          [,1]     [,2]\n[1,] 0.4130701 0.413059\n\n\nThey are the same!\n\n\nCode\nlist(raw = tlc_gls$varBeta,\n     base3=tlc_baseline_3$varBeta)\n\n\n$raw\n            (Intercept)        trtA     weekw1     weekw4      weekw6\n(Intercept)  0.50451606 -0.50451606 -0.1223674 -0.1105218 -0.06048317\ntrtA        -0.50451606  1.00903212  0.1223674  0.1105218  0.06048317\nweekw1      -0.12236737  0.12236737  0.6271371  0.4390662  0.27183933\nweekw4      -0.11052175  0.11052175  0.4390662  0.6640654  0.27889731\nweekw6      -0.06048317  0.06048317  0.2718393  0.2788973  0.78947714\ntrtA:weekw1  0.12236737 -0.24473475 -0.6271371 -0.4390662 -0.27183933\ntrtA:weekw4  0.11052175 -0.22104350 -0.4390662 -0.6640654 -0.27889731\ntrtA:weekw6  0.06048317 -0.12096634 -0.2718393 -0.2788973 -0.78947714\n            trtA:weekw1 trtA:weekw4 trtA:weekw6\n(Intercept)   0.1223674   0.1105218  0.06048317\ntrtA         -0.2447347  -0.2210435 -0.12096634\nweekw1       -0.6271371  -0.4390662 -0.27183933\nweekw4       -0.4390662  -0.6640654 -0.27889731\nweekw6       -0.2718393  -0.2788973 -0.78947714\ntrtA:weekw1   1.2542741   0.8781324  0.54367866\ntrtA:weekw4   0.8781324   1.3281308  0.55779461\ntrtA:weekw6   0.5436787   0.5577946  1.57895428\n\n$base3\n            (Intercept)       trtA     weekw4     weekw6 trtA:weekw4\n(Intercept)   0.6271416 -0.6271416 -0.1880546 -0.3553027   0.1880546\ntrtA         -0.6271416  1.2542831  0.1880546  0.3553027  -0.3761093\nweekw4       -0.1880546  0.1880546  0.4130590  0.1951241  -0.4130590\nweekw6       -0.3553027  0.3553027  0.1951241  0.8729404  -0.1951241\ntrtA:weekw4   0.1880546 -0.3761093 -0.4130590 -0.1951241   0.8261179\ntrtA:weekw6   0.3553027 -0.7106053 -0.1951241 -0.8729404   0.3902482\n            trtA:weekw6\n(Intercept)   0.3553027\ntrtA         -0.7106053\nweekw4       -0.1951241\nweekw6       -0.8729404\ntrtA:weekw4   0.3902482\ntrtA:weekw6   1.7458807\n\n\nThe marginal covariances are also the same if we change the estimation method to ML.\nThe idea here was that we could calculate the change from baseline covariance estimate from the raw model estimates of covariance.\n\n\\begin{aligned}\n\\underbrace{\\widehat{\\mathrm{Cov}}(Y_{i2} -Y_{i1}, Y_{i2} -Y_{i1})}_{\\Delta \\text{ baseline model estimates}} = \\underbrace{\\widehat{\\mathrm{Var}}(Y_{{i2}}) - 2\\widehat{\\mathrm{Cov}}(Y_{i2}, Y_{i1}) + \\widehat{\\mathrm{Var}}(Y_{i1})}_{\\text{Raw model estimates}}\n\\end{aligned}\n\n\n\nCode\n# We note this is NOT true when we use the REML estimates\n# linear \nL <- matrix(c(-1, 1, 0, 0,\n              -1, 0, 1, 0,\n              -1, 0, 0, 1), byrow = TRUE, ncol = 4)\n\n# not the same!\nlist(`from_baseline_change_model` = getVarCov(tlc_baseline_3),\n     `from_raw_model` =  L %*% getVarCov(tlc_baseline_1) %*% t(L))\n\n\n$from_baseline_change_model\nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]\n[1,] 31.357 21.954 13.592\n[2,] 21.954 33.205 13.945\n[3,] 13.592 13.945 39.474\n  Standard Deviations: 5.5997 5.7623 6.2828 \n\n$from_raw_model\n         [,1]     [,2]     [,3]\n[1,] 31.35685 21.95331 13.59197\n[2,] 21.95331 33.20327 13.94487\n[3,] 13.59197 13.94487 39.47386\n\n\nWith REML estimates, they are NOT equal. But if we refit the models with ML,\n\n\nCode\n# what if we try ML?\ntlc_baseline_3_ml <- tlc_baseline_3 %>% update(method = \"ML\")\ntlc_baseline_1_ml <- tlc_gls %>% update(method = \"ML\")\n\n\n\n\nCode\nlist(`from_baseline_change_model` = getVarCov(tlc_baseline_3_ml),\n     `from_raw_model` =  L %*% getVarCov(tlc_baseline_1_ml) %*% t(L))\n\n\n$from_baseline_change_model\nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]\n[1,] 30.729 21.515 13.319\n[2,] 21.515 32.541 13.666\n[3,] 13.319 13.666 38.684\n  Standard Deviations: 5.5434 5.7044 6.2196 \n\n$from_raw_model\n         [,1]     [,2]     [,3]\n[1,] 30.72886 21.51453 13.31984\n[2,] 21.51453 32.54042 13.66637\n[3,] 13.31984 13.66637 38.68419\n\n\n\n\n20.1.4 4. ancova change\nThis method results in standard error estimates that are more efficient, but generally only recommended for randomized studies. In observational studies, there’s danger in introducing bias.\nWe can do ancova on the raw values, or change from baseline values. That is, we could consider either:\nBoth of these will give the same estimates (maybe obviously when written out this way), but \\gamma' = \\gamma + 1.\n\n\nCode\n# ancova\ntlc_baseline_4 <- gls(lead ~ trt*week + lead_base,\n                      correlation = corSymm(form = ~ time-1 | id), # unstructured\n                      weights = varIdent(form = ~ 1 | week),  # heterogeneous\n                      data = tlc_diff)\n\ntlc_baseline_4b <- gls(lead_diff ~ trt*week + lead_base,\n                       correlation = corSymm(form = ~ time-1 | id), # unstructured\n                       weights = varIdent(form = ~ 1 | week),  # heterogeneous\n                       data = tlc_diff)\n\ncbind(coef(tlc_baseline_4),\n               coef(tlc_baseline_4b)) %>% \n  `colnames<-`(c(\"Ancova\", \"Ancova Diff\")) %>% \n  as.data.frame() %>% \n  rownames_to_column(\"term\")\n\n\n         term      Ancova Ancova Diff\n1 (Intercept)   3.5236964   3.5236964\n2        trtA -11.3536109 -11.3536109\n3      weekw4  -0.5900000  -0.5900000\n4      weekw6  -1.0140000  -1.0140000\n5   lead_base   0.8045183  -0.1954817\n6 trtA:weekw4   2.5820000   2.5820000\n7 trtA:weekw6   8.2540000   8.2540000"
  },
  {
    "objectID": "longitudinal/longitudinal.html#comparison",
    "href": "longitudinal/longitudinal.html#comparison",
    "title": "13  Longitudinal Data Analysis",
    "section": "20.2 Comparison",
    "text": "20.2 Comparison\nIt’s hard to compare the estimates of the coefficients directly, so we compare them for the hypothesis test of the treatment by time interaction.\n\n\nCode\n# contrast matrix for tlc_baseline_3\n# trtA (interaction w1), trtA + trtA:w4, trtA + trtA:w6\nL <- matrix(c(0, 1, 0, 0, 0, 0,\n              0, 1, 0, 0, 1, 0,\n              0, 1, 0, 0, 0, 1), byrow = T, nrow = 3)\nrbind(\n  `raw` = anova(tlc_baseline_1, Terms = 4),\n  `constrain baseline` = anova(tlc_baseline_2, Terms = 3:5),\n  `baseline change` = anova(tlc_baseline_3, L = L), # same as tlc_baseline_1 as expected\n  `ancova` = anova(tlc_baseline_4, Terms = c(2,5))) %>%  # similarly efficient as baseline 2 model\n  as.data.frame() %>% \n  mutate(`Chisq` = numDF * `F-value`,\n         .after= `F-value`)\n\n\n                   numDF  F-value    Chisq      p-value\nraw                    3 35.92934 107.7880 1.557061e-20\nconstrain baseline     3 37.31422 111.9427 3.074120e-21\nbaseline change        3 35.92872 107.7862 8.227085e-20\nancova                 3 37.04297 111.1289 2.517144e-20\n\n\nWe see that 2 and 4 have larger F-statistics, implying that there is slightly more power to detect the group differences (assuming the assumptions are reasonable).\nFitzmaurice, Laird, and Ware (2011) concludes that 2 should be recommended, because we get similar efficiency gains, and there is an implicit assumption about the covariance matrix of the ancova model. In the ANCOVA model, we only have a 3x3 covariance matrix, where as in baseline change model, we estimated a 4x4 covariance matrix. Hence, we are implicitly assuming that \\mathrm{Cov}(Y_{i1}, Y_{i2}) = \\mathrm{Cov}(Y_{i1}, Y_{i3}) = \\mathrm{Cov}(Y_{i1}, Y_{i4})\n\n\nCode\ntlc_baseline_4 %>% getVarCov()\n\n\nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]\n[1,] 30.151 20.864 12.991\n[2,] 20.864 32.230 13.460\n[3,] 12.991 13.460 39.478\n  Standard Deviations: 5.491 5.6771 6.2831"
  },
  {
    "objectID": "longitudinal/longitudinal.html#references",
    "href": "longitudinal/longitudinal.html#references",
    "title": "13  Longitudinal Data Analysis",
    "section": "20.3 References",
    "text": "20.3 References\n\nEuropean Agency for evaluation of medicinal products from a quick google search"
  },
  {
    "objectID": "longitudinal/longitudinal.html#decorrelation",
    "href": "longitudinal/longitudinal.html#decorrelation",
    "title": "13  Longitudinal Data Analysis",
    "section": "21.1 Decorrelation",
    "text": "21.1 Decorrelation\n\n\nCode\n# can extract residuals with type = \"normalized\"\nfat_post %>% count(id)\n\n\n     id  n\n1     1  6\n2     2  9\n3     3  8\n4     4  6\n5     5  7\n6     6  8\n7     7  9\n8     8  3\n9     9  7\n10   10  7\n11   11  7\n12   12  6\n13   13  5\n14   14  8\n15   15  9\n16   16 10\n17   17  8\n18   18  9\n19   19  9\n20   20  8\n21   21  9\n22   22  6\n23   23  5\n24   24  8\n25   25  7\n26   26  7\n27   27  5\n28   28  7\n29   29  6\n30   30  8\n31   31  6\n32   32  5\n33   33  5\n34   34  5\n35   35  6\n36   36  7\n37   37  6\n38   38  8\n39   39  6\n40   40  6\n41   41  5\n42   42  9\n43   43  5\n44   44  3\n45   45  7\n46   46  7\n47   47  6\n48   48  5\n49   49  7\n50   50  8\n51   51  8\n52   52  7\n53   53  8\n54   54  7\n55   55  9\n56   56  8\n57   57  6\n58   58  5\n59   59  5\n60   60  5\n61   61  8\n62   62  8\n63   63  7\n64   64  6\n65   65  7\n66   66  8\n67   67  8\n68   68  7\n69   69  7\n70   70  3\n71   71  8\n72   72  6\n73   73  8\n74   74  8\n75   75  7\n76   76  6\n77   77  3\n78   78  8\n79   79  7\n80   80  8\n81   81  6\n82   82  6\n83   83  6\n84   84  6\n85   85  6\n86   86  8\n87   87  8\n88   88  5\n89   89  3\n90   90  3\n91   91  5\n92   92  8\n93   93  7\n94   94  6\n95   95  7\n96   96  6\n97   97  5\n98   98  8\n99   99  8\n100 100  8\n101 101  8\n102 102  8\n103 103  6\n104 104  5\n105 105  5\n106 106  5\n107 107  6\n108 108  4\n109 109  7\n110 110  7\n111 111  8\n112 112  7\n113 113  7\n114 114  7\n115 115  7\n116 116  7\n117 117  6\n118 118  5\n119 119  5\n120 120  7\n121 121  7\n122 122  7\n123 123  7\n124 124  5\n125 125  6\n126 126  4\n127 127  5\n128 128  7\n129 129  7\n130 130  6\n131 131  6\n132 132  7\n133 133  7\n134 134  4\n135 135  7\n136 136  6\n137 137  7\n138 138  7\n139 139  7\n140 140  7\n141 141  7\n142 142  7\n143 143  5\n144 144  6\n145 145  7\n146 146  7\n147 147  5\n148 148  5\n149 149  5\n150 150  6\n151 151  6\n152 152  6\n153 153  6\n154 154  6\n155 155  6\n156 156  6\n157 157  6\n158 158  6\n159 159  5\n160 160  3\n161 161  5\n162 162  5\n\n\nCode\nS <- getVarCov(fat_lme, type = \"marginal\", individuals = 1)\nS\n\n\nid 1 \nMarginal variance covariance matrix\n       1      2      3      4      5      6\n1 60.288 46.991 43.546 39.949 36.007 32.886\n2 46.991 54.304 42.885 40.853 38.553 35.311\n3 43.546 42.885 51.763 41.668 40.846 37.496\n4 39.949 40.853 41.668 51.991 43.240 39.776\n5 36.007 38.553 40.846 43.240 55.056 42.044\n6 32.886 35.311 37.496 39.776 42.044 48.858\n  Standard Deviations: 7.7645 7.3691 7.1946 7.2105 7.42 6.9898 \n\n\nCode\nR <- S[[1]] %>% chol()\nR\n\n\n         1        2        3        4        5        6\n1 7.764524 6.051983 5.608287 5.145088 4.637412 4.235458\n2 0.000000 4.204493 2.127138 2.310672 2.494246 2.301845\n3 0.000000 0.000000 3.973052 1.987756 2.399240 2.226406\n4 0.000000 0.000000 0.000000 4.028538 2.196216 2.045436\n5 0.000000 0.000000 0.000000 0.000000 4.092661 1.668148\n6 0.000000 0.000000 0.000000 0.000000 0.000000 3.700942\n\n\nCode\ncrossprod(R) # R'R\n\n\n         1        2        3        4        5        6\n1 60.28784 46.99077 43.54568 39.94916 36.00730 32.88632\n2 46.99077 54.30426 42.88479 40.85319 38.55258 35.31101\n3 43.54568 42.88479 51.76274 41.66771 40.84585 37.49563\n4 39.94916 40.85319 41.66771 51.99143 43.23992 39.77628\n5 36.00730 38.55258 40.84585 43.23992 55.05645 42.04400\n6 32.88632 35.31101 37.49563 39.77628 42.04400 48.85798\n\n\n\n\nCode\neps_hat <- residuals(fat_lme, type = \"pearson\")\neps_hat[fat_post$id == 1]\n\n\n         1          1          1          1          1          1 \n-1.4696365  0.6821217 -0.3313941  2.4943915 -2.0320743  0.2480700 \n\n\nCode\nRinv <- solve(R)\neps_hat[fat_post$id == 1] %*% Rinv\n\n\n              1         2           3         4          5          6\n[1,] -0.1892758 0.4346816 -0.04895698 0.6357493 -0.8594188 0.07874242\n\n\n\n\nCode\nresiduals(fat_lme, type = \"normalized\")[fat_post$id == 1]\n\n\n         1          1          1          1          1          1 \n-1.4696365  0.6821217 -0.3313941  2.4943915 -2.0320743  0.2480700 \n\n\nCode\nfat_lme$modelStruct\n\n\nreStruct  parameters:\n       id1        id2        id3        id4        id5        id6 \n 0.7894148 -0.9241612 -1.3828910  0.1210868 -0.2928406 -0.3762166 \n\n\n\n\nCode\n?nlme:::recalc\nnlme:::residuals.lme\n\n\nfunction (object, level = Q, type = c(\"response\", \"pearson\", \n    \"normalized\"), asList = FALSE, ...) \n{\n    type <- match.arg(type)\n    Q <- object$dims$Q\n    val <- object[[\"residuals\"]]\n    if (is.character(level)) {\n        nlevel <- match(level, names(val))\n        if (any(aux <- is.na(nlevel))) {\n            stop(sprintf(ngettext(sum(aux), \"nonexistent level %s\", \n                \"nonexistent levels %s\"), level[aux]), domain = NA)\n        }\n        level <- nlevel\n    }\n    else {\n        level <- 1 + level\n    }\n    if (type != \"response\") {\n        val <- val[, level]/attr(val, \"std\")\n    }\n    else {\n        val <- val[, level]\n    }\n    if (type == \"normalized\") {\n        if (!is.null(cSt <- object$modelStruct$corStruct)) {\n            val <- recalc(cSt, list(Xy = as.matrix(val)))$Xy[, \n                seq_along(level)]\n        }\n        else {\n            type <- \"pearson\"\n        }\n    }\n    if (length(level) == 1) {\n        grps <- as.character(object[[\"groups\"]][, max(c(1, level - \n            1))])\n        if (asList) {\n            val <- as.list(split(val, ordered(grps, levels = unique(grps))))\n        }\n        else {\n            grp.nm <- row.names(object[[\"groups\"]])\n            val <- naresid(object$na.action, val)\n            names(val) <- grps[match(names(val), grp.nm)]\n        }\n        attr(val, \"label\") <- switch(type, response = {\n            if (!is.null(aux <- attr(object, \"units\")$y)) paste(\"Residuals\", \n                aux) else \"Residuals\"\n        }, pearson = \"Standardized residuals\", normalized = \"Normalized residuals\")\n        val\n    }\n    else naresid(object$na.action, val)\n}\n<bytecode: 0x7f7ee5f19a40>\n<environment: namespace:nlme>"
  },
  {
    "objectID": "longitudinal/longitudinal.html#aggregation",
    "href": "longitudinal/longitudinal.html#aggregation",
    "title": "13  Longitudinal Data Analysis",
    "section": "21.2 Aggregation",
    "text": "21.2 Aggregation\nThe next method deals with aggregation"
  },
  {
    "objectID": "longitudinal/longitudinal.html#plm",
    "href": "longitudinal/longitudinal.html#plm",
    "title": "13  Longitudinal Data Analysis",
    "section": "24.1 plm",
    "text": "24.1 plm\n\nthe fixed effects model (“within”),\nthe pooling model (“pooling”),\nthe first-difference model (“fd”),\nthe between model (“between”),\nthe error components model (“random”).\n\n\n24.1.1 EmplUK\n\nfirm - firm index\nyear - year\nsector - the sector of activity\nemp - employment\nwage - wages\ncapital - capital\noutput - output\n\n\n\nCode\ndata(\"EmplUK\")\n\n\n\n\nCode\n# pdata.frame creats a dataframe for plm to work with\nE <- pdata.frame(EmplUK,                 # The orig dataframe\n            index=c(\"firm\",\"year\"),      # The \"individual\" and \"time\" variable names\n            drop.index = TRUE,           # Don't show index columns\n            row.names = TRUE)            # Show rownames that combine individual-time\n\nclass(E) # pdata.frame\n\n\n[1] \"pdata.frame\" \"data.frame\" \n\n\nCode\nsummary(E$emp) # Selecting the column from a pdata.frame give a \"pseries\" object\n\n\ntotal sum of squares: 261539.4 \n         id        time \n0.980765381 0.009108488 \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.104   1.181   2.287   7.892   7.020 108.562 \n\n\nCode\nmethods(class=\"pseries\") # Looking at the methods with a pseries object\n\n\n [1] as.matrix         between           Between           Complex          \n [5] D                 diff              F                 G                \n [9] index             is.pbalanced      is.pconsecutive   L                \n[13] lag               lead              make.pbalanced    make.pconsecutive\n[17] Math              Ops               pcdtest           pdim             \n[21] plot              print             pvar              Sum              \n[25] summary           Within           \nsee '?methods' for accessing help and source code\n\n\nCode\n# plm:::print.summary.pseries\n# plm:::summary.pseries # The function for summarizing the series\n\n\npseries datatype comes with various functions to operate on them to compute correctly time lag within each individual\n\n\nCode\nbetween(E$emp) # means across the time periods\nSum(E$emp) # sum across the time periods\nWithin(E$emp) # deviation from mean\nhead(plm:::lag.pseries(E$emp, 0:2)) # Lag the sequence within subject by however many\n\n\n\n\n24.1.2 Grunfield example\n\n\nCode\ndata(Grunfeld)\nGrunfeld$firm <- factor(Grunfeld$firm)\nGrunfeld$year <- factor(Grunfeld$year)\n\nskim(Grunfeld)\n\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nGrunfeld\n\n\n\n\nNumber of rows\n\n\n200\n\n\n\n\nNumber of columns\n\n\n5\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nfactor\n\n\n2\n\n\n\n\nnumeric\n\n\n3\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: factor\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nordered\n\n\nn_unique\n\n\ntop_counts\n\n\n\n\n\n\nfirm\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n10\n\n\n1: 20, 2: 20, 3: 20, 4: 20\n\n\n\n\nyear\n\n\n0\n\n\n1\n\n\nFALSE\n\n\n20\n\n\n193: 10, 193: 10, 193: 10, 193: 10\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\ninv\n\n\n0\n\n\n1\n\n\n145.96\n\n\n216.88\n\n\n0.93\n\n\n33.56\n\n\n57.48\n\n\n138.04\n\n\n1486.7\n\n\n▇▁▁▁▁\n\n\n\n\nvalue\n\n\n0\n\n\n1\n\n\n1081.68\n\n\n1314.47\n\n\n58.12\n\n\n199.98\n\n\n517.95\n\n\n1679.85\n\n\n6241.7\n\n\n▇▂▁▁▁\n\n\n\n\ncapital\n\n\n0\n\n\n1\n\n\n276.02\n\n\n301.10\n\n\n0.80\n\n\n79.17\n\n\n205.60\n\n\n358.10\n\n\n2226.3\n\n\n▇▁▁▁▁\n\n\n\n\n\n\n\n\n\nCode\nGrunfeld %>% ggplot(aes(year, inv, color = factor(firm))) + \n  geom_line()\n\n\ngeom_path: Each group consists of only one observation. Do you need to adjust\nthe group aesthetic?\n\n\n\n\n\n\n\nCode\ngrun.po <- plm(inv~value + capital, data = Grunfeld, model = \"pooling\") # pooled\ngrun.fe <- plm(inv~value + capital, data = Grunfeld, model = \"within\") # fixed effects\ngrun.re <- plm(inv~value + capital, data = Grunfeld, model = \"random\") # random effects\ngrun.fd <- plm(inv~value + capital, data = Grunfeld, model = \"fd\") # first difference\n\nsummary(grun.po)\n\n\nPooling Model\n\nCall:\nplm(formula = inv ~ value + capital, data = Grunfeld, model = \"pooling\")\n\nBalanced Panel: n = 10, T = 20, N = 200\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-291.6757  -30.0137    5.3033   34.8293  369.4464 \n\nCoefficients:\n               Estimate  Std. Error t-value  Pr(>|t|)    \n(Intercept) -42.7143694   9.5116760 -4.4907 1.207e-05 ***\nvalue         0.1155622   0.0058357 19.8026 < 2.2e-16 ***\ncapital       0.2306785   0.0254758  9.0548 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    9359900\nResidual Sum of Squares: 1755900\nR-Squared:      0.81241\nAdj. R-Squared: 0.8105\nF-statistic: 426.576 on 2 and 197 DF, p-value: < 2.22e-16\n\n\nCode\nsummary(grun.fe)\n\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = inv ~ value + capital, data = Grunfeld, model = \"within\")\n\nBalanced Panel: n = 10, T = 20, N = 200\n\nResiduals:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n-184.00857  -17.64316    0.56337   19.19222  250.70974 \n\nCoefficients:\n        Estimate Std. Error t-value  Pr(>|t|)    \nvalue   0.110124   0.011857  9.2879 < 2.2e-16 ***\ncapital 0.310065   0.017355 17.8666 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    2244400\nResidual Sum of Squares: 523480\nR-Squared:      0.76676\nAdj. R-Squared: 0.75311\nF-statistic: 309.014 on 2 and 188 DF, p-value: < 2.22e-16\n\n\nCode\nsummary(grun.re)\n\n\nOneway (individual) effect Random Effect Model \n   (Swamy-Arora's transformation)\n\nCall:\nplm(formula = inv ~ value + capital, data = Grunfeld, model = \"random\")\n\nBalanced Panel: n = 10, T = 20, N = 200\n\nEffects:\n                  var std.dev share\nidiosyncratic 2784.46   52.77 0.282\nindividual    7089.80   84.20 0.718\ntheta: 0.8612\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-177.6063  -19.7350    4.6851   19.5105  252.8743 \n\nCoefficients:\n              Estimate Std. Error z-value Pr(>|z|)    \n(Intercept) -57.834415  28.898935 -2.0013  0.04536 *  \nvalue         0.109781   0.010493 10.4627  < 2e-16 ***\ncapital       0.308113   0.017180 17.9339  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    2381400\nResidual Sum of Squares: 548900\nR-Squared:      0.7695\nAdj. R-Squared: 0.76716\nChisq: 657.674 on 2 DF, p-value: < 2.22e-16\n\n\nCode\nsummary(grun.fd)\n\n\nOneway (individual) effect First-Difference Model\n\nCall:\nplm(formula = inv ~ value + capital, data = Grunfeld, model = \"fd\")\n\nBalanced Panel: n = 10, T = 20, N = 200\nObservations used in estimation: 190\n\nResiduals:\n       Min.     1st Qu.      Median     3rd Qu.        Max. \n-200.889558  -13.889063    0.016677    9.504223  195.634938 \n\nCoefficients:\n              Estimate Std. Error t-value  Pr(>|t|)    \n(Intercept) -1.8188902  3.5655931 -0.5101    0.6106    \nvalue        0.0897625  0.0083636 10.7325 < 2.2e-16 ***\ncapital      0.2917667  0.0537516  5.4281 1.752e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    584410\nResidual Sum of Squares: 345460\nR-Squared:      0.40888\nAdj. R-Squared: 0.40256\nF-statistic: 64.6736 on 2 and 187 DF, p-value: < 2.22e-16\n\n\n\n\nCode\n# Statistical equivalent\nsummary(lm(inv~value + capital, data = Grunfeld)) # pooled\n\n\n\nCall:\nlm(formula = inv ~ value + capital, data = Grunfeld)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-291.68  -30.01    5.30   34.83  369.45 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -42.714369   9.511676  -4.491 1.21e-05 ***\nvalue         0.115562   0.005836  19.803  < 2e-16 ***\ncapital       0.230678   0.025476   9.055  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 94.41 on 197 degrees of freedom\nMultiple R-squared:  0.8124,    Adjusted R-squared:  0.8105 \nF-statistic: 426.6 on 2 and 197 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(lm(inv~firm + capital + value, data = Grunfeld)) # fixed effects\n\n\n\nCall:\nlm(formula = inv ~ firm + capital + value, data = Grunfeld)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-184.009  -17.643    0.563   19.192  250.710 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -70.29672   49.70796  -1.414    0.159    \nfirm2        172.20253   31.16126   5.526 1.08e-07 ***\nfirm3       -165.27512   31.77556  -5.201 5.14e-07 ***\nfirm4         42.48742   43.90988   0.968    0.334    \nfirm5        -44.32010   50.49226  -0.878    0.381    \nfirm6         47.13542   46.81068   1.007    0.315    \nfirm7          3.74324   50.56493   0.074    0.941    \nfirm8         12.75106   44.05263   0.289    0.773    \nfirm9        -16.92555   48.45327  -0.349    0.727    \nfirm10        63.72887   50.33023   1.266    0.207    \ncapital        0.31007    0.01735  17.867  < 2e-16 ***\nvalue          0.11012    0.01186   9.288  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52.77 on 188 degrees of freedom\nMultiple R-squared:  0.9441,    Adjusted R-squared:  0.9408 \nF-statistic: 288.5 on 11 and 188 DF,  p-value: < 2.2e-16\n\n\nCode\nsummary(lmer(inv~ (1|firm) + capital + value, data = Grunfeld)) # random model, but different random effect estimators\n\n\nWarning: Some predictor variables are on very different scales: consider\nrescaling\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: inv ~ (1 | firm) + capital + value\n   Data: Grunfeld\n\nREML criterion at convergence: 2195.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.4319 -0.3498  0.0210  0.3592  4.8145 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n firm     (Intercept) 7367     85.83   \n Residual             2781     52.74   \nNumber of obs: 200, groups:  firm, 10\n\nFixed effects:\n             Estimate Std. Error t value\n(Intercept) -57.86442   29.37776   -1.97\ncapital       0.30819    0.01717   17.95\nvalue         0.10979    0.01053   10.43\n\nCorrelation of Fixed Effects:\n        (Intr) capitl\ncapital -0.019       \nvalue   -0.328 -0.368\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling"
  },
  {
    "objectID": "longitudinal/randomcoef.html",
    "href": "longitudinal/randomcoef.html",
    "title": "14  Random Coefficient Models (CALS)",
    "section": "",
    "text": "15 Modeling Heterogeneity: Fixed Effect models vs Random Coefficient Models\nIn continuation of looking at heterogenous variance modeling, random coefficient models are a specific type of mixed effect model that allow richness in modeling the variance through random effects. This is adding to the toolbox that we’ve built upon from last week in we looked at modeling the covariance matrix directly. Since we are introducing two “angles” of attack and giving structure to the variance Var(Y), SAS commonly refers to this method of covariance modeling through random effects as “G”-side. Modeling the covariance directly as we did last week is referred to as “R”-side for “residuals”. The terminology becomes more clear with the framework:\n\\begin{aligned}\nY = X\\beta + Zu + e\n\\end{aligned}\nwhere:\n\\begin{aligned}\nVar(Y) &= Z\\underbrace{\\operatorname{Var}(u)}_GZ' + \\underbrace{\\operatorname{Var}(e)}_{R} \\\\\n&= \\Sigma\n\\end{aligned}\n Note we can also define the following variances when additional random effects are present, such that:\nThese are directly extractable through lme with getVarCov(obj, type = c(\"random.effects\", \"conditional\", \"marginal\")).\nIn the fixed effects models, we have\n\\begin{aligned}\nY = X\\beta + \\varepsilon\n\\end{aligned}\nwhere:\nAgain, we have that \\operatorname{Var}(Y) = \\Sigma. This is exclusively “R”-side modeling when the ZGZ' matrix is 0, which is the case when we don’t have any random effects.\nIn context of this dataset, we note some research questions in particular that we are interested in modeling:\nIn this section, we try to “push” fixed effects as far as we can by modeling the mean and covariance (R-side) and see how good of a model we can get. We’ll first focus on modeling the mean.\nWe’re working with a different toolbox now, as far as covariance modeling goes. We can now control the covariance matrix directly (R-side) as we did above or implicitly through the use of random effects/coefficients (G-side).\nThe advantages:\nThe disadvantages:\nThe reason I recommend lme over lmer is that you have more control over the structure of “G” and “R” matrices in lme. lmer is only capable of fitting diagonal and unstructured covariances for G, and homogenous diagonal matrix for “R”. (And i recommend SAS over lme because the syntax is much easier!)\nHTML SAS Output\nHere we’ll compare the\nThis idea has come up a number of times, and I’ve mentioned this plot a few times so I thought I’d actually just show the effect that’s happening with this dataset.\nThis will also offer some extra intuition as to how the structure of the G matrix is affecting the effects of the individuals\n\\begin{aligned}\nE[u | y] &= GZ'V^{-1}(y - X\\beta) \\\\\neBLUP(u) &= \\hat GZ'\\hat V^{-1}(y - X\\hat\\beta)\n\\end{aligned}\nRandom effects represent this “middle ground” between fixed effects and error, so there are more decisions into which level effects fall under for modeling. Can you explain the differences between the following models? All of them loosely fall under the category of “fitting line to each child”.\nAll give slightly different predictions, but similar enough in terms of functional form and information that is modeled.\nNote that the predictions are similar between some models, but enough differences that it’s probably not “rounding” error. The closest ones are 2,6 and 3,7.\nWhen looking at the predictions at a whole glance though, they are mostly the same, capturing all of the main trends.\nThe number of parameters for each of these models is wildly different, and thus the information criteria. What’s going on?\nI think the differences in modeling are quite a little nuanced, but largely I think of this as a reframing of the fixed vs random debate. The random coefficients just make this even more confusing to me in terms of separation of the two effects… Definitionally, treating something as fixed implies that you’re interested in making inferences on those topics, whereas treating something as random is accounting for population randomness in the population you want to make inferences about. In experimental designs, the split is somewhat easier, because you have “experimental” components of your design like treatments you wish to distinguish, which really should be fixed effects, and everything else about your experiment, like blocks and the units you randomize your treatments to. Walter Stroup advocates an ANOVA-esque approach to this, and apparently IMS bulletins discuss this approach as “What would Fisher Do?”, Stroup calls the blocks and units the “topographical features” and the treatments “treatment features”. I think Miliken and Johnson describe a similar split with “experiemental” and “design” components of your study.\nAt the end of the day, I consider these things:\nThe models in which childid appears as a fixed effect AND a random effect grouping is a little unreasonable, as those fits will be fighting for the same information. Using age as both a fixed and random covariate seems appropriate because there will be populational growth rate I’m interested in studying, as well as variation in that among the children I’ve measured. That leaves model 4.\nThe “randomness I’m assuming in the population” can be more clearly visualized by just showing random effect components of the estimates. We can also plot the fitted values alongside them to show the split of fixed and random among all these models!\nModels in which we overparameterize the fixed effects, we see no random variability. Since the whole point of using a random model is to capture the variability of the population, 2, 3, 6, 7 simply don’t reflect that distribution. The difference between 1 and 4 is simply whether or not the population randomness is centered around a population trend. In 4, they fan out around 0, because the fixed age accounts for the population level trend, whereas model 1 fans out above 0."
  },
  {
    "objectID": "longitudinal/randomcoef.html#modeling-the-mean",
    "href": "longitudinal/randomcoef.html#modeling-the-mean",
    "title": "14  Random Coefficient Models (CALS)",
    "section": "17.1 Modeling the Mean",
    "text": "17.1 Modeling the Mean\nFrom the exploratory plots, we loosely fit linear/quadratic/stick models to the data.\n\n\nCode\n# without child id\nautism_lm <- lm(vsae ~ age*sicdegp, data = autism_complete) \n\n# with child id, varying intercept\nautism_lm_id <- lm(vsae ~ age*sicdegp + childid, data = autism_complete)\n\n# with child id varying slope and intercept\n# Note: these estimates are the same as if we subset dataset to just the single child and ran lm. \n# sum(coef(autism_lm_id_slope)[c(\"(Intercept)\", \"childid10\")])\n# sum(coef(autism_lm_id_slope)[c(\"age\", \"age:childid10\")])\n# coef(lm(vsae~age, data = subset(autism, childid == 10)))\nautism_lm_id_slope <- lm(vsae ~ age + childid + childid:age, data = autism_complete)\n\n\n# stick models\nautism_stick <- autism_complete %>% \n  mutate(age_gt_9 = as.numeric(age > 9), # create indicator for stick models\n         age_relu_9 = age_gt_9 * (age - 9)) # ReLu function for adding slope above 9\n\nautism_lm_stick <- lm(vsae ~ age*sicdegp + age_relu_9:sicdegp, data = autism_stick) # without child id, \"pooled\"\nautism_lm_stick_id <- lm(vsae ~ age*sicdegp + age_relu_9:sicdegp + childid, data = autism_stick) # without child id, \"pooled\"\nautism_lm_stick_id_slope <- lm(vsae ~ age + childid + age:childid + age_relu_9:childid, data = autism_stick) # individual stick model\n\n# quadratic trend\nautism_lm_quad <- lm(vsae ~ age*sicdegp + I(age^2):sicdegp, data = autism_complete)\nautism_lm_quad_id <- lm(vsae ~ age*sicdegp + I(age^2):sicdegp + childid, data = autism_complete) # id specific intercept\nautism_lm_quad_id_slope1 <- lm(vsae ~ age*childid + I(age^2):sicdegp, data = autism_complete) # id specific intercept and linear terms\nautism_lm_quad_id_slope2 <- lm(vsae ~ age*childid + I(age^2)*childid, data = autism_complete) # id specific intercept, linear and quadratic\n\n\n# using poly macro\n# autism_lm_poly2 <- lm(vsae ~ poly(age, degree = 2) * sicdegp, data = autism)\n# autism_lm_poly2 <- lm(vsae ~ poly(age, degree = 2) * child_id, data = autism)\n\n\n\n\nCode\n# predictions, plotted raw\nautism_predict <- autism %>% filter(complete.cases(.)) %>% \n  add_column(\n  yhat_lm = predict(autism_lm),\n  yhat_lm_id = predict(autism_lm_id),\n  yhat_lm_id_slope = predict(autism_lm_id_slope),\n  yhat_lm_stick = predict(autism_lm_stick),\n  yhat_lm_stick_id_slope = predict(autism_lm_stick_id_slope),\n  yhat_lm_quad = predict(autism_lm_quad),\n  yhat_lm_quad_id = predict(autism_lm_quad_id),\n  yhat_lm_quad_id_slope1 = predict(autism_lm_quad_id_slope1),\n  yhat_lm_quad_id_slope2 = predict(autism_lm_quad_id_slope2),\n  )\n\nautism_predict %>% \n  pivot_longer(cols = c(vsae, starts_with(\"yhat\")),\n                                names_to = \"type\",\n                                values_to = \"y\") %>% \n  arrange(childid, age) %>% \n  ggplot(aes(age, y, group = childid, color = type)) +\n  geom_line(alpha = .4) +\n  facet_grid(type~sicdegp)\n\n\n\n\n\n\nThe stick model estimates look a little funky because there are some individuals with only observations at age = 2, 13, thus a stick model would be degenerate in those individuals and the plotting is simply showing the direct line instead of the stick estimates. I think the same thing is happening in the id quad estimates.\n\n\n\nCode\nautism_fixed_ic <- AIC(autism_lm,\n    autism_lm_id,\n    autism_lm_id_slope,\n    autism_lm_stick,\n    autism_lm_stick_id_slope,\n    autism_lm_quad,\n    autism_lm_quad_id,\n    autism_lm_quad_id_slope1,\n    autism_lm_quad_id_slope2) %>% \n  add_column(\n    BIC = BIC(autism_lm,\n              autism_lm_id,\n              autism_lm_id_slope,\n              autism_lm_stick,\n              autism_lm_stick_id_slope,\n              autism_lm_quad,\n              autism_lm_quad_id,\n              autism_lm_quad_id_slope1,\n              autism_lm_quad_id_slope2)$BIC)\n\nautism_fixed_ic %>% \n  kbl(format = \"html\",\n      caption = \"Fixed Model Mean Information Criteria\",\n      table.attr = \"style='width:50%;'\") %>% \n  kable_classic(full_width = TRUE) %>% \n  column_spec(3, color = c(\"black\", \"red\")[as.numeric(autism_fixed_ic$AIC == min(autism_fixed_ic$AIC)) + 1]) %>% \n  column_spec(4, color = c(\"black\", \"red\")[as.numeric(autism_fixed_ic$BIC == min(autism_fixed_ic$BIC)) + 1])\n\n\n\n\nFixed Model Mean Information Criteria\n \n  \n      \n    df \n    AIC \n    BIC \n  \n \n\n  \n    autism_lm \n    7 \n    5538.140 \n    5569.035 \n  \n  \n    autism_lm_id \n    162 \n    5440.346 \n    6155.327 \n  \n  \n    autism_lm_id_slope \n    315 \n    4553.386 \n    5943.626 \n  \n  \n    autism_lm_stick \n    10 \n    5538.678 \n    5582.813 \n  \n  \n    autism_lm_stick_id_slope \n    408 \n    4153.635 \n    5954.326 \n  \n  \n    autism_lm_quad \n    10 \n    5539.403 \n    5583.538 \n  \n  \n    autism_lm_quad_id \n    165 \n    5440.750 \n    6168.970 \n  \n  \n    autism_lm_quad_id_slope1 \n    318 \n    4542.668 \n    5946.148 \n  \n  \n    autism_lm_quad_id_slope2 \n    457 \n    4237.499 \n    6254.450 \n  \n\n\n\n\n\nThe information criteria here show something very interesting! AIC chooses one of the most complex models, with 408 parameters, while BIC chooses the model with 7 parameters! How you go about model selection is very much a philosophical decision, and the information criteria tend to reflect those camps of thinking. I’ll be moving forward with autism_lm.\n\n17.1.1 Diagnostics\nWe can use the diagnostic plots to examine missing trends and start to get a sense of the variance modeling we will need.\n\nSimple Linear Model\n\n\nCode\npar(mfrow = c(2,2))\nplot(autism_lm)\n\n\n\n\n\n\n\nIndividual Stick Model\n\n\nCode\npar(mfrow = c(2,2))\nplot(autism_lm_stick_id_slope) # danger with this model is that there are a number of points with leverage one.\n\n\n\n\n\n\n\n\n\nBased on these plots, we see that when we choose the super parameterized models, we are risking overfitting with a number of values with leverage 1.\nI would ultimately base modeling decisions on specific research questions the scientist had. If there was greater interest in ages 10 - 13, I may be more hesitant to choose the stick model because it’s quite overfit in this domain. If they wanted to summarize “rules of thumb” for presentation or big picture of this trait, I’d likely choose the simple linear model. If they were hoping for prediction applications based on younger children and valued accuracy of predictions, I may opt for the over-parameterized models like quadratic or stick model."
  },
  {
    "objectID": "longitudinal/randomcoef.html#modeling-the-variance",
    "href": "longitudinal/randomcoef.html#modeling-the-variance",
    "title": "14  Random Coefficient Models (CALS)",
    "section": "17.2 Modeling the variance",
    "text": "17.2 Modeling the variance\nFor simplicity, we’ll chose the linear functional form, as it is quite simple, and performs reasonably well for the number of parameters that it uses. Note, obviously the variance modeling will depend on your chosen mean model.\nSimilar to how we modeled the mean, we start with trying to visualize the covariance matrix with as few restrictions as possible, so we fit an unstructured covariance matrix with gls.\n\n\nCode\n# start with unstructured covariance matrix, and let the optimizer tell us the best estimate with no restrictions.\nautism_gls_un <- gls(vsae~age*sicdegp,\n                     correlation = corSymm(form = ~1 | childid), # unstructured correlation\n                     weights = varIdent(form = ~1 | agef), # parameter for each entry along diagonal\n                     data = autism_complete)\ngetVarCov(autism_gls_un)\n\n\nMarginal variance covariance matrix\n        [,1]     [,2]     [,3]    [,4]     [,5]\n[1,] 10.8500   8.9204   9.4352   25.49   48.503\n[2,]  8.9204  55.6110  45.2460  108.23  203.970\n[3,]  9.4352  45.2460 135.6600  223.11  404.000\n[4,] 25.4900 108.2300 223.1100  765.99 1206.800\n[5,] 48.5030 203.9700 404.0000 1206.80 2319.400\n  Standard Deviations: 3.2939 7.4573 11.647 27.677 48.16 \n\n\nThere are some visual guides that we can use to help us model the covariance:\n\nline graph of matrix entries, grouped by row\nVariogram\nAutocorrelation Function\n\n\n17.2.1 1. Line Graph of matrix entries, grouped by row\nI picked up this visualization from Generalized Linear Mixed Models by Walter Stroup, one of the authors of the SAS for Mixed Models book. I like this visualization better than a heat map because instead of using a color channel for the variance, it uses y-position which is much more clear.\n\n\n\n\n\n\n\n\n\nCode\nsigmahat <- getVarCov(autism_gls_un)\n\nsigmahat_df <- data.frame(row = rep(1:5, each = 5),\n           col = rep(1:5, 5),\n           age = rep(c(2, 3, 5, 9, 13), 5), # un\n           cov = c(sigmahat)) %>% \n  filter(row <= col) # pull upper triangle w/ diagonal entries\n\nsigmahat_df %>% \n  ggplot(aes(col, cov, color = factor(row), group = factor(row))) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\nThis graphic is confusing at first, but I think it’s one of the more intuitive visualizations once you’re used to it. each point in the graph is the estimated covariance/variance. The lines show the trend that is happening in each row, as you move away from the main diagonal. For example, the red line is the first row of \\hat \\Sigma. There are 5 dots because the first row starts in column 1. The main diagonal (all the variances) are the left most point in each of the trend lines.\nWe see a similar pattern of the estimated covariances increasing as the age increases. column 5 represents age 13, and we can see the estimated variance is ~2500.\nSince age has a meaning on a continuous scale, it would be slightly more helpful to visualize the appropriate distances in the x axis when looking at the estimated covariances.\n\n\nCode\nsigmahat_age_plot <- sigmahat_df %>%\n  mutate(diag = col - row) %>% \n  ggplot(aes(age, cov, color = factor(row), group = factor(row))) +\n  geom_point() +\n  geom_line()\nsigmahat_age_plot\n\n\n\n\n\nWe can see some resemblance of a power relationship for the main diagonal, and there is a pretty clean pattern in the covariance here, so it’s likely we can capture most of the trends with just a few extra parameters. I would like to graphically test what i’m thinking, so i’ll add a smoothed parametric fit to the left most points of each colored trend (main diagonal).\n\n\nCode\n# hackish way to checking how a power relationship on age might fit for the heterogenous variance function\nsigmahat_age_plot + \n  geom_line(stat = \"smooth\",\n            method = \"lm\",\n              aes(group = diag),\n              formula = (y~ I(x^3)), # can play with this form to visualize how the variance estimates might look. Try exponential here, doesn't fit well! (so AR covariance models probably inappropriate)\n              se = FALSE,\n              size = .2,\n              linetype = 2,\n              alpha = .4,\n              color = \"black\") \n\n\n\n\n\nThe main diagonal band here seems like a pretty decent fit, which is primarily what I’m interested in here. The fact that the off-diagonal bands also fit this power relationship pretty well implies that I can probably get away with a fairly simple covariance matrix structure (few parameters for off diagonals, like compound symmetry) and still describe this covariance matrix quite well.\n\n\n17.2.2 2. Semi-(Variogram)\nThis is another method of visualizing covariances that is popular in spatial statistics. This is more useful when one (or more) of your datapoints have a continuous/spatial interpretation. In this case, we are interested in how the correlation of observations between ages as the distance in age increases.\nThe semi-variogram is defined as: (for isotropic and stationary processes)\n\n\\begin{aligned}\n\\gamma(h) &= \\frac{1}{2}Var(\\epsilon_{age} - \\epsilon_{age+h}) \\\\\n&= \\frac{1}{2}E(\\epsilon_{age} - \\epsilon_{age+h})^2\n\\end{aligned}\n\nEmpirically to estimate this, we take pairs of observations certain distances apart, and average the squared distances of the estimated residuals.\n\n\nCode\nvario_autism <- Variogram(autism_gls_un, form = ~ age | childid, resType = \"response\")\nvario_autism\n\n\n     variog dist n.pairs\n1  22.25863    1     147\n2  41.44491    2      86\n3  53.94080    3      91\n4 303.74329    4     153\n5 275.86605    6     115\n6 382.44231    7     118\n7 707.25001    8      49\n8 841.57324   10      91\n9 927.39613   11      93\n\n\nWe have oddly spaced age observations, age = c(2, 3, 5, 9, 13), so in this case, it’s pretty distinctive that our estimate of dist = 1 come from age = 2,3, and dist = 8 comes from age = 5, 13.\n\n\nCode\nplot(vario_autism)\n\n\n\n\n\n\n\n \n\nFigures from Mixed Effects Models\n\n\n\n\n\n17.2.3 3. Auto Correlation Functions\n(sample) auto correlation functions show the correlation with lagged versions of itself.\n\n\\begin{aligned}\nACF(k) = \\frac{\\operatorname{Cov}(\\varepsilon, \\varepsilon_{lag(k)})}{\\operatorname{Var}(\\varepsilon)}\n\\end{aligned}\n\nThe ACF is most useful when we have data that is more time series like, with many measured timepoints and somewhat equally spaced measurements because we’re calculating correlation with itself.\n\n\nCode\nACF(autism_gls_un, form = ~ age | childid, resType = \"response\")\n\n\n  lag        ACF\n1   0 1.00000000\n2   1 0.46705931\n3   2 0.19978821\n4   3 0.11193700\n5   4 0.09748354\n\n\nCode\nplot(ACF(autism_gls_un, form = ~ age | childid, resType = \"response\"), alpha = .01) # observed - fitted, not accounting for covariance estimates\n\n\n\n\n\nWe can ignore the first bar, but we’re looking at the trend made by the top of the bars. An AR1 model would have the tops of the bars decrease exponentially quickly. The fact that the bars poke out from the dotted alpha = .01 curve means that there’s likely some sequential correlation happening in our data that we haven’t accounted for.\n\n\nCode\nplot(ACF(autism_gls_un, form = ~ age | childid, resType = \"normalized\"), alpha = .01) # accounting for our estimated covariance matrix\n\n\n\n\n\nIf we plot the ACF of residuals that account for our unstructured covariance matrix, we can see the sequential correlations drop out of significance.\n\n\n17.2.4 Fitting variance models\nThere are many covariance shapes we can try here, and SAS has even more! See SAS Repeated Statement.\n\n\nCode\n# heterogeneous, diagonal\nautism_gls_het <- gls(vsae~age*sicdegp,\n                      weights = varIdent(form = ~ 1 | agef),\n                      data = autism_complete)\n\n# should be a bad fit since we already know there is strong heterogeneity\nautism_gls_cs <- gls(vsae~age*sicdegp,\n                     correlation = corCompSymm(form = ~1 | childid),\n                     data = autism_complete)\n\n# heterogeneous, compound symmetry\nautism_gls_csh <- gls(vsae~age*sicdegp,\n                     correlation = corCompSymm(form = ~1 | childid),\n                     weights = varIdent(form = ~ 1 | agef),\n                     data = autism_complete)\n\n# continuous autoregressive\n# correlation = \\phi^distance\nautism_gls_carh <- gls(vsae~age*sicdegp,\n                      correlation = corCAR1(form = ~ age | childid), # i.e. exponential decay with distance\n                      weights = varIdent(form = ~1 | agef),\n                      data = autism_complete)\n\n# based on the visualizations, should be decent\n# variance = age^\\theta\nautism_gls_pow <- gls(vsae~age*sicdegp,\n                      correlation = corCompSymm(form= ~1 | childid),\n                      weights = varPower(form = ~age), # fit heterogeneous variance function\n                      data = autism_complete)\n\nautism_gls_cpow <- gls(vsae~age*sicdegp,\n                      correlation = corCompSymm(form= ~1 | childid),\n                      weights = varConstPower(form = ~age), # additional constant variable to power relationship\n                      data = autism_complete)\n\n# with linear paramterization of corstruct\nautism_gls_powlin <- gls(vsae~age*sicdegp,\n                      correlation = corLin(form= ~age | childid),\n                      weights = varPower(form = ~age), # fit heterogeneous variance function\n                      data = autism_complete)\n\n\n\n\nCode\nautism_gls_ic <- AIC(\n    autism_gls_het,\n    autism_gls_cs,\n    autism_gls_csh,\n    autism_gls_carh,\n    autism_gls_un,\n    autism_gls_pow,\n    autism_gls_cpow,\n    autism_gls_powlin) %>% \n  add_column(\n    BIC = BIC(autism_gls_het,\n              autism_gls_cs,\n              autism_gls_csh,\n              autism_gls_carh,\n              autism_gls_un,\n              autism_gls_pow,\n              autism_gls_cpow,\n              autism_gls_powlin)$BIC)\n\nautism_gls_ic %>% kbl(caption = \"Information Criteria for \") %>% \n  kable_classic(full_width = F) %>%\n  column_spec(3, color = c(\"black\", \"red\")[as.numeric(autism_gls_ic$AIC == min(autism_gls_ic$AIC)) + 1]) %>% \n  column_spec(4, color = c(\"black\", \"red\")[as.numeric(autism_gls_ic$BIC == min(autism_gls_ic$BIC)) + 1])\n\n\n\n\nInformation Criteria for \n \n  \n      \n    df \n    AIC \n    BIC \n  \n \n\n  \n    autism_gls_het \n    11 \n    4661.843 \n    4710.282 \n  \n  \n    autism_gls_cs \n    8 \n    5461.844 \n    5497.072 \n  \n  \n    autism_gls_csh \n    12 \n    4529.853 \n    4582.696 \n  \n  \n    autism_gls_carh \n    12 \n    4604.352 \n    4657.195 \n  \n  \n    autism_gls_un \n    21 \n    4482.384 \n    4574.859 \n  \n  \n    autism_gls_pow \n    9 \n    4536.168 \n    4575.800 \n  \n  \n    autism_gls_cpow \n    10 \n    4538.168 \n    4582.203 \n  \n  \n    autism_gls_powlin \n    9 \n    4654.410 \n    4694.043 \n  \n\n\n\n\n\nAt this stage, it seems the unstructured covariance matrix would fit the best if we follow either AIC or BIC, though notably the power variance structure has a very comparable BIC with only 9 parameters vs 21 parameters total. Since I’ve already abided by principles of parsimony in choosing to use the very simple linear model, I’d probably elect to use the autism_gls_pow as my final model for fixed effects modeling for the same reasons. The model is also very appealing for explaining variance as simply a power function of age (respecting the continuous scale) and similar performance in information criteria."
  },
  {
    "objectID": "longitudinal/randomcoef.html#final-model",
    "href": "longitudinal/randomcoef.html#final-model",
    "title": "14  Random Coefficient Models (CALS)",
    "section": "17.3 Final Model",
    "text": "17.3 Final Model\n\n\nCode\nplot(autism_gls_pow)\n\n\n\n\n\nspread among the residuals looks much better against the fitted values.\nSome basic inference from the fixed effect model…\n\n\nCode\nintervals(autism_gls_pow) # asymptotic normal approximation, based on inverse hessian\n\n\nApproximate 95% confidence intervals\n\n Coefficients:\n                  lower       est.      upper\n(Intercept)  -0.5248735  1.2011843  2.9272421\nage           2.2896001  3.1507950  4.0119900\nsicdegp2     -1.9420685  0.3411552  2.6243789\nsicdegp3     -5.7898786 -3.2664553 -0.7430321\nage:sicdegp2 -0.5218454  0.6171416  1.7561285\nage:sicdegp3  3.0205486  4.2837888  5.5470290\n\n Correlation structure:\n        lower      est.     upper\nRho 0.3682194 0.4549824 0.5387205\n\n Variance function:\n         lower     est.    upper\npower 1.217197 1.291734 1.366271\n\n Residual standard error:\n   lower     est.    upper \n1.372947 1.575796 1.808615"
  },
  {
    "objectID": "longitudinal/randomcoef.html#weighted-least-squares",
    "href": "longitudinal/randomcoef.html#weighted-least-squares",
    "title": "14  Random Coefficient Models (CALS)",
    "section": "17.4 Weighted Least Squares",
    "text": "17.4 Weighted Least Squares\nI mentioned weighted least squares as a very simple quick fix that give similar qualities of inference as you’d get with more sophisticated fitting procedures (increased variance for larger ages). You can see here that it works decently, but the fact that we’re not accounting for measurements from individuals (correlation) means that we can do much better.\n\n\nCode\ngls_un_var <- getVarCov(autism_gls_un, individual = 2) %>% diag()\n\nautism_wls <- lm(vsae~age*sicdegp, weights = 1/(gls_un_var[autism$agef]), data = autism)\n\nAIC(autism_wls)\n\n\n[1] 4655.256\n\n\nCode\nBIC(autism_wls)\n\n\n[1] 4686.151\n\n\nI believe comparing the information criteria in WLS to GLS is legitimate? but I’m not certain… the former situation we’re assuming variance parameters are known which makes this comparison weird.\n\n\nCode\nautism_new <- expand.grid(age = 2:13, sicdegp = factor(1:3))\n\n# from direct lm\nautism_new %>% bind_cols(predict(autism_lm, newdata = autism_new, interval = \"confidence\")) %>% pivot_longer(cols = fit:upr, names_to = \"type\") %>% \n  ggplot(aes(age, value, linetype = type, group = type)) +\n  geom_line() + \n  facet_wrap(~sicdegp)\n\n\n\n\n\nCode\n# from weighted least squares\nautism_new %>% bind_cols(predict(autism_wls, newdata = autism_new, interval = \"confidence\")) %>% pivot_longer(cols = fit:upr, names_to = \"type\") %>%\n  ggplot(aes(age, value, linetype = type, group = type)) +\n  geom_line() + \n  facet_wrap(~sicdegp)"
  },
  {
    "objectID": "longitudinal/randomcoef.html#fitting-models",
    "href": "longitudinal/randomcoef.html#fitting-models",
    "title": "14  Random Coefficient Models (CALS)",
    "section": "18.1 Fitting Models",
    "text": "18.1 Fitting Models\n\n\nCode\n# random intercept\nautism_lme_id <- lme(fixed = vsae~sicdegp*age,\n    random = ~ 1 | childid,\n    data = autism_complete)\n\n# random slopes model\n# doesn't converge!!\nautism_lme_id_slope <- lme(fixed = vsae~sicdegp*age,\n    random = ~ age | childid,\n    data = autism_complete)\n\n\nError in lme.formula(fixed = vsae ~ sicdegp * age, random = ~age | childid, : nlminb problem, convergence error code = 1\n  message = iteration limit reached without convergence (10)\n\n\nWe hit some optimizer problems trying to fit the random slopes model. By default, lme uses the outdated nlminb optimizer, which is similar to “BFGS”, a quasi-newton optimization routine. 1. It’s mostly used for compatibility reasons, and optim is the general optimizer that is now preferred. lmeControl has the option opt = \"optim\", which switches the optimizer, and now looks for optimMethod = \"BFGS\" which says to run BFGS algorithm in optim.\nWe can also switch the function call to lmer, because this is a model that can be handled in that library as well. The default callback is lmer -> nloptwrap (wrapper function) -> nloptr (R interface into NLopt) -> NLopt (Free/Open Source library for Nonlinear optimization) -> NLOPT_LN_BOBYQA (BOBYQA routine written in C). BOBYQA is a derivative free optimization program.\nI try to avoid diving down the optimizer rabbit hole as much as possible… Fix 2 is normally the route I take, if you’re curious, in which I fit successively simpler models until boundary issues don’t exist.\n\n\nCode\n# Fix 1: change the optimizer to \"optim\" (BFGW) in lme\nautism_lme_id_slope <- lme(fixed = vsae~sicdegp*age,\n    random = ~ age | childid,\n    data = autism_complete,\n    control = lmeControl(opt = \"optim\"))\n# summary(autism_lme_id_slope) # note correlation of random effects is _very_ close to boundary -1 (even though fits with no complaints)\n\n# Fix 2: change optimizer to use ------------------\n# lmer fits, but warns about boundary...\nautism_lmer_id_slope <- lmer(vsae~sicdegp*age + (age | childid), data = autism_complete)\n## boundary (singular) fit: see help('isSingular')\n# looking at summary, we see that correlation of random effects is -1 (boundary)\n# summary(autism_lmer_id_slope)\n\n# A useful function is \"allFit\", which tries to fit the model with \"all\" appropriate optimizers.\n# they all have boundary warnings.\n# allFit(autism_lmer_id_slope)\n\n# try the uncorrelated model, still boundary w/ intercept variance estimated as 0.\nautism_lmer_id_slope_nocor <- lmer(vsae~sicdegp*age + (age || childid), data = autism_complete)\n## boundary (singular) fit: see help('isSingular')\n# summary(autism_lmer_id_slope_nocor)\n\n# take out random intercept, finally no boundary estimates, and no warnings!\nautism_lmer_id_slope_noint <- lmer(vsae~sicdegp*age + (0 + age | childid), data = autism_complete)\n\n\nBased on these issues, I’m skeptical the optimizer will be able to handle more complicated random coefficient models reliably. But we’ll try! We’ll try to fit the same gamut of linear/quadratic/stick models that we had fit in the fixed case. Since lmer is likely more familiar, I show these fits in lmer first, and the lme equivalent underneath.\n\n\nCode\n### Fitting in lmer ---------------------------------------------------------------------\n## Quadratic Models\nautism_lmer_quad_id <- lmer(vsae ~ age*sicdegp + I(age^2):sicdegp + (1 | childid), # random intercept\n                            data = autism_complete) # no warnings\n\n# quadratic, with random linear term.\nautism_lmer_quad_id_slope1 <- lmer(vsae~age + I(age^2):sicdegp + (1 + age | childid), # random intercept, linear\n                                   data = autism_complete) # boundary warning\n## boundary (singular) fit: see help('isSingular')\nautism_lmer_quad_id_slope1_nocor <- lmer(vsae~age + I(age^2):sicdegp + (1 + age || childid), # random intercept, linear, uncorrelated\n                                   data = autism_complete) # boundary warning\n## boundary (singular) fit: see help('isSingular')\nautism_lmer_quad_id_slope1_noint <- lmer(vsae~age + I(age^2):sicdegp + (0  + age | childid), # random linear, no intercept\n                                   data = autism_complete) # no warnings\n\n# Quadratic: unstructured, heterogenous G matrix, diagonal, homogenous R\nautism_lmer_quad_id_slope2 <- lmer(vsae ~ age*sicdegp + I(age^2):sicdegp + (1 + age + I(age^2) | childid), data = autism_complete) # singular, corr = -1\n## boundary (singular) fit: see help('isSingular')\n\n# Quadratic: diagonal, heterogenous G matrix, diagonal, homogenous R\nautism_lmer_quad_id_slope2_nocor <- lmer(vsae ~ age*sicdegp + I(age^2):sicdegp + (1 + age + I(age^2) || childid), data = autism_complete) # singular, intercept ≈ 0\n## boundary (singular) fit: see help('isSingular')\n\n\n# Quadratic, no intercept: unstructured, heterogenous G matrix, diagonal, homogenous R\nautism_lmer_quad_id_slope2_noint <- lmer(vsae ~ age*sicdegp + I(age^2):sicdegp + (0 + age + I(age^2) | childid), data = autism_complete, \n                                         control = lmerControl(optimizer = \"Nelder_Mead\")) # no warnings\n\n# Quadratic, no intercept: diagonal, heterogenous G matrix, diagonal, homogenous R\n# autism_lmer_quad_id_slope2_noint_nocor <- lmer(vsae ~ age*sicdegp + I(age^2):sicdegp + (0 + age + I(age^2) || childid), data = autism_complete) # fail to converge\nautism_lmer_quad_id_slope2_noint_nocor <- lmer(vsae ~ age*sicdegp + I(age^2):sicdegp + (0 + age + I(age^2) || childid), data = autism_complete, \n                                         control = lmerControl(optimizer = \"Nelder_Mead\")) # no warnings\n\n## Stick Models\nautism_lmer_stick_id <- lmer(vsae ~ age*sicdegp + age_relu_9:sicdegp + (1 | childid), data = autism_stick) \nautism_lmer_stick_id_slope <- lmer(vsae ~ age*sicdegp + age_relu_9:sicdegp + (0 + age + age_relu_9 | childid), data = autism_stick)\n\n\n\n\nCode\n# Fitting in lme ---------------------------------------------------------------------\n# Quadratic, random intercept: diagonal, homogenous R\nautism_lme_quad_id <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp, # quadratic \n                       random = ~ 1 | childid, # random intercept\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\n\n# Quadratic, random linear: unstructured, heterogenous G matrix, diagonal, homogenous R\nautism_lme_quad_id_slope1 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = ~ age | childid, # random intercept and linear term\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\n# Quadratic, random linear: diagonal, heterogenous G matrix:  diagonal, homogenous R\nautism_lme_quad_id_slope1_nocor <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid = pdDiag(form = ~ age)), # random intercept and linear term, no correlation\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\nautism_lme_quad_id_slope1_noint <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid = pdDiag(form = ~ 0 + age)), # random intercept and linear term, no intercept\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\n\n# Quadratic, random quadratic: unstructured, heterogenous G matrix:  diagonal, homogenous R\nautism_lme_quad_id_slope2 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = ~ age + I(age^2) | childid, # random intercept, linear and quadratic term\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\n# Quadratic, random quadratic: diagonal, heterogenous G matrix:  diagonal, homogenous R\nautism_lme_quad_id_slope2_nocor <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid=pdDiag(form = ~ age + I(age^2))), # random intercept, linear and quadratic term, no correlation\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\n# Quadratic, random quadratic (no int): unstructured, heterogenous G matrix:  diagonal, homogenous R\nautism_lme_quad_id_slope2_noint <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid=pdSymm(form = ~ 0 + age + I(age^2))), # Unstructured G\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\n# Quadratic, random quadratic (no int): diagonal, heterogenous G matrix:  diagonal, homogenous R\nautism_lme_quad_id_slope2_noint_nocor <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid=pdDiag(form = ~ 0 + age + I(age^2))),\n                       data = autism_complete,\n                       control = lmeControl(opt = \"optim\"))\n\n## Stick models\nautism_lme_stick_id <- lme(vsae ~ age*sicdegp + age_relu_9:sicdegp,\n                       random = ~ 1 | childid, # random intercept\n                       data = autism_stick,\n                       control = lmeControl(opt = \"optim\"))\nautism_lme_stick_id_slope <- lme(vsae ~ age*sicdegp + age_relu_9:sicdegp,\n                       random = ~ 0 + age + age_relu_9 | childid, # random linear and quadratic term, no intercept\n                       data = autism_stick,\n                       control = lmeControl(opt = \"optim\"))\n\n\n\n\nCode\n# see predicted values from some random effect models\nautism_random_predict <- autism_complete %>% add_column(yhat_lme_id = predict(autism_lme_id),\n                                 yhat_lme_id_slope = predict(autism_lme_id_slope),\n                                 yhat_lme_quad_id = predict(autism_lme_quad_id),\n                                 yhat_lme_quad_id_slope1 = predict(autism_lme_quad_id_slope1),\n                                 yhat_lme_quad_id_slope2 = predict(autism_lme_quad_id_slope2),\n                                 yhat_lme_stick_id = predict(autism_lme_stick_id),\n                                 yhat_lme_stick_id_slope = predict(autism_lme_stick_id_slope),\n                                 yhat_lmer_quad_id_slope2_noint = predict(autism_lmer_quad_id_slope2_noint),\n                                 yhat_lmer_stick_id = predict(autism_lmer_stick_id),\n                                 yhat_lmer_stick_id_slope = predict(autism_lmer_stick_id_slope))\n\nautism_random_predict %>% pivot_longer(cols = c(vsae, starts_with(\"yhat\")),\n                                names_to = \"type\",\n                                values_to = \"y\") %>% \n  arrange(childid, age) %>% \n  ggplot(aes(age, y, group = childid, color = type)) +\n  geom_line(alpha = .4) +\n  facet_grid(type~sicdegp)\n\n\n\n\n\n\n\nCode\nautism_lme_ic <- AIC(\n  autism_lme_id,\n  autism_lme_id_slope, # warning here too, probably because includes individuals that \n  autism_lme_quad_id,\n  autism_lme_quad_id_slope1,\n  autism_lme_quad_id_slope2,\n  autism_lmer_quad_id_slope2, # lmer version, get warning when included, not quite sure why, maybe refit as ML? but very close to lme counterpart\n  autism_lme_quad_id_slope2_nocor,\n  autism_lmer_quad_id_slope2_nocor, # lmer version\n  autism_lme_quad_id_slope2_noint,\n  autism_lmer_quad_id_slope2_noint, # lmer version\n  autism_lme_quad_id_slope2_noint_nocor,\n  autism_lme_stick_id,\n  autism_lme_stick_id_slope\n    ) %>% \n  add_column(\n    BIC = BIC(\n      autism_lme_id,\n    autism_lme_id_slope,\n    autism_lme_quad_id,\n    autism_lme_quad_id_slope1,\n    autism_lme_quad_id_slope2,\n    autism_lmer_quad_id_slope2, # lmer version\n    autism_lme_quad_id_slope2_nocor,\n    autism_lmer_quad_id_slope2_nocor, # lmer version\n    autism_lme_quad_id_slope2_noint,\n    autism_lmer_quad_id_slope2_noint, # lmer version\n    autism_lme_quad_id_slope2_noint_nocor,\n    autism_lme_stick_id,\n    autism_lme_stick_id_slope)$BIC)\n\n\nWarning in AIC.default(autism_lme_id, autism_lme_id_slope, autism_lme_quad_id, :\nmodels are not all fitted to the same number of observations\n\n\nWarning in BIC.default(autism_lme_id, autism_lme_id_slope, autism_lme_quad_id, :\nmodels are not all fitted to the same number of observations\n\n\nCode\nautism_lme_ic %>% kbl() %>% \n  kable_classic(full_width = F) %>% \n  column_spec(3, color = c(\"black\", \"red\")[as.numeric(autism_lme_ic$AIC == min(autism_lme_ic$AIC)) + 1]) %>% \n  column_spec(4, color = c(\"black\", \"red\")[as.numeric(autism_lme_ic$BIC == min(autism_lme_ic$BIC)) + 1])\n\n\n\n\n \n  \n      \n    df \n    AIC \n    BIC \n  \n \n\n  \n    autism_lme_id \n    8 \n    5461.844 \n    5497.072 \n  \n  \n    autism_lme_id_slope \n    10 \n    4716.130 \n    4760.166 \n  \n  \n    autism_lme_quad_id \n    11 \n    5470.017 \n    5518.402 \n  \n  \n    autism_lme_quad_id_slope1 \n    13 \n    4724.230 \n    4781.412 \n  \n  \n    autism_lme_quad_id_slope2 \n    16 \n    4640.705 \n    4711.083 \n  \n  \n    autism_lmer_quad_id_slope2 \n    16 \n    4639.983 \n    4710.598 \n  \n  \n    autism_lme_quad_id_slope2_nocor \n    13 \n    4680.454 \n    4737.636 \n  \n  \n    autism_lmer_quad_id_slope2_nocor \n    13 \n    4680.391 \n    4737.766 \n  \n  \n    autism_lme_quad_id_slope2_noint \n    13 \n    4680.009 \n    4737.191 \n  \n  \n    autism_lmer_quad_id_slope2_noint \n    13 \n    4680.009 \n    4737.384 \n  \n  \n    autism_lme_quad_id_slope2_noint_nocor \n    12 \n    4678.391 \n    4731.174 \n  \n  \n    autism_lme_stick_id \n    11 \n    5454.106 \n    5502.490 \n  \n  \n    autism_lme_stick_id_slope \n    13 \n    4748.713 \n    4805.895 \n  \n\n\n\n\n\nWe’ve fit all these “intuitive” models, and we can see most of them give pretty intuitive predicted values for our dataset. They capture the main trends we’re after pretty well. Given that we’re using random coefficient models, we’ll probably rule out the models that only vary the intercept. It seems from these predictive plots that we at the very least need to be modeling complexity at the linear or quadratic level (or splines). We’ll look further into these models in the diagnostics, as well as the variance modeling in the diagnostics.\nThe information criteria seems to pick out the “autism_lme_quad_id_slope2”, which is the random effect model with random coefficients up to quadratic order, and unstructured correlation matrix in G. thought we remember there were some boundary warnings with that model, so removing the intercept for a model that is not near the boundary may be desired."
  },
  {
    "objectID": "longitudinal/randomcoef.html#diagnostics-1",
    "href": "longitudinal/randomcoef.html#diagnostics-1",
    "title": "14  Random Coefficient Models (CALS)",
    "section": "18.2 Diagnostics",
    "text": "18.2 Diagnostics\nLet’s look at the mean structure with standard residual plots first. We’ll just pick out a few plots to look at.\n\n18.2.1 Quad\n\n\nCode\nplot(autism_lme_quad_id_slope2, form = resid(.,type = \"normalized\") ~ age | sicdegp) # normalized\n\n\n\n\n\nIn order to study the variance covariance pattern of this, I’ll examine how closely it matches up with the unstructured estimate of the covariance. we can also look at the sample covariance for some direction.\n\n\nCode\nautism_gls_quad_un <- gls(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       correlation = corSymm(form = ~1 | childid),\n                       weights = varIdent(form = ~ 1 | age),\n                       data = autism_complete)\n\ngetVarCov(autism_gls_quad_un)\n\n\nMarginal variance covariance matrix\n        [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 10.8700   9.0082   9.3212   25.916   48.667\n[2,]  9.0082  55.7270  45.7570  110.010  201.300\n[3,]  9.3212  45.7570 135.7700  222.120  397.840\n[4,] 25.9160 110.0100 222.1200  771.040 1197.400\n[5,] 48.6670 201.3000 397.8400 1197.400 2287.700\n  Standard Deviations: 3.297 7.4651 11.652 27.768 47.83 \n\n\nCode\ngetVarCov(autism_lme_quad_id_slope2, type = \"marginal\") # ZGZ + R\n\n\nchildid 1 \nMarginal variance covariance matrix\n        1        2        3        4        5\n1 39.0350   2.0468   5.4054   21.354   49.612\n2  2.0468  53.4340  43.3380   96.860  148.630\n3  5.4054  43.3380 156.7100  270.900  422.580\n4 21.3540  96.8600 270.9000  748.630 1274.200\n5 49.6120 148.6300 422.5800 1274.200 2568.100\n  Standard Deviations: 6.2478 7.3098 12.518 27.361 50.677 \n\n\nCode\n# we can also look at the sample covariance to get a rough sense for how the variances and covariances should be behaving\n\n\nIt seems the variance pattern in the random effect model is severely overestimating the variance at age 2. There is also an interesting pattern in which it seems to underestimate the covariance between age 2 and 3. Other than that, the covariance looks to be within reason of capturing the overall trends. there are likely some optimizations in parameterization that can be made, but it’s difficult to know exactly how…\n\n\n18.2.2 Quad simplified, no intercept\n\n\nCode\nplot(autism_lme_quad_id_slope2_noint_nocor, form = resid(.,type = \"normalized\") ~ age | sicdegp) # normalized\n\n\n\n\n\n\n\nCode\ngetVarCov(autism_gls_quad_un)\n\n\nMarginal variance covariance matrix\n        [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 10.8700   9.0082   9.3212   25.916   48.667\n[2,]  9.0082  55.7270  45.7570  110.010  201.300\n[3,]  9.3212  45.7570 135.7700  222.120  397.840\n[4,] 25.9160 110.0100 222.1200  771.040 1197.400\n[5,] 48.6670 201.3000 397.8400 1197.400 2287.700\n  Standard Deviations: 3.297 7.4651 11.652 27.768 47.83 \n\n\nCode\ngetVarCov(autism_lme_quad_id_slope2_noint_nocor, type = \"marginal\") # ZGZ + R\n\n\nchildid 1 \nMarginal variance covariance matrix\n       1       2       3        4        5\n1 55.373  11.014  21.709   51.146   91.311\n2 11.014  67.485  38.850   97.086  179.460\n3 21.709  38.850 134.400  229.700  440.750\n4 51.146  97.086 229.700  706.560 1303.300\n5 91.311 179.460 440.750 1303.300 2667.700\n  Standard Deviations: 7.4413 8.2149 11.593 26.581 51.65 \n\n\nThe model with the simplified G structure is also lacking in the earlier ages, and also the covariances with age2 seems to be increasing too quickly. The second diagonal band is also both over and underestimated sometimes."
  },
  {
    "objectID": "longitudinal/randomcoef.html#further-covariance-adjustments",
    "href": "longitudinal/randomcoef.html#further-covariance-adjustments",
    "title": "14  Random Coefficient Models (CALS)",
    "section": "18.3 Further covariance adjustments",
    "text": "18.3 Further covariance adjustments\nThe mixed effects model already has some complexity in the variance that is modeled, but it’s missing some parts of the covariance that we can try to adjust for on the R side of things. I find that there aren’t many guardrails when modeling things in this manner, so likelihood is generally my guide. You can try to create plots to clue you in on certain patterns but often it’s just faster to fit a bunch of parameterizations and check the results.\nFor the diagonal of the R matrix, it’s useful to know how some of the variance classes can be combined and to know what your options are, you can do this by exploring ?varClasses.\n\nvarIdent allows for a different level for each variance on the diagonal.\nvarExp is a (fitted) exponential relationship to covariate\nvarPower is an (fitted) power relationship\nvarFixed allows for a constant (fixed) covariate value\nvarComb allows combinations of any of the above\n\nFor the structure of the R matrix, you can see ?corStructs\n\ncorAR1 allows for exponential decay in rows, as measured from distance from diagonal\ncorCAR1 allows for exponential decay in rows, as measured from distance in continuous covariate\ncorARMA allows for exponential decay in rows, as distance from diagonal, AND first q diagonal bands\ncorCompSymm constant off diagonals\n\nFor the structure of the G matrix, there are also a number of spatial related matrices, which have a functional form of how correlation drops off in relation to distance.\n\npdDiag is useful for specifying that you only want a diagonal matrix for G. This is the (1 + age || childid) double bar option in lmer\npdSymm specifies that you want to esetimate an unstructured matrix for G. This is (1 + age | childid) default option in lmer.\npdBlocked is useful for composing matrix structures for nested effects in the G matrix.\npdCompSymm compound symmetry in G matrix. Only possible with flexLambda branch in lmer.\n\n\n\nCode\n# This first model tries to add a parameter to adjust the variance of age=2, since the diagnostics above were close except for this term.\nautism_lme_quad_id_slope2_het2 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                                     random = list(childid = pdSymm(form = ~ 1 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | I(age == 2)),\n                       control = lmeControl(opt = \"optim\", maxIter = 1000, msMaxIter = 1000, msVerbose = TRUE)) # unfortunately it's a fight with the optimizer, so we need to simplify.\n## initial  value 3387.370642\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 1\n## Error in logLik.reStruct(object, conLin): NA/NaN/Inf in foreign function call (arg 3)\n\n# try removing the intercept, and fit simple model\nautism_lme_quad_id_slope2_noint_het2 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                                     random = list(childid = pdSymm(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | I(age == 2)),\n                       control = lmeControl(opt = \"optim\", maxIter = 1000, msMaxIter = 1000, msVerbose = TRUE)) # converged w/ warnings\n## initial  value 3397.116545\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 1\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 47\n## iter  10 value 3324.372416\n## final  value 3324.349202 \n## converged\n\n# the more flexible diagonal values\nautism_lme_quad_id_slope2_noint_het <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                                     random = list(childid = pdSymm(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age),\n                       control = lmeControl(opt = \"optim\")) # converged w/ warnings\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 1\n\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 47\n\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 47\n\n# we'll try more heterogenous matrices, but with the simplified G matrix and without intercept to avoid optimizer issues\nautism_lme_quad_id_slope2_noint_nocor_het <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                                     random = list(childid = pdDiag(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age),\n                       control = lmeControl(opt = \"optim\"))\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 1\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 4\n\nautism_lme_quad_id_slope2_noint_nocor_het2 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                                     random = list(childid = pdDiag(form = ~0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | I(age == 2)),\n                       control = lmeControl(opt = \"optim\"))\n\n# sincethe model onl\nautism_lme_quad_id_slope2_noint_nocor_het2 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                                     random = list(childid = pdDiag(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age),\n                       control = lmeControl(opt = \"optim\"))\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 1\n\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 4\n\nautism_lme_quad_id_slope2_noint_nocor_ar1 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                                    random = list(childid = pdDiag(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       correlation = corAR1(form = ~ 1 | childid),\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age),\n                       control = lmeControl(opt = \"optim\"))\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 1\n\n## Warning in logLik.reStruct(object, conLin): Singular precision matrix in level\n## -1, block 4\n\nautism_lme_quad_id_slope2_noint_nocor_ma1 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid = pdDiag(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       correlation = corARMA(p = 0, q = 1),\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age))\n\nautism_lme_quad_id_slope2_noint_nocor_ma2 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid = pdDiag(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       correlation = corARMA(p = 0, q = 2),\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age))\n\nautism_lme_quad_id_slope2_noint_nocor_ar1ma2 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid = pdDiag(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       correlation = corARMA(p = 1, q = 2),\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age))\n\nautism_lme_quad_id_slope2_noint_nocor_car1 <- lme(vsae ~ age*sicdegp + I(age^2):sicdegp,\n                       random = list(childid = pdDiag(form = ~ 0 + age + I(age^2))), # random linear and quadratic term, no intercept\n                       correlation = corCAR1(form = ~ 1 | childid),\n                       data = autism_complete,\n                       weights = varIdent(form = ~ 1 | age))\n\n\n\n\nCode\nautism_lme_cor_ic <- AIC(\n  autism_lmer_quad_id_slope2,\n  autism_lme_quad_id_slope2_noint_het,\n  autism_lme_quad_id_slope2_noint_het2,\n  autism_lme_quad_id_slope2_noint_nocor,\n  autism_lme_quad_id_slope2_noint_nocor_het,\n  autism_lme_quad_id_slope2_noint_nocor_het2,\n     autism_lme_quad_id_slope2_noint_nocor_ma1,\n     autism_lme_quad_id_slope2_noint_nocor_ma2,\n     autism_lme_quad_id_slope2_noint_nocor_ar1,\n  autism_lme_quad_id_slope2_noint_nocor_ar1ma2,\n     autism_lme_quad_id_slope2_noint_nocor_car1) %>% \n  add_column(BIC = BIC(\n  autism_lmer_quad_id_slope2,\n  autism_lme_quad_id_slope2_noint_het,\n  autism_lme_quad_id_slope2_noint_het2,\n    autism_lme_quad_id_slope2_noint_nocor,\n    autism_lme_quad_id_slope2_noint_nocor_het,\n  autism_lme_quad_id_slope2_noint_nocor_het2,\n    autism_lme_quad_id_slope2_noint_nocor_ma1,\n    autism_lme_quad_id_slope2_noint_nocor_ma2,\n    autism_lme_quad_id_slope2_noint_nocor_ar1,\n    autism_lme_quad_id_slope2_noint_nocor_ar1ma2,\n    autism_lme_quad_id_slope2_noint_nocor_car1)$BIC)\n## Warning in AIC.default(autism_lmer_quad_id_slope2,\n## autism_lme_quad_id_slope2_noint_het, : models are not all fitted to the same\n## number of observations\n## Warning in BIC.default(autism_lmer_quad_id_slope2,\n## autism_lme_quad_id_slope2_noint_het, : models are not all fitted to the same\n## number of observations\n\nautism_lme_cor_ic %>% kbl() %>% \n  kable_classic(full_width=F) %>% \n  column_spec(3, color = c(\"black\", \"red\")[as.numeric(autism_lme_cor_ic$AIC == min(autism_lme_cor_ic$AIC)) + 1]) %>%\n  column_spec(4, color = c(\"black\", \"red\")[as.numeric(autism_lme_cor_ic$BIC == min(autism_lme_cor_ic$BIC)) + 1])\n\n\n\n\n \n  \n      \n    df \n    AIC \n    BIC \n  \n \n\n  \n    autism_lmer_quad_id_slope2 \n    16 \n    4639.983 \n    4710.598 \n  \n  \n    autism_lme_quad_id_slope2_noint_het \n    17 \n    4484.389 \n    4559.165 \n  \n  \n    autism_lme_quad_id_slope2_noint_het2 \n    14 \n    4536.707 \n    4598.287 \n  \n  \n    autism_lme_quad_id_slope2_noint_nocor \n    12 \n    4678.391 \n    4731.174 \n  \n  \n    autism_lme_quad_id_slope2_noint_nocor_het \n    16 \n    4482.832 \n    4553.209 \n  \n  \n    autism_lme_quad_id_slope2_noint_nocor_het2 \n    16 \n    4482.832 \n    4553.209 \n  \n  \n    autism_lme_quad_id_slope2_noint_nocor_ma1 \n    17 \n    4483.408 \n    4558.184 \n  \n  \n    autism_lme_quad_id_slope2_noint_nocor_ma2 \n    18 \n    4484.452 \n    4563.627 \n  \n  \n    autism_lme_quad_id_slope2_noint_nocor_ar1 \n    17 \n    4483.537 \n    4558.313 \n  \n  \n    autism_lme_quad_id_slope2_noint_nocor_ar1ma2 \n    19 \n    4486.409 \n    4569.982 \n  \n  \n    autism_lme_quad_id_slope2_noint_nocor_car1 \n    17 \n    4483.531 \n    4558.307 \n  \n\n\n\n\n\nIt seems like our efforts paid off by improving the model, but some of the parameterizations did not improve the fit as much as we may have been hoping. Many are quite close, so I honestly believe we’ve done most of the corrections appropriate for this model and we’re fighting for scraps now, at least with these modeling techniques. The best fit by both AIC and BIC (among what we tried) is the simplified G model with heterogeneous structure in R.\nIf you want more formal quantification of model comparisons, use the likelihood ratio test.\n\n\nCode\nanova(autism_lme_quad_id_slope2_noint_nocor, autism_lme_quad_id_slope2_noint_nocor_het)\n\n\n                                          Model df      AIC      BIC    logLik\nautism_lme_quad_id_slope2_noint_nocor         1 12 4678.391 4731.174 -2327.196\nautism_lme_quad_id_slope2_noint_nocor_het     2 16 4482.832 4553.209 -2225.416\n                                            Test  L.Ratio p-value\nautism_lme_quad_id_slope2_noint_nocor                            \nautism_lme_quad_id_slope2_noint_nocor_het 1 vs 2 203.5595  <.0001"
  },
  {
    "objectID": "longitudinal/randomcoef.html#final-model-1",
    "href": "longitudinal/randomcoef.html#final-model-1",
    "title": "14  Random Coefficient Models (CALS)",
    "section": "18.4 Final Model",
    "text": "18.4 Final Model\nWe have probably done enough modeling to come up with a final model, we’ll use the best fit from the last iteration of improvements, though our final model that was selected from AIC/BIC still has some singularity issues and troubles with the optimizer. We’ll have to spend some extra effort to track down why that’s happening.\n\n\nCode\ngetVarCov(autism_gls_quad_un)\n\n\nMarginal variance covariance matrix\n        [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 10.8700   9.0082   9.3212   25.916   48.667\n[2,]  9.0082  55.7270  45.7570  110.010  201.300\n[3,]  9.3212  45.7570 135.7700  222.120  397.840\n[4,] 25.9160 110.0100 222.1200  771.040 1197.400\n[5,] 48.6670 201.3000 397.8400 1197.400 2287.700\n  Standard Deviations: 3.297 7.4651 11.652 27.768 47.83 \n\n\nCode\ngetVarCov(autism_lme_quad_id_slope2_noint_nocor_het, type = \"marginal\") # seems to reflect the pattern in covariance quite well.\n\n\nchildid 1 \nMarginal variance covariance matrix\n        1        2       3        4        5\n1 12.1870   6.9707  14.602   37.024   68.995\n2  6.9707  47.3600  27.497   73.663  141.310\n3 14.6020  27.4970 128.690  183.190  361.580\n4 37.0240  73.6630 183.190  752.910 1104.700\n5 68.9950 141.3100 361.580 1104.700 2252.800\n  Standard Deviations: 3.4909 6.8819 11.344 27.439 47.464 \n\n\n\n\nCode\nplot(autism_lme_quad_id_slope2_noint_nocor_het, form = resid(., type = \"normalized\")~fitted(.) | sicdegp, ylim = c(-8, 8))\n\n\n\n\n\n\n\nCode\nintervals(autism_lme_quad_id_slope2_noint_nocor_het)\n\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                        lower        est.     upper\n(Intercept)       -2.25946345  0.77647542 3.8124143\nage                1.68325878  3.26560911 4.8479594\nsicdegp2          -2.31980385  1.70808682 5.7359775\nsicdegp3          -7.07004509 -2.63431483 1.8014154\nage:sicdegp2      -2.17606966 -0.09458308 1.9869035\nage:sicdegp3       1.65369494  3.93648717 6.2192794\nsicdegp1:I(age^2) -0.15892339 -0.01247629 0.1339708\nsicdegp2:I(age^2) -0.08121223  0.04156100 0.1643342\nsicdegp3:I(age^2) -0.13239419  0.01868912 0.1697724\n\n Random Effects:\n  Level: childid \n                 lower      est.     upper\nsd(age)      0.5372926 0.8451180 1.3293025\nsd(I(age^2)) 0.2364546 0.2731189 0.3154684\n\n Variance function:\n         lower      est.       upper\n3  1.713870157 2.0708134    2.502096\n5  2.243430178 2.8094052    3.518165\n9  4.001851151 5.0275041    6.316026\n13 0.000146793 0.4443621 1345.144133\n\n Within-group standard error:\n   lower     est.    upper \n2.460852 2.852393 3.306230 \n\n\nit seems like the variance function confidence interval for age is quite wide, which hints to where the singularity the optimizer was warning about.\n\n\nCode\n# try this model instead\nalt_mod <- lmer(vsae ~ age*sicdegp + I(age^2):sicdegp + (1 + age + I(age^2) | sicdegp/childid), data = autism_complete) # singular, corr = -1\n\n\nboundary (singular) fit: see help('isSingular')\n\n\nCode\nplot_redres(alt_mod, type = \"raw_mar\", xvar = \"age\")\n\n\nLoading required namespace: testthat\n\n\n\n\n\nCode\nplot_redres(alt_mod, type = \"raw_mar\")\n\n\n\n\n\nCode\nhigh_idx <- fitted(alt_mod) > 100\n\nautism_complete %>% filter(high_idx)\n\n\n   age vsae sicdegp childid agef\n1    9  114       3       4    9\n2   13  107       3      19   13\n3   13  124       3      42   13\n4   13  153       3      49   13\n5   13  104       3      95   13\n6    9  135       3     100    9\n7    9  171       3     105    9\n8   13  107       3     106   13\n9   13  135       3     115   13\n10   9  116       3     136    9\n11  13  192       3     136   13\n12  13  165       3     151   13\n13  13  165       3     180   13\n14  13  130       3     190   13\n15   9  110       2      91    9\n16  13  198       2      91   13\n17  13  124       2      96   13\n18  13  107       2     101   13\n19  13  144       2     107   13\n20  13  171       2     139   13\n21  13  104       2     150   13\n22  13  165       2     193   13\n23  13  147       2     212   13\n24  13  120       1      80   13\n25  13  110       1     124   13\n26  13  126       1     187   13\n27   9  130       1     210    9\n\n\nCode\nalt_df <- autism_complete %>% \n  add_column(xb = predict(alt_mod, re.form = NA),\n             xb_zb = predict(alt_mod))\n\nalt_df %>% filter(high_idx) %>% \n  ggplot() +\n  geom_point(aes(age, xb)) +\n  geom_point(aes(age, vsae), color = \"blue\") + \n  geom_point(aes(age, xb_zb), color = \"red\") +\n  facet_wrap(~sicdegp)\n\n\n\n\n\nCode\ncompute_redres(alt_mod, type = \"raw_mar\")[high_idx]\n\n\n [1]  51.014254   6.922364  23.922364  52.922364   3.922364  72.014254\n [7] 108.014254   6.922364  34.922364  53.014254  91.922364  64.922364\n[13]  64.922364  29.922364  76.881986 148.440308  74.440308  57.440308\n[19]  94.440308 121.440308  54.440308 115.440308  97.440308  78.275818\n[25]  68.275818  84.275818 102.239244\n\n\n\n\nCode\nautism_complete\n\n\n    age vsae sicdegp childid agef\n1     2    6       3       1    2\n2     3    7       3       1    3\n3     5   18       3       1    5\n4     9   25       3       1    9\n5    13   27       3       1   13\n6     2   17       3       3    2\n7     3   18       3       3    3\n8     5   12       3       3    5\n9     9   18       3       3    9\n10   13   24       3       3   13\n11    2   12       3       4    2\n12    3   14       3       4    3\n13    5   38       3       4    5\n14    9  114       3       4    9\n15    2   17       3      19    2\n16    3   27       3      19    3\n17    9   75       3      19    9\n18   13  107       3      19   13\n19    2   11       3      21    2\n20    3   21       3      21    3\n21    9   50       3      21    9\n22   13   69       3      21   13\n23    2    8       3      27    2\n24    3   17       3      27    3\n25    5   24       3      27    5\n26    9   31       3      27    9\n27   13   29       3      27   13\n28    2   11       3      36    2\n29    3   17       3      36    3\n30    9   15       3      36    9\n31    2   10       3      42    2\n32    3   27       3      42    3\n33    9   77       3      42    9\n34   13  124       3      42   13\n35    2   13       3      46    2\n36    3   26       3      46    3\n37    5   71       3      46    5\n38    9   31       3      46    9\n39   13   77       3      46   13\n40    2   19       3      48    2\n41    3   22       3      48    3\n42    5   26       3      48    5\n43    9   52       3      48    9\n44    2    9       3      49    2\n45    3   26       3      49    3\n46    5   77       3      49    5\n47    9   81       3      49    9\n48   13  153       3      49   13\n49    2   10       3      51    2\n50    3   20       3      51    3\n51    5   21       3      51    5\n52    3   16       3      60    3\n53    9   34       3      60    9\n54   13   66       3      60   13\n55    2   16       3      61    2\n56    5   33       3      61    5\n57    2   10       3      62    2\n58    3   19       3      62    3\n59    5   25       3      62    5\n60    9   67       3      62    9\n61   13   88       3      62   13\n62    2   11       3      78    2\n63    3   12       3      78    3\n64    5   15       3      78    5\n65    2   12       3      95    2\n66    3   18       3      95    3\n67    5   38       3      95    5\n68    9   73       3      95    9\n69   13  104       3      95   13\n70    2   14       3      97    2\n71    3   11       3      97    3\n72    5   25       3      97    5\n73    9   67       3      97    9\n74   13   50       3      97   13\n75    2   15       3     100    2\n76    3   24       3     100    3\n77    5   37       3     100    5\n78    9  135       3     100    9\n79    2   17       3     105    2\n80    5   65       3     105    5\n81    9  171       3     105    9\n82    2   10       3     106    2\n83    3   39       3     106    3\n84    9   63       3     106    9\n85   13  107       3     106   13\n86    2   15       3     115    2\n87    3   24       3     115    3\n88    9   71       3     115    9\n89   13  135       3     115   13\n90    2   20       3     122    2\n91    3   21       3     122    3\n92    5   34       3     122    5\n93    9   65       3     122    9\n94   13   81       3     122   13\n95    2    9       3     123    2\n96    3   13       3     123    3\n97    9   27       3     123    9\n98   13   66       3     123   13\n99    2   13       3     131    2\n100   3   31       3     131    3\n101   5   22       3     131    5\n102   9   50       3     131    9\n103  13   71       3     131   13\n104   2   12       3     133    2\n105   3   18       3     133    3\n106   5   22       3     133    5\n107   2    9       3     136    2\n108   3   33       3     136    3\n109   9  116       3     136    9\n110  13  192       3     136   13\n111   2   12       3     141    2\n112   3   15       3     141    3\n113   5   26       3     141    5\n114   9   53       3     141    9\n115  13   72       3     141   13\n116   2   18       3     151    2\n117   3   20       3     151    3\n118   5   33       3     151    5\n119   9  101       3     151    9\n120  13  165       3     151   13\n121   2   15       3     154    2\n122   3   20       3     154    3\n123   5   36       3     154    5\n124   2   13       3     158    2\n125   3   24       3     158    3\n126   5   37       3     158    5\n127   9   57       3     158    9\n128   2   12       3     174    2\n129   3   20       3     174    3\n130   5   39       3     174    5\n131   9   90       3     174    9\n132   2   15       3     180    2\n133   3   63       3     180    3\n134   9   68       3     180    9\n135  13  165       3     180   13\n136   2   11       3     182    2\n137   3   19       3     182    3\n138   9   26       3     182    9\n139  13   39       3     182   13\n140   2   15       3     183    2\n141   3   22       3     183    3\n142   5   35       3     183    5\n143   9   65       3     183    9\n144  13   62       3     183   13\n145   2    8       3     190    2\n146   3   22       3     190    3\n147   5   35       3     190    5\n148   9   69       3     190    9\n149  13  130       3     190   13\n150   2    8       3     195    2\n151   3   19       3     195    3\n152   9   59       3     195    9\n153   2    8       3     198    2\n154   3   14       3     198    3\n155   9   34       3     198    9\n156  13   55       3     198   13\n157   2   12       3     201    2\n158   3   14       3     201    3\n159   5   38       3     201    5\n160   9   93       3     201    9\n161   2    8       3     207    2\n162   3   14       3     207    3\n163   9   22       3     207    9\n164  13   48       3     207   13\n165   2   15       3     211    2\n166   2   12       2       9    2\n167   3   21       2       9    3\n168   9   66       2       9    9\n169  13   68       2       9   13\n170   2    7       2      12    2\n171   3   10       2      12    3\n172   5    8       2      12    5\n173   2   12       2      14    2\n174   3   19       2      14    3\n175   5   14       2      14    5\n176   9   28       2      14    9\n177  13   68       2      14   13\n178   2   13       2      15    2\n179   3    8       2      15    3\n180   5   29       2      15    5\n181   9   24       2      15    9\n182  13   44       2      15   13\n183   2    7       2      16    2\n184   3    6       2      16    3\n185   9   39       2      16    9\n186  13   24       2      16   13\n187   2    5       2      17    2\n188   3   10       2      17    3\n189   5   29       2      17    5\n190   9   32       2      17    9\n191  13   67       2      17   13\n192   2   10       2      18    2\n193   3    6       2      18    3\n194   2   11       2      24    2\n195   3   13       2      24    3\n196   5   20       2      24    5\n197   9   33       2      24    9\n198   2    3       2      28    2\n199   3   10       2      28    3\n200   5   17       2      28    5\n201  13   42       2      28   13\n202   2   10       2      30    2\n203   3   15       2      30    3\n204   9   40       2      30    9\n205  13  101       2      30   13\n206   2    7       2      33    2\n207   3   10       2      33    3\n208   5   12       2      33    5\n209   9   15       2      33    9\n210  13   10       2      33   13\n211   2    7       2      35    2\n212   3   10       2      35    3\n213   2    6       2      37    2\n214  13   15       2      37   13\n215   2    7       2      39    2\n216   3   12       2      39    3\n217   5    9       2      39    5\n218   9    6       2      39    9\n219   2    8       2      40    2\n220   3   22       2      40    3\n221   5   24       2      40    5\n222   9   54       2      40    9\n223  13   54       2      40   13\n224   2   17       2      44    2\n225   3   18       2      44    3\n226   9   21       2      44    9\n227   2    8       2      47    2\n228   3    9       2      47    3\n229   5   19       2      47    5\n230   2    7       2      55    2\n231   3    9       2      55    3\n232   5   13       2      55    5\n233   9   29       2      55    9\n234  13   36       2      55   13\n235   2   10       2      58    2\n236   3   19       2      58    3\n237   9   53       2      58    9\n238  13   95       2      58   13\n239   2    6       2      63    2\n240   3    7       2      63    3\n241   5   14       2      63    5\n242   9   32       2      63    9\n243   2    7       2      64    2\n244   3   14       2      64    3\n245   5    9       2      64    5\n246  13   40       2      64   13\n247   2    6       2      65    2\n248   3   10       2      65    3\n249   5   11       2      65    5\n250   9   15       2      65    9\n251   2    3       2      67    2\n252   3   11       2      67    3\n253   9   15       2      67    9\n254  13   17       2      67   13\n255   2   14       2      76    2\n256   3   21       2      76    3\n257   5   22       2      76    5\n258   2    9       2      81    2\n259   3   13       2      81    3\n260   9   22       2      81    9\n261  13   44       2      81   13\n262   2   10       2      85    2\n263   5   22       2      85    5\n264   2   12       2      87    2\n265   3   27       2      87    3\n266   5   18       2      87    5\n267   9    8       2      87    9\n268  13    9       2      87   13\n269   2   10       2      91    2\n270   3   25       2      91    3\n271   9  110       2      91    9\n272  13  198       2      91   13\n273   2    6       2      92    2\n274   5    4       2      92    5\n275   9   15       2      92    9\n276  13   18       2      92   13\n277   2   17       2      96    2\n278   3   18       2      96    3\n279   5   27       2      96    5\n280   9   66       2      96    9\n281  13  124       2      96   13\n282   2    8       2     101    2\n283   3   24       2     101    3\n284   9   75       2     101    9\n285  13  107       2     101   13\n286   2   12       2     107    2\n287   3   15       2     107    3\n288   5   31       2     107    5\n289  13  144       2     107   13\n290   2   10       2     108    2\n291   3   20       2     108    3\n292   9   40       2     108    9\n293  13   73       2     108   13\n294   2   12       2     110    2\n295   3   22       2     110    3\n296   9   63       2     110    9\n297  13   44       2     110   13\n298   2   11       2     113    2\n299   3   13       2     113    3\n300   5   10       2     113    5\n301   9   18       2     113    9\n302  13   24       2     113   13\n303   2    1       2     116    2\n304   3    9       2     116    3\n305   9   15       2     116    9\n306  13   14       2     116   13\n307   2    4       2     119    2\n308   3    6       2     119    3\n309   5   12       2     119    5\n310   2   12       2     120    2\n311   3    9       2     120    3\n312   5    8       2     120    5\n313   2    9       2     126    2\n314   3    8       2     126    3\n315   5   18       2     126    5\n316   9   12       2     126    9\n317   2    9       2     128    2\n318   3   14       2     128    3\n319   9   14       2     128    9\n320   2    4       2     129    2\n321   3   14       2     129    3\n322   2   13       2     139    2\n323   3   18       2     139    3\n324   5   30       2     139    5\n325   9   73       2     139    9\n326  13  171       2     139   13\n327   2    3       2     142    2\n328   3   13       2     142    3\n329   9   12       2     142    9\n330  13   17       2     142   13\n331   2    7       2     145    2\n332   3    8       2     145    3\n333   5   12       2     145    5\n334   9   21       2     145    9\n335  13   28       2     145   13\n336   2   11       2     146    2\n337   3   18       2     146    3\n338   9   21       2     146    9\n339  13   32       2     146   13\n340   2    9       2     147    2\n341   3   12       2     147    3\n342   5   11       2     147    5\n343   9   13       2     147    9\n344   2    6       2     148    2\n345   3    9       2     148    3\n346   5    9       2     148    5\n347   9   21       2     148    9\n348  13   15       2     148   13\n349   2   18       2     150    2\n350   3   21       2     150    3\n351   5   35       2     150    5\n352   9   66       2     150    9\n353  13  104       2     150   13\n354   2    7       2     152    2\n355   3   18       2     152    3\n356   9   10       2     152    9\n357  13   24       2     152   13\n358   2   10       2     155    2\n359   3   25       2     155    3\n360   9   59       2     155    9\n361  13   40       2     155   13\n362   2   10       2     159    2\n363   3   11       2     159    3\n364   5   16       2     159    5\n365   2    8       2     165    2\n366   3   13       2     165    3\n367   9   18       2     165    9\n368   2    6       2     169    2\n369   3    9       2     169    3\n370   5   17       2     169    5\n371   9   13       2     169    9\n372  13   14       2     169   13\n373   2    7       2     170    2\n374   3   14       2     170    3\n375   2    9       2     173    2\n376   3   11       2     173    3\n377   2    4       2     175    2\n378   3   10       2     175    3\n379   5   24       2     175    5\n380   9   33       2     175    9\n381  13   54       2     175   13\n382   2   15       2     178    2\n383   3   15       2     178    3\n384   9   54       2     178    9\n385   2   11       2     181    2\n386   3   11       2     181    3\n387   5   26       2     181    5\n388   9   13       2     181    9\n389   2    8       2     191    2\n390   3   11       2     191    3\n391   5   11       2     191    5\n392   9   12       2     191    9\n393  13   14       2     191   13\n394   3   39       2     193    3\n395  13  165       2     193   13\n396   2    5       2     194    2\n397   3   13       2     194    3\n398   2    7       2     196    2\n399   3   12       2     196    3\n400   9   15       2     196    9\n401  13    9       2     196   13\n402   2    9       2     204    2\n403   3   10       2     204    3\n404   5   17       2     204    5\n405   9   26       2     204    9\n406   2   11       2     205    2\n407   3   15       2     205    3\n408   9   18       2     205    9\n409  13   66       2     205   13\n410   2   10       2     208    2\n411   3   13       2     208    3\n412   2    2       2     209    2\n413   3    4       2     209    3\n414   9   12       2     209    9\n415  13   32       2     209   13\n416   2    7       2     212    2\n417   3   21       2     212    3\n418   5   29       2     212    5\n419   9   72       2     212    9\n420  13  147       2     212   13\n421   2    6       1       2    2\n422   3    7       1       2    3\n423   5    7       1       2    5\n424   9    8       1       2    9\n425  13   14       1       2   13\n426   2    6       1       6    2\n427   3   12       1       6    3\n428   9   12       1       6    9\n429  13   45       1       6   13\n430   2    9       1       8    2\n431   3   12       1       8    3\n432   5   14       1       8    5\n433   2    9       1      10    2\n434   3   11       1      10    3\n435   9   18       1      10    9\n436  13   39       1      10   13\n437   2    6       1      13    2\n438   3   10       1      13    3\n439   5   12       1      13    5\n440   2   11       1      22    2\n441   3   14       1      22    3\n442   5   22       1      22    5\n443   2    6       1      31    2\n444   3   13       1      31    3\n445   5   15       1      31    5\n446   9   54       1      31    9\n447   2    5       1      32    2\n448   3   11       1      32    3\n449   5   12       1      32    5\n450   2   10       1      38    2\n451   3    6       1      38    3\n452   5   11       1      38    5\n453   9    3       1      38    9\n454  13    8       1      38   13\n455   2    8       1      41    2\n456   3   23       1      41    3\n457   5   28       1      41    5\n458   2    6       1      43    2\n459   3    9       1      43    3\n460   5   10       1      43    5\n461   9   10       1      43    9\n462  13   15       1      43   13\n463   2    5       1      45    2\n464   3   10       1      45    3\n465   9   13       1      45    9\n466  13   16       1      45   13\n467   2    9       1      50    2\n468   3   14       1      50    3\n469   9   15       1      50    9\n470  13   54       1      50   13\n471   2    7       1      57    2\n472   3   11       1      57    3\n473   5   15       1      57    5\n474   9   26       1      57    9\n475   2    7       1      59    2\n476   3    8       1      59    3\n477   5   26       1      59    5\n478   9   11       1      59    9\n479  13   12       1      59   13\n480   2    6       1      66    2\n481   3    9       1      66    3\n482   5   12       1      66    5\n483   9   11       1      66    9\n484  13   13       1      66   13\n485   2    7       1      70    2\n486   3   11       1      70    3\n487   9    7       1      70    9\n488   2    5       1      71    2\n489   3   11       1      71    3\n490   9   42       1      71    9\n491   2   10       1      77    2\n492   3   12       1      77    3\n493   5   13       1      77    5\n494   9   16       1      77    9\n495  13   83       1      77   13\n496   2   13       1      80    2\n497   3   45       1      80    3\n498   9   73       1      80    9\n499  13  120       1      80   13\n500   2   10       1      82    2\n501   3    8       1      82    3\n502   5   12       1      82    5\n503   9   14       1      82    9\n504   2    5       1      86    2\n505   3   10       1      86    3\n506   5   13       1      86    5\n508   2    4       1      88    2\n509   3   12       1      88    3\n510   2    9       1      99    2\n511   3   13       1      99    3\n512   5   10       1      99    5\n513   9   12       1      99    9\n514   2   12       1     104    2\n515   3   14       1     104    3\n516   5   13       1     104    5\n517   9   21       1     104    9\n518   2    6       1     109    2\n519   3    5       1     109    3\n520   5    6       1     109    5\n521   9    7       1     109    9\n522  13    8       1     109   13\n523   2   10       1     111    2\n524   3   10       1     111    3\n525   5   12       1     111    5\n526   9   13       1     111    9\n527  13   12       1     111   13\n528   2    7       1     112    2\n529   3    7       1     112    3\n530   5   11       1     112    5\n531   9   16       1     112    9\n532   2    6       1     114    2\n533   2    9       1     117    2\n534   3   12       1     117    3\n535   9   12       1     117    9\n536  13   12       1     117   13\n537   2    5       1     124    2\n538   9  114       1     124    9\n539  13  110       1     124   13\n540   2    4       1     134    2\n541   3   12       1     134    3\n542   9   23       1     134    9\n543  13   21       1     134   13\n544   2    4       1     135    2\n545   3   10       1     135    3\n546   9   13       1     135    9\n547  13   35       1     135   13\n548   2    5       1     143    2\n549   3   11       1     143    3\n550   9   20       1     143    9\n551  13   45       1     143   13\n552   2    8       1     153    2\n554   5   17       1     153    5\n555   9   22       1     153    9\n556  13   29       1     153   13\n557   2    8       1     156    2\n558   3   16       1     156    3\n559   5   42       1     156    5\n560   2   13       1     161    2\n561   3   15       1     161    3\n562   5   30       1     161    5\n563  13   79       1     161   13\n564   2    4       1     167    2\n565   3    8       1     167    3\n566   2    1       1     168    2\n567   3    6       1     168    3\n568   5   12       1     168    5\n569   9   11       1     168    9\n570  13   10       1     168   13\n571   2    4       1     171    2\n572   3    8       1     171    3\n573   5   16       1     171    5\n574   9   33       1     171    9\n575  13   49       1     171   13\n576   2    7       1     172    2\n577   3   11       1     172    3\n578   9   10       1     172    9\n579  13   15       1     172   13\n580   2    1       1     179    2\n581   3    6       1     179    3\n582   5    7       1     179    5\n583  13    7       1     179   13\n584   2    4       1     184    2\n585   3   12       1     184    3\n586   5   13       1     184    5\n587   9   15       1     184    9\n588   2    6       1     185    2\n589   3   15       1     185    3\n590   9   18       1     185    9\n591  13   42       1     185   13\n592   2    9       1     186    2\n593   3   18       1     186    3\n594   5   19       1     186    5\n595   9   20       1     186    9\n596   2    6       1     187    2\n597   3   13       1     187    3\n598   9   65       1     187    9\n599  13  126       1     187   13\n600   2    9       1     197    2\n601   3   12       1     197    3\n602   9   12       1     197    9\n603  13    8       1     197   13\n604   2   11       1     200    2\n605   3    8       1     200    3\n606   2    8       1     202    2\n607   3    9       1     202    3\n608   5    6       1     202    5\n609  13   12       1     202   13\n610   2    4       1     210    2\n611   3   25       1     210    3\n612   9  130       1     210    9\n\n\nCode\nfitted(autism_lmer_quad_id_slope2)\n\n\n         1          2          3          4          5          6          7 \n 12.313226  14.430375  18.241976  24.174403  27.852459  12.395592  13.304193 \n         8          9         10         11         12         13         14 \n 15.222047  19.460365  24.235497  16.170573  24.152411  45.161463 107.361071 \n        15         16         17         18         19         20         21 \n 13.629456  21.967135  73.091758 108.221079  12.989889  18.398589  49.778019 \n        22         23         24         25         26         27         28 \n 69.675951  12.069754  16.235883  23.062853  30.695639  30.300220  12.094076 \n        29         30         31         32         33         34         35 \n 13.545404  17.829659  14.166637  21.833236  77.436628 123.652032  12.602822 \n        36         37         38         39         40         41         42 \n 20.702286  35.199620  57.387903  70.501009  13.542575  17.920102  27.875873 \n        43         44         45         46         47         48         49 \n 52.590288  14.241925  25.942024  49.626245  98.130782 148.150113  13.262949 \n        50         51         52         53         54         55         56 \n 16.697904  24.529874  15.235062  38.083033  64.518021  13.878495  32.947150 \n        57         58         59         60         61         62         63 \n 13.466873  19.571263  32.311947  59.920928  90.366726  12.991207  14.447106 \n        64         65         66         67         68         69         70 \n 18.413099  13.640252  21.354394  37.150001  70.210506 105.230066  12.242385 \n        71         72         73         74         75         76         77 \n 20.135835  33.605942  51.278979  56.595781  17.139047  26.143664  50.708238 \n        78         79         80         81         82         83         84 \n126.058748  17.628913  69.290723 164.001448  13.789259  20.580422  67.830205 \n        85         86         87         88         89         90         91 \n105.523210  14.782266  20.111561  74.784843 132.850376  13.068419  20.505418 \n        92         93         94         95         96         97         98 \n 34.755578  60.760541  83.438364  13.600581  13.822077  33.188316  63.277493 \n        99        100        101        102        103        104        105 \n 13.136834  17.959221  27.905883  49.006763  71.717714  13.339514  16.786846 \n       106        107        108        109        110        111        112 \n 24.783822  15.485703  27.020928 114.949508 191.394498  13.165329  18.099080 \n       113        114        115        116        117        118        119 \n 28.288123  49.952377  73.331519  15.382148  22.903610  41.649184  93.950938 \n       120        121        122        123        124        125        126 \n166.000169  13.956704  20.546750  35.032095  13.207279  19.921226  33.206897 \n       127        128        129        130        131        132        133 \n 59.209347  14.826611  22.645858  40.851234  87.529510  15.479535  20.898888 \n       134        135        136        137        138        139        140 \n 85.101530 158.080933  12.584575  14.908647  29.025613  38.601241  12.372833 \n       141        142        143        144        145        146        147 \n 21.172602  36.424444  57.537339  66.129185  14.619951  20.064712  33.856215 \n       148        149        150        151        152        153        154 \n 73.047142 127.715298  13.561293  19.199636  58.990763  12.995255  15.557264 \n       155        156        157        158        159        160        161 \n 36.001681  54.462111  15.119631  22.468128  40.426692  89.390102  13.070192 \n       162        163        164        165        166        167        168 \n 13.597671  27.163064  46.111915  13.832097   9.426625  18.542731  58.677894 \n       169        170        171        172        173        174        175 \n 71.566596   9.245111   9.627182  10.632153  10.530568  11.251127  15.247897 \n       176        177        178        179        180        181        182 \n 33.464048  65.310348   9.534244  12.550592  18.564961  30.520388  42.378067 \n       183        184        185        186        187        188        189 \n  8.736631  14.254922  31.395095  27.612759  10.153066  13.160093  20.322694 \n       190        191        192        193        194        195        196 \n 39.242083  64.287056   9.493371  10.689731   9.712336  12.791451  19.247219 \n       197        198        199        200        201        202        203 \n 33.348907   9.614452  12.003108  17.104312  41.827684  11.398791  12.298342 \n       204        205        206        207        208        209        210 \n 46.635458  97.088592   8.863234  10.452103  12.784462  14.067662  10.842172 \n       211        212        213        214        215        216        217 \n  9.546941  11.762262   8.973412  15.439253   8.788553   9.241661   9.508242 \n       218        219        220        221        222        223        224 \n  7.482875   9.363647  16.219079  28.420411  46.784940  57.098625   9.309624 \n       225        226        227        228        229        230        231 \n 11.380442  22.737046   9.658775  12.327023  17.985555   9.510663  11.673792 \n       232        233        234        235        236        237        238 \n 16.197459  26.034428  36.924243  10.777448  15.131933  54.644818  93.735290 \n       239        240        241        242        243        244        245 \n  9.946879  11.738965  16.454198  30.408907   9.784800  10.634290  13.446794 \n       246        247        248        249        250        251        252 \n 39.543782   9.173339  10.138338  11.996459  15.425196   9.066062  10.446956 \n       253        254        255        256        257        258        259 \n 15.929162  16.914295  10.088128  14.269057  23.299460   9.848249  10.850226 \n       260        261        262        263        264        265        266 \n 25.166817  42.620479   9.879266  21.267206   8.679327  10.938629  14.061540 \n       267        268        269        270        271        272        273 \n 14.724598   7.943970  12.874612  22.403533 109.535512 196.155360   9.319229 \n       274        275        276        277        278        279        280 \n  9.846023  12.850566  18.486140  11.557316  16.120018  28.561203  66.706694 \n       281        282        283        284        285        286        287 \n122.536347  10.542735  19.081239  71.679521 108.047192  11.987365  17.383216 \n       288        289        290        291        292        293        294 \n 32.045666 142.305417  10.324534  13.566102  42.812639  71.640924   8.627624 \n       295        296        297        298        299        300        301 \n 18.879554  52.885235  49.359591   9.323848  10.358239  12.617105  17.895176 \n       302        303        304        305        306        307        308 \n 24.187033   8.974839  10.452840  15.109307  14.202632   9.249247  10.233207 \n       309        310        311        312        313        314        315 \n 12.266257   9.391514   9.589576  10.558014   8.801750  10.643243  13.288186 \n       316        317        318        319        320        321        322 \n 14.425899   9.024984  10.423305  15.432000   9.600511  12.834794  12.976064 \n       323        324        325        326        327        328        329 \n 16.228751  29.117984  80.431878 165.793013   9.124003  10.018363  14.382501 \n       330        331        332        333        334        335        336 \n 16.337619   9.398701  10.691067  13.528045  20.210982  28.239226   9.387462 \n       337        338        339        340        341        342        343 \n 11.359653  23.359869  31.519122   9.082187  10.024664  11.673147  14.024218 \n       344        345        346        347        348        349        350 \n  9.008839  10.724108  13.544624  16.745577  16.693090  10.670133  17.637212 \n       351        352        353        354        355        356        357 \n 32.485366  65.837659 104.064600   9.385313   9.666184  15.099252  22.290668 \n       358        359        360        361        362        363        364 \n  8.549260  18.490214  50.261778  44.896004   9.636706  11.776664  16.498147 \n       365        366        367        368        369        370        371 \n  9.175527  10.869210  19.002063   8.927391  10.628135  13.270822  15.520996 \n       372        373        374        375        376        377        378 \n 13.724235   9.707307  12.955068   9.658230  12.140647   9.799907  13.052686 \n       379        380        381        382        383        384        385 \n 19.967567  35.434621  53.084734  10.526255  15.286603  53.061066   8.490054 \n       386        387        388        389        390        391        392 \n 11.855571  16.497665  17.426083   9.066235   9.890038  11.307342  13.220742 \n       393        394        395        396        397        398        399 \n 13.905866  22.580534 164.298198   9.596018  12.576684   8.777818  10.734808 \n       400        401        402        403        404        405        406 \n 14.648869   9.803123   9.511384  11.735515  16.363946  26.341487  10.736915 \n       407        408        409        410        411        412        413 \n  9.456753  26.651682  61.806303   9.774011  12.777141   9.772482   8.967104 \n       414        415        416        417        418        419        420 \n 15.298580  30.151702  12.150547  16.758810  30.393123  75.332905 143.834228 \n       421        422        423        424        425        426        427 \n  7.950103   7.743019   7.753710   9.474538  13.461292   8.878659   7.835014 \n       428        429        430        431        432        433        434 \n 18.558896  41.885056   8.319251  10.380780  14.931520   8.519414   9.044830 \n       435        436        437        438        439        440        441 \n 21.104604  37.627575   8.131068   9.655584  12.942021   8.690439  12.612237 \n       442        443        444        445        446        447        448 \n 21.014656   9.631713  12.873242  21.893077  50.079863   8.110661   9.737559 \n       449        450        451        452        453        454        455 \n 13.159163   7.756369   7.626380   7.403338   7.104996   7.003642   8.910275 \n       456        457        458        459        460        461        462 \n 14.933917  27.312822   7.867558   8.398595   9.505249  11.896877  14.526264 \n       463        464        465        466        467        468        469 \n  7.822006   8.907238  13.948313  15.908728   9.108414   8.160644  22.280460 \n       470        471        472        473        474        475        476 \n 50.556945   8.278063  10.513856  15.282527  26.008208   7.402012  10.358043 \n       477        478        479        480        481        482        483 \n 14.699736  17.101654  11.128277   7.737870   8.802123  10.568715  12.654241 \n       484        485        486        487        488        489        490 \n 12.809556   7.583098   8.221749   8.390500   8.796311  12.593755  40.949588 \n       491        492        493        494        495        496        497 \n 10.018678   7.920314   8.616073  29.577545  76.632290   9.549987  18.360377 \n       498        499        500        501        502        503        504 \n 76.020802 119.030685   7.897566   8.973225  11.059548  14.972210   8.135320 \n       505        506        508        509        510        511        512 \n  9.897384  13.594502   8.248076  10.935928   7.784617   8.816855  10.617110 \n       513        514        515        516        517        518        519 \n 13.160734   8.133265   9.919524  13.653791  21.769312   7.800100   7.565070 \n       520        521        522        523        524        525        526 \n  7.246226   7.213400   7.987058   7.672886   9.091652  11.336963  13.458702 \n       527        528        529        530        531        532        533 \n 12.421929   8.014459   9.007845  11.174583  16.227927   8.263066   7.693886 \n       534        535        536        537        538        539        540 \n  8.934786  12.890696  12.204648   8.279086  97.049843 117.339002   7.701847 \n       541        542        543        544        545        546        547 \n 10.702548  21.566891  22.009918   8.506604   8.265251  17.191919  33.023782 \n       548        549        550        551        552        554        555 \n  8.668426   9.253363  23.505340  43.237478   7.987193  14.988224  22.869013 \n       556        557        558        559        560        561        562 \n 29.088181   9.333139  17.822941  35.177812   8.896548  14.622649  26.471353 \n       563        564        565        566        567        568        569 \n 79.152863   8.087710   9.743122   7.647100   8.791028  10.521940  11.755987 \n       570        571        572        573        574        575        576 \n 10.019664   8.524305  11.157592  17.063273  31.431060  49.207413   7.862572 \n       577        578        579        580        581        582        583 \n  8.428136  11.994262  14.536197   7.704075   7.924851   8.198044   7.046037 \n       584        585        586        587        588        589        590 \n  7.753380   9.475242  12.385761  16.073983   8.569591   9.220894  22.428053 \n       591        592        593        594        595        596        597 \n 40.089343   7.719017  10.848544  16.080643  22.437024  10.349085  14.826560 \n       598        599        600        601        602        603        604 \n 66.257393 123.940789   7.549049   9.072484  12.213593   8.593856   8.336900 \n       605        606        607        608        609        610        611 \n 10.023762   7.862503   7.979278   8.374183  12.105195  11.566102  24.507304 \n       612 \n124.684849 \n\n\nCode\npredict(autism_lmer_quad_id_slope2)\n\n\n         1          2          3          4          5          6          7 \n 12.313226  14.430375  18.241976  24.174403  27.852459  12.395592  13.304193 \n         8          9         10         11         12         13         14 \n 15.222047  19.460365  24.235497  16.170573  24.152411  45.161463 107.361071 \n        15         16         17         18         19         20         21 \n 13.629456  21.967135  73.091758 108.221079  12.989889  18.398589  49.778019 \n        22         23         24         25         26         27         28 \n 69.675951  12.069754  16.235883  23.062853  30.695639  30.300220  12.094076 \n        29         30         31         32         33         34         35 \n 13.545404  17.829659  14.166637  21.833236  77.436628 123.652032  12.602822 \n        36         37         38         39         40         41         42 \n 20.702286  35.199620  57.387903  70.501009  13.542575  17.920102  27.875873 \n        43         44         45         46         47         48         49 \n 52.590288  14.241925  25.942024  49.626245  98.130782 148.150113  13.262949 \n        50         51         52         53         54         55         56 \n 16.697904  24.529874  15.235062  38.083033  64.518021  13.878495  32.947150 \n        57         58         59         60         61         62         63 \n 13.466873  19.571263  32.311947  59.920928  90.366726  12.991207  14.447106 \n        64         65         66         67         68         69         70 \n 18.413099  13.640252  21.354394  37.150001  70.210506 105.230066  12.242385 \n        71         72         73         74         75         76         77 \n 20.135835  33.605942  51.278979  56.595781  17.139047  26.143664  50.708238 \n        78         79         80         81         82         83         84 \n126.058748  17.628913  69.290723 164.001448  13.789259  20.580422  67.830205 \n        85         86         87         88         89         90         91 \n105.523210  14.782266  20.111561  74.784843 132.850376  13.068419  20.505418 \n        92         93         94         95         96         97         98 \n 34.755578  60.760541  83.438364  13.600581  13.822077  33.188316  63.277493 \n        99        100        101        102        103        104        105 \n 13.136834  17.959221  27.905883  49.006763  71.717714  13.339514  16.786846 \n       106        107        108        109        110        111        112 \n 24.783822  15.485703  27.020928 114.949508 191.394498  13.165329  18.099080 \n       113        114        115        116        117        118        119 \n 28.288123  49.952377  73.331519  15.382148  22.903610  41.649184  93.950938 \n       120        121        122        123        124        125        126 \n166.000169  13.956704  20.546750  35.032095  13.207279  19.921226  33.206897 \n       127        128        129        130        131        132        133 \n 59.209347  14.826611  22.645858  40.851234  87.529510  15.479535  20.898888 \n       134        135        136        137        138        139        140 \n 85.101530 158.080933  12.584575  14.908647  29.025613  38.601241  12.372833 \n       141        142        143        144        145        146        147 \n 21.172602  36.424444  57.537339  66.129185  14.619951  20.064712  33.856215 \n       148        149        150        151        152        153        154 \n 73.047142 127.715298  13.561293  19.199636  58.990763  12.995255  15.557264 \n       155        156        157        158        159        160        161 \n 36.001681  54.462111  15.119631  22.468128  40.426692  89.390102  13.070192 \n       162        163        164        165        166        167        168 \n 13.597671  27.163064  46.111915  13.832097   9.426625  18.542731  58.677894 \n       169        170        171        172        173        174        175 \n 71.566596   9.245111   9.627182  10.632153  10.530568  11.251127  15.247897 \n       176        177        178        179        180        181        182 \n 33.464048  65.310348   9.534244  12.550592  18.564961  30.520388  42.378067 \n       183        184        185        186        187        188        189 \n  8.736631  14.254922  31.395095  27.612759  10.153066  13.160093  20.322694 \n       190        191        192        193        194        195        196 \n 39.242083  64.287056   9.493371  10.689731   9.712336  12.791451  19.247219 \n       197        198        199        200        201        202        203 \n 33.348907   9.614452  12.003108  17.104312  41.827684  11.398791  12.298342 \n       204        205        206        207        208        209        210 \n 46.635458  97.088592   8.863234  10.452103  12.784462  14.067662  10.842172 \n       211        212        213        214        215        216        217 \n  9.546941  11.762262   8.973412  15.439253   8.788553   9.241661   9.508242 \n       218        219        220        221        222        223        224 \n  7.482875   9.363647  16.219079  28.420411  46.784940  57.098625   9.309624 \n       225        226        227        228        229        230        231 \n 11.380442  22.737046   9.658775  12.327023  17.985555   9.510663  11.673792 \n       232        233        234        235        236        237        238 \n 16.197459  26.034428  36.924243  10.777448  15.131933  54.644818  93.735290 \n       239        240        241        242        243        244        245 \n  9.946879  11.738965  16.454198  30.408907   9.784800  10.634290  13.446794 \n       246        247        248        249        250        251        252 \n 39.543782   9.173339  10.138338  11.996459  15.425196   9.066062  10.446956 \n       253        254        255        256        257        258        259 \n 15.929162  16.914295  10.088128  14.269057  23.299460   9.848249  10.850226 \n       260        261        262        263        264        265        266 \n 25.166817  42.620479   9.879266  21.267206   8.679327  10.938629  14.061540 \n       267        268        269        270        271        272        273 \n 14.724598   7.943970  12.874612  22.403533 109.535512 196.155360   9.319229 \n       274        275        276        277        278        279        280 \n  9.846023  12.850566  18.486140  11.557316  16.120018  28.561203  66.706694 \n       281        282        283        284        285        286        287 \n122.536347  10.542735  19.081239  71.679521 108.047192  11.987365  17.383216 \n       288        289        290        291        292        293        294 \n 32.045666 142.305417  10.324534  13.566102  42.812639  71.640924   8.627624 \n       295        296        297        298        299        300        301 \n 18.879554  52.885235  49.359591   9.323848  10.358239  12.617105  17.895176 \n       302        303        304        305        306        307        308 \n 24.187033   8.974839  10.452840  15.109307  14.202632   9.249247  10.233207 \n       309        310        311        312        313        314        315 \n 12.266257   9.391514   9.589576  10.558014   8.801750  10.643243  13.288186 \n       316        317        318        319        320        321        322 \n 14.425899   9.024984  10.423305  15.432000   9.600511  12.834794  12.976064 \n       323        324        325        326        327        328        329 \n 16.228751  29.117984  80.431878 165.793013   9.124003  10.018363  14.382501 \n       330        331        332        333        334        335        336 \n 16.337619   9.398701  10.691067  13.528045  20.210982  28.239226   9.387462 \n       337        338        339        340        341        342        343 \n 11.359653  23.359869  31.519122   9.082187  10.024664  11.673147  14.024218 \n       344        345        346        347        348        349        350 \n  9.008839  10.724108  13.544624  16.745577  16.693090  10.670133  17.637212 \n       351        352        353        354        355        356        357 \n 32.485366  65.837659 104.064600   9.385313   9.666184  15.099252  22.290668 \n       358        359        360        361        362        363        364 \n  8.549260  18.490214  50.261778  44.896004   9.636706  11.776664  16.498147 \n       365        366        367        368        369        370        371 \n  9.175527  10.869210  19.002063   8.927391  10.628135  13.270822  15.520996 \n       372        373        374        375        376        377        378 \n 13.724235   9.707307  12.955068   9.658230  12.140647   9.799907  13.052686 \n       379        380        381        382        383        384        385 \n 19.967567  35.434621  53.084734  10.526255  15.286603  53.061066   8.490054 \n       386        387        388        389        390        391        392 \n 11.855571  16.497665  17.426083   9.066235   9.890038  11.307342  13.220742 \n       393        394        395        396        397        398        399 \n 13.905866  22.580534 164.298198   9.596018  12.576684   8.777818  10.734808 \n       400        401        402        403        404        405        406 \n 14.648869   9.803123   9.511384  11.735515  16.363946  26.341487  10.736915 \n       407        408        409        410        411        412        413 \n  9.456753  26.651682  61.806303   9.774011  12.777141   9.772482   8.967104 \n       414        415        416        417        418        419        420 \n 15.298580  30.151702  12.150547  16.758810  30.393123  75.332905 143.834228 \n       421        422        423        424        425        426        427 \n  7.950103   7.743019   7.753710   9.474538  13.461292   8.878659   7.835014 \n       428        429        430        431        432        433        434 \n 18.558896  41.885056   8.319251  10.380780  14.931520   8.519414   9.044830 \n       435        436        437        438        439        440        441 \n 21.104604  37.627575   8.131068   9.655584  12.942021   8.690439  12.612237 \n       442        443        444        445        446        447        448 \n 21.014656   9.631713  12.873242  21.893077  50.079863   8.110661   9.737559 \n       449        450        451        452        453        454        455 \n 13.159163   7.756369   7.626380   7.403338   7.104996   7.003642   8.910275 \n       456        457        458        459        460        461        462 \n 14.933917  27.312822   7.867558   8.398595   9.505249  11.896877  14.526264 \n       463        464        465        466        467        468        469 \n  7.822006   8.907238  13.948313  15.908728   9.108414   8.160644  22.280460 \n       470        471        472        473        474        475        476 \n 50.556945   8.278063  10.513856  15.282527  26.008208   7.402012  10.358043 \n       477        478        479        480        481        482        483 \n 14.699736  17.101654  11.128277   7.737870   8.802123  10.568715  12.654241 \n       484        485        486        487        488        489        490 \n 12.809556   7.583098   8.221749   8.390500   8.796311  12.593755  40.949588 \n       491        492        493        494        495        496        497 \n 10.018678   7.920314   8.616073  29.577545  76.632290   9.549987  18.360377 \n       498        499        500        501        502        503        504 \n 76.020802 119.030685   7.897566   8.973225  11.059548  14.972210   8.135320 \n       505        506        508        509        510        511        512 \n  9.897384  13.594502   8.248076  10.935928   7.784617   8.816855  10.617110 \n       513        514        515        516        517        518        519 \n 13.160734   8.133265   9.919524  13.653791  21.769312   7.800100   7.565070 \n       520        521        522        523        524        525        526 \n  7.246226   7.213400   7.987058   7.672886   9.091652  11.336963  13.458702 \n       527        528        529        530        531        532        533 \n 12.421929   8.014459   9.007845  11.174583  16.227927   8.263066   7.693886 \n       534        535        536        537        538        539        540 \n  8.934786  12.890696  12.204648   8.279086  97.049843 117.339002   7.701847 \n       541        542        543        544        545        546        547 \n 10.702548  21.566891  22.009918   8.506604   8.265251  17.191919  33.023782 \n       548        549        550        551        552        554        555 \n  8.668426   9.253363  23.505340  43.237478   7.987193  14.988224  22.869013 \n       556        557        558        559        560        561        562 \n 29.088181   9.333139  17.822941  35.177812   8.896548  14.622649  26.471353 \n       563        564        565        566        567        568        569 \n 79.152863   8.087710   9.743122   7.647100   8.791028  10.521940  11.755987 \n       570        571        572        573        574        575        576 \n 10.019664   8.524305  11.157592  17.063273  31.431060  49.207413   7.862572 \n       577        578        579        580        581        582        583 \n  8.428136  11.994262  14.536197   7.704075   7.924851   8.198044   7.046037 \n       584        585        586        587        588        589        590 \n  7.753380   9.475242  12.385761  16.073983   8.569591   9.220894  22.428053 \n       591        592        593        594        595        596        597 \n 40.089343   7.719017  10.848544  16.080643  22.437024  10.349085  14.826560 \n       598        599        600        601        602        603        604 \n 66.257393 123.940789   7.549049   9.072484  12.213593   8.593856   8.336900 \n       605        606        607        608        609        610        611 \n 10.023762   7.862503   7.979278   8.374183  12.105195  11.566102  24.507304 \n       612 \n124.684849 \n\n\nCode\nranef(autism_lmer_quad_id_slope2)$childid %>% colMeans()\n\n\n  (Intercept)           age      I(age^2) \n 3.224847e-13 -1.754406e-13  1.440551e-14"
  },
  {
    "objectID": "longitudinal/randomcoef.html#g-effect-on-blups",
    "href": "longitudinal/randomcoef.html#g-effect-on-blups",
    "title": "14  Random Coefficient Models (CALS)",
    "section": "20.1 G effect on BLUPs",
    "text": "20.1 G effect on BLUPs\nLet’s just look at panel 1 for simplicity of visualization.\n\n\nCode\n# the pooled model\nautism_lm_1 <- lm(vsae ~ age, data = autism_complete %>% filter(sicdegp == 1)) # the pooled model\n\npooled_data <- data.frame(as.list(coef(autism_lm_1)[c(1, 2 )])) %>% \n  rename(\"intercept\" = 1,\n         \"slope\" = \"age\") %>% \n  add_column(model = \"pooled\")\n\n\n\n\nCode\n# the individual model\nautism_lm_id_slope_1 <- lmList(vsae~age | childid, data = autism_complete %>% filter(sicdegp == 1)) # individual slopes model, just easier to extract coefs with this function\n\n# sig1_children <- autism_complete %>% filter(sicdegp == 1) %>% distinct(childid) %>% pull(childid) \nindiv_data <- coef(autism_lm_id_slope_1) %>% \n  rownames_to_column(var = \"child_id\") %>% \n  rename(\"intercept\" = 2,\n         \"slope\" = 3) %>% \n  add_column(model = \"indiv\", .after = \"child_id\")\n\n\n\n\nCode\n# the mixed models\n# G matrix affect shape of pull for \"BLUPs\"\n# G symm\nautism_lme_id_slope_1_un <-  lme(vsae ~ age,\n                              random = list(childid = pdSymm(form = ~age)),\n                              data =  autism_complete %>% filter(sicdegp == 1),\n                              control = lmeControl(opt = \"optim\"))\n\n# G diag, het\nautism_lme_id_slope_1_diag <-  lme(vsae ~ age,\n                              random = list(childid = pdDiag(form = ~age)),\n                              data =  autism_complete %>% filter(sicdegp == 1),\n                              control = lmeControl(opt = \"optim\"))\n\n# G diag, homo\nautism_lme_id_slope_1_ident <- lme(vsae ~ age,\n                                   random = list(childid = pdIdent(form = ~age)),\n                                   data =  autism_complete %>% filter(sicdegp == 1),\n                                   control = lmeControl(opt = \"optim\"))\n\n# G CS, homo\nautism_lme_id_slope_1_cs <- lme(vsae ~ age,\n                                   random = list(childid = pdCompSymm(form = ~age)),\n                                   data =  autism_complete %>% filter(sicdegp == 1),\n                                   control = lmeControl(opt = \"optim\"))\n\n\n# list of lme models\nlme_models <-  list(mixed_un = autism_lme_id_slope_1_un, \n                   mixed_diag = autism_lme_id_slope_1_diag,\n                   mixed_ident = autism_lme_id_slope_1_ident,\n                   mixed_cs = autism_lme_id_slope_1_cs)\n\n# mixed, fixed effects\nmixed_data <- lme_models %>%  \n  map_dfr(~data.frame(as.list(fixed.effects(.x))), # extract fixed effects as data frame\n          .id = \"model\") %>% \n  rename(\"intercept\" = 2,\n         \"slope\" = 3)\n\n# helper function for extracting individual lines\nget_indiv_lines <- function(lme_object) {\n  coef(lme_object) %>% as.matrix() %>% \n    as.data.frame() %>% \n    rownames_to_column(\"child_id\") %>% \n    select(\"child_id\", `(Intercept)`, `age`) %>% \n    rename(\"intercept\" = 2,\n           \"slope\" = 3)\n}\n\n# mixed model individuals\nmixed_indiv_data <- lme_models %>% map_dfr(~get_indiv_lines(.x),\n                       .id = \"model\")\n\n\n\n\nCode\ncombined_indiv_data <- bind_rows(indiv_data, mixed_indiv_data)\ncombined_avg_data <- combined_indiv_data %>%\n  group_by(model) %>%\n  summarize(across(intercept:slope, mean, na.rm = T))\nindiv_avg_data <- indiv_data %>% group_by(model) %>%\n  summarize(across(intercept:slope, mean, na.rm = T))\n\n\nWe’ve done all the fitting, and data extraction so now we’re ready to plot and create dataframes for plotting\n\n\nCode\n# hack for each specifying what data goes to which \"facet\" of ggplot\n# needs to duplicate data that will appear in multiple facets.\nmixed_data_facet <- mixed_data %>% \n  mutate(facet_id = c(\"mixed_un\" = 1, \"mixed_ident\" = 2, \"mixed_diag\" = 3, \"mixed_cs\" = 4)[model])\nmixed_indiv_data_facet <- mixed_indiv_data %>%\n  mutate(facet_id = c(\"mixed_un\" = 1, \"mixed_ident\" = 2, \"mixed_diag\" = 3, \"mixed_cs\" = 4)[model])\nindiv_data_facet <-  1:4 %>% map_dfr(~add_column(indiv_data, facet_id = .)) # 4 copies of individual data, one for each facet\ncombined_indiv_data_facet <- bind_rows(mixed_indiv_data_facet, indiv_data_facet) %>% arrange(model)\n\n# ellipse level curves of G\nmixed_ellipses <- lme_models %>% \n  map_dfr(\n    function(lme_obj) {\n      G_hat <- getVarCov(lme_obj)\n      mu <- fixed.effects(lme_obj) \n      ellipse(G_hat, centre = mu) %>% \n        as_tibble(.name_repair = ~c(\"intercept\", \"slope\"))\n    },\n    .id = \"model\") %>% \n  mutate(facet_id = c(\"mixed_un\" = 1, \"mixed_ident\" = 2, \"mixed_diag\" = 3, \"mixed_cs\" = 4)[model]) # specify which facet for each ellipse \n\n\n\n\nCode\nggplot(data = combined_indiv_data_facet) +\n  geom_path(mapping = aes(intercept, slope, group = child_id), # fixed -> mixed\n            arrow = arrow(ends = \"last\", length = unit(.1, \"cm\")),\n            alpha =.7) +\n  geom_point(mapping = aes(intercept, slope, color = model, alpha = model), # mixed estimates\n             pch = 16) + \n  geom_point(mapping = aes(intercept, slope, color = model, alpha = model), # mixed center\n             data = mixed_data_facet, size = 8, shape = 4) +\n  geom_path(mapping = aes(intercept, slope, color = model), # G level ellipses\n            data = mixed_ellipses,\n            linetype = 2,\n            alpha = .4) + \n  facet_wrap(~facet_id) +\n  scale_color_manual(values = c(\"#000000\", \"#E69F00\", \"#009E73\", \"#0072B2\", \"#CC79A7\")) +\n  scale_alpha_manual(values = c(.4, rep(.6, 4))) +\n  labs(title = \"G structure effect on mixed effect estimation\")\n\n\nWarning: Removed 4 row(s) containing missing values (geom_path).\n\n\nWarning: Removed 4 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nCode\n# show G hats for each of the models\nlme_models %>% map(getVarCov)\n\n\n$mixed_un\nRandom effects variance covariance matrix\n            (Intercept)     age\n(Intercept)      47.664 -24.470\nage             -24.470  12.565\n  Standard Deviations: 6.9039 3.5447 \n\n$mixed_diag\nRandom effects variance covariance matrix\n            (Intercept)    age\n(Intercept)    0.013694 0.0000\nage            0.000000 6.4369\n  Standard Deviations: 0.11702 2.5371 \n\n$mixed_ident\nRandom effects variance covariance matrix\n            (Intercept)    age\n(Intercept)      6.4617 0.0000\nage              0.0000 6.4617\n  Standard Deviations: 2.542 2.542 \n\n$mixed_cs\nRandom effects variance covariance matrix\n            (Intercept)     age\n(Intercept)      8.9804 -8.9780\nage             -8.9780  8.9804\n  Standard Deviations: 2.9967 2.9967 \n\n\n\n\nCode\ng_mu <- ggplot(mapping = aes(intercept, slope, color = model, alpha = model)) +\n  geom_point(data = indiv_data, pch = 16) + \n  geom_point(data = mixed_data_facet, shape = 4, size = 4) +\n  geom_point(data = pooled_data, shape = 4, size = 4) + \n  scale_color_manual(values = c(\"#000000\", \"#E69F00\", \"#009E73\", \"#0072B2\", \"#CC79A7\", \"347827\")) +\n  scale_alpha_manual(values = c(.4, rep(.6, 5)))\ng_mu_zoom <- g_mu + coord_cartesian(xlim = c(1,3), ylim = c(2,3.5)) +\n  theme(legend.position = \"none\")\n\nvp_zoom <- viewport(width = .3, height = .3, x = .67, y = .85)\n\nprint(g_mu)\n\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\nCode\nprint(g_mu_zoom, vp = vp_zoom)\n\n\nWarning: Removed 1 rows containing missing values (geom_point)."
  },
  {
    "objectID": "longitudinal/randomcoef.html#r-effect-on-blups",
    "href": "longitudinal/randomcoef.html#r-effect-on-blups",
    "title": "14  Random Coefficient Models (CALS)",
    "section": "20.2 R effect on BLUPs",
    "text": "20.2 R effect on BLUPs\nThis section we’ll play with the estimates through “R” a little, and see how that affects the BLUPs. This should affect the BLUPs by changing the estimate of “V” in the equation:\n\n\\begin{aligned}\nE[u | y] &= GZ'V^{-1}(y - X\\beta) \\\\\neBLUP(u) &= \\hat GZ'\\hat V^{-1}(y - X\\hat\\beta)\n\\end{aligned}\n\n\n\nCode\n# heterogenous\nautism_lme_id_slope_1_ident_het <- lme(vsae ~ age,\n                                   random = list(childid = pdIdent(form = ~age)),\n                                   data =  autism_complete %>% filter(sicdegp == 1),\n                                   weights = varIdent(form = ~ age),\n                                   control = lmeControl(opt = \"optim\"))\n\nautism_lme_id_slope_1_ident_cs <- lme(vsae ~ age,\n                                   random = list(childid = pdIdent(form = ~age)),\n                                   data =  autism_complete %>% filter(sicdegp == 1),\n                                   correlation = corCompSymm(form = ~ 1 | childid),\n                                   control = lmeControl(opt = \"optim\"))\n\nautism_lme_id_slope_1_ident_car <- lme(vsae ~ age,\n                                   random = list(childid = pdIdent(form = ~age)),\n                                   data =  autism_complete %>% filter(sicdegp == 1),\n                                   correlation = corCAR1(form = ~ age | childid),\n                                   weights = varIdent(form = ~ age),\n                                   control = lmeControl(opt = \"optim\"))\n\nautism_lme_id_slope_1_ident_un <- lme(vsae ~ age,\n                                   random = list(childid = pdIdent(form = ~age)),\n                                   data =  autism_complete %>% filter(sicdegp == 1),\n                                   correlation = corSymm(form = ~ 1 | childid),\n                                   weights = varIdent(form = ~ age),\n                                   control = lmeControl(opt = \"optim\"))\n\n\n\n\nCode\nlme_r_models <- list(mixed_r_ident = autism_lme_id_slope_1_ident,\n                     mixed_r_un = autism_lme_id_slope_1_ident_un,\n                     mixed_r_cs = autism_lme_id_slope_1_ident_cs,\n                     mixed_r_car = autism_lme_id_slope_1_ident_car)\n\nmixed_r_indiv_data <- lme_r_models %>% map_dfr(~get_indiv_lines(.x),\n                       .id = \"model\")\n\n\n\n\nCode\n# plotting dfs\n# for faceting\n# can use same one\n# indiv_data_facet <-  1:4 %>% map_dfr(~add_column(indiv_data, facet_id = .)) # 4 copies of individual data, one for each facet\nmixed_r_indiv_data_facet <- mixed_r_indiv_data %>% mutate(facet_id = c(\"mixed_r_ident\" = 1, \"mixed_r_un\" = 2, \"mixed_r_cs\" = 3, \"mixed_r_car\" = 4)[model])\ncombined_r_indiv_data_facet <- bind_rows(mixed_r_indiv_data_facet, indiv_data_facet) %>% arrange(model)\n\n\n\n\nCode\ncombined_r_indiv_data_facet %>% \n  ggplot(aes(intercept, slope, color = model, alpha = model)) +\n  geom_path(aes(intercept, slope, group = child_id),\n            arrow = arrow(ends = \"last\", length = unit(.1, \"cm\")),\n            color = \"black\",\n            alpha =.7) +\n  geom_point(pch = 16) +\n  scale_color_manual(values = c(\"#000000\", \"#E69F00\", \"#009E73\", \"#0072B2\", \"#CC79A7\")) +\n  scale_alpha_manual(values = c(.4, rep(.7, 4))) +\n  facet_wrap(~facet_id)\n\n\nWarning: Removed 4 row(s) containing missing values (geom_path).\n\n\nWarning: Removed 4 rows containing missing values (geom_point).\n\n\n\n\n\n\n\nCode\nlme_r_models %>% map(getVarCov, type = \"marginal\")\n\n\n$mixed_r_ident\nchildid 2 \nMarginal variance covariance matrix\n        1       2       3      4       5\n1  85.996  45.232  71.079 122.77  174.47\n2  45.232 118.300 103.390 180.93  258.47\n3  71.079 103.390 221.690 297.24  426.47\n4 122.770 180.930 297.240 583.55  762.48\n5 174.470 258.470 426.470 762.48 1152.20\n  Standard Deviations: 9.2734 10.877 14.889 24.157 33.944 \n\n$mixed_r_un\nchildid 2 \nMarginal variance covariance matrix\n        1      2      3      4       5\n1  87.144  75.38 103.39 124.70  202.00\n2  75.380 124.58 116.16 202.06  296.72\n3 103.390 116.16 244.38 329.34  465.61\n4 124.700 202.06 329.34 663.67  856.77\n5 202.000 296.72 465.61 856.77 1322.50\n  Standard Deviations: 9.3351 11.162 15.633 25.762 36.367 \n\n$mixed_r_cs\nchildid 2 \nMarginal variance covariance matrix\n        1       2       3      4       5\n1  76.107  28.296  51.154  96.87  142.59\n2  28.296 104.680  79.726 148.30  216.88\n3  51.154  79.726 196.110 251.16  365.45\n4  96.870 148.300 251.160 516.13  662.61\n5 142.590 216.880 365.450 662.61 1019.00\n  Standard Deviations: 8.724 10.231 14.004 22.718 31.922 \n\n$mixed_r_car\nchildid 2 \nMarginal variance covariance matrix\n        1       2       3      4       5\n1  99.642  92.726  96.156 134.07  184.84\n2  92.726 133.680 139.880 197.49  273.81\n3  96.156 139.880 242.580 327.73  452.51\n4 134.070 197.490 327.730 623.76  817.82\n5 184.840 273.810 452.510 817.82 1222.80\n  Standard Deviations: 9.9821 11.562 15.575 24.975 34.968"
  },
  {
    "objectID": "longitudinal/within_between.html#introduction",
    "href": "longitudinal/within_between.html#introduction",
    "title": "15  The Within-Between Model",
    "section": "15.1 Introduction",
    "text": "15.1 Introduction\nThis material is largely sourced from Chapter 9 of Longitudinal Data Analysis.\nFor fixed effects models,\n\nX_{ij} is independent of \\varepsilon_{ij} and \\varepsilon_{ij'}.\n\nan observation will not help determine the value of the next\n\nFE models “clear out” any higher level trends, and just focus on the within estimate\nThe \\hat\\beta estimate will be consistent even when the higher level effect is correlated with the covariates."
  },
  {
    "objectID": "longitudinal/within_between.html#the-model",
    "href": "longitudinal/within_between.html#the-model",
    "title": "15  The Within-Between Model",
    "section": "15.2 The Model",
    "text": "15.2 The Model\nTo introduce the model, we will look at a simulated example where there is a higher level effect\n\n\nCode\n# simulated  \ndat_skel <- data.frame(cohort = rep(1:3, each = 16),\n           id = rep(1:12, each = 4),\n           age = c(rep(c(4, 6, 7, 10), 4), # cohort 1\n                   rep(c(4, 5, 8, 9) + 1, 4), # cohort 2 \n                   rep(c(4, 7, 8, 10) + 2, 4))) # cohort 3\n# creating data\nset.seed(1)\nid_int <- rnorm(12, 2) + 10\nid_slope <- rnorm(12, mean = 1, sd = .01) # random slope\ncohort_int <- c(1, 4, 7)\n\ndat <- dat_skel %>% \n  mutate(y = id_int + id_slope * age + cohort_int[cohort] + rnorm(nrow(.), sd = 5),\n         id = factor(id),\n         cohort = factor(cohort))\n\ndat %>% head()\n\n\n  cohort id age        y\n1      1  1   4 19.44783\n2      1  1   6 18.77012\n3      1  1   7 18.46414\n4      1  1  10 17.23703\n5      1  2   4 14.93811\n6      1  2   6 20.32587\n\n\n\n\nCode\ndat %>% ggplot(aes(age, y, color = cohort, group = id)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\nCode\ndat_centered <- dat %>% arrange(id, age) %>%\n  group_by(id) %>% \n  mutate(age_mean = mean(age),\n         y_mean = mean(y),\n         age_centered = age - age_mean,\n         y_centered = y - y_mean,\n         age_baseline = age - first(age),\n         y_baseline = y - first(y))\n\ndat_baseline <- dat_centered %>% \n  filter(age_baseline != 0)\n\n\n\n\nCode\n# Fixed Effect Estimate (within model)\ncentered_lm <- lm(y_centered ~ age_centered, data = dat_centered) # centered regression\n\n# between model\nbaseline_lm <- lm(y_baseline~age_baseline, data = dat_baseline) # baseline adjusted (centered)\nid_lm <- lm(y~age + id, data = dat_centered) # id model (centered)\n\nmean_lm <- lm(y_mean ~ age_mean, data = dat_centered) # mean regression (pooled model)\n\npooled_lm <- lm(y~age, data = dat_centered)\n\ncohort_lm <- lm(y~age + cohort, data = dat_centered)\n\n# mixed model\nmmod <- lmer(y ~ age + (1|id), data = dat)\n\n# within-between model\nwb_mmod <- lmer(y~age_centered + age_mean + (1 | id), data = dat_centered)\n\n\nboundary (singular) fit: see help('isSingular')\n\n\n\n\nCode\ncbind(\n  baseline_lm = coef(baseline_lm),\n  mean = coef(mean_lm),\n  cohort = coef(cohort_lm)[1:2],\n  id = coef(id_lm)[1:2],\n  pooled = coef(pooled_lm),\n  centered = coef(centered_lm),\n  mixed = fixef(mmod)) %>% rbind(NA) %>% \n  cbind(within_between = fixef(wb_mmod))\n\n\n             baseline_lm      mean    cohort       id    pooled     centered\n(Intercept)    -0.508907 -3.670061 11.107892 9.596570 10.993983 4.411316e-16\nage_baseline    1.316630  3.640294  1.316031 1.316031  1.768289 1.316031e+00\n                      NA        NA        NA       NA        NA           NA\n                 mixed within_between\n(Intercept)  12.131986      -3.670061\nage_baseline  1.623012       1.316031\n                    NA       3.640294\n\n\n\n\\begin{aligned}\nw = \\frac{(1 - \\rho_y)\\rho_x}{(1 - \\rho_y) + n\\rho_y(1-\\rho_x)}\n\\end{aligned}\n and\n\n\\begin{aligned}\n\\hat \\beta_2^{(RE)} = (1 - w)\\hat\\beta_2^{(FE)} + w\\hat\\beta_2^{(B)}\n\\end{aligned}\n\n\n\nCode\n# help function for weight\nfixed_random_weight <- function(rho_x, rho_y, n) {\n  (1-rho_y) * rho_x / ((1-rho_y) + n * rho_y * (1 - rho_x))\n}\n\n\n\n\nCode\n# response correlation\nrho_y <- VarCorr(mmod) %>% as.data.frame() %>% pull(vcov) %>% \n  as_mapper(~.x[1]/sum(.x))()\n\n# correlation of age\nage_lm <- lm(age~id, data = dat_centered)\nrho_x <- summary(age_lm)$r.squared\n\nn <- 4\nw <- fixed_random_weight(rho_x, rho_y, n)\n(1-w) * fixef(wb_mmod)[2] + w * fixef(wb_mmod)[3]\n\n\nage_centered \n    1.623012 \n\n\n\n\nCode\ndat %>% ggplot(aes(age, y, color = cohort, group = id)) +\n  geom_point(alpha = .2) +\n  geom_line(alpha = .2) +\n  geom_abline(slope = coef(centered_lm)[2], intercept = coef(centered_lm)[1] + 10) + # centered estimate\n  geom_abline(slope = coef(mean_lm)[2], intercept = coef(mean_lm)[1]) + # mean estimate\n  geom_abline(slope = fixef(mmod)[2], intercept = fixef(mmod)[1], linetype = 2) + # mixed effects estimate\n  theme_minimal()"
  },
  {
    "objectID": "longitudinal/within_between.html#fev-example",
    "href": "longitudinal/within_between.html#fev-example",
    "title": "15  The Within-Between Model",
    "section": "15.3 FEV example",
    "text": "15.3 FEV example\n\n\nCode\nlibrary(foreign)\nfev <- read.dta(\"data/fev1.dta\") %>% \n  filter(id != 197) %>%\n  mutate(id = factor(id),\n         y =  logfev1 - 2 * (log(ht)))\n\nfev_centered <- fev %>% group_by(id) %>% \n  mutate(\n    y_mean = mean(y),\n    age_mean = mean(age),\n    age_centered = age - age_mean)\n\n\n\n\nCode\n# fitting all models\nfev_lm <- lm(y ~ age + id, data = fev) # lm\nfev_mer <- lmer(y~age + (1|id), data = fev) # random\n\n# within/between model\nfev_wb <- lmer(y ~ age_centered + age_mean + (1|id),\n               data = fev_centered,\n               REML = FALSE)\n\n# cross sectional\nfev_mean_lm <- lm(y_mean ~ age_mean, data = fev_centered)\n\n\n# age correlation (between/within)\nfev_age_lm <- lm(age ~ id, data = fev)\n\n\n\n\nCode\n# comparing the coefficients\ncbind(fixed = coef(fev_lm)[1:2],\n      random = fixef(fev_mer),\n      mean = coef(fev_mean_lm)) %>% \n  rbind(NA) %>% cbind(within_between = fixef(fev_wb))\n\n\n                  fixed      random        mean within_between\n(Intercept) -0.39873981 -0.35517115 -0.37154929    -0.34833979\nage          0.02982264  0.02980696  0.03109488     0.02982264\n                     NA          NA          NA     0.02923539\n\n\n\n\nCode\nrho_y <- VarCorr(fev_mer) %>% \n  as.data.frame() %>%\n  pull(vcov) %>%\n  as_mapper(~.x[1]/sum(.x))() # .689\n\n# age covariance\n# fev_age_ss <- anova(fev_age_lm)$`Sum Sq`\n# rho_x <- fev_age_ss[1] / sum(fev_age_ss) \nrho_x <- summary(fev_age_lm)$r.squared # .172\n\nnbar <- fev %>% count(id) %>% pull(n) %>% mean()\n\nw <- fixed_random_weight(rho_x, rho_y, nbar)\nw\n\n\n[1] 0.01297669\n\n\nSince this value of w is very low we expect that the mixed estimate is very close to the fixed estimate.\n\n\nCode\n# hausman test manually\n\nhaus_contr <- cbind(c(0, -1, 1))\nhaus_mean <- fixef(fev_wb) %*% haus_contr\nhaus_var <- t(haus_contr) %*% vcov(fev_wb) %*% haus_contr\nhaus_z <- haus_mean / sqrt(haus_var)[1]\npchisq(haus_z^2, 1, lower.tail = FALSE) # p = .84\n\n\n          [,1]\n[1,] 0.8412728\n\n\nCode\n# hausman with plm\n\n# phtest(y~age, data = fev_centered, index = c(\"id\"))\nwi <- plm(y~age, data = fev_centered, index = c(\"id\"), model = \"within\" )\nfe <- plm(y~age, data = fev_centered, index = c(\"id\"), model = \"between\")\nmi <- plm(y~age, data = fev_centered, index = c(\"id\"), model = \"random\")\n\nphtest(wi, mi)\n\n\n\n    Hausman Test\n\ndata:  y ~ age\nchisq = 0.042027, df = 1, p-value = 0.8376\nalternative hypothesis: one model is inconsistent"
  },
  {
    "objectID": "longitudinal/within_between.html#resources",
    "href": "longitudinal/within_between.html#resources",
    "title": "15  The Within-Between Model",
    "section": "15.4 Resources",
    "text": "15.4 Resources\n\nSeparation of individual-level and cluster-level covariate effects in regression analysis of correlated data"
  },
  {
    "objectID": "matrix/matrix.html#basic-matrices",
    "href": "matrix/matrix.html#basic-matrices",
    "title": "17  Matrix Math",
    "section": "17.1 Basic Matrices",
    "text": "17.1 Basic Matrices\nLet j be the vector of all 1’s, and J be the matrix of all ones.\n\n\\begin{aligned}\njj' &= J \\\\\nj'x  &= \\sum x \\\\\nj'A  &= \\operatorname{colsums}(A) \\\\\nAj &= \\operatorname{rowsums}(A) \\\\\nJx &= \\begin{bmatrix}\n  \\sum x \\\\\n  \\vdots \\\\\n  \\sum x \\\\\n\\end{bmatrix}\n\\end{aligned}"
  },
  {
    "objectID": "matrix/matrix.html#adjacency-matrix",
    "href": "matrix/matrix.html#adjacency-matrix",
    "title": "17  Matrix Math",
    "section": "17.2 Adjacency Matrix",
    "text": "17.2 Adjacency Matrix\n\n17.2.1 Spectral Bounds\n\nSimple\nAll 3 of these proofs and results can be found in Newman (2018), section 18.2.3.\n\nLargest Eigenvalue, undirected, unweighted, lower bound by average degree\n\n\n\\begin{aligned}\n\\lambda_1(A) \\geq d_{avg}\n\\end{aligned}\n\nproof: use 1 vector as test vector in courant fischer\n\nLargest Eigenvalue, undirected, unweighted, lower bound by max degree\n\n\n\\begin{aligned}\n\\lambda_1(A) \\geq \\sqrt{d_{max}}\n\\end{aligned}\n proof: use [sqrt(dmax), 1, 0] for max degree node, connected to max degree, else (respectively) as test vector in courant fischer\n\nSmallest Eigenvalue, undirected, unweighted, upper bound by max degree\n\n\n\\begin{aligned}\n\\lambda_n (A) \\leq -\\sqrt{k_{max}}\n\\end{aligned}\n proof: use [sqrt(dmax), -1, 0] for max degree node, connected to max degree, else (respectively) as test vector in courant fischer.\n\n\n\n17.2.2 Example Spectrums\nFirst we’ll examine how all the adjacency matrices vary in terms of their eigenvalues.\n\n\nCode\ng5_list <- read_file6(\"data/graph5c.g6\", type = \"adjacency\")\n\ng5_list |> enframe(name = \"gid\", value = \"adj\") |> \n  rowwise() |> \n  mutate(eig_values = list(zapsmall(eigen(adj)$values)),\n         gid = factor(gid),\n         max_eig = max(eig_values)) |>\n  mutate(gid_name = recode(gid,\n                           \"9\" = \"9 (line)\",\n                           \"1\" = \"1 (cross)\",\n                           \"21\" = \"21 (complete)\",\n                           \"7\" = \"7 (wakanda)\")) |> \n  dplyr::select(gid, gid_name, eig_values, max_eig) |> \n  unnest_longer(col = c(\"eig_values\")) |>\n  group_by(gid, gid_name, eig_values, max_eig) |> \n  summarize(\n    multiplicity = factor(n(), levels = 1:4, ordered = TRUE),\n    .groups = \"drop_last\") |> \n  ggplot(aes(x = fct_reorder(gid_name, max_eig), y = eig_values, color = multiplicity)) +\n  geom_point() +\n  scale_color_manual(values = colorRampPalette(colors = c(\"#69CAFF\", \"#9900FF\"), interpolate = \"spline\")(4)) +\n  labs(x = \"Graph Number\",\n       y = \"Eigenvalue\",\n       color = \"Multiplicity\") +\n  coord_cartesian(ylim = c(-4, 4)) +\n  theme_test() +\n  theme(panel.grid.major.y = element_line(color = \"gray90\"),\n        panel.border = element_blank(),\n        axis.line = element_line(color = \"black\"))\n\n\n\n\n\nEigenvalues of undirected, adjacency matrices. All matrices are connected 5 graphs. We note that there are both postive and negative eigenvalues unless the graph has no edges.\n\n\n\n\n\n\n17.2.3 Spectral Radius Bounds\n\n\n\n\n\n\nTheorem 2.3: Adjacency Matrix Spectral Radius Bound (Hong, Shu, and Fang 2001)\n\n\n\n\n\\begin{aligned}\n\\rho(A) \\leq \\frac{d_{min} -1 + \\sqrt{(d_{min} + 1)^2 + 4(2m-nd_{min})}}{2}\n\\end{aligned}\n\n\nd_{min} is minimum degree on the graph\nn,m is number of nodes, edges in the graph\nequality is reached for regular graphs, or bi-degreed graphs of either d_{min} or n-1\n\nMore simply, for a simply connected graph d_{min} = 1 the expression simplifies\n\n\\begin{align*}\n\\rho(A) \\leq \\sqrt{1 + 2m - n}\n\\end{align*}\n\\tag{17.1}\n\nequality reached for complete or star graph\n\n\n\n\n\nCode\nhong_upper_adj_spectral <- function(d_min, n, m, easy = FALSE) {\n  if (easy) return(sqrt(1 + 2*m - n))\n  (d_min - 1 + sqrt((d_min + 1)^2 + 4 * (2 * m - n * d_min))) / 2\n}\n\n\n\n\nCode\ng5_list <- read_file6(\"data/graph5c.g6\", type = \"igraph\")\n\nspectrum(g5_list[[5]], which = list(pos = \"LM\", howmany = 1))$values\n\n\n[1] 2.135779\n\n\nCode\neigen(as_adj(g5_list[[2]]))$values[1]\n\n\n[1] 1.847759\n\n\nCode\ng5_hong_df <- tibble(g = g5_list) |> \n  rownames_to_column(\"g_id\") |> \n  rowwise() |> \n  mutate(min_deg = min(degree(g)),\n         n_edge = ecount(g),\n         n_vertex = vcount(g),\n         spectral_radius = eigen(as_adj(g))$values[1],\n         upper_spectral = hong_upper_adj_spectral(min_deg, n_vertex, n_edge),\n         easy_upper_spectral = hong_upper_adj_spectral(min_deg, n_vertex, n_edge, easy = TRUE)) |> \n  arrange(upper_spectral, easy_upper_spectral, spectral_radius)\n\ng5_hong_df |> ggplot(aes(factor(g_id, unique(g_id)), spectral_radius)) +\n  geom_point(aes(color = \"Spectral Radius\")) +\n  geom_line(aes(y = upper_spectral, group = 1, color = \"Hong Upper Bound\", linetype = \"Hong Upper Bound\")) +\n  geom_line(aes(y = easy_upper_spectral, group = 1, color = \"Hong Easy Upper Bound\", linetype = \"Hong Easy Upper Bound\")) +\n  guides(colour = guide_legend(override.aes = list(shape = c(26, 26, 19), linetype = c(1, 2, 0)))) +\n  scale_color_manual(\"\", values = c(\"red\", \"red\", \"black\")) +\n  scale_shape(guide = \"none\") + \n  scale_linetype(guide = \"none\") +\n  labs(x = \"graph\",\n       y = \"Adj. Spectral Radius\",\n       title = \"Hong Spectral Radius of Adjacency for all connected 5-graphs\")"
  },
  {
    "objectID": "matrix/matrix.html#laplacian",
    "href": "matrix/matrix.html#laplacian",
    "title": "17  Matrix Math",
    "section": "17.3 Laplacian",
    "text": "17.3 Laplacian\n\n17.3.1 Example Spectrums\n\nComponentsComplete GraphStar GraphBipartite GraphPath\n\n\nIf there are 2 connected components, 2 of the eigenvalues will be 0.\n\n\nCode\ng1 <- graph(~1-2-3-1-4-5-6-4)\nL1 <- laplacian_matrix(g1)\nplot(g1)\n\n\n\n\n\nCode\neigen(L1)$values |> zapsmall()\n\n\n[1] 4.561553 3.000000 3.000000 3.000000 0.438447 0.000000\n\n\nCode\ng2 <- graph(~1-2-3-1, 4-5-6-4)\nplot(g2)\n\n\n\n\n\nCode\nL2 <- laplacian_matrix(g2)\neigen(L2)$values |> zapsmall()\n\n\n[1] 3 3 3 3 0 0\n\n\n\n\n\n\nCode\ng <- make_full_graph(5)\nL <- laplacian_matrix(g)\nplot(g)\n\n\n\n\n\nCode\neigen(L)$values |> zapsmall()\n\n\n[1] 5 5 5 5 0\n\n\n\n\n\n\nCode\ng <- make_star(5, mode = \"undirected\")\nL <- laplacian_matrix(g)\nplot(g)\n\n\n\n\n\nCode\neigen(L)$values |> zapsmall()\n\n\n[1] 5 1 1 1 0\n\n\n\n\n\n\nCode\ng <- make_full_bipartite_graph(5, 3)\nL <- laplacian_matrix(g, normalized = F)\nplot(g)\n\n\n\n\n\nCode\neigen(L)$values |> zapsmall()\n\n\n[1] 8 5 5 3 3 3 3 0\n\n\n\n\n\n\nCode\ng <- make_graph(~1-2-3-4-5-6-7-8-9-10)\nL <- laplacian_matrix(g, normalized = F)\nplot(g)\n\n\n\n\n\nCode\neigen(L)$values |> zapsmall()\n\n\n [1] 3.902113 3.618034 3.175571 2.618034 2.000000 1.381966 0.824429 0.381966\n [9] 0.097887 0.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n17.3.2 Spectral Radius Bounds\n\nSimple\nThere are 3 simple bounds on the Laplacian presented by (Anderson and Morley 1985),\n\nnumber of nodes in graph\n\n \\lambda_1(L) \\leq n\n\nequality reached on complete graph, for example.\n\n\nmax degree\n\n \\lambda_1(L) \\leq 2d_{max}\n\nequality nears on a path graph.\n\n\nmaximal ends of an edge\n\n \\lambda_1(L) \\leq \\max_{i\\sim j} w_i + w_j \n\nwhere maximum is over edges in graph, and w_i + w_j is the sum of weights of that edge’s endpoints.\nI believe this is true over any weighted, connected, undirected graph\nequality is reached when bipartite graph\n\n\n\n\n17.3.3 Laplacian for Distributed Summation\n\n\n\n\n\n\nSource\n\n\n\nThis insight comes from Sivan Toledo, where he has a nice description of the problem.\n\n\nIf the eventual algorithm goal is to have the sum of the graph sitting on every node, the repeated application of a matrix multiplication should converge to the matrix of all ones J. The spectrum of J is simply n and the rest zeros. \\frac{1}{n}J is special because it sets each node to have the mean, and the single non-zero eigenvalue is 1.\n\n\nCode\neigen(matrix(rep(1/4, 16), nrow = 4))$values |> zapsmall()\n\n\n[1] 1 0 0 0\n\n\nSuppose the initial state of the graph y is values at each of the n nodes of the graph.\n\n\\begin{aligned}\nn(I - \\frac{1}{n}L)^ky\n\\end{aligned}\n\nAs k increases, we’d expect that the matrix spectrum converges to match J, since L has a zero eigenvalue, one eigen value limit is 1. And since the Laplacian has spectral radius upper bounded by number of nodes,\n\n\nCode\nset.seed(1)\ng <- sample_gnp(10, .5)\nA <- g %>% as_adj()\nL <- Diagonal(x = rowSums(A)) - A\n\n\n\n\nCode\nJ <- Matrix(1, nrow = 10, ncol = 10)\nJ %*% cbind(1:10)\n\n\n10 x 1 Matrix of class \"dgeMatrix\"\n      [,1]\n [1,]   55\n [2,]   55\n [3,]   55\n [4,]   55\n [5,]   55\n [6,]   55\n [7,]   55\n [8,]   55\n [9,]   55\n[10,]   55\n\n\n\n\nCode\nJe <- eigen(J)\nLn <- laplacian_matrix(g, normalized = TRUE)\neigen(Ln)\n\n\neigen() decomposition\n$values\n [1] 1.647271e+00 1.467555e+00 1.450327e+00 1.166667e+00 1.084726e+00\n [6] 1.000000e+00 9.451136e-01 7.853920e-01 4.529494e-01 2.442491e-15\n\n$vectors\n             [,1]        [,2]         [,3]          [,4]        [,5]\n [1,] -0.16557306 -0.35884141  0.414024644  4.378811e-16 -0.41543869\n [2,]  0.01245928  0.32716246 -0.261033301  7.071068e-01 -0.26421090\n [3,]  0.07815309  0.07003764  0.522362456 -2.958733e-16  0.30735544\n [4,]  0.49476837 -0.29623231 -0.126497326  3.013764e-16 -0.34924549\n [5,] -0.26126348 -0.45016699 -0.226430018  3.655149e-16 -0.17830741\n [6,]  0.01453893 -0.34738481 -0.412927706 -6.383782e-16  0.64394097\n [7,]  0.29116939 -0.06983555  0.410643861 -1.133005e-16  0.11871973\n [8,]  0.39734608  0.39945650 -0.007375584 -5.551115e-17  0.08547476\n [9,]  0.01245928  0.32716246 -0.261033301 -7.071068e-01 -0.26421090\n[10,] -0.64049820  0.27700977  0.113930213 -2.030681e-18  0.05918033\n               [,6]        [,7]       [,8]        [,9]      [,10]\n [1,] -3.131423e-15  0.23529056  0.5901861  0.09578594 -0.2948839\n [2,] -3.906316e-15  0.22258023 -0.1102821  0.25044926 -0.3611576\n [3,] -6.324555e-01 -0.02451212 -0.2642891  0.25347743 -0.2948839\n [4,] -3.162278e-01  0.13949439 -0.2339562 -0.57753213 -0.1474420\n [5,]  6.578071e-15 -0.66595972 -0.2431392  0.18631243 -0.3296902\n [6,] -1.421198e-15  0.30002682  0.2257963 -0.02773400 -0.3900947\n [7,]  7.071068e-01  0.04045948 -0.3410158 -0.03156311 -0.3296902\n [8,]  6.679915e-15 -0.53861397  0.5160385 -0.17841799 -0.2948839\n [9,] -2.959103e-15  0.22258023 -0.1102821  0.25044926 -0.3611576\n[10,] -1.665335e-16  0.01531268 -0.1004178 -0.63187856 -0.2948839\n\n\nCode\n# not correct\n# M <- diag(10) - L / (diag(L) + 1) # direct averaging of over all nodes ***\n# M <- diag(L) - L/2 # average across edges?\n\n# correct\nM <- diag(10) - L / 10\n\nmat_pow <- function(M, t = 10) {\n  if (t == 1) {return(M)}\n  return(M %*% mat_pow(M, t - 1))\n}\n# mat_pow(M, 100)\n\ny <- cbind(1:10)\nfor (i in 1:100) {\n  y <- M %*% y\n}\ny\n\n\n10 x 1 Matrix of class \"dgeMatrix\"\n          [,1]\n [1,] 5.499981\n [2,] 5.499980\n [3,] 5.499978\n [4,] 5.500109\n [5,] 5.499981\n [6,] 5.499988\n [7,] 5.499989\n [8,] 5.499992\n [9,] 5.499980\n[10,] 5.500024\n\n\n\n\n17.3.4 Normalized Laplacian\nThe primary reason for looking at the normalized laplacian is because it removes dependence on the number of nodes in the graph, which would change bounds. Rather, the eigenvalues of a normalized laplacian will range from 0 \\leq 2, reaching 2 for bipartite graphs.\n\n\nCode\nD <- 1 / sqrt(diag(L))\nNL <- Diagonal(x = D) %*% L %*% Diagonal(x = D)\nNL\n\n\n10 x 10 sparse Matrix of class \"dgCMatrix\"\n                                                                            \n [1,]  1.0000000 -0.2041241  .          .    .         -0.1889822  .        \n [2,] -0.2041241  1.0000000 -0.2041241  .   -0.1825742 -0.1543033 -0.1825742\n [3,]  .         -0.2041241  1.0000000  .   -0.2236068 -0.1889822  .        \n [4,]  .          .          .          1.0  .          .          .        \n [5,]  .         -0.1825742 -0.2236068  .    1.0000000  .         -0.2000000\n [6,] -0.1889822 -0.1543033 -0.1889822  .    .          1.0000000 -0.1690309\n [7,]  .         -0.1825742  .          .   -0.2000000 -0.1690309  1.0000000\n [8,] -0.2500000  .          .          .   -0.2236068 -0.1889822  .        \n [9,] -0.2041241 -0.1666667 -0.2041241  .   -0.1825742 -0.1543033 -0.1825742\n[10,]  .          .          .         -0.5  .         -0.1889822 -0.2236068\n                                      \n [1,] -0.2500000 -0.2041241  .        \n [2,]  .         -0.1666667  .        \n [3,]  .         -0.2041241  .        \n [4,]  .          .         -0.5000000\n [5,] -0.2236068 -0.1825742  .        \n [6,] -0.1889822 -0.1543033 -0.1889822\n [7,]  .         -0.1825742 -0.2236068\n [8,]  1.0000000  .         -0.2500000\n [9,]  .          1.0000000  .        \n[10,] -0.2500000  .          1.0000000\n\n\n\nExample Spectrums of Normalized Laplacian\n\nRingPathCompleteBipartite\n\n\n\n\nCode\ng <- make_ring(5)\nplot(g)\n\n\n\n\n\nCode\nN <- laplacian_matrix(g, normalized = TRUE)\n# 1 - cos(2 * pi * 0:4 / 5) |> sort() # exact\neigen(N)$values |> zapsmall()\n\n\n[1] 1.809017 1.809017 0.690983 0.690983 0.000000\n\n\n\n\n\n\nCode\ng <- graph(~1-2-3-4-5)\nN <- laplacian_matrix(g, normalized = TRUE)\n# 1 - cos(pi * 0:4 / 4) |> sort() # exact\neigen(N)$values |> zapsmall()\n\n\n[1] 2.0000000 1.7071068 1.0000000 0.2928932 0.0000000\n\n\n\n\n\n\nCode\ng <- make_full_graph(5)\nN <- laplacian_matrix(g, normalized = TRUE)\neigen(N)$values |> zapsmall() # n / n-1\n\n\n[1] 1.25 1.25 1.25 1.25 0.00\n\n\n\n\n\n\nCode\ng <- make_full_bipartite_graph(6, 4)\nN <- laplacian_matrix(g, normalized = TRUE)\neigen(N)$values |> zapsmall() # 0, 1 (n + m - 2), 2\n\n\n [1] 2 1 1 1 1 1 1 1 1 0\n\n\n\n\n\n\n\n\n17.3.5 Fiedler Bounds (normalized Laplacian)\nFiedler Eigenvalue is the smallest non-zero eigenvalue.\n\nSimple\nFor k-regular graph, and diameter > 4, we have\n\n\\begin{aligned}\n\\limsup \\lambda_{n-1} \\leq 1 - 2\\frac{\\sqrt{k-1}}{k}\n\\end{aligned}\n\n\nequality is reached for ramanujan graphs\nnote that the conditions exclude bipartite graphs\nthe fact that it’s “lim sup” makes this quite useless, because it’s asymptotically toward infinity”. In fact, for some regular 4 graphs, 14 node, diameter 5 graphs, the Fiedler value is still well above this bound.\n\nRather, a more general (and useful upper bound):\n\n\n\n\n\n\nLemma 1.14: Fiedler Upper Bound (dia \\geq 4) (Chung 1997)\n\n\n\nLet G be a graph with diameter D \\geq 4, and let k denote the maximum degree of G. Then,\n\n\\begin{aligned}\n\\lambda_{n-1} \\leq 1 - 2\\frac{\\sqrt{k-1}}{k}\\left(1 - \\frac{2}{D}\\right) + \\frac{2}{D}\n\\end{aligned}\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nBy contracting weighted graphs, and using Rayleigh quotient to upper bound.\n\n\n\n\n\nNote that the original reference (Nilli 1991) , studies this problem for non-standardized Laplacian.\n\n\nCode\nupper_fiedler_nilli <- function(k, D) {\n  if (D < 4) abort(\"upper bound only valid when diameter greater than 4\")\n  return(1 - 2 * sqrt(k -1) / k * (1 - 2 / D) + 2 / D)\n}\n\n\nLet’s generate all permutations of connected 8 graphs, and pick out those with diameter greater than 4. All graph generation is done with the program geng and filtered with pickg (McKay and Piperno 2013).\n\n\nCode\ng8_dia4plus_list <- read_file6(\"data/graph8cdia4p.g6\", type = \"igraph\")\n\ng8_dia4plus_df <- tibble(g = g8_dia4plus_list) |> \n  rownames_to_column(\"g_id\") |> \n  rowwise() |> \n  mutate(fiedler = net_fiedler(g, normalize = T),\n         k = max(degree(g)),\n         D = diameter(g),\n         upper_fiedler = upper_fiedler_nilli(k, D)) |> \n  arrange(desc(upper_fiedler), desc(fiedler))\n  \ng8_dia4plus_df |> \n  ggplot(aes(x = factor(g_id, levels = unique(g_id)),\n             y = fiedler, color = \"Fiedler Value\")) +\n  geom_point(size = .5, alpha = .7) +\n  geom_line(aes(y = upper_fiedler, group = upper_fiedler, color = \"Upper Bound\")) +\n  labs(color = \"Color\",\n       title = \"Nilli Bound on Fiedler Value for connected-8 Graphs\",\n       x = \"Unique Graph Combinations\",\n       y = \"Fiedler Value\") +\n  scale_color_manual(values = c(\"black\", \"red\")) +\n  scale_x_discrete(breaks = NULL, labels = NULL) + \n  theme_minimal()\n\n\n\n\n\n\n\nCode\n# files with regular, diameter larger than 4\n# These are all 4 regular graphs, connected, 13/14 nodes, d\nreg4_dia4plus_files <- c(\"data/reg4c13dia4.g6\", \"data/reg4c13dia5.g6\", \"data/reg4c14dia4.g6\", \"data/reg4c14dia5.g6\") \nreg4_dia4plus_list <- reg4_dia4plus_files |> \n  map(read_file6, type = \"igraph\")\n\nreg4_dia4plus_df <- tibble(g = reg4_dia4plus_list, path = reg4_dia4plus_files)  |> \n  unnest_longer(g, indices_include = TRUE, indices_to = \"graph_id\") |> \n  rowwise() |> \n  mutate(match = list(str_match(path, \"data/reg4c(\\\\d*)dia(\\\\d*)\")),\n         nodes = match[2],\n         dia = match[3],\n         k = 4,\n         fiedler = net_fiedler(g, normalized = T),\n         upper_fiedler = 1 - 2*sqrt(4-1) / 4) |> \n  select(-c(match, path)) |> arrange(desc(dia), nodes)\n\nreg4_dia4plus_df <- reg4_dia4plus_df |> mutate(dia = as.numeric(dia),\n                           islower = upper_fiedler > fiedler,\n                           other_upper = 1 - 2 * sqrt(4 - 1)/4 * (1 - 2 / dia) + 2/dia,\n                           other_islower = other_upper > fiedler) |> \n  arrange(islower, other_islower, desc(dia))\n\n# among regular graphs, 13-14 nodes, 4+ diameter\nreg4_dia4plus_df\n\n\n# A tibble: 4,271 × 10\n# Rowwise: \n   g        graph_id nodes   dia     k fiedler upper_f…¹ islower other…² other…³\n   <list>      <int> <chr> <dbl> <dbl>   <dbl>     <dbl> <lgl>     <dbl> <lgl>  \n 1 <igraph>        1 13        4     4   0.203     0.134 FALSE      1.07 TRUE   \n 2 <igraph>        2 13        4     4   0.250     0.134 FALSE      1.07 TRUE   \n 3 <igraph>        3 13        4     4   0.207     0.134 FALSE      1.07 TRUE   \n 4 <igraph>        4 13        4     4   0.205     0.134 FALSE      1.07 TRUE   \n 5 <igraph>        5 13        4     4   0.204     0.134 FALSE      1.07 TRUE   \n 6 <igraph>        6 13        4     4   0.257     0.134 FALSE      1.07 TRUE   \n 7 <igraph>        7 13        4     4   0.255     0.134 FALSE      1.07 TRUE   \n 8 <igraph>        8 13        4     4   0.257     0.134 FALSE      1.07 TRUE   \n 9 <igraph>        9 13        4     4   0.188     0.134 FALSE      1.07 TRUE   \n10 <igraph>       10 13        4     4   0.192     0.134 FALSE      1.07 TRUE   \n# … with 4,261 more rows, and abbreviated variable names ¹​upper_fiedler,\n#   ²​other_upper, ³​other_islower\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\nreg4_dia4plus_df |> unite(col = \"g_uid\", !!!c(\"nodes\", \"dia\", \"graph_id\"), remove = FALSE) |>\n  ggplot(aes(fct_reorder2(g_uid, fiedler, dia, .desc = T), fiedler)) +\n  geom_point() + \n  geom_point(aes(y = upper_fiedler), color = \"red\") +\n  geom_point(aes(y = other_upper), color = \"red\")\n\n\n\n\n\nCode\n# reg4_dia4plus_df\n# reg4_dia4plus_df # well... they are all lower bounded? if it's an infinite family of regular graphs... it should get closer right? b/c it's lim sup?\n\n\n\n\nBy Volume (global)\nA loose lower bound for the smallest non-trivial eigenvalue (Chung 1997, 7):\n\n\\begin{aligned}\n\\lambda_{n-1} \\geq \\frac{1}{D\\operatorname{vol}(G)}\n\\end{aligned}\n where D is the diameter of the graph, and volume of the graph is the sum of degrees for each node.\n\n\n\n\n\n\nDefinition of Volume\n\n\n\nNote that Chung (1997) uses the definition \\operatorname{vol}(G) = \\sum_{x\\in S}d_x where d_x is the degree of vertex x. Other references use \\operatorname{vol}(G) = |G| which is the number of nodes, or giving each node weight 1.\n\n\nLet’s find the Fiedler value of every graph of size 5 and graph them against the bound,\n\n\nCode\ng5_list <- read_file6(\"data/graph5c.g6\", type = \"igraph\")\n\n# fiedler values\nfiedler5 <- lapply(g5_list, function(g) {\n  vol <- sum(degree(g)) # conservative\n  # vol <- vcount(g)\n  M <- laplacian_matrix(g, normalized = TRUE)\n  dia <- diameter(g, directed = FALSE)\n  list(g = list(g),\n       fiedler = eigen(M)$values[4],\n       vol = vol,\n       dia = dia)\n})\n\ngraph_fiedler <- tibble(gg = fiedler5) |> unnest_wider(gg) |> unnest(cols = c(g)) |> \n  mutate(bound = 1/ (dia * vol),\n         diff = fiedler - bound) |> \n  arrange(diff)\n\n\n# calculate the lower bound.\nfiedler_bound_line <- tibble(dia = rep(list(seq(1, 4, .1)), 3), vol = c(5, 10, 20)) |> rowwise() |> \n  mutate(bound = list(1 / vol / dia)) |> \n  unnest_longer(col = c(dia, bound))\n\n\n# The bound is generally quite bad for graphs \ngraph_fiedler |> ggplot(aes(dia, fiedler, color = vol)) +\n  geom_line(data = fiedler_bound_line, mapping = aes(dia, bound, color = vol, group = vol)) +\n  geom_point() +\n  labs(title = \"Simple Fiedler Bound\")\n\n\n\n\n\nEven using the vertex count definition for volume (less conservative), the bound is quite low for most of the Fiedler values.\n\n\nBy Cheeger’s (Sparsest Cut)\nCheeger’s constant is loosely defined in english, as the minimal ratio, of cost of cutting edges, to the size of sets it cuts off. That is, a “dumbbell” shape graph, where large vertex sets are on both side, and only cutting 1 edge in the middle would have a very very low cheeger constant.\n\n\\begin{aligned}\nh_G &= \\min_S \\frac{|\\delta S|}{\\min \\{|S|,|\\bar S| \\}} \\\\&= \\frac{\\text{cutting edges cost}}{\\text{vertex set volume}}\n\\end{aligned}\n\nCalculating Cheeger’s constant is an NP-Hard problem, meaning that the problem is likely non-polynomial for solution and checking.\nThe bounds on Fiedler’s value, with cheegers constants have the form,\n\n\\begin{aligned}\n\\frac{h_G^2}{2d_{max}} \\leq \\lambda_{n-1}  \\leq 2h_G\n\\end{aligned}\n\n\n\nCode\n# graph_boundary_edges <- function(g, vs) {\n#   if(!inherits(g, \"igraph\")) {\n#     abort(\"`g` must be of class `igraph`\")\n#   }\n#   inc_edges <- igraph::incident_edges(g, vs)\n#   purrr::reduce()\n# }\n\n# graph_interior_edges <- function(g, vs) {\n#   if(!inherits(g, \"igraph\")) {\n#     abort(\"`g` must be of class `igraph`\")\n#   }\n#   inc_edges <- igraph:incident_edges(g, vs)\n# }\n\n\n# forget the functions... just do on matrices\n\n\n\n\nCode\n#TODO: implemented assuming undirected graph, generalize?\ncheeger <- function(A, S) {\n  # calculate vertex boundary by taking neighbors minus initial set\n  adj_vec_set <- A[S,, drop = F] |> colSums()\n  ego1 <- which(adj_vec_set > 0)\n  dS <- setdiff(ego1, S) # vertex boundary\n  \n  # calculate edge boundary by subsetting matrix from S to !S\n  bridge_matrix <- A[S, dS, drop = F]\n  \n  bridge_edges <- bridge_matrix |> \n    as(\"lgCMatrix\") |> # also assumes unweighted here when converting to logical\n    which(arr.ind = T) \n  bridge_edges[,\"row\"] <- S[bridge_edges[,\"row\"]] # convert indices\n  bridge_edges[,\"col\"] <- dS[bridge_edges[,\"col\"]]\n  vol_dS <- sum(bridge_matrix)\n  min_vol_S <- min(length(S), nrow(A) - length(S)) # use length as volume metric\n  return(vol_dS / min_vol_S)\n}\n\n# generate subsets for each graph\nall_comb <- mapply(function(x) combn(5, x, simplify = F), 1:5)\nall_comb_df <- tibble(elem = all_comb) |>\n  unnest_longer(elem)\n\ngraph_df <- g5_list |> lapply(as_adj) |>\n  tibble(A = _) |> rownames_to_column(var = \"id\") |> \n  mutate(id = as.numeric(id))\n\ncheeger_df <- full_join(graph_df, all_comb_df, by = character()) |> rowwise() |> \n  mutate(cheeger_values = cheeger(A, elem))\n\ncheeger_constant_df <- cheeger_df |> filter(cheeger_values > 0) |>  group_by(id) |> arrange(id, cheeger_values) |> slice(1) |> ungroup()\n\n# cheeger_constant_df |> slice(2) |> unlist()\n\nfiedler_value_df <- tibble(gg = fiedler5) |> unnest_wider(gg) |> unnest(cols = c(g)) |> \n  rownames_to_column(var = \"id\") |> \n  rowwise() |> \n  mutate(id = as.numeric(id),\n         max_deg = max(degree(g))) |> \n  select(id, fiedler, dia, vol, max_deg) |> \n  ungroup()\n\n\ncheeger_fiedler_df <- cheeger_constant_df |> select(id, elem, cheeger_values) |> left_join(fiedler_value_df, by = \"id\") |> \n  mutate(upper_cheeger = cheeger_values * 2,\n         lower_cheeger = cheeger_values^2 / 2/ max_deg) # dividing by max deg\n# cheeger_fiedler_df |> mutate(inbound = fiedler > lower_cheeger & fiedler < upper_cheeger)\n\ncheeger_fiedler_df |>\n  ggplot() +\n  geom_point(aes(id, lower_cheeger), color = \"red\") +\n  geom_point(aes(id, fiedler, color = vol)) +\n  geom_point(aes(id, upper_cheeger), color = \"red\") +\n  labs(y = \"Eigenvalue\",\n       x = \"Graph ID\",\n       title = \"\")\n\n\n\n\n\nCode\n# many variations of cheeger's unfortunately\n# seems incorrect for\n\n\n\n\n\n17.3.6 Laplacian decomposition as incidence matrix\nIf we define an incidence matrix as a |V| \\times |E| matrix, in which each column has a 1 and -1 for in positions the edge connects the vertices,\n\n\nCode\n# ve_incidence <- function(g) {\n#   stopifnot(class(g) == \"igraph\")\n#   g %>% get.adjedgelist() %>% map_dbl(~-onehot(.x, n = ecount(g)))\n# }\n\n\nonehot <- function(x, n = max(x)) {\n  y <- vector(mode = \"numeric\",length = n)\n  y[x] <- 1\n  return(y)\n}\n\n\n# edge list\nve_incidence_matrix <- function(g) {\n  stopifnot(class(g) == \"igraph\")\n  onehot_edge <- function(x, n = max(x)) {\n    y <- vector(mode = \"numeric\",length = n)\n    y[x * sign(x)] <- sign(x)\n    return(y)\n  }\n  g_el <- get.edgelist(g) \n  g_el[,2] <- -g_el[,2]\n  g_el %>% apply(1, onehot_edge, n = vcount(g))\n}\n\nset.seed(1)\ng <- sample_gnp(10, .5)\nA <- g %>% as_adj()\nL <- Diagonal(x = rowSums(A)) - A\nB <- ve_incidence_matrix(g) # incidence matrix\n\n\nOur incidence matrix B looks like\n\n\nB = \\begin{bmatrix} 1 &0 &0 &0 &1 &0 &0 &0 &0 &0 &1 &0 &0 &1 &0 &0 &0 &0 &0 &0 &0 &0 &0 \\\\-1 &1 &1 &0 &0 &1 &0 &1 &0 &0 &0 &0 &0 &0 &1 &0 &0 &0 &0 &0 &0 &0 &0 \\\\0 &-1 &0 &1 &0 &0 &1 &0 &0 &0 &0 &0 &0 &0 &0 &1 &0 &0 &0 &0 &0 &0 &0 \\\\0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &1 &0 &0 &0 \\\\0 &0 &-1 &-1 &0 &0 &0 &0 &1 &0 &0 &1 &0 &0 &0 &0 &1 &0 &0 &0 &0 &0 &0 \\\\0 &0 &0 &0 &-1 &-1 &-1 &0 &0 &1 &0 &0 &1 &0 &0 &0 &0 &1 &0 &0 &1 &0 &0 \\\\0 &0 &0 &0 &0 &0 &0 &-1 &-1 &-1 &0 &0 &0 &0 &0 &0 &0 &0 &1 &0 &0 &1 &0 \\\\0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &-1 &-1 &-1 &0 &0 &0 &0 &0 &0 &0 &0 &0 &1 \\\\0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &-1 &-1 &-1 &-1 &-1 &-1 &0 &0 &0 &0 \\\\0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &0 &-1 &-1 &-1 &-1 \\\\ \\end{bmatrix}\n\n\n\n\nL= \\begin{bmatrix} 4 &-1 &0 &0 &0 &-1 &0 &-1 &-1 &0 \\\\-1 &6 &-1 &0 &-1 &-1 &-1 &0 &-1 &0 \\\\0 &-1 &4 &0 &-1 &-1 &0 &0 &-1 &0 \\\\0 &0 &0 &1 &0 &0 &0 &0 &0 &-1 \\\\0 &-1 &-1 &0 &5 &0 &-1 &-1 &-1 &0 \\\\-1 &-1 &-1 &0 &0 &7 &-1 &-1 &-1 &-1 \\\\0 &-1 &0 &0 &-1 &-1 &5 &0 &-1 &-1 \\\\-1 &0 &0 &0 &-1 &-1 &0 &4 &0 &-1 \\\\-1 &-1 &-1 &0 &-1 &-1 &-1 &0 &6 &0 \\\\0 &0 &0 &-1 &0 &-1 &-1 &-1 &0 &4 \\\\ \\end{bmatrix}\n\n\nJust to see the sparsity pattern of B, (we try spam’s display routine)\n\n\n\n\n\n\n\n17.3.7 Laplacian Stochastic Matrix\nI - D^{\\dagger}A"
  },
  {
    "objectID": "matrix/matrix.html#stochastic-matrices",
    "href": "matrix/matrix.html#stochastic-matrices",
    "title": "17  Matrix Math",
    "section": "17.4 Stochastic Matrices",
    "text": "17.4 Stochastic Matrices\nmatrices in which each matrix row sums to 1."
  },
  {
    "objectID": "matrix/matrix.html#matrix-norms",
    "href": "matrix/matrix.html#matrix-norms",
    "title": "17  Matrix Math",
    "section": "17.5 Matrix Norms",
    "text": "17.5 Matrix Norms\nThis section aims to quantify and give intuition behind the following matrix norms.\n\nspectral\nfrobenius\nL^n"
  },
  {
    "objectID": "meta/meta.html#effect-size",
    "href": "meta/meta.html#effect-size",
    "title": "18  Meta Analysis",
    "section": "18.1 Effect Size",
    "text": "18.1 Effect Size\nIn order to pool studies, the most important item is the effect size. Largely effect size can be categorized into relative or absolute. Another breakdown is whether the effect size is standardized or not.\n\n18.1.1 Mean\n\n\n18.1.2 Median\n\n\n18.1.3 Proportion\n\n\n18.1.4 Logit Proportion\n\n\n18.1.5 Correlation\nStandard error formulas are not trivial. Even something like the correlation coefficients have been glossed over and never revisited. We’ve said “good enough”! The commonly reported values are given here:\n\n(1 - \\rho^2)^2 / n is reported by Hald (2008) pp. 126\n\\frac{(1-\\rho^2)^2}{n-1} is reported by Dingman and Perry (1956)\n\\frac{(1-\\rho^2)^2}{n-2} is reported by\n\\frac{\\rho^2}{n}\\left(\\frac{M_{22}}{\\rho^2\\sigma_x^2 \\sigma_y^2} + \\frac{M_{40}}{4\\sigma_x^4} + \\frac{M_{04}}{4\\sigma_y^4} - \\frac{M_{13}}{\\rho \\sigma_x\\sigma_y^3} - \\frac{M_{31}}{\\rho \\sigma_x^3\\sigma_y1} + \\frac{M_{22}}{\\rho \\sigma_x^2\\sigma_y^2}\\right) is reported by Sheppard and Forsyth (1899) (in the bivariate normal case, this reduces to the first case)\n\n\n\nCode\nsample_cor_norm <- function(mean1, mean2, sd1, sd2, n) {\n  x <- rnorm(n, mean1, sd2)\n  y <- rnorm(n, mean2, sd2)\n  cor(x, y)\n}\nsample_cor_norm_std <- partial(sample_cor_norm, mean1 = 0, mean2=0, sd1=1, sd2=1)\n\n# resampling simulation, (for n = 10, 20, 30, repeat the sampling 10 times.)\ndat_resample_cor_norm_std <- seq(10, 410, 20) |> \n  enframe(name = NULL, value = \"n\") |> \n  rowwise() |> \n  mutate(sample_cor = list(replicate(10, sample_cor_norm_std(n))), # increase replicates to make it closer to truth\n         sample_cor_se = sd(sample_cor))\n\ntheoretical_resample_cor_se <- tibble(x = seq(3, 410),\n                             y = 1/sqrt(x),\n                             y1 = 1/sqrt(x -1),\n                             y2 = 1/sqrt(x-2)) |> \n  pivot_longer(cols = y:y2)\n\ndat_resample_cor_norm_std |> ggplot() +\n  geom_point(aes(n, sample_cor_se)) +\n  geom_line(data = theoretical_resample_cor_se,\n            aes(x, value, linetype = name), color = \"red\")\n\n\n\n\n\nIt doesn’t really matter the denominator, it’s not a big difference. This looks at the actual values of sampling correlation coefficient as n increases.\n\n\nCode\n# Alternative way of looking at the drop off...\ndat_cor_norm_std <- 5:200 |> \n  enframe(name = NULL, value = \"n\") |> \n  rowwise() |> \n  mutate(sample_cor = sample_cor_norm_std(n))\n\ntheoretical_cor_se <- tibble(x = seq(5,200, .5),\n                             y2 = 2/sqrt(x),\n                             y_2 = -2/sqrt(x)) |> \n  pivot_longer(y2:y_2)\ndat_cor_norm_std |> ggplot(aes(n, sample_cor)) +\n    geom_line(data = theoretical_cor_se,\n            mapping = aes(x, value, group = name), linetype = 2, color = \"red\") +\n  geom_line(color = \"grey50\") +\n  theme_minimal() +\n  lims(y = c(-.8, .8)) +\n  labs(y = \"Sample Pearson Correlation\",\n       title = \"2-SE Envelope of Sample Correlation as function of n\")\n\n\nWarning: Removed 6 row(s) containing missing values (geom_path).\n\n\n\n\n\nWe have the luxury of just resampling from our target population, so we don’t really have to bootstrap, but evaluating how good the bootstrap is in various situations is probably worth looking into as well.\nNow we’re curious how bad these distributions can get if the distributions are skewed. Let’s do a correlated example with exponential distributions.\n\n\nCode\n# simulating function for correlated variables\nsample_cor_exp <- function(lambda1, lambda2, n) {\n  x <- rexp(n, lambda1)\n  y <- rexp(n, lambda2)\n  cor(x, x + y) # expected correlation is 1/lambda1 + 1/lambda2\n}\n\ndat_resample_cor_exp <- seq(10, 410, 20) |> \n  enframe(name = NULL, value = \"n\") |> \n  rowwise() |> \n  mutate(sample_cor = list(replicate(50, sample_cor_exp(2, 3, n))), # increase replicates to make it closer to truth\n         sample_cor_se = sd(sample_cor))\n\nrho <- 5/6\ntheoretical_resample_cor_se <- tibble(x = seq(3, 410),\n                             y = (1 - rho^2)/sqrt(x))\n\ntheoretical_resample_cor_se |> ggplot() +\n  geom_line(mapping = aes(x = x, y = y), \n            color = \"red\",\n            linetype = 2) +\n  geom_point(data = dat_resample_cor_exp,\n             mapping = aes(n, sample_cor_se)) +\n  labs(y=\"standard error\",\n       title = \"Standard Errors vs \\u03C1 = 5/6\")\n\n\n\n\n\nOverall, when the distribution is skewed, we’re notably underestimating the standard error almost across the board.\n\n\nCode\n# envelope graph\ndat_sample_cor_exp <- 5:200 |> \n  enframe(name = NULL, value = \"n\") |> \n  rowwise() |> \n  mutate(sample_cor = sample_cor_exp(2, 3, n)) # 5/6 cor theoretical\n\n# theoretical envelop, upper lower 2nd std dev, around (d)\nrho <- 5/6\ntheoretical_cor_se <- tibble(x = seq(5,200, .5),\n                             y_upper = 2 * (1 - rho^2)/sqrt(x) + rho,\n                             y_lower = -2 * (1 - rho^2)/sqrt(x) + rho) |> \n  pivot_longer(starts_with(\"y\"))\n\ndat_sample_cor_exp |> ggplot() +\n  geom_line(aes(x = n, y= sample_cor), color = \"grey50\") +\n  geom_line(data = theoretical_cor_se, \n            mapping = aes(x = x, value, group = name),\n            color = \"red\") +\n  theme_minimal() +\n  labs(title = \"2-SE Envelope of Sample Correlation of Exponential RV when \\u03C1 = 5/6\")\n\n\n\n\n\nWhen the distributions are skewed, it seems they peak the bottom a little more, but this matches the story above in which we are underestimating the standard error a little, because it’s poking out a little more all over.\n\n\nZ transformed Pearson"
  },
  {
    "objectID": "meta/meta.html#diversity-indices",
    "href": "meta/meta.html#diversity-indices",
    "title": "18  Meta Analysis",
    "section": "18.2 Diversity Indices",
    "text": "18.2 Diversity Indices\n\nShannon diversity\nHill Diversity Indices\n\nIt has been noted that there are biases when bootstrapping these diversity indices.1 Furthermore, there have been attempts to quantify the standard error as well.2\n\n18.2.1 Hill/Renyi Diversity\n\n\\begin{align*}\nD_a &= \\left(\\sum_i^S p_i^a\\right)^{\\frac{1}{1-a}} \\\\\nD_0 &= \\sum_i^S p_i^0 = S & \\text{Species Richness}\\\\\nD_1 &= \\exp\\left(-\\sum_i p_i \\log p_i\\right) & \\text{Exp(Shannon-Wiener Entropy Index)} \\\\\nD_2 &= \\frac{1}{\\sum_i^S p_i^2} & \\text{Simpson's Reciprocol Index}\n\\end{align*}\n\n\n\n\nLimit details for D_1\n\nWhen a = 1, the exponent becomes 1/0, so we must use the limiting definition as a\\rightarrow 1 to see how the diversity metric reduces to entropy.\n\n\\begin{align*}\n\\lim_{a\\rightarrow 1} \\left(\\sum_{i}^S p_i^a\\right)^{\\frac{1}{1-a}} &= \\lim_{a\\rightarrow 1} \\exp\\left(\\frac{1}{1-a}\\log\\left(\\sum_{i}^S p_i^a\\right)\\right) \\\\\n&= \\exp\\left(\\lim_{a\\rightarrow 1}\\frac{\\log\\left(\\sum_{i}^S p_i^a\\right)}{1-a}\\right) \\\\\n&= \\exp\\left(\\lim_{a\\rightarrow 1}\\frac{\\sum_i^S p^a_i \\log_i p_i}{-\\sum_i^S p_i^a}\\right) \\\\\n&= \\exp\\left(-\\sum_i^S p \\log_i p_i\\right)\n\\end{align*}\n\nwhere we’ve used L’hopitals rule in step 2, and \\sum_i^S p_i = 1 in step 3.\n\n\n\n\n18.2.2 Species Diversity\nThe most basic metric, basically binarizing the counts, and saying these are all the species that we see.\n\n\n18.2.3 Shannon diversity\nShannon diversity is defined as:\n\n\\begin{align*}\nH = -\\sum p_i \\log p_i\n\\end{align*}\n\n\np_i is the proportion among all the species, so \\sum p_i = 1.\nH is maximized when proportions are the same among proportions\nSpecies with count 0 do not contribute to entropy (they also don’t change the proportion of existing species).\n\nHowever if using Shannon Equitability Index, the total species count does matter.\n\n\nIs Shannon diversity dependent on number of species? Assume uniform distribution.\n\n\nCode\nS <- 2:200\nH <- S |> map_dbl(~entropy(rep(20, .x)))\nH_equitable <- H / log(S)\ntibble(S, H, H_equitable) |> ggplot() +\n  geom_line(aes(S, H, color = \"Shannon Index (H)\")) +\n  geom_line(aes(S, H_equitable, color = \"Equitable Shannon Index\")) +\n  theme_minimal() +\n  labs(y = \"Entropy\",\n       title = \"Uniform distribution of species\")\n\n\n\n\n\nWhat’s the sampling distribution of Shannon’s diversity index?\n\n\nCode\n#' simulate uniform counts\n#'\n#' @param S number of species\n#' @param n sequencing depth\n#'\n#' @return\n#' @export\n#'\n#' @examples\nsim_uniform_species <- function(S, n = S*5){\n  y <- rdunif(n, S)\n  table(y)[as.character(1:S)] %|% 0L |> as.numeric()\n}\n\n# gives sample community matrix, with uniform distribution across both margins.\nsim_uniform_community_matrix <- function(S, sample_n = 10, sample_min = 100, sample_max = 400) {\n  # sequencing effort will vary between samples\n  n <- runif(sample_n, sample_min, sample_max)\n\n  # each line is a count from samples\n  mapply(n, FUN = sim_uniform_species, S = S) |> \n    t()\n}\n\n\n# Bias of entropy estimation as a function of sequencing depth.\nentropy_uniform_se <- expand.grid(n = c(2, 5, 10, 50, 200, 500, 1000),\n                                  S = 2:200) |> rowwise() |> \n  mutate(H = entropy(sim_uniform_species(S, n)))\n\nS <- seq(2:200)\nH_theoretical <- S |> map_dbl(~entropy(rep(10, .x)))\ntheoretical_entropy_uniform <- tibble(S, H_theoretical)\n\nentropy_uniform_se |> ggplot() +\n  geom_line(aes(S, H, color = n, group = n)) +\n  geom_line(data = theoretical_entropy_uniform,\n            mapping = aes(S, H_theoretical)) +\n  scale_color_gradient(trans = \"log\", breaks = c(2, 10, 100, 1000))\n\n\n\n\n\nThe bias it seems can be quite massive if you choose different sequencing efforts. Thus, we must standardize our metric before comparing alpha diversity.\nNormalization Techniques (Slides with more3)\n\nSubsampling to a common depth (rarefaction)\nEqualize depths by scaling OTU counts to common depth (different from rarefaction, scaling vs subsampling)\nTransform counts into relative abundance (denominator is each sample)\nNormalize data based on 16S\n\nNow we’ll try to quantify this bias in the Shannon Diversity Metric, using different estimators of the uniform.\n\n\nCode\n# Chao Shen entropy estimator\nS <- 100\nn <- 50:1000\n\nset.seed(1)\ndat_entropy_bias <- expand_grid(n, S) |> \n  rowwise() |> \n  mutate(y = list(sim_uniform_species(S, n)),\n         entropy_est = list(list(H_ML = entropy(y),\n                            H_MM = entropy(y, method = \"MM\"),\n                            H_Jeffreys = entropy(y, method = \"Jeffreys\"),\n                            H_Laplace = entropy(y, method = \"Laplace\"),\n                            H_minimax = entropy(y, method = \"minimax\"),\n                            H_CS = entropy(y, method = \"CS\"),\n                            H_shrink = entropy(y, method = \"shrink\", verbose = FALSE))),\n         H_Z = Entropy.z(y)) |> \n  unnest_wider(entropy_est) |> \n  pivot_longer(cols = starts_with(\"H\"), names_to = \"H_type\" , values_to = \"H\")\n\ndat_entropy_true <- tibble(n, H_theory = entropy(rep(1, 100)))\n\ng <- dat_entropy_bias |> ggplot() +\n  geom_line(data = dat_entropy_true,\n            mapping = aes(n, H_theory), color = \"red\") + \n  geom_line(aes(n, H, color = H_type), alpha = .5)\n\nconfig(ggplotly(g), modeBarButtonsToRemove = c(\"zoom\", \"pan\", \"select\", \"zoomIn\", \"zoomOut\",\n                                               \"autoScale\", \"select2d\", \"hoverClosestCartesian\", \"hoverCompareCartesian\"), displaylogo = FALSE)\n\n\n\n\n\n\nThe H_ML is the standard plug in estimator of the entropy. We can see that most of them underestimate the true entropy, but the shrinkage estimator does surprisingly well. This is something to look into, I think it shrinks toward the uniform distribution anyway, so it’s not really a fair comparison. overall we note that:\n\nH_CS probably has the highest standard error\n\n\n\n18.2.4 Rarefaction (aside)\nThere is a difference in terminology rarefaction and rarifying. Rarefaction curves are the curves, plotting affect of the alpha diversity metric against the sampling effort. rarifying is the practice of subsampling to a similar depth.\nThe main idea, is that having more sample counts means that more species are likely to be represented. Thus, in order to make samples comparable, we should calculate an “expected species” count with some sort of common effort. Rarefied estimators for species counts are statistically inadmissable (McMurdie and Holmes 2014), due to the simple fact that we’re throwing away data, and thus artificially increasing standard error when we don’t need to. However, it still remains a common technique so understanding why it’s not optimal is still relevant. Since rarefying is basically based on hypergeometric subsampling, exact formulas for standard error and expected value exist.\nIn general, this is what rarefied curves look like:\n\n\nCode\ndata(BCI)\nrarecurve(BCI)\nabline(v=300, col = 2)\n\n\n\n\n\nA single curve is one sample (row), and plots the expected species count as a function of sub-sampling depth.\n\n\nCode\nset.seed(1)\nuniform_cm <- sim_uniform_community_matrix(100)\n# rarefying gives expected species diversity.\nrarefy(uniform_cm, 118, se = T)\n\n\n        [,1]     [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\nS  67.860019 68.13418 68.816150 69.537161 70.031099 69.334350 68.897680\nse  2.417449  2.72915  2.953008  3.075216  2.255168  3.063407  3.047504\n        [,8]      [,9] [,10]\nS  69.540933 69.126184    69\nse  3.020944  2.948892     0\nattr(,\"Subsample\")\n[1] 118\n\n\nCode\n# sum(uniform_cm[10,] != 0) # 69, since this is actual data for 10, the se is zero.\nuniform_drare <- drarefy(uniform_cm, 118) # margin sums are the expected species count, so each cell is the probability of seeing that species \n\n# subsample first one 179 down to 118.\ncbind(uniform_drare[1, ],\n      1 - exp(lchoose(179 - uniform_cm[1,], 118) - lchoose(179, 118))) |> head()\n\n\n      [,1]      [,2]\n 0.8851296 0.8851296\n 0.9617099 0.9617099\n 0.0000000 0.0000000\n 0.0000000 0.0000000\n 0.6592179 0.6592179\n 0.6592179 0.6592179\n\n\ndrarify formula counts subsampling arrangements without replacement. Let’s say sample 1 had 200 counts total, and we wanted to subsample down to 120. Then, we the total number of subsamples is taking those 120 of those 200 counts. We can imagine that we’re pulling from a pool of S_1, S_2, S_2, S_3, for which the sample counts would be c(1, 2, 1).\n\n\\begin{align*}\nP(\\text{at least one }S_1 \\text{ sampled}) =1 - \\frac{\\binom{200 - S_1}{120}}{\\binom{200}{120}}\n\\end{align*}\n\n\n\nCode\nrarecurve(uniform_cm, sample = 118)\n\n\n\n\n\nWhen all the curves are coming from the same distribution, it’s reasonable that the rarefied curves will all look the same."
  },
  {
    "objectID": "meta/meta.html#estimators-for-species-diversity",
    "href": "meta/meta.html#estimators-for-species-diversity",
    "title": "18  Meta Analysis",
    "section": "18.3 Estimators for Species Diversity",
    "text": "18.3 Estimators for Species Diversity\n\nbreakaway - estimates through nonlinear regression of probability ratios, fraction of singleton’s to empty, doubles to singles, etc. Kemp has characterized the distribution of these ratios.\nDivNet Paper\nEntropy Estimators\n\nAsymptotic Normality of Entropy Estimator\nEntropic Representation and Estimation of Diversity Indices"
  },
  {
    "objectID": "meta/meta.html#entropy",
    "href": "meta/meta.html#entropy",
    "title": "18  Meta Analysis",
    "section": "18.4 Entropy",
    "text": "18.4 Entropy\n\n\nCode\n# se calculated by delta method (for binomial at least)\nentropy_delta_se <- function(n, p) {\n  sqrt(p * (1 - p) / n * (1 + log(p))^2)\n}\n\np <- c(.01, .1, .3, .5)\nn <- seq(5, 200, 15)\nsim_df <- expand_grid(n, p)\nentropy_se_df <- sim_df |> rowwise() |> \n  mutate(y = list(rbinom(200, n, p)),\n         p_hat = list(y / n),\n         sample_entropy = list(p_hat * log(p_hat)),\n         se_sample_entropy = sd(sample_entropy, na.rm = TRUE),\n         delta_se = entropy_delta_se(n, p))\n\n# deltamethod(~ 1 / (x1 + x2))\n# deriv(expression(x * log(x)), \"x\", function.arg = TRUE)\n# deriv(~x * log(x), \"x\")\n\n\nentropy_se_df |> ggplot(aes(n, se_sample_entropy, color = factor(p))) +\n  geom_point() +\n  geom_line(mapping = aes(n, delta_se),\n            linetype = 2) +\n  facet_wrap(~p, nrow = 2)"
  },
  {
    "objectID": "meta/meta.html#group-differences",
    "href": "meta/meta.html#group-differences",
    "title": "18  Meta Analysis",
    "section": "18.5 Group Differences",
    "text": "18.5 Group Differences\nThere’s a lot of these, so here’s a quick list\n\n18.5.1 Mean Difference\n\n\n18.5.2 Standardized Mean Difference (Cohen’s d)\nNote that the standard t statistic is defined as \\frac{\\sqrt{n}(\\bar x_2 - \\bar x_1)}{s_p} which differs from d by a factor of \\sqrt{n}.\n\nAdjustments\n\n\nName\n\nEstimator\nSE\n\n\n\n\nMD\n\n\\bar x_2 - \\bar x_1\n\ns_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\n\n\nSMD\nCohen’s d\n\\frac{\\bar x_2 - \\bar x_1}{s_{p}}\n\n\\sqrt{\\frac{n_1 + n_2}{n_1n_2} + \\frac{d^2}{2(n_1 + n_2)}}\n\nSO Derivation of SE\n\n\n\nHedge’s g\n\n\n\n\n(log) Risk Ratio\n\n\\log\\left(\\frac{a/n_{1}}{b/n_2}\\right)\n\n\n\n(log) Odds Ratio\n\n\\log\\left(\\frac{a/b}{c/d}\\right)\n\n\n\n(log) Incidence Risk Ratio\n\n\n\n\n\nHazard Ratio\n\n\n\n\n\n\n\nreliability / test-retest / attenuated/ error in measurement models\n\nThese types of estimators standardize by \\sqrt{r_{xx}}, when available and many variants thereafter\n\nSmall Sample Adjustments\n\nGenerally small samples have quite a large bias"
  },
  {
    "objectID": "meta/meta.html#pooling-effect-size",
    "href": "meta/meta.html#pooling-effect-size",
    "title": "18  Meta Analysis",
    "section": "18.6 Pooling Effect Size",
    "text": "18.6 Pooling Effect Size\nNow that we have an understanding of how effect sizes matter for different variables, how do we actually pool the studies?\n\n\n\n\n\n\nDon’t use lme or lm\n\n\n\nAlthough lme() and lm() have a weights argument, they are NOT (directly) appropriate for meta analyses. The weights that are provided to those arguments are relative, and thus still estimate an additional error component. The (inverse) weights from the meta analyses are different in that they should fully specify the error variance, and they should be treated as fixed variance. This class of models falls under “small area estimation” or Fay-Harriot models.\nViechtbauer (author of metafor R package) has a great post about this topic.\n\n\nWe’ll follow along with the Chapter 4: Doing Meta Analysis in R.\n\n\nCode\n## Fixed Efffect estimating\ndata(SuicidePrevention)\n# Since all raw data, need to calculate standardized effect size metrics\nSP_calc <- esc_mean_sd(grp1m = SuicidePrevention$mean.e,\n                       grp1sd = SuicidePrevention$sd.e,\n                       grp1n = SuicidePrevention$n.e,\n                       grp2m = SuicidePrevention$mean.c,\n                       grp2sd = SuicidePrevention$sd.c,\n                       grp2n = SuicidePrevention$n.c,\n                       study = SuicidePrevention$author,\n                       es.type = \"g\") %>% \n                     as.data.frame()\n\n\nWarning in missing(grp1sd) || is.null(grp1sd) || is.na(grp1sd): 'length(x) = 9 >\n1' in coercion to 'logical(1)'\n\n\nWarning in missing(grp2sd) || is.null(grp2sd) || is.na(grp2sd): 'length(x) = 9 >\n1' in coercion to 'logical(1)'\n\n\nCode\ncbind(SP_calc$weight, \n1/ SP_calc$se^2) # weights are calculated by inverse se^2\n\n\n          [,1]     [,2]\n [1,] 46.09784 46.09784\n [2,] 34.77314 34.77314\n [3,] 14.97625 14.97625\n [4,] 32.18243 32.18243\n [5,] 24.52054 24.52054\n [6,] 54.50431 54.50431\n [7,] 29.99941 29.99941\n [8,] 19.84837 19.84837\n [9,] 26.63768 26.63768\n\n\nCode\nfe_est <- with(SP_calc, sum(es * weight) / sum(weight)) # pooled estimate \n\n\nRandom effect weighting adds and additional\nConsider a Knapp and Hartung (2003) adjustment, changing tests to use t-distribution for the extra parameter estimated in the random effect model. Sidik and Jonkman (2005) evaluates the estimators and finds Knapp Hartung variance estimates to be the best protection against error. The main problem is estimating the weights used in the meta-analyses.\n\n\nCode\n## Random effect\nlibrary(meta)\ndata(ThirdWave)\nm.gen <- metagen(TE = TE,\n                 seTE = seTE,\n                 studlab = Author,\n                 data = ThirdWave,\n                 sm = \"SMD\",\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"REML\",\n                 hakn = TRUE,\n                 title = \"Third Wave Psychotherapies\")\n\nm.gen <- metagen(TE = TE,\n                 seTE = seTE,\n                 studlab = Author,\n                 data = ThirdWave,\n                 sm = \"SMD\",\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"DL\",\n                 hakn = FALSE,\n                 title = \"Third Wave Psychotherapies\")\n\nsummary(m.gen)\n\n\nReview:     Third Wave Psychotherapies\n\n                          SMD            95%-CI %W(random)\nCall et al.            0.7091 [ 0.1979; 1.2203]        5.0\nCavanagh et al.        0.3549 [-0.0300; 0.7397]        6.3\nDanitzOrsillo          1.7912 [ 1.1139; 2.4685]        3.7\nde Vibe et al.         0.1825 [-0.0484; 0.4133]        8.0\nFrazier et al.         0.4219 [ 0.1380; 0.7057]        7.4\nFrogeli et al.         0.6300 [ 0.2458; 1.0142]        6.3\nGallego et al.         0.7249 [ 0.2846; 1.1652]        5.7\nHazlett-Stevens & Oren 0.5287 [ 0.1162; 0.9412]        6.0\nHintz et al.           0.2840 [-0.0453; 0.6133]        6.9\nKang et al.            1.2751 [ 0.6142; 1.9360]        3.8\nKuhlmann et al.        0.1036 [-0.2781; 0.4853]        6.3\nLever Taylor et al.    0.3884 [-0.0639; 0.8407]        5.6\nPhang et al.           0.5407 [ 0.0619; 1.0196]        5.3\nRasanen et al.         0.4262 [-0.0794; 0.9317]        5.1\nRatanasiripong         0.5154 [-0.1731; 1.2039]        3.6\nShapiro et al.         1.4797 [ 0.8618; 2.0977]        4.1\nSong & Lindquist       0.6126 [ 0.1683; 1.0569]        5.7\nWarnecke et al.        0.6000 [ 0.1120; 1.0880]        5.2\n\nNumber of studies combined: k = 18\n\n                        SMD           95%-CI    z  p-value\nRandom effects model 0.5741 [0.4082; 0.7399] 6.78 < 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0752 [0.0357; 0.3046]; tau = 0.2743 [0.1891; 0.5519]\n I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 45.50   17  0.0002\n\nDetails on meta-analytical method:\n- Inverse variance method\n- DerSimonian-Laird estimator for tau^2\n- Jackson method for confidence interval of tau^2 and tau\n\n\n\n\nCode\nweights <- 1/(ThirdWave$seTE^2)\nfe_est <- with(ThirdWave, sum(TE * weights) / sum(weights)) # pooled estimate \nQ <- sum(with(ThirdWave, (TE - fe_est)^2 / seTE^2))\n\n\n\n\nCode\nk <- nrow(ThirdWave)\nmax(0, (Q - (k - 1))/  (sum(weights) - sum(weights^2) / sum(weights))) # DL estimator\n\n\n[1] 0.07523267\n\n\n\n\nCode\nlibrary(esc)\n# Define the data we need to calculate SMD/d\ngrp1m <- 50   # mean of group 1\ngrp2m <- 60   # mean of group 2\ngrp1sd <- 10  # sd of group 1\ngrp2sd <- 10  # sd of group 2\ngrp1n <- 100  # n of group1\ngrp2n <- 100  # n of group2\n\n# Calculate effect size\nesc_mean_sd(grp1m = grp1m, grp2m = grp2m, \n            grp1sd = grp1sd, grp2sd = grp2sd, \n            grp1n = grp1n, grp2n = grp2n)\n\n\n\nEffect Size Calculation for Meta Analysis\n\n     Conversion: mean and sd to effect size d\n    Effect Size:  -1.0000\n Standard Error:   0.1500\n       Variance:   0.0225\n       Lower CI:  -1.2940\n       Upper CI:  -0.7060\n         Weight:  44.4444"
  },
  {
    "objectID": "mixed/mixed.html",
    "href": "mixed/mixed.html",
    "title": "19  Mixed Models",
    "section": "",
    "text": "20 Introduction to Mixed Models\nFor a model that shows this ,\n\\begin{equation}\n         Y_i = X_i\\beta + Zu_i + e_i\n\\end{equation}\nwith assumptions,\nA step-by-step proof of the mixed model equations can be found on Kevin Liu’s Website: Linear Mixed Model Equations.\nThe main concept of REML, is that instead of using the likelihood of y \\sim N(X\\beta, V), we should instead look at the likelihood of Ky\nA wonderful resource for discussing inference in mixed models is given by Faraway’s Inferential Methods for Linear Mixed Models"
  },
  {
    "objectID": "mixed/mixed.html#mixed-model-ecosystem",
    "href": "mixed/mixed.html#mixed-model-ecosystem",
    "title": "19  Mixed Models",
    "section": "20.1 Mixed Model Ecosystem",
    "text": "20.1 Mixed Model Ecosystem\nMixed model software is widely available for most use case scenarios in most commonly used statistical software. Although there are some implementational differences between the software (optimizers, defaults, design philosophy).\n\n20.1.1 R Package Overview\n\nlme4\nnlme\n\n\n\n20.1.2 SAS Overview"
  },
  {
    "objectID": "mixed/mixed.html#example-datasets",
    "href": "mixed/mixed.html#example-datasets",
    "title": "19  Mixed Models",
    "section": "20.2 Example Datasets",
    "text": "20.2 Example Datasets\n\nOats from nlme package (balanced split plot design)\nsleepstudy from lme4 package\n\n\n20.2.1 Oats\n\nOats is a good example of a balanced split-plot design, from the nlme package.\nWe will remove some data observations to make it incomplete, like they do in emmeans documentation\n\nOats has 3 factors, yield ~ nitro | block/variety\n\nyield (continuous)\nnitro (4 levels)\n\n0.0\n0.2\n0.4\n0.6\n\nVariety (3 levels)\n\nVictory\nGolden Rain\nMarvellous\n\nBlocks (6 levels)\n\n\n\nCode\n# intentionally removing some observations\noats_miss <- Oats[-c(1, 2, 3, 5, 8, 13, 21, 34, 55),]\n\n\n\n\n20.2.2 Sleepstudy\n\nsleepstudy is a good example of random intercept and coefficient with subject random effect.\nsleep study is accessible from lme4 package with sleepstudy\nmost examples for lme4 will feature this dataset."
  },
  {
    "objectID": "mixed/mixed.html#resources",
    "href": "mixed/mixed.html#resources",
    "title": "19  Mixed Models",
    "section": "20.3 Resources",
    "text": "20.3 Resources\n\nBen’s Introduction to Mixed Models - basic lab with examples of how to approach mixed models by a prominant mixed models expert."
  },
  {
    "objectID": "mixed/mixed.html#nlme",
    "href": "mixed/mixed.html#nlme",
    "title": "19  Mixed Models",
    "section": "23.1 nlme",
    "text": "23.1 nlme\nThis section we use the “Oxide” example p167 in Mixed-Effects Models in S and S-PLUS.\nnlme introduces the class “groupedData”, for which has a natural methods for which we can use\nlme also has a LOT of variance classes that can fit R variances of individuals according to covariates as well! See ?varClasses for the documentation of this and see MLM Heterogenous Variance for an example.\nMany methods that apply to class lme objects,\n\nintervals(lmod) - confidence intervals around parameters\n\n“using a normal approximation to the distribution of the (restricted) maximum likelihood estimators (the estimators are assumed to have a normal distribution centered at the true parameter values and with covariance matrix equal to the negative inverse Hessian matrix of the (restricted) log-likelihood evaluated at the estimated parameters).” ~ from documentation\n\naugPred - produces preditions of the response for each group, and produces plot of predicted lines for each subject superposed on the original data.\nplot(compareFits(coef(mod), coef(mod2))) - plot estimates of two models, (like REML vs ML) estimates.\ngetVarCov - gives marginal and conditional covariance matrices, very useful!\nVarCorr - instead of covariances, list the correlations\n\n\n\nCode\n# random defaults to the rhs of grouped Data\nformula(Oxide) # shows that it has a grouping structure\n\n\nThickness ~ 1 | Lot/Wafer\n\n\n\n\nCode\n# The example of how to create a new grouped object\nOrth.new <-  # create a new copy of the groupedData object\n  groupedData(distance ~ age | Subject,\n              data = as.data.frame( Orthodont ),\n              FUN = mean,\n              outer = ~ Sex,\n              labels = list( x = \"Age\",\n                y = \"Distance from pituitary to pterygomaxillary fissure\" ),\n              units = list( x = \"(yr)\", y = \"(mm)\") )\n# formula(Orth.new)\n# plot(Orth.new)\n# gsummary(Orth.new)\north_lmod <- lme(Orth.new)\n# orth_vario <- Variogram(orth_lmod) # doesn't quite look right...."
  },
  {
    "objectID": "mixed/mixed.html#lme4",
    "href": "mixed/mixed.html#lme4",
    "title": "19  Mixed Models",
    "section": "23.2 lme4",
    "text": "23.2 lme4\nSome common accessor functions for working with mixed models and what they return:\n\nsummary() - all the important information\nfixef() - the \\beta part of the equation\nranef() - the u part of the equation\ngetME() - Get very specific components of an object of class merMod. See the help function for full list\n\nX - fixed-effects model matrix\nZ - random-effects model matrix\nmu - conditional mean of response\nb - conditional mode of random effects variable\ntheta - just the parameters in the covariance matrix G\nsigma - relative variance factor\nLambda - squareroot of random effect covariance, G = \\sigma^2LL'\n\npredict(mod, re.form = \"NA | NULL | ~subject\") - predictions with different levels of random effect structures"
  },
  {
    "objectID": "mixed/mixed.html#lmeinfo",
    "href": "mixed/mixed.html#lmeinfo",
    "title": "19  Mixed Models",
    "section": "23.3 lmeInfo",
    "text": "23.3 lmeInfo\nGives expected information and variance components from information matrices of “lme” models. Useful because lme doesn’t report the standard errors from the objects"
  },
  {
    "objectID": "mixed/mixed.html#sommer",
    "href": "mixed/mixed.html#sommer",
    "title": "19  Mixed Models",
    "section": "23.4 sommer",
    "text": "23.4 sommer"
  },
  {
    "objectID": "mixed/mixed.html#glmmtmb",
    "href": "mixed/mixed.html#glmmtmb",
    "title": "19  Mixed Models",
    "section": "23.5 glmmTMB",
    "text": "23.5 glmmTMB"
  },
  {
    "objectID": "mixed/mixed.html#fixed-effects",
    "href": "mixed/mixed.html#fixed-effects",
    "title": "19  Mixed Models",
    "section": "24.1 Fixed Effects",
    "text": "24.1 Fixed Effects\nThe reported fixed effects from fixef(mod) is really just the average of all across all the individual fits that were created\n\n\nCode\n# These are the same! \nfixef(sleep_mod) # %>% as.data.frame() # makes it nicer to work with\n## (Intercept)        Days \n##   251.40510    10.46729\ncolMeans(coef(sleep_mod)$Subject)\n## (Intercept)        Days \n##   251.40510    10.46729\n\n\n\n\nCode\n# Results aren't shown, but this is how to get them.\nsummary(oats_mod)\nranef(oats_mod) # %>% as.data.frame() # makes it nicer to work with\nfixef(oats_mod) # %>% as.data.frame() # makes it nicer to work with\n\n# Something to do with subject level standard errors\nattr(ranef(oats_mod, condVar = TRUE, drop = TRUE)$`Variety:Block`, \"postVar\")\n\n# Standard errors for random effects\nprint(vc <- VarCorr(oats_mod), comp = c(\"Variance\", \"Std.Dev.\")) %>%\n  as.data.frame(order = \"lower.tri\")"
  },
  {
    "objectID": "mixed/mixed.html#random-effects",
    "href": "mixed/mixed.html#random-effects",
    "title": "19  Mixed Models",
    "section": "24.2 Random Effects",
    "text": "24.2 Random Effects\n\n\nCode\n# From the help on ranef\nstr(ranef(sleep_mod))\nas.data.frame(ranef(sleep_mod))\nrr <- ranef(sleep_mod)\ndd <- as.data.frame(rr)\ndotplot(rr,scales = list(x = list(relation = 'free')))[[\"Subject\"]] # This just allows the x access to be different between the two objects\n\n\n\n\n\n\n\nCode\n# How to replicate dotplot in ggplot!\n# The \"free_x\" is allowing different value scales...\nggplot(dd, aes(y=grp,x=condval)) +\n  geom_point() + facet_wrap(~term,scales=\"free_x\") +\n  geom_errorbarh(aes(xmin=condval -2*condsd,\n                     xmax=condval +2*condsd), height=0)\n\n\n\n\n\n\n24.2.1 Random Effect Variance\nMore specifically, these are the variances of the conditional modes\n\n\nCode\ncov_mat <- VarCorr(sleep_mod)[[\"Subject\"]]\ncov_mat # shows the correlation between random effects as well\n\n\n            (Intercept)      Days\n(Intercept)  612.100158  9.604409\nDays           9.604409 35.071714\nattr(,\"stddev\")\n(Intercept)        Days \n  24.740658    5.922138 \nattr(,\"correlation\")\n            (Intercept)       Days\n(Intercept)  1.00000000 0.06555124\nDays         0.06555124 1.00000000\n\n\nCode\nprint(VarCorr(sleep_mod), comp=c(\"Variance\")) # extract the variance estimates of random effects (shown in summary)\n\n\n Groups   Name        Variance Corr \n Subject  (Intercept) 612.100       \n          Days         35.072  0.066\n Residual             654.940       \n\n\nSimilarly, the conditional variance-covariance matrix of the random effects can be extracted by the following. These variances are conditional on the data and\n\n\nCode\nattr(ranef(sleep_mod)$Subject, \"postVar\")\n\n\n, , 1\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 2\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 3\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 4\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 5\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 6\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 7\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 8\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 9\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 10\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 11\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 12\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 13\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 14\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 15\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 16\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 17\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n, , 18\n\n         [,1]       [,2]\n[1,] 145.7056 -21.444504\n[2,] -21.4445   5.312283\n\n\nCode\n# ?ranef\n\n\nA cleaner data frame version of the above with standard deviations, (no correlation)\n\n\nCode\nas.data.frame(ranef(sleep_mod))\n\n\n    grpvar        term grp     condval    condsd\n1  Subject (Intercept) 308   2.2585509 12.070857\n2  Subject (Intercept) 309 -40.3987381 12.070857\n3  Subject (Intercept) 310 -38.9604090 12.070857\n4  Subject (Intercept) 330  23.6906196 12.070857\n5  Subject (Intercept) 331  22.2603126 12.070857\n6  Subject (Intercept) 332   9.0395679 12.070857\n7  Subject (Intercept) 333  16.8405086 12.070857\n8  Subject (Intercept) 334  -7.2326151 12.070857\n9  Subject (Intercept) 335  -0.3336684 12.070857\n10 Subject (Intercept) 337  34.8904868 12.070857\n11 Subject (Intercept) 349 -25.2102286 12.070857\n12 Subject (Intercept) 350 -13.0700342 12.070857\n13 Subject (Intercept) 351   4.5778642 12.070857\n14 Subject (Intercept) 352  20.8636782 12.070857\n15 Subject (Intercept) 369   3.2754656 12.070857\n16 Subject (Intercept) 370 -25.6129993 12.070857\n17 Subject (Intercept) 371   0.8070461 12.070857\n18 Subject (Intercept) 372  12.3145921 12.070857\n19 Subject        Days 308   9.1989758  2.304839\n20 Subject        Days 309  -8.6196806  2.304839\n21 Subject        Days 310  -5.4488565  2.304839\n22 Subject        Days 330  -4.8143503  2.304839\n23 Subject        Days 331  -3.0699116  2.304839\n24 Subject        Days 332  -0.2721770  2.304839\n25 Subject        Days 333  -0.2236361  2.304839\n26 Subject        Days 334   1.0745816  2.304839\n27 Subject        Days 335 -10.7521652  2.304839\n28 Subject        Days 337   8.6282652  2.304839\n29 Subject        Days 349   1.1734322  2.304839\n30 Subject        Days 350   6.6142178  2.304839\n31 Subject        Days 351  -3.0152621  2.304839\n32 Subject        Days 352   3.5360011  2.304839\n33 Subject        Days 369   0.8722149  2.304839\n34 Subject        Days 370   4.8224850  2.304839\n35 Subject        Days 371  -0.9881562  2.304839\n36 Subject        Days 372   1.2840221  2.304839\n\n\nCode\n# broom.mixed alternative, with same output\nbroom.mixed::tidy(sleep_mod, effects = \"ran_vals\")\n\n\n# A tibble: 36 × 6\n   effect   group   level term        estimate std.error\n   <chr>    <chr>   <chr> <chr>          <dbl>     <dbl>\n 1 ran_vals Subject 308   (Intercept)    2.26       12.1\n 2 ran_vals Subject 309   (Intercept)  -40.4        12.1\n 3 ran_vals Subject 310   (Intercept)  -39.0        12.1\n 4 ran_vals Subject 330   (Intercept)   23.7        12.1\n 5 ran_vals Subject 331   (Intercept)   22.3        12.1\n 6 ran_vals Subject 332   (Intercept)    9.04       12.1\n 7 ran_vals Subject 333   (Intercept)   16.8        12.1\n 8 ran_vals Subject 334   (Intercept)   -7.23       12.1\n 9 ran_vals Subject 335   (Intercept)   -0.334      12.1\n10 ran_vals Subject 337   (Intercept)   34.9        12.1\n# … with 26 more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "mixed/mixed.html#plotting-effects",
    "href": "mixed/mixed.html#plotting-effects",
    "title": "19  Mixed Models",
    "section": "24.3 Plotting Effects",
    "text": "24.3 Plotting Effects\nThere are a few packages that automate the effect plotting\n\n“effects”\n“ggeffects”\n“sjPlot”\n\n\n\nCode\n# sjPLot\nplot_model(oats_mod,\n           show.values = TRUE,  # show overlay\n           value.offset = .3) # offset above\n\n\n\n\n\nsome useful options\n\nterms = c(\"factor(nitro)0.2\", \"factor(nitro)0.4\""
  },
  {
    "objectID": "mixed/mixed.html#varfunc-variance-functions",
    "href": "mixed/mixed.html#varfunc-variance-functions",
    "title": "19  Mixed Models",
    "section": "26.1 varFunc variance functions",
    "text": "26.1 varFunc variance functions\nvarfunc is for modeling variance with covariate, for heterogeneity\n\n\nCode\nDialyzer$QB\n\n\n  [1] 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200\n [19] 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200\n [37] 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200\n [55] 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 200 300 300\n [73] 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300\n [91] 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300\n[109] 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300 300\n[127] 300 300 300 300 300 300 300 300 300 300 300 300 300 300\nLevels: 200 300\n\n\nCode\nDialyzer %>% ggplot(aes(pressure, rate, group = Subject)) + \n  geom_point() +\n  geom_line() + \n  facet_wrap(~QB)\n\n\n\n\n\nCode\n# relevel so 300 is first\nDialyzer$QB <- factor(Dialyzer$QB, levels = c(300, 200))\ncontrasts(Dialyzer$QB) <- contr.sum(levels(Dialyzer$QB))\n\ndial_lme <- lme(rate ~ pressure*QB + I(pressure^2)*QB + I(pressure^3)*QB + I(pressure^4)*QB,\n                data =  Dialyzer,\n                random = ~pressure + pressure^2)\n\nsummary(dial_lme)\n\n\nLinear mixed-effects model fit by REML\n  Data: Dialyzer \n       AIC      BIC    logLik\n  705.0693 745.2148 -338.5347\n\nRandom effects:\n Formula: ~pressure + pressure^2 | Subject\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev   Corr  \n(Intercept) 2.334516 (Intr)\npressure    1.546108 -0.398\nResidual    2.188620       \n\nFixed effects:  rate ~ pressure * QB + I(pressure^2) * QB + I(pressure^3) * QB +      I(pressure^4) * QB \n                      Value Std.Error  DF   t-value p-value\n(Intercept)       -16.70523  1.723312 112 -9.693679  0.0000\npressure           89.12680  6.790445 112 13.125327  0.0000\nQB1                -1.02095  1.723312  18 -0.592433  0.5609\nI(pressure^2)     -43.31114  8.110964 112 -5.339827  0.0000\nI(pressure^3)       9.49376  3.669924 112  2.586910  0.0110\nI(pressure^4)      -0.81949  0.556104 112 -1.473624  0.1434\npressure:QB1        1.84205  6.790445 112  0.271271  0.7867\nQB1:I(pressure^2)  -0.16107  8.110964 112 -0.019859  0.9842\nQB1:I(pressure^3)   0.80053  3.669924 112  0.218133  0.8277\nQB1:I(pressure^4)  -0.19529  0.556104 112 -0.351169  0.7261\n Correlation: \n                  (Intr) pressr QB1    I(p^2) I(p^3) I(p^4) pr:QB1 QB1:I(^2\npressure          -0.923                                                   \nQB1                0.098 -0.087                                            \nI(pressure^2)      0.872 -0.984  0.077                                     \nI(pressure^3)     -0.828  0.955 -0.070 -0.992                              \nI(pressure^4)      0.789 -0.923  0.064  0.974 -0.995                       \npressure:QB1      -0.087  0.076 -0.923 -0.066  0.058 -0.053                \nQB1:I(pressure^2)  0.077 -0.066  0.872  0.056 -0.049  0.044 -0.984         \nQB1:I(pressure^3) -0.070  0.058 -0.828 -0.049  0.043 -0.038  0.955 -0.992  \nQB1:I(pressure^4)  0.064 -0.053  0.789  0.044 -0.038  0.034 -0.923  0.974  \n                  QB1:I(^3\npressure                  \nQB1                       \nI(pressure^2)             \nI(pressure^3)             \nI(pressure^4)             \npressure:QB1              \nQB1:I(pressure^2)         \nQB1:I(pressure^3)         \nQB1:I(pressure^4) -0.995  \n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-2.4923834 -0.5199016 -0.0168656  0.4604300  2.6001042 \n\nNumber of Observations: 140\nNumber of Groups: 20 \n\n\nCode\nplot(dial_lme, resid(., type = \"pearson\")~pressure, abline = 0)\n\n\n\n\n\nCode\n# update the weights by pressure\ndial_lme2 <- update(dial_lme,\n       weights = varPower(form = ~pressure))\n\nsummary(dial_lme2)\n\n\nLinear mixed-effects model fit by REML\n  Data: Dialyzer \n       AIC      BIC    logLik\n  681.9085 724.9216 -325.9543\n\nRandom effects:\n Formula: ~pressure + pressure^2 | Subject\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev   Corr  \n(Intercept) 1.899460 (Intr)\npressure    1.679911 -0.344\nResidual    1.712085       \n\nVariance function:\n Structure: Power of variance covariate\n Formula: ~pressure \n Parameter estimates:\n    power \n0.6416526 \nFixed effects:  rate ~ pressure * QB + I(pressure^2) * QB + I(pressure^3) * QB +      I(pressure^4) * QB \n                      Value Std.Error  DF    t-value p-value\n(Intercept)       -17.54492  0.964974 112 -18.181745  0.0000\npressure           93.06697  4.242482 112  21.936918  0.0000\nQB1                -1.07348  0.964974  18  -1.112445  0.2806\nI(pressure^2)     -48.35597  5.761060 112  -8.393588  0.0000\nI(pressure^3)      11.85545  2.886244 112   4.107570  0.0001\nI(pressure^4)      -1.18275  0.472928 112  -2.500914  0.0138\npressure:QB1        1.97424  4.242482 112   0.465350  0.6426\nQB1:I(pressure^2)  -0.26514  5.761060 112  -0.046023  0.9634\nQB1:I(pressure^3)   0.83421  2.886244 112   0.289030  0.7731\nQB1:I(pressure^4)  -0.19923  0.472928 112  -0.421260  0.6744\n Correlation: \n                  (Intr) pressr QB1    I(p^2) I(p^3) I(p^4) pr:QB1 QB1:I(^2\npressure          -0.886                                                   \nQB1                0.115 -0.100                                            \nI(pressure^2)      0.831 -0.977  0.084                                     \nI(pressure^3)     -0.783  0.939 -0.072 -0.989                              \nI(pressure^4)      0.738 -0.898  0.064  0.965 -0.993                       \npressure:QB1      -0.100  0.084 -0.886 -0.068  0.057 -0.050                \nQB1:I(pressure^2)  0.084 -0.068  0.831  0.054 -0.045  0.039 -0.977         \nQB1:I(pressure^3) -0.072  0.057 -0.783 -0.045  0.037 -0.032  0.939 -0.989  \nQB1:I(pressure^4)  0.064 -0.050  0.738  0.039 -0.032  0.027 -0.898  0.965  \n                  QB1:I(^3\npressure                  \nQB1                       \nI(pressure^2)             \nI(pressure^3)             \nI(pressure^4)             \npressure:QB1              \nQB1:I(pressure^2)         \nQB1:I(pressure^3)         \nQB1:I(pressure^4) -0.993  \n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.08149251 -0.57786661 -0.04607249  0.47502441  2.58366062 \n\nNumber of Observations: 140\nNumber of Groups: 20 \n\n\nCode\ncbind(AIC(dial_lme),\n      AIC(dial_lme2))\n\n\n         [,1]     [,2]\n[1,] 705.0693 681.9085\n\n\nCode\nplot(dial_lme2, \n     resid(., type = \"pearson\")~pressure,\n     abline = 0)\n\n\n\n\n\nCode\nplot(dial_lme2,\n     resid(., type = \"pearson\")~pressure | QB,\n           abline =0) # increasing heterogeneity in both \n\n\n\n\n\nCode\nplot(dial_lme,\n     resid(., type = \"pearson\")~pressure | QB,\n           abline =0) # increasing heterogeneity in both \n\n\n\n\n\nCode\ngetVarCov(dial_lme2, type = \"marginal\")\n\n\nSubject 1 \nMarginal variance covariance matrix\n       1      2      3       4       5       6       7\n1 3.7128 3.1317 2.9254  2.7190  2.4938  2.2938  2.0937\n2 3.1317 4.4381 3.3784  3.5385  3.7134  3.8686  4.0238\n3 2.9254 3.3784 7.1286  5.0538  5.9685  6.7805  7.5926\n4 2.7190 3.5385 5.0538 11.4380  8.2236  9.6925 11.1610\n5 2.4938 3.7134 5.9685  8.2236 17.9120 12.8720 15.0580\n6 2.2938 3.8686 6.7805  9.6925 12.8720 25.1700 18.5170\n7 2.0937 4.0238 7.5926 11.1610 15.0580 18.5170 33.8280\n  Standard Deviations: 1.9269 2.1067 2.6699 3.382 4.2323 5.017 5.8162 \n\n\nCode\ndist(Dialyzer$pressure[Dialyzer$Subject == 1])\n\n\n      1     2     3     4     5     6\n2 0.265                              \n3 0.755 0.490                        \n4 1.245 0.980 0.490                  \n5 1.780 1.515 1.025 0.535            \n6 2.255 1.990 1.500 1.010 0.475      \n7 2.730 2.465 1.975 1.485 0.950 0.475"
  },
  {
    "objectID": "mixed/mixed.html#lme-corstruct",
    "href": "mixed/mixed.html#lme-corstruct",
    "title": "19  Mixed Models",
    "section": "26.2 lme corstruct",
    "text": "26.2 lme corstruct\ncorstructs are for modeling with “distances”\n\n\nCode\nspatDat <- data.frame(x = c(0, .25, .5, .75, 1), y = c(0, .25, .5, .75, 1))\nexample_cormat <- Initialize(corExp(c(1,.2),\n                                    form = ~x + y, # specifies the position vector and | grouping variable\n                                    nugget = TRUE), spatDat)\n\ncorMatrix(example_cormat)\n\n\n          [,1]      [,2]      [,3]      [,4]      [,5]\n[1,] 1.0000000 0.5617508 0.3944550 0.2769817 0.1944934\n[2,] 0.5617508 1.0000000 0.5617508 0.3944550 0.2769817\n[3,] 0.3944550 0.5617508 1.0000000 0.5617508 0.3944550\n[4,] 0.2769817 0.3944550 0.5617508 1.0000000 0.5617508\n[5,] 0.1944934 0.2769817 0.3944550 0.5617508 1.0000000\n\n\nCode\nplot(Variogram(example_cormat, distance = seq(0, 3, .1)))"
  },
  {
    "objectID": "mixed/mixed.html#variograms",
    "href": "mixed/mixed.html#variograms",
    "title": "19  Mixed Models",
    "section": "26.3 Variograms",
    "text": "26.3 Variograms\n(semi) variogram are a way of plotting and visualizing correlations over time/space\n\n\\begin{aligned}\n\\gamma[d(\\varepsilon_x, \\varepsilon_y), \\mathbf{\\lambda}] &= \\frac{1}{2}\\operatorname{Var}(\\varepsilon_x - \\varepsilon_y) \\\\\n&= \\frac{1}{2}E[\\varepsilon_x - \\varepsilon_y]^2\n\\end{aligned}\n\n\n\nCode\n# calculating the variogram manually\nplot(Variogram(c(1, 3, -1, 5), # time points\n               c(1, .3, .5, 2, 7, .2))) # values at time points\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = FALSE, :\npseudoinverse used at 0.166\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = FALSE, :\nneighborhood radius 0.334\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = FALSE, :\nreciprocal condition number 0\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = FALSE, :\nThere are other near singularities as well. 36.409\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = FALSE, :\npseudoinverse used at 0.166\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = FALSE, :\nneighborhood radius 0.334\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = FALSE, :\nreciprocal condition number 0\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = FALSE, :\nThere are other near singularities as well. 36.409\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = FALSE, :\npseudoinverse used at 0.166\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = FALSE, :\nneighborhood radius 0.334\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = FALSE, :\nreciprocal condition number 0\n\n\nWarning in simpleLoess(y, x, w, span, degree = degree, parametric = FALSE, :\nThere are other near singularities as well. 36.409\n\n\n\n\n\nCode\nval <- outer(c(1, 3, -1, 5), c(1, 3, -1, 5), function(x, y) (y - x)^2 / 2)\nval <- val[lower.tri(val)]\nval # the variogram values\n\n\n[1]  2  2  8  8  2 18\n\n\n\n\nCode\nn <- 20 # 20 obs\nN <- 100 # nodes\n\nrandomwalk <- function(N) {\n  cumsum(rnorm(N))\n}\nset.seed(1)\nrw_mat <- replicate(1000,\n          randomwalk(6))\n\n\n\n\nCode\nrw_dat <- rw_mat %>% melt(c(\"node\",\"obs\"))\n\nrw_mat %>% t() %>% cov() # sample covariance\n\n\n         [,1]     [,2]     [,3]     [,4]     [,5]     [,6]\n[1,] 1.065027 1.069246 1.083659 1.067816 1.093611 1.149804\n[2,] 1.069246 2.068872 2.093093 2.113653 2.145208 2.199581\n[3,] 1.083659 2.093093 3.211379 3.242694 3.287997 3.324031\n[4,] 1.067816 2.113653 3.242694 4.286559 4.353058 4.374326\n[5,] 1.093611 2.145208 3.287997 4.353058 5.426274 5.461257\n[6,] 1.149804 2.199581 3.324031 4.374326 5.461257 6.552920\n\n\nCode\nrw_dat %>% ggplot(aes(node, value)) +\n  geom_point()\n\n\n\n\n\nCode\nrw_gls <- gls(value~1,  data = rw_dat,\n          correlation = corSymm(form = ~1|obs),\n          weights = varIdent(form = ~1 | node))\n\n\ngetVarCov(rw_gls)\n\n\nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]   [,5]   [,6]\n[1,] 1.0650 1.0692 1.0836 1.0677 1.0935 1.1497\n[2,] 1.0692 2.0679 2.0919 2.1127 2.1440 2.1983\n[3,] 1.0836 2.0919 3.2093 3.2401 3.2857 3.3218\n[4,] 1.0677 2.1127 3.2401 4.2836 4.3496 4.3707\n[5,] 1.0935 2.1440 3.2857 4.3496 5.4218 5.4567\n[6,] 1.1497 2.1983 3.3218 4.3707 5.4567 6.5473\n  Standard Deviations: 1.032 1.438 1.7915 2.0697 2.3285 2.5588 \n\n\nCode\ngetVarCov(rw_gls, individual = 4)\n\n\nMarginal variance covariance matrix\n       [,1]   [,2]   [,3]   [,4]   [,5]   [,6]\n[1,] 1.0650 1.0692 1.0836 1.0677 1.0935 1.1497\n[2,] 1.0692 2.0679 2.0919 2.1127 2.1440 2.1983\n[3,] 1.0836 2.0919 3.2093 3.2401 3.2857 3.3218\n[4,] 1.0677 2.1127 3.2401 4.2836 4.3496 4.3707\n[5,] 1.0935 2.1440 3.2857 4.3496 5.4218 5.4567\n[6,] 1.1497 2.1983 3.3218 4.3707 5.4567 6.5473\n  Standard Deviations: 1.032 1.438 1.7915 2.0697 2.3285 2.5588 \n\n\nCode\nrw_cs_cov <- corMatrix(Initialize(corCompSymm(value = .3, form = ~1 | obs), rw_dat))\n\nrw_cs_cov %>% length()\n\n\n[1] 1000"
  },
  {
    "objectID": "mixed/mixed.html#example-bodyweight",
    "href": "mixed/mixed.html#example-bodyweight",
    "title": "19  Mixed Models",
    "section": "26.4 Example: BodyWeight",
    "text": "26.4 Example: BodyWeight\nbody weight of rats measured over 64 days and 3 different diets\n\n\nCode\nBodyWeight %>% ggplot(aes(Time, weight, group = Rat)) +\n  geom_line() +\n  facet_wrap(~Diet)\n\n\n\n\n\nCode\n# Residuals\nbw_lme <- lme(weight ~ Time * Diet, BodyWeight,\n    random = ~Time | Rat)\n\n\n\n# model with varPower\nbw_lme2 <- update(bw_lme, weights = varPower())\nplot(bw_lme, resid(., type = \"p\") ~ Time) # Residual plot\n\n\n\n\n\nCode\nplot(bw_lme2, resid(., type = \"p\")~Time) # such a subtle difference... how much effect does it really have. yet it's strongly statistically significant\n\n\n\n\n\nCode\nanova(bw_lme, bw_lme2)\n\n\n        Model df      AIC      BIC    logLik   Test  L.Ratio p-value\nbw_lme      1 10 1171.720 1203.078 -575.8599                        \nbw_lme2     2 11 1163.921 1198.415 -570.9607 1 vs 2 9.798326  0.0017\n\n\nThe anova gives a significant result, meaning that the covariance with heterogenous modeling is better, but if you look summary of the results, the standard errors are really not that different, maybe a difference of about 10%.\n\n\nCode\nsummary(bw_lme)\n\n\nLinear mixed-effects model fit by REML\n  Data: BodyWeight \n      AIC      BIC    logLik\n  1171.72 1203.078 -575.8599\n\nRandom effects:\n Formula: ~Time | Rat\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 36.9390723 (Intr)\nTime         0.2484113 -0.149\nResidual     4.4436052       \n\nFixed effects:  weight ~ Time * Diet \n                Value Std.Error  DF   t-value p-value\n(Intercept) 251.65165 13.094025 157 19.218816  0.0000\nTime          0.35964  0.091140 157  3.946019  0.0001\nDiet2       200.66549 22.679516  13  8.847873  0.0000\nDiet3       252.07168 22.679516  13 11.114509  0.0000\nTime:Diet2    0.60584  0.157859 157  3.837858  0.0002\nTime:Diet3    0.29834  0.157859 157  1.889903  0.0606\n Correlation: \n           (Intr) Time   Diet2  Diet3  Tm:Dt2\nTime       -0.160                            \nDiet2      -0.577  0.092                     \nDiet3      -0.577  0.092  0.333              \nTime:Diet2  0.092 -0.577 -0.160 -0.053       \nTime:Diet3  0.092 -0.577 -0.053 -0.160  0.333\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-3.25558796 -0.42196874  0.08229384  0.59933559  2.78994477 \n\nNumber of Observations: 176\nNumber of Groups: 16 \n\n\nCode\nsummary(bw_lme2)\n\n\nLinear mixed-effects model fit by REML\n  Data: BodyWeight \n       AIC      BIC    logLik\n  1163.921 1198.415 -570.9607\n\nRandom effects:\n Formula: ~Time | Rat\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 36.8980214 (Intr)\nTime         0.2436518 -0.145\nResidual     0.1751704       \n\nVariance function:\n Structure: Power of variance covariate\n Formula: ~fitted(.) \n Parameter estimates:\n    power \n0.5428447 \nFixed effects:  weight ~ Time * Diet \n                Value Std.Error  DF   t-value p-value\n(Intercept) 251.60215 13.067335 157 19.254281  0.0000\nTime          0.36109  0.088378 157  4.085720  0.0001\nDiet2       200.77697 22.656249  13  8.861881  0.0000\nDiet3       252.17017 22.661603  13 11.127640  0.0000\nTime:Diet2    0.60183  0.155424 157  3.872180  0.0002\nTime:Diet3    0.29524  0.155892 157  1.893847  0.0601\n Correlation: \n           (Intr) Time   Diet2  Diet3  Tm:Dt2\nTime       -0.152                            \nDiet2      -0.577  0.088                     \nDiet3      -0.577  0.088  0.333              \nTime:Diet2  0.087 -0.569 -0.157 -0.050       \nTime:Diet3  0.086 -0.567 -0.050 -0.158  0.322\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.93736805 -0.44386633  0.07986351  0.58084818  2.26489822 \n\nNumber of Observations: 176\nNumber of Groups: 16 \n\n\n\n\nCode\n# variogram seems to increase at 20 days, so model with exponential spatial\nplot(Variogram(bw_lme, form = ~Time, maxDist = 42)) # loess smoother\n\n\n\n\n\nCode\nbw_lme3 <- update(bw_lme, corr = corExp(form = ~Time))\n\nplot(Variogram(bw_lme3, maxDist = 42, form = ~Time, \n               resType = \"normalized\",\n               robust = T), ylim = c(0, 1.4)) # fit with time variogram\n\n\n\n\n\nUse the normalized residuals and robust"
  },
  {
    "objectID": "mixed/mixed.html#using-covariate-as-both-random-and-fixed-effect",
    "href": "mixed/mixed.html#using-covariate-as-both-random-and-fixed-effect",
    "title": "19  Mixed Models",
    "section": "28.1 Using covariate as both random and fixed effect",
    "text": "28.1 Using covariate as both random and fixed effect\nThere is a blog post with this phenomena"
  },
  {
    "objectID": "mixed/mixed.html#plot-of-shrinkage-sleepstudy",
    "href": "mixed/mixed.html#plot-of-shrinkage-sleepstudy",
    "title": "19  Mixed Models",
    "section": "28.2 Plot of Shrinkage (sleepstudy)",
    "text": "28.2 Plot of Shrinkage (sleepstudy)\nThis stuff is from this blog post: https://www.tjmahr.com/plotting-partial-pooling-in-mixed-effects-models/\nLooking at the shrinkage stuff though, there’s also from Bates\n\n\nCode\nsleepstudy <- sleepstudy %>% as_tibble() %>% mutate(Subject = as.character(Subject))\nsleepstudy\n\n\n# A tibble: 180 × 3\n   Reaction  Days Subject\n      <dbl> <dbl> <chr>  \n 1     250.     0 308    \n 2     259.     1 308    \n 3     251.     2 308    \n 4     321.     3 308    \n 5     357.     4 308    \n 6     415.     5 308    \n 7     382.     6 308    \n 8     290.     7 308    \n 9     431.     8 308    \n10     466.     9 308    \n# … with 170 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\n# Adding two participants to illustrate partial pooling\ndf_sleep <- bind_rows(\n  sleepstudy,\n  tibble(Reaction = c(286, 288), Days = 0:1, Subject = \"374\"),\n  tibble(Reaction = 245, Days = 0, Subject = \"373\"))\n\n\n\n\nCode\nggplot(df_sleep, aes(Days, Reaction)) + \n  facet_wrap(~Subject) +\n  geom_point() +\n  geom_smooth(method=lm, se=FALSE)\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nFitting all the linear models all at once, lme4 provides a convenience function for that with s3 methods defined as well: coef, confint, fitted, fixef, formula, logLik, pairs, plot, predict, print, qqnorm, ranef, residuals, sigma, summary, and update\n\n28.2.1 No Pooling\n\n\nCode\n# Creating the no pooling model\n# This is basically an ANCOVA lm(Reaction ~ Days + Subject)\n\ndf_no_pooling <- lmList(Reaction ~ Days | Subject, data=df_sleep) %>% \n  coef() %>%\n  rownames_to_column(\"Subject\") %>%\n  rename(Intercept = `(Intercept)`, Slope_Days = Days) %>% \n  add_column(Model = \"No Pooling\") %>% \n  filter(Subject != \"373\")\n# ?lmList # Creates a linear model for each one.\n\n\n\n\n28.2.2 Pooled\n\n\nCode\n# Creating pooled model\nm_pooled <- lm(Reaction ~ Days, df_sleep)\ndf_pooled <- tibble(\n  Model = \"Complete pooling\",\n  Subject = unique(df_sleep$Subject),\n  Intercept = coef(m_pooled)[1], \n  Slope_Days = coef(m_pooled)[2])\n\n\n\n\nCode\n# combine the two models\ndf_models <- bind_rows(df_pooled, df_no_pooling) %>%\n  left_join(df_sleep, by = \"Subject\")\ndf_models\n\n\n# A tibble: 365 × 6\n   Model            Subject Intercept Slope_Days Reaction  Days\n   <chr>            <chr>       <dbl>      <dbl>    <dbl> <dbl>\n 1 Complete pooling 308          252.       10.3     250.     0\n 2 Complete pooling 308          252.       10.3     259.     1\n 3 Complete pooling 308          252.       10.3     251.     2\n 4 Complete pooling 308          252.       10.3     321.     3\n 5 Complete pooling 308          252.       10.3     357.     4\n 6 Complete pooling 308          252.       10.3     415.     5\n 7 Complete pooling 308          252.       10.3     382.     6\n 8 Complete pooling 308          252.       10.3     290.     7\n 9 Complete pooling 308          252.       10.3     431.     8\n10 Complete pooling 308          252.       10.3     466.     9\n# … with 355 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\n28.2.3 Partial Pooling\n\n\nCode\nlmm <- lmer(Reaction ~ 1 + Days + (1 + Days | Subject), df_sleep)\n\n# Make a dataframe with the fitted effects\ndf_partial_pooling <- coef(lmm)[[\"Subject\"]] %>% \n  rownames_to_column(\"Subject\") %>% \n  as_tibble() %>% \n  rename(Intercept = `(Intercept)`, Slope_Days = Days) %>% \n  add_column(Model = \"Partial pooling\")\n\ndf_models <- bind_rows(df_pooled, df_no_pooling, df_partial_pooling) %>% \n  left_join(df_sleep, by = \"Subject\")\n\n\n\n\n28.2.4 Plot comparing all versions\n\n\nCode\n# Create initial base plot without partial pooling lines\np_model_comparison <- ggplot(df_models) + \n  aes(x = Days, y = Reaction) + \n  # Set the color mapping in this layer so the points don't get a color\n  geom_abline(aes(intercept = Intercept, slope = Slope_Days, color = Model),\n              size = .75) +\n  geom_point() + \n  facet_wrap(~Subject) +\n  scale_x_continuous(breaks = 0:4 * 2) + \n  labs(x = \"Days of Sleep Deprivation\", y = \"Average Reaction Time\") +\n  scale_color_brewer(palette = \"Dark2\") + \n  theme(legend.position = \"top\")\n# p_model_comparison # to view without partial pooling\n\np_model_comparison\n\n\n\n\n\n\n\n28.2.5 Linear Model Version Estimates\nThe fixed version of this is the same as the no pooling case. The plot completely overlaps with the no pooling\n\n\nCode\nm <- lm(formula = Reaction ~ Days*Subject, data = df_sleep)\ncoef(m)\n\n\n    (Intercept)            Days      Subject309      Subject310      Subject330 \n    244.1926691      21.7647024     -39.1377236     -40.7084436      45.4924236 \n     Subject331      Subject332      Subject333      Subject334      Subject335 \n     41.5462964      20.0589455      30.8264364      -4.0297545      18.8420236 \n     Subject337      Subject349      Subject350      Subject351      Subject352 \n     45.9114582     -29.0808964     -18.3580655      16.9543418      32.1794000 \n     Subject369      Subject370      Subject371      Subject372      Subject373 \n     10.7754800     -33.7435782       9.4433691      22.8521309       0.8073309 \n     Subject374 Days:Subject309 Days:Subject310 Days:Subject330 Days:Subject331 \n     41.8073309     -19.5029170     -15.6498036     -18.7566297     -16.4986836 \nDays:Subject332 Days:Subject333 Days:Subject334 Days:Subject335 Days:Subject337 \n    -12.1979345     -12.6226570      -9.5115612     -24.6457364      -2.7387285 \nDays:Subject349 Days:Subject350 Days:Subject351 Days:Subject352 Days:Subject369 \n     -8.2707697      -2.2606855     -15.3312048      -8.1981533     -10.4165933 \nDays:Subject370 Days:Subject371 Days:Subject372 Days:Subject373 Days:Subject374 \n     -3.7085515     -12.5762576     -10.4666291              NA     -19.7647024 \n\n\nCode\n# In this information, there is really a line for each subject, need to create that matrix\n\nmcoef <- tibble(Subject = \"308\", Intercept = coef(m)[1], Slope_Days = coef(m)[2])\nmcoef <- rbind(mcoef, \n               tibble(\n                 Subject = substr(names(coef(m)[3:21]), 8,10),\n                 Intercept = coef(m)[3:21] + coef(m)[1],\n                 Slope_Days = coef(m)[22:40] + coef(m)[2]))\nmcoef$Model <- \"Linear Model\"\ndf_ancova <- mcoef\n\ndf_models <- bind_rows(df_pooled, df_no_pooling, df_partial_pooling, df_ancova) %>% \n  left_join(df_sleep, by = \"Subject\")\n\n# Oh it's the same as no pooling\ndf_models %>% filter((Model == \"Linear Model\" | Model == \"No Pooling\") & Subject == \"370\" & Days == 0)\n\n\n# A tibble: 2 × 6\n  Model        Subject Intercept Slope_Days Reaction  Days\n  <chr>        <chr>       <dbl>      <dbl>    <dbl> <dbl>\n1 No Pooling   370          210.       18.1     225.     0\n2 Linear Model 370          210.       18.1     225.     0\n\n\nEffectively, this is the same as filtering the data and running a linear regression on each subset. That is what is meant by “no pooling”\n\n\nCode\nmf <- lm(Reaction ~ Days, subset(df_sleep, Subject == \"308\"))\ncoef(mf)\n\n\n(Intercept)        Days \n   244.1927     21.7647 \n\n\nCode\ncoef(m)[1:2]\n\n\n(Intercept)        Days \n   244.1927     21.7647 \n\n\nCode\nfixef(lmm)\n\n\n(Intercept)        Days \n  252.54256    10.45212 \n\n\nCode\ncoef(lmm)\n\n\n$Subject\n    (Intercept)       Days\n308    253.9478 19.6264337\n309    211.7331  1.7319161\n310    213.1582  4.9061511\n330    275.1425  5.6436007\n331    273.7286  7.3862730\n332    260.6504 10.1632571\n333    268.3683 10.2246059\n334    244.5524 11.4837802\n335    251.3702 -0.3355788\n337    286.2319 19.1090424\n349    226.7663 11.5531844\n350    238.7807 17.0156827\n351    256.2344  7.4119456\n352    272.3511 13.9920878\n369    254.9484 11.2985770\n370    226.3701 15.2027877\n371    252.5051  9.4335409\n372    263.8916 11.7253429\n373    248.9753 10.3915288\n374    271.1450 11.0782516\n\nattr(,\"class\")\n[1] \"coef.mer\"\n\n\nThe standard errors is where it matters… let’s look at that… say I’m interested in saying something about the subject 308. How would I go about that? The answer is that it’s difficult without making some egregarious assumptions.\nhttps://stackoverflow.com/questions/26198958/extracting-coefficients-and-their-standard-error-from-lme\n\n\nCode\n# filtered model for patient 308\ncoef(summary(mf))\n\n\n            Estimate Std. Error  t value     Pr(>|t|)\n(Intercept) 244.1927   28.08269 8.695486 2.385022e-05\nDays         21.7647    5.26037 4.137485 3.264657e-03\n\n\nCode\n# Coef of blocked model\ncoef(summary(m))\n\n\n                   Estimate Std. Error     t value     Pr(>|t|)\n(Intercept)     244.1926691  15.041687 16.23439348 2.419368e-34\nDays             21.7647024   2.817566  7.72464642 1.741840e-12\nSubject309      -39.1377236  21.272158 -1.83985675 6.784831e-02\nSubject310      -40.7084436  21.272158 -1.91369599 5.764319e-02\nSubject330       45.4924236  21.272158  2.13858996 3.415609e-02\nSubject331       41.5462964  21.272158  1.95308328 5.274863e-02\nSubject332       20.0589455  21.272158  0.94296711 3.472771e-01\nSubject333       30.8264364  21.272158  1.44914476 1.494714e-01\nSubject334       -4.0297545  21.272158 -0.18943797 8.500163e-01\nSubject335       18.8420236  21.272158  0.88575985 3.772237e-01\nSubject337       45.9114582  21.272158  2.15828869 3.256308e-02\nSubject349      -29.0808964  21.272158 -1.36708726 1.737283e-01\nSubject350      -18.3580655  21.272158 -0.86300907 3.895675e-01\nSubject351       16.9543418  21.272158  0.79702030 4.267514e-01\nSubject352       32.1794000  21.272158  1.51274731 1.325355e-01\nSubject369       10.7754800  21.272158  0.50655321 6.132431e-01\nSubject370      -33.7435782  21.272158 -1.58627902 1.148695e-01\nSubject371        9.4433691  21.272158  0.44393094 6.577589e-01\nSubject372       22.8521309  21.272158  1.07427421 2.844966e-01\nSubject373        0.8073309  29.684902  0.02719668 9.783405e-01\nSubject374       41.8073309  29.684902  1.40837020 1.611769e-01\nDays:Subject309 -19.5029170   3.984640 -4.89452386 2.609126e-06\nDays:Subject310 -15.6498036   3.984640 -3.92753235 1.326622e-04\nDays:Subject330 -18.7566297   3.984640 -4.70723286 5.841112e-06\nDays:Subject331 -16.4986836   3.984640 -4.14057040 5.875009e-05\nDays:Subject332 -12.1979345   3.984640 -3.06123857 2.630447e-03\nDays:Subject333 -12.6226570   3.984640 -3.16782847 1.875589e-03\nDays:Subject334  -9.5115612   3.984640 -2.38705643 1.828240e-02\nDays:Subject335 -24.6457364   3.984640 -6.18518475 6.072281e-09\nDays:Subject337  -2.7387285   3.984640 -0.68732139 4.929857e-01\nDays:Subject349  -8.2707697   3.984640 -2.07566282 3.970366e-02\nDays:Subject350  -2.2606855   3.984640 -0.56734995 5.713599e-01\nDays:Subject351 -15.3312048   3.984640 -3.84757562 1.786891e-04\nDays:Subject352  -8.1981533   3.984640 -2.05743875 4.144801e-02\nDays:Subject369 -10.4165933   3.984640 -2.61418662 9.894962e-03\nDays:Subject370  -3.7085515   3.984640 -0.93071174 3.535604e-01\nDays:Subject371 -12.5762576   3.984640 -3.15618391 1.946990e-03\nDays:Subject372 -10.4666291   3.984640 -2.62674378 9.553514e-03\nDays:Subject374 -19.7647024  36.301801 -0.54445515 5.869704e-01\n\n\nCode\n# CAUTION: Ignoring Covariances, and directly adding the variances for Conditional Variance and fixed eff variance\nsqrt(diag(vcov(lmm)) + diag(attr(ranef(lmm)$Subject, \"postVar\")[,,1]))\n\n\n(Intercept)        Days \n  13.575606    2.758914 \n\n\n\n\nCode\ndf_fixef <- tibble(\n  Model = \"Partial pooling (average)\",\n  Intercept = fixef(lmm)[1],\n  Slope_Days = fixef(lmm)[2])\n\n# Complete pooling / fixed effects are center of gravity in the plot\ndf_gravity <- df_pooled %>% \n  distinct(Model, Intercept, Slope_Days) %>% \n  bind_rows(df_fixef)\ndf_gravity\n\n\n# A tibble: 2 × 3\n  Model                     Intercept Slope_Days\n  <chr>                         <dbl>      <dbl>\n1 Complete pooling               252.       10.3\n2 Partial pooling (average)      253.       10.5\n\n\nCode\ndf_pulled <-  bind_rows(df_no_pooling, df_partial_pooling)\nhead(df_pulled)\n\n\n  Subject Intercept Slope_Days      Model\n1     308  244.1927  21.764702 No Pooling\n2     309  205.0549   2.261785 No Pooling\n3     310  203.4842   6.114899 No Pooling\n4     330  289.6851   3.008073 No Pooling\n5     331  285.7390   5.266019 No Pooling\n6     332  264.2516   9.566768 No Pooling\n\n\nCode\nggplot(df_pulled, aes(Intercept, Slope_Days, color=Model)) + \n  geom_point() +\n  geom_point(data=df_gravity, size=4) +\n  geom_path(aes(group=Subject), arrow=arrow(length=unit(.02, \"npc\"))) +\n  geom_text(aes(label=Subject), data=df_no_pooling, nudge_y = .8)"
  },
  {
    "objectID": "mixed/mixed.html#getting-subject-level-standard-errors",
    "href": "mixed/mixed.html#getting-subject-level-standard-errors",
    "title": "19  Mixed Models",
    "section": "28.3 Getting Subject level standard errors",
    "text": "28.3 Getting Subject level standard errors\nComplicated Issue, according to Ben Bolker\n\n\nCode\nfixed_vars <- diag(vcov(mod))\nattributes(ranef(mod, condVar=TRUE, drop=TRUE)$Subject)\n\nattributes(ranef(mod)$Subject)\nattr(ranef(mod)$Subject, \"postVar\")\ncmode_vars <- t(apply(attr(ranef(mod)$Subject, \"postVar\"), 3, diag)) # columns of intercept variance and day variance\n\nsqrt(sweep(cmode_vars, 2, fixed_vars, \"+\")) # Standard errors  (basically add fixed_vars to respective columns)"
  },
  {
    "objectID": "mixed/mixed.html#emmeans-with-mixed-models-oats",
    "href": "mixed/mixed.html#emmeans-with-mixed-models-oats",
    "title": "19  Mixed Models",
    "section": "28.4 Emmeans with Mixed Models (oats)",
    "text": "28.4 Emmeans with Mixed Models (oats)\n\n\nCode\noats_emm <- emmeans(oats_mod, c(\"Variety\", \"nitro\"), lmer.df = \"satterthwaite\") # Also affects the SE slightly\noats_emm\n\n\n Variety     nitro emmean   SE    df lower.CL upper.CL\n Golden Rain   0.0   80.6 8.18 10.54     62.5     98.7\n Marvellous    0.0   83.8 8.15 10.40     65.8    101.9\n Victory       0.0   72.3 8.34 11.26     54.0     90.6\n Golden Rain   0.2   98.7 8.02  9.76     80.8    116.6\n Marvellous    0.2  102.0 8.08 10.07     84.0    119.9\n Victory       0.2   90.4 8.17 10.39     72.3    108.5\n Golden Rain   0.4  115.9 8.09 10.14     97.9    133.9\n Marvellous    0.4  119.1 8.00  9.69    101.2    137.0\n Victory       0.4  107.6 8.17 10.39     89.5    125.7\n Golden Rain   0.6  125.8 8.09 10.07    107.8    143.8\n Marvellous    0.6  129.0 7.99  9.61    111.1    146.9\n Victory       0.6  117.5 8.03  9.79     99.5    135.4\n\nDegrees-of-freedom method: satterthwaite \nConfidence level used: 0.95 \n\n\n\n28.4.1 Manual calculation of emmeans and SE\nEmmeans simply takes advantage of the fact that the marginal means are the linear combinations of the coefficients. Thus, if the model coefficient standard deviations are incorrect, then the estimated marginal means standard errrors are incorrect.\nTo replicate the standard errors within emmeans, you must use REML estimates of standards errors.\nReminder: modesl by default in R use the factor encoding, so to get (0.0 nitro, marvellous), we know the (0.0 nitro, Victory) is the intercept, and the coefficeint for variety marvellous is the effect, so (0.0 nitro, Marevellous) = (Intercept) + VarietyMarvellous.\nOverall, we can create the reference grid manually and calculate the marginal means from the grid\n\n\nCode\n# calculate reference grid manually\noats_grid <- with(oats_miss, expand.grid(Variety = levels(Variety),\n                                         nitro = levels(factor(nitro))))\noats_X <- model.matrix(~Variety + nitro, data = oats_grid) # the combinations\noats_B <- fixef(oats_mod)\noats_V <- vcov(oats_mod)\n\n# Calculate my version\noats_my_emm <- list(emmean = c(oats_X %*% oats_B), # marginal means are just X\\betahat\n                    SE = sqrt(diag(oats_X %*% oats_V %*% t(oats_X)))) # cov(X\\beta) = Xcov(\\betahat)X'\n\n# Calculate the package version\noats_package_emm <- oats_emm %>% as_tibble() %>% dplyr::select(emmean, SE) %>% as.list()\n\n\n# Show that emmean estimates are the same\ncbind(oats_my_emm$emmean, oats_package_emm$emmean)\n\n\n           [,1]      [,2]\n [1,]  80.58462  80.58462\n [2,]  83.81332  83.81332\n [3,]  72.27827  72.27827\n [4,]  98.72680  98.72680\n [5,] 101.95550 101.95550\n [6,]  90.42045  90.42045\n [7,] 115.89071 115.89071\n [8,] 119.11941 119.11941\n [9,] 107.58436 107.58436\n[10,] 125.76112 125.76112\n[11,] 128.98982 128.98982\n[12,] 117.45477 117.45477\n\n\nCode\n# Show that SE calculations are the same (note, they're only the same when you use Satter)\ncbind(oats_my_emm$SE, oats_package_emm$SE)\n\n\n       [,1]     [,2]\n1  8.180084 8.180084\n2  8.145113 8.145113\n3  8.342494 8.342494\n4  8.015913 8.015913\n5  8.078163 8.078163\n6  8.171648 8.171648\n7  8.093027 8.093027\n8  8.001749 8.001749\n9  8.171293 8.171293\n10 8.087191 8.087191\n11 7.985790 7.985790\n12 8.030997 8.030997\n\n\nNote:\n\nSE are the same when you calculate them with REML = TRUE and lmer.df = \"satterthwaite\".\n\nIn order to average these down (by Variety) we then just get the appropriate linear combinations and calculate them with the appropriate variance-covariance."
  },
  {
    "objectID": "mixed/mixed_variance.html",
    "href": "mixed/mixed_variance.html",
    "title": "20  Mixed Models (Variance)",
    "section": "",
    "text": "21 Introduction\nUsing random effects into the modeling equation gives us an opportunity to account for correlations among observations in a structured manner. Having a lot of flexibility in modeling a covariance structure is generally a pretty computationally difficult problem, but modern software have made this problem rather manageable. There are many numerical methods of solving the same equations, and thus a wide array of software to consider. Given the numerical complexity of the problem, the software will often times give different estimates and standard errors.\nThe example that we use throughout this section is a multi-environment potato RCBD (DT_example from the sommer package). The covariates in the dataset are:\nUnder construction\nThe R packages that can do forms of variance covariance structures (not an exhaustive list):\nThe ratings are rather subjective, based on stability, ease of use, flexibility and target applications. Personally, the order in which I would use the software (and what I recommend to others) would be: * emmREML - implements the mixed model association, mentioned above.\nnlme will offer many options of variance structure with similar performance if you need more flexibility, but does not deal with crossed random effects. SAS has a very intuitive interface for specifying the “G”-side and “R”-side and will likely be able to handle your use case. glmmTMB also has a very intuitive syntax, mostly following lme4 syntax, but currently the “R”-side modeling is kind of hackish and a workaround (sets the residual error to something very small). sommer I think is the newest package, and thus the components of design are still settling. It is by far the most flexible for variance structure designs, and targeted specifically to those working in multi-environment trails but I think the syntax does take some getting used to.\nRight now these resources are relatively unorganized, but contain a wealth of information. It seems like the variance modeling is scattered throughout the packages in R.\nFor more of a philosophical take on confirmatory analyses:"
  },
  {
    "objectID": "mixed/mixed_variance.html#nlme",
    "href": "mixed/mixed_variance.html#nlme",
    "title": "20  Mixed Models (Variance)",
    "section": "23.1 nlme",
    "text": "23.1 nlme\nnlme is a rather stable package with a lot of facilities to account for many different variance structures on both the “G”-side and the “R”-side.\nSee ?pdClasses and ?varClasses for functional variance modeling."
  },
  {
    "objectID": "mixed/mixed_variance.html#sommer",
    "href": "mixed/mixed_variance.html#sommer",
    "title": "20  Mixed Models (Variance)",
    "section": "23.2 sommer",
    "text": "23.2 sommer\nYou can probably think of sommer as the free version of AS-REML. It particularly specializes in multienvironment, and heterogenous variance structures.\nThe variance component specification is a little cryptic, but based mostly on hadamard matrix multiplication.\n\nmmer\n\nvsr = “Variance Structure for Random effects”, use this to build the variance, outer function\n\ndsr = diagonal covariance structure\nusr = unstructure covraince structure\n\n\nmmec - the main “average information” algorithm for solving\n\nvsc = “variance structure for coefficients”, random=~vsc(e, f, h, g) means var(g) = G.e @ G.f @ G.h @ I.g\n\nisc = identity covariance structure\n\n\n\nThe numbering in the matrix means it’s a constraint\n\nnot to be estimated\nestimated and constrained to be positive (i.e. variance component)\nestimated and unconstrained (i.e., normally a covariance)\nnot estimated but fixed (provided by Gti arg)\n\n\n\nCode\nx <- as.factor(c(1:5,2:6,1:5))\nusr(x) # list(vector, vcov structure)\n\n\n[[1]]\n   1 2 3 4 5 6\n1  1 0 0 0 0 0\n2  0 1 0 0 0 0\n3  0 0 1 0 0 0\n4  0 0 0 1 0 0\n5  0 0 0 0 1 0\n6  0 1 0 0 0 0\n7  0 0 1 0 0 0\n8  0 0 0 1 0 0\n9  0 0 0 0 1 0\n10 0 0 0 0 0 1\n11 1 0 0 0 0 0\n12 0 1 0 0 0 0\n13 0 0 1 0 0 0\n14 0 0 0 1 0 0\n15 0 0 0 0 1 0\nattr(,\"assign\")\n[1] 1 1 1 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$dummy\n[1] \"contr.treatment\"\n\n\n[[2]]\n  1 2 3 4 5 6\n1 1 2 2 2 2 2\n2 2 1 2 2 2 2\n3 2 2 1 2 2 2\n4 2 2 2 1 2 2\n5 2 2 2 2 1 2\n6 2 2 2 2 2 1\n\n\nCode\nx <- as.factor(c(1:5,1:5,1:6))\ncsr(x, matrix(1, 6, 6))\n\n\n[[1]]\n   1 2 3 4 5 6\n1  1 0 0 0 0 0\n2  0 1 0 0 0 0\n3  0 0 1 0 0 0\n4  0 0 0 1 0 0\n5  0 0 0 0 1 0\n6  1 0 0 0 0 0\n7  0 1 0 0 0 0\n8  0 0 1 0 0 0\n9  0 0 0 1 0 0\n10 0 0 0 0 1 0\n11 1 0 0 0 0 0\n12 0 1 0 0 0 0\n13 0 0 1 0 0 0\n14 0 0 0 1 0 0\n15 0 0 0 0 1 0\n16 0 0 0 0 0 1\nattr(,\"assign\")\n[1] 1 1 1 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$dummy\n[1] \"contr.treatment\"\n\n\n[[2]]\n  1 2 3 4 5 6\n1 1 1 1 1 1 1\n2 1 1 1 1 1 1\n3 1 1 1 1 1 1\n4 1 1 1 1 1 1\n5 1 1 1 1 1 1\n6 1 1 1 1 1 1\n\n\n\n\nCode\nx <- as.factor(c(1:5,2:6,1:5))\ndsr(x) # list(vector, vcov)\n\n\n[[1]]\n   1 2 3 4 5 6\n1  1 0 0 0 0 0\n2  0 1 0 0 0 0\n3  0 0 1 0 0 0\n4  0 0 0 1 0 0\n5  0 0 0 0 1 0\n6  0 1 0 0 0 0\n7  0 0 1 0 0 0\n8  0 0 0 1 0 0\n9  0 0 0 0 1 0\n10 0 0 0 0 0 1\n11 1 0 0 0 0 0\n12 0 1 0 0 0 0\n13 0 0 1 0 0 0\n14 0 0 0 1 0 0\n15 0 0 0 0 1 0\nattr(,\"assign\")\n[1] 1 1 1 1 1 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$dummy\n[1] \"contr.treatment\"\n\n\n[[2]]\n  1 2 3 4 5 6\n1 1 0 0 0 0 0\n2 0 1 0 0 0 0\n3 0 0 1 0 0 0\n4 0 0 0 1 0 0\n5 0 0 0 0 1 0\n6 0 0 0 0 0 1\n\n\n\nFurther Reading\nThey overview their package in this paper, and implement three programs for fitting:\n\nNewton Rapheson Direct Inversion\n“Average Information REML” - original paper by same authors of the AS-REML package. A great example of this optimization step with an example is given in a book chapter by author of “sommer” package Overview of Major Computer Packages for Genomic Prediction of Complex Traits\n“Efficient Mixed Model Association (EMMA)”"
  },
  {
    "objectID": "mixed/mixed_variance.html#sas",
    "href": "mixed/mixed_variance.html#sas",
    "title": "20  Mixed Models (Variance)",
    "section": "23.3 SAS",
    "text": "23.3 SAS\n\nTips and Strategies for Mixed Modeling with SAS"
  },
  {
    "objectID": "mixed/mixed_variance.html#glmmtmb",
    "href": "mixed/mixed_variance.html#glmmtmb",
    "title": "20  Mixed Models (Variance)",
    "section": "23.4 glmmTMB",
    "text": "23.4 glmmTMB\nThe glmmTMB variance covariance structures vignette has a lot of information."
  },
  {
    "objectID": "mixed/mixed_variance.html#homogenous-variance",
    "href": "mixed/mixed_variance.html#homogenous-variance",
    "title": "20  Mixed Models (Variance)",
    "section": "24.1 Homogenous variance",
    "text": "24.1 Homogenous variance\n\n\n\n\n\n\n24.1.1 lme4\n\n\nCode\n# lmer approach\nmmod1 <- lmer(Yield ~ Env + (1 | Name) + (1|Env:Name), data = DT_example)\n# mmod1_alt <- lmer(Yield ~ Env + (1 | Name/Env), data = DT_example) # same way of specifying model identical results\n\n# summary(mmod1_alt)\nas.data.frame(VarCorr(mmod1))$vcov\n\n\n[1] 5.173234 3.681927 4.366211\n\n\nCode\n # Groups   Name        Std.Dev.\n # Env:Name (Intercept) 2.2745  \n # Name     (Intercept) 1.9188  \n # Residual             2.0895\n\n# profiling for standard errors\npr_mmod1 <- profile(mmod1)\n# xyplot (pr_mmod1 , aspect =1.3)\n# splom(pr_mmod1)\n# densityplot(pr_mmod1)\n\n# taking the half the width of a .68 confidence interval as an estimate of a standard error of the estimate\nconfint(pr_mmod1, c(\".sig01\", \".sig02\", \".sigma\"), level = .68)^2 %>% # square for variance\n  as.data.frame() %>%\n  mutate(var_se_ish = (`84 %` - `16 %`) / 2)\n\n\n           16 %     84 % var_se_ish\n.sig01 3.665686 6.471304  1.4028093\n.sig02 2.248953 5.305835  1.5284410\n.sigma 3.779901 5.084092  0.6520955\n\n\n\n\n24.1.2 sommer\n\n\nCode\n# Univariate, homogenous variance models,\n# for p > n\nans1r <- mmer(Yield ~ Env,\n              random = ~ Name + Env:Name,\n              rcov = ~ units, # residual structure\n              data = DT_example, verbose = FALSE)\n\n\nVersion out of date. Please update sommer to the newest version using:\ninstall.packages('sommer') in a new session\n Use the 'dateWarning' argument to disable the warning message.\n\n\nCode\nsummary(ans1r)$varcomp\n\n\n                      VarComp VarCompSE   Zratio Constraint\nName.Yield-Yield     3.681877 1.6909561 2.177394   Positive\nEnv:Name.Yield-Yield 5.173062 1.4952313 3.459707   Positive\nunits.Yield-Yield    4.366285 0.6470458 6.748031   Positive\n\n\n\n\nCode\n# MME-based Average information, for records > coefficients?\nans1c <- mmec(Yield~Env,\n              random= ~ Name + Env:Name,\n              rcov= ~ units,\n              data=DT_example, verbose = FALSE)\n\n\nVersion out of date. Please update sommer to the newest version using:\ninstall.packages('sommer') in a new session\n Use the 'dateWarning' argument to disable the warning message.\n\n\nCode\nsummary(ans1c)$varcomp\n\n\n                  VarComp VarCompSE   Zratio Constraint\nName:isc:isc     3.702831  1.698858 2.179600   Positive\nEnv:Name:isc:isc 5.132119  1.875676 2.736143   Positive\nunits:isc:isc    4.465685  1.611163 2.771715   Positive\n\n\n\n\n24.1.3 nlme\n\n\nCode\n# lme method for fitting homogenous variance model, nested\nlmod1 <- lme(Yield~Env,\n             random = list(Name = pdSymm(form = ~1),\n                           Env = pdSymm(form = ~1)),\n             method = \"REML\",\n             data = DT_example)\n\nsummary(lmod1)\n\n\nLinear mixed-effects model fit by REML\n  Data: DT_example \n      AIC     BIC    logLik\n  948.197 967.421 -468.0985\n\nRandom effects:\n Formula: ~1 | Name\n        (Intercept)\nStdDev:     1.91884\n\n Formula: ~1 | Env %in% Name\n        (Intercept) Residual\nStdDev:    2.274477 2.089546\n\nFixed effects:  Yield ~ Env \n                Value Std.Error DF   t-value p-value\n(Intercept) 16.496350 0.6855031 91 24.064589       0\nEnvCA.2012  -5.776757 0.7558206 51 -7.643027       0\nEnvCA.2013  -6.380478 0.7960544 51 -8.015128       0\n Correlation: \n           (Intr) ECA.2012\nEnvCA.2012 -0.732         \nEnvCA.2013 -0.712  0.643  \n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.95612756 -0.46306587 -0.03282517  0.37592452  4.22246535 \n\nNumber of Observations: 185\nNumber of Groups: \n         Name Env %in% Name \n           41            94 \n\n\n\n\n24.1.4 SAS\n\n\nCode\n# SAS example\nproc mixed data=dt covtest;\nclass env name;\nmodel yield = env;\nrandom name name*env;\nrun;\n\n\n\ncovtest - for the standard errors of covariance estimates\n\n\nSAS output, homogenous variance\n\n\nCov Parm\nEstimate\nStandard Error\nZ-Value\nPr > Z\n\n\n\n\nname\n3.7861\n1.6078\n2.35\n0.0093\n\n\nenv*name\n5.1616\n1.4675\n3.52\n0.0002\n\n\nResidual\n4.3678\n0.6477\n6.74\n<.0001\n\n\n\n\n\n24.1.5 glmmTMB\n\n\nCode\ntmod1 <- glmmTMB(Yield ~ Env + (1 | Name) + (1 | Env:Name), \n                 REML = TRUE,\n                 data = DT_example)\nsummary(tmod1)\n\n\n Family: gaussian  ( identity )\nFormula:          Yield ~ Env + (1 | Name) + (1 | Env:Name)\nData: DT_example\n\n     AIC      BIC   logLik deviance df.resid \n   948.2    967.5   -468.1    936.2      182 \n\nRandom effects:\n\nConditional model:\n Groups   Name        Variance Std.Dev.\n Name     (Intercept) 3.682    1.919   \n Env:Name (Intercept) 5.173    2.274   \n Residual             4.366    2.090   \nNumber of obs: 185, groups:  Name, 41; Env:Name, 94\n\nDispersion estimate for gaussian family (sigma^2): 4.37 \n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  16.4964     0.6871  24.010  < 2e-16 ***\nEnvCA.2012   -5.7768     0.7581  -7.620 2.54e-14 ***\nEnvCA.2013   -6.3805     0.8008  -7.968 1.61e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "mixed/mixed_variance.html#section",
    "href": "mixed/mixed_variance.html#section",
    "title": "20  Mixed Models (Variance)",
    "section": "",
    "text": "Comparison\nThe results with lmer are quite close, but the effect from average information is quite different. The conclusion should be that even for simple models, the numerical differences of the variances can be quite different depending on the optimization algorithm."
  },
  {
    "objectID": "mixed/mixed_variance.html#heterogeneous-g-homogenous-r",
    "href": "mixed/mixed_variance.html#heterogeneous-g-homogenous-r",
    "title": "20  Mixed Models (Variance)",
    "section": "24.2 Heterogeneous G, homogenous R",
    "text": "24.2 Heterogeneous G, homogenous R\nThis is an example of heterogenous VC effects for G2, and still identity for R.\n\n\n\n\n\n\n24.2.1 lme4\nYou can force lmer to fit several random effects by expanding the variable to be a dummy variable\n\n\nCode\nDT_lmer <- bind_cols(DT_example,\n                     model.matrix(~0 + Env, data=DT_example))\nDT_lmer %>% names()\n\n\n [1] \"Name\"       \"Env\"        \"Loc\"        \"Year\"       \"Block\"     \n [6] \"Yield\"      \"Weight\"     \"EnvCA.2011\" \"EnvCA.2012\" \"EnvCA.2013\"\n\n\nCode\nmmod2 <- lmer(Yield ~ Env + (1|Name) + (0 + EnvCA.2011 + EnvCA.2012 + EnvCA.2013 || Name), \n              data = DT_lmer)\n\nsummary(mmod2)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: \nYield ~ Env + (1 | Name) + (0 + EnvCA.2011 + EnvCA.2012 + EnvCA.2013 ||  \n    Name)\n   Data: DT_lmer\n\nREML criterion at convergence: 932.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6628 -0.4984 -0.0448  0.4083  4.4902 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Name     (Intercept)  2.964   1.722   \n Name.1   EnvCA.2011  10.426   3.229   \n Name.2   EnvCA.2012   2.659   1.631   \n Name.3   EnvCA.2013   5.702   2.388   \n Residual              4.398   2.097   \nNumber of obs: 185, groups:  Name, 41\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)  16.5109     0.8269 23.0600  19.966 4.73e-16 ***\nEnvCA.2012   -5.8089     0.8593 22.5089  -6.760 7.60e-07 ***\nEnvCA.2013   -6.4233     0.9359 30.3461  -6.863 1.21e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) ECA.2012\nEnvCA.2012 -0.860         \nEnvCA.2013 -0.799  0.767  \n\n\n\n\n24.2.2 sommer\n\n\nCode\nans2r <- mmer(Yield~Env,\n              random= ~Name + vsr(dsr(Env),Name), # the second one is an interaction term, and specifies\n              rcov= ~units,\n              data=DT_example, verbose = FALSE)\n\n\nVersion out of date. Please update sommer to the newest version using:\ninstall.packages('sommer') in a new session\n Use the 'dateWarning' argument to disable the warning message.\n\n\nCode\nsummary(ans2r)$varcomp\n\n\n                           VarComp VarCompSE   Zratio Constraint\nName.Yield-Yield          2.964764  1.505450 1.969354   Positive\nCA.2011:Name.Yield-Yield 10.423874  4.454365 2.340148   Positive\nCA.2012:Name.Yield-Yield  2.657574  1.803218 1.473796   Positive\nCA.2013:Name.Yield-Yield  5.702131  2.511301 2.270588   Positive\nunits.Yield-Yield         4.397686  0.651709 6.747930   Positive\n\n\n\n\nCode\nans2e <- mmec(Yield~Env,\n              random = ~Name + vsc(dsc(Env), isc(Name)),\n              rcov = ~units,\n              data = DT_example,\n              verbose = FALSE)\n\n\nVersion out of date. Please update sommer to the newest version using:\ninstall.packages('sommer') in a new session\n Use the 'dateWarning' argument to disable the warning message.\n\n\nCode\nsummary(ans2e)$varcomp\n\n\n                           VarComp VarCompSE   Zratio Constraint\nName:isc:isc              3.026478  1.525291 1.984198   Positive\nEnv:Name:CA.2011:CA.2011 10.135905  3.763618 2.693128   Positive\nEnv:Name:CA.2012:CA.2012  2.642893  2.210557 1.195578   Positive\nEnv:Name:CA.2013:CA.2013  5.780788  3.144418 1.838429   Positive\nunits:isc:isc             4.152878  1.520209 2.731780   Positive\n\n\n\n\n24.2.3 glmmTMB\n\n\nCode\n# glmmTMB\ntmod2 <- glmmTMB(Yield ~ Env + (1|Name) + diag(Env+0|Name), # just for more intuitive summary results, (changes interpretation of covariance paramters)\n                 data = DT_example,\n                 REML = TRUE)\nsummary(tmod2)\n\n\n Family: gaussian  ( identity )\nFormula:          Yield ~ Env + (1 | Name) + diag(Env + 0 | Name)\nData: DT_example\n\n     AIC      BIC   logLik deviance df.resid \n   948.2    974.0   -466.1    932.2      180 \n\nRandom effects:\n\nConditional model:\n Groups   Name        Variance Std.Dev. Corr      \n Name     (Intercept)  2.964   1.722              \n Name.1   EnvCA.2011  10.426   3.229              \n          EnvCA.2012   2.659   1.631    0.00      \n          EnvCA.2013   5.702   2.388    0.00 0.00 \n Residual              4.398   2.097              \nNumber of obs: 185, groups:  Name, 41\n\nDispersion estimate for gaussian family (sigma^2):  4.4 \n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  16.5109     0.8281  19.939  < 2e-16 ***\nEnvCA.2012   -5.8089     0.8613  -6.744 1.54e-11 ***\nEnvCA.2013   -6.4233     0.9399  -6.834 8.24e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n24.2.4 nlme\n\n\nCode\n# lme method of fitting heterogenous variance structure\nlmod2 <- lme(Yield ~ Env,\n             random = list(Name = pdBlocked(\n               list(pdIdent(~1), # Identity for Name\n                    pdDiag(~Env-1)))), # Diagonal structure for Environment:Name\n             method = \"REML\",\n             data = DT_example)\n\nVarCorr(lmod2) # Nice, they match!\n\n\nName = pdIdent(1), pdDiag(Env - 1) \n            Variance  StdDev  \n(Intercept)  2.963725 1.721547\nEnvCA.2011  10.425967 3.228927\nEnvCA.2012   2.658958 1.630631\nEnvCA.2013   5.702071 2.387901\nResidual     4.397553 2.097034\n\n\nCode\nsummary(lmod2)\n\n\nLinear mixed-effects model fit by REML\n  Data: DT_example \n       AIC      BIC    logLik\n  948.2295 973.8615 -466.1147\n\nRandom effects:\n Composite Structure: Blocked\n\n Block 1: (Intercept)\n Formula: ~1 | Name\n        (Intercept)\nStdDev:    1.721547\n\n Block 2: EnvCA.2011, EnvCA.2012, EnvCA.2013\n Formula: ~Env - 1 | Name\n Structure: Diagonal\n        EnvCA.2011 EnvCA.2012 EnvCA.2013 Residual\nStdDev:   3.228927   1.630631   2.387901 2.097034\n\nFixed effects:  Yield ~ Env \n                Value Std.Error  DF   t-value p-value\n(Intercept) 16.510938 0.8269335 142 19.966464       0\nEnvCA.2012  -5.808897 0.8593216 142 -6.759864       0\nEnvCA.2013  -6.423279 0.9358819 142 -6.863343       0\n Correlation: \n           (Intr) ECA.2012\nEnvCA.2012 -0.860         \nEnvCA.2013 -0.799  0.767  \n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.66278393 -0.49845117 -0.04478272  0.40832173  4.49017462 \n\nNumber of Observations: 185\nNumber of Groups: 41 \n\n\n\n\n24.2.5 afex\n\n\nCode\nset_default_contrasts()\n\n\nsetting contr.treatment globally: options(contrasts=c('contr.treatment', 'contr.poly'))\n\n\nCode\namod2 <- mixed(Yield ~ Env + (1|Name) + (0 + Env || Name),\n      data = DT_example,\n      check_contrasts = FALSE,\n      expand_re = TRUE)\nsummary(amod2)\n\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: Yield ~ Env + (1 | Name) + (0 + re2.EnvCA.2011 + re2.EnvCA.2012 +  \n    re2.EnvCA.2013 || Name)\n   Data: data\n\nREML criterion at convergence: 932.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.6628 -0.4984 -0.0448  0.4083  4.4902 \n\nRandom effects:\n Groups   Name           Variance Std.Dev.\n Name     (Intercept)     2.964   1.722   \n Name.1   re2.EnvCA.2011 10.426   3.229   \n Name.2   re2.EnvCA.2012  2.659   1.631   \n Name.3   re2.EnvCA.2013  5.702   2.388   \n Residual                 4.398   2.097   \nNumber of obs: 185, groups:  Name, 41\n\nFixed effects:\n            Estimate Std. Error      df t value Pr(>|t|)    \n(Intercept)  16.5109     0.8269 23.0600  19.966 4.73e-16 ***\nEnvCA.2012   -5.8089     0.8593 22.5089  -6.760 7.60e-07 ***\nEnvCA.2013   -6.4233     0.9359 30.3461  -6.863 1.21e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) ECA.2012\nEnvCA.2012 -0.860         \nEnvCA.2013 -0.799  0.767  \n\n\n\n\n24.2.6 SAS\n\n\nCode\n# SAS\nproc mixed data=dt;\nclass env name;\nmodel yield = env;\nrandom name / G;\nrandom name*env / group = env;\nrun;"
  },
  {
    "objectID": "mixed/mixed_variance.html#heterogenous-g-and-r",
    "href": "mixed/mixed_variance.html#heterogenous-g-and-r",
    "title": "20  Mixed Models (Variance)",
    "section": "24.3 Heterogenous G and R",
    "text": "24.3 Heterogenous G and R\nAlso called a Heterogeneous CS model.\n\n\n\n\n\n\n24.3.1 sommer\n\n\nCode\nans3r <- mmer(Yield~Env,\n              random= ~Name + vsr(dsr(Env),Name), # the second one is an interaction term, and specifies\n              rcov= ~vsr(dsr(Env),units),\n              data=DT_example, \n              verbose = FALSE)\n\n\nVersion out of date. Please update sommer to the newest version using:\ninstall.packages('sommer') in a new session\n Use the 'dateWarning' argument to disable the warning message.\n\n\nCode\nsqrt(summary(ans3r)$varcomp$VarComp)\n\n\n[1] 1.721293 3.185337 1.370230 2.574714 2.223162 2.392690 1.599962\n\n\nCode\nsummary(ans3r)$varcomp %>% mutate(sdComp = sqrt(VarComp),\n                                  .after = VarComp)\n\n\n                            VarComp   sdComp VarCompSE   Zratio Constraint\nName.Yield-Yield           2.962851 1.721293 1.4962000 1.980251   Positive\nCA.2011:Name.Yield-Yield  10.146369 3.185337 4.5073271 2.251083   Positive\nCA.2012:Name.Yield-Yield   1.877530 1.370230 1.8697568 1.004158   Positive\nCA.2013:Name.Yield-Yield   6.629152 2.574714 2.5028114 2.648682   Positive\nCA.2011:units.Yield-Yield  4.942450 2.223162 1.5245057 3.242001   Positive\nCA.2012:units.Yield-Yield  5.724963 2.392690 1.3123015 4.362536   Positive\nCA.2013:units.Yield-Yield  2.559880 1.599962 0.6399685 4.000010   Positive\n\n\n\n\nCode\n# Apparently order of your data matters, and can result in an error message of not fitting... can't find documentation as to why.\nDT <- DT_example[with(DT_example, order(Env)), ]\n\nans3e <- mmec(Yield~Env,\n              random = ~Name + vsc(dsc(Env), isc(Name)),\n              rcov = ~vsc(dsc(Env), isc(units)),\n              data = DT, # DT_example here will result in failure...\n              verbose = FALSE)\n\n\nVersion out of date. Please update sommer to the newest version using:\ninstall.packages('sommer') in a new session\n Use the 'dateWarning' argument to disable the warning message.\n\n\nCode\nsummary(ans3e)$varcomp\n\n\n                            VarComp VarCompSE   Zratio Constraint\nName:isc:isc               2.969457 1.3199316 2.249705   Positive\nEnv:Name:CA.2011:CA.2011  10.113270 4.2317594 2.389850   Positive\nEnv:Name:CA.2012:CA.2012   1.874589 1.3918727 1.346810   Positive\nEnv:Name:CA.2013:CA.2013   6.625506 2.3850257 2.777960   Positive\nEnv:units:CA.2011:CA.2011  4.953675 1.3389505 3.699670   Positive\nEnv:units:CA.2012:CA.2012  5.725455 1.2010534 4.767027   Positive\nEnv:units:CA.2013:CA.2013  2.561167 0.5872852 4.361029   Positive\n\n\n\n\n24.3.2 nlme\n\n\nCode\n# lme example with heterogenous G and R\nlmod3 <- lme(Yield ~ Env,\n             random = list(Name = pdBlocked(list(\n               pdIdent(form = ~1), # Name\n               pdDiag(form = ~Env-1)))), # Name:Env\n             weights = varIdent(form = ~1 | Env),\n             method = \"REML\",\n             data = DT_example)\n\nsummary(lmod3)\n\n\nLinear mixed-effects model fit by REML\n  Data: DT_example \n       AIC      BIC    logLik\n  946.7659 978.8059 -463.3829\n\nRandom effects:\n Composite Structure: Blocked\n\n Block 1: (Intercept)\n Formula: ~1 | Name\n        (Intercept)\nStdDev:    1.721021\n\n Block 2: EnvCA.2011, EnvCA.2012, EnvCA.2013\n Formula: ~Env - 1 | Name\n Structure: Diagonal\n        EnvCA.2011 EnvCA.2012 EnvCA.2013 Residual\nStdDev:   3.185572   1.370587   2.574817  2.39263\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | Env \n Parameter estimates:\n  CA.2012   CA.2013   CA.2011 \n1.0000000 0.6686978 0.9291778 \nFixed effects:  Yield ~ Env \n                Value Std.Error  DF   t-value p-value\n(Intercept) 16.507683 0.8268640 142 19.964204       0\nEnvCA.2012  -5.816899 0.8575800 142 -6.782923       0\nEnvCA.2013  -6.412440 0.9356534 142 -6.853436       0\n Correlation: \n           (Intr) ECA.2012\nEnvCA.2012 -0.861         \nEnvCA.2013 -0.799  0.768  \n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.07307332 -0.53566469 -0.05597277  0.46043655  4.19617802 \n\nNumber of Observations: 185\nNumber of Groups: 41 \n\n\nCode\nVarCorr(lmod3)\n\n\nName = pdIdent(1), pdDiag(Env - 1) \n            Variance  StdDev  \n(Intercept)  2.961912 1.721021\nEnvCA.2011  10.147871 3.185572\nEnvCA.2012   1.878509 1.370587\nEnvCA.2013   6.629680 2.574817\nResidual     5.724679 2.392630\n\n\nCode\nsigma_hat <- summary(lmod3)$sigma # for some reason, defaults to CA.2012 for the \"reference\" sigma\nsigma_weights <- coef(lmod3$modelStruct$varStruct, unconstrained=FALSE) # extract the weights estimated for each Env\nc(CA.2012 = sigma_hat, sigma_hat*sigma_weights)^2 # Multiply reference sigma to weights for final sd estimates (square for variance)\n\n\n CA.2012  CA.2013  CA.2011 \n5.724679 2.559829 4.942524 \n\n\nCode\nintervals(lmod3) # for the approximate confidence intervals\n\n\nApproximate 95% confidence intervals\n\n Fixed effects:\n                lower      est.     upper\n(Intercept) 14.873129 16.507683 18.142237\nEnvCA.2012  -7.512173 -5.816899 -4.121626\nEnvCA.2013  -8.262050 -6.412440 -4.562831\n\n Random Effects:\n  Level: Name \n                   lower     est.    upper\nsd(1)          1.0650575 1.721021 2.780988\nsd(EnvCA.2011) 2.0264211 3.185572 5.007780\nsd(EnvCA.2012) 0.4935377 1.370587 3.806212\nsd(EnvCA.2013) 1.7766278 2.574817 3.731609\n\n Variance function:\n            lower      est.     upper\nCA.2013 0.4773923 0.6686978 0.9366651\nCA.2011 0.6366733 0.9291778 1.3560666\n\n Within-group standard error:\n   lower     est.    upper \n1.897397 2.392630 3.017122 \n\n\n\n\nCode\n# picked A01143-3C because it is present in all 3 environments\ngetVarCov(lmod3, type = \"random.effects\") # G\n\n\nRandom effects variance covariance matrix\n            (Intercept) EnvCA.2011 EnvCA.2012 EnvCA.2013\n(Intercept)      2.9619      0.000     0.0000     0.0000\nEnvCA.2011       0.0000     10.148     0.0000     0.0000\nEnvCA.2012       0.0000      0.000     1.8785     0.0000\nEnvCA.2013       0.0000      0.000     0.0000     6.6297\n  Standard Deviations: 1.721 3.1856 1.3706 2.5748 \n\n\nCode\ngetVarCov(lmod3, individuals = \"A01143-3C\", type = \"conditional\") # R \n\n\nName A01143-3C \nConditional variance covariance matrix\n       1      2      3      4      5      6\n1 5.7247 0.0000 0.0000 0.0000 0.0000 0.0000\n2 0.0000 5.7247 0.0000 0.0000 0.0000 0.0000\n3 0.0000 0.0000 2.5598 0.0000 0.0000 0.0000\n4 0.0000 0.0000 0.0000 4.9425 0.0000 0.0000\n5 0.0000 0.0000 0.0000 0.0000 4.9425 0.0000\n6 0.0000 0.0000 0.0000 0.0000 0.0000 2.5598\n  Standard Deviations: 2.3926 2.3926 1.5999 2.2232 2.2232 1.5999 \n\n\nCode\ngetVarCov(lmod3, individuals = \"A01143-3C\", type = \"marginal\") # ZGZ + R\n\n\nName A01143-3C \nMarginal variance covariance matrix\n        1       2       3       4       5       6\n1 10.5650  4.8404  2.9619  2.9619  2.9619  2.9619\n2  4.8404 10.5650  2.9619  2.9619  2.9619  2.9619\n3  2.9619  2.9619 12.1510  2.9619  2.9619  9.5916\n4  2.9619  2.9619  2.9619 18.0520 13.1100  2.9619\n5  2.9619  2.9619  2.9619 13.1100 18.0520  2.9619\n6  2.9619  2.9619  9.5916  2.9619  2.9619 12.1510\n  Standard Deviations: 3.2504 3.2504 3.4859 4.2488 4.2488 3.4859 \n\n\n\n\n24.3.3 glmmTMB\n\n\nCode\n# glmmTMB workaround for heterogenous R, need to code individual units as factor\nDT_TMB_example <- DT_example %>% mutate(units = as.factor(1:n()))\n\n# gives \"false convergence warning\": https://cran.r-project.org/web/packages/glmmTMB/vignettes/troubleshooting.html\n# tough to diagnose\ntmod3 <- glmmTMB(Yield ~ Env + (1|Name) + diag(Env + 0 | Name) + diag(Env + 0 | units), \n                 dispformula = ~0,\n                 REML = TRUE,\n                 data = DT_TMB_example)\n\n# update the optimizer method\ntmod3_bfgs <- update(tmod3, control = glmmTMBControl(optimizer = optim,\n                                                     optArgs = list(method=\"BFGS\")))\n\n# extract variance components\ntmod3_bfgs %>% tidy(effects = \"ran_pars\") %>% \n  filter(startsWith(term, \"sd\")) %>% \n  separate(term, into = c(\"term\", \"partition\"), sep = \"__\") %>% \n  mutate(var_est = estimate^2) %>% \n  dplyr::select(group, partition, var_est)\n\n\n# A tibble: 7 × 3\n  group  partition   var_est\n  <chr>  <chr>         <dbl>\n1 Name   (Intercept)    2.96\n2 Name.1 EnvCA.2011    10.1 \n3 Name.1 EnvCA.2012     1.88\n4 Name.1 EnvCA.2013     6.63\n5 units  EnvCA.2011     4.94\n6 units  EnvCA.2012     5.72\n7 units  EnvCA.2013     2.56\n\n\n\n\n24.3.4 SAS\n\n\nCode\nproc mixed data=dt;\nclass env name;\nmodel yield = env;\nrandom name;\nrandom name*env / group = env;\nrepeated / group = env;\nrun;\n\n\n\nSAS output for above code\n\n\nCov\nParm\nGroup\nEstimate\n\n\n\n\nname\n\n\n3.0647\n\n\nenv*name\nenv\n“CA.2011\n10.0346\n\n\nenv*name\nenv\n“CA.2012\n1.8875\n\n\nenv*name\nenv\n“CA.2013\n6.5388\n\n\nResidual\nenv\n“CA.2011\n4.9424\n\n\nResidual\nenv\n“CA.2012\n5.7143\n\n\nResidual\nenv\n“CA.2013\n2.5599"
  },
  {
    "objectID": "mixed/mixed_variance.html#unstructured-g-heterogenous-r",
    "href": "mixed/mixed_variance.html#unstructured-g-heterogenous-r",
    "title": "20  Mixed Models (Variance)",
    "section": "24.4 Unstructured G, Heterogenous R",
    "text": "24.4 Unstructured G, Heterogenous R\n\n\n\n\n\n\n24.4.1 sommer\n\n\nCode\nans4r <- mmer(Yield~Env,\n              random=~ vsr(usr(Env), Name), # we drop name from the model just for illustration (too many variables)\n              rcov=~vsr(dsr(Env),units),\n              data=DT_example, verbose = FALSE)\n\n\nVersion out of date. Please update sommer to the newest version using:\ninstall.packages('sommer') in a new session\n Use the 'dateWarning' argument to disable the warning message.\n\n\nCode\nsummary(ans4r)$varcomp\n\n\n                                    VarComp VarCompSE    Zratio Constraint\nCA.2011:Name.Yield-Yield         15.6650010 5.4206906 2.8898534   Positive\nCA.2012:CA.2011:Name.Yield-Yield  6.1101600 2.4850649 2.4587527   Unconstr\nCA.2012:Name.Yield-Yield          4.5296090 1.8208107 2.4876881   Positive\nCA.2013:CA.2011:Name.Yield-Yield  6.3844808 3.0658977 2.0824181   Unconstr\nCA.2013:CA.2012:Name.Yield-Yield  0.3929997 1.5233985 0.2579757   Unconstr\nCA.2013:Name.Yield-Yield          8.5972750 2.4837814 3.4613654   Positive\nCA.2011:units.Yield-Yield         4.9698460 1.5322540 3.2434870   Positive\nCA.2012:units.Yield-Yield         5.6729333 1.3007862 4.3611574   Positive\nCA.2013:units.Yield-Yield         2.5570940 0.6392821 3.9999462   Positive\n\n\n\n\nCode\n# Display the Variance components as a matrix\nG_vec <- summary(ans4r)$varcomp %>% slice(1:6) %>% pull(VarComp)\nG <- Matrix(0,\n            nrow = 3,\n            ncol = 3)\nG[upper.tri(G, diag = TRUE)] <- G_vec\ncolnames(G) <- c(\"CA.2011\", \"CA.2012\", \"CA.2013\")\nrownames(G) <- c(\"CA.2011\", \"CA.2012\", \"CA.2013\")\nG\n\n\n3 x 3 Matrix of class \"dtrMatrix\"\n           CA.2011    CA.2012    CA.2013\nCA.2011 15.6650010  6.1101600  6.3844808\nCA.2012          .  4.5296090  0.3929997\nCA.2013          .          .  8.5972750\n\n\n\n\n24.4.2 nlme\n\n\nCode\n# lme version. Note we dropped \"Name\" from the model here because otherwise the std error of the variance estimates were HUGE. checked with interval(lmod4)\nlmod4 <- lme(Yield ~ Env,\n             random = list(Name = pdSymm(form = ~Env-1)), # Name:Env\n             weights = varIdent(form = ~1 | Env),\n             method = \"REML\",\n             data = DT_example)\ngetVarCov(lmod4) # G, now that we specify the same likelihood, the answers are the same\n\n\nRandom effects variance covariance matrix\n           EnvCA.2011 EnvCA.2012 EnvCA.2013\nEnvCA.2011    15.6640    6.11080    6.38400\nEnvCA.2012     6.1108    4.53100    0.39141\nEnvCA.2013     6.3840    0.39141    8.59800\n  Standard Deviations: 3.9578 2.1286 2.9322 \n\n\nCode\ngetVarCov(lmod4, individual = \"A01143-3C\",type = \"conditional\") # R\n\n\nName A01143-3C \nConditional variance covariance matrix\n       1      2     3    4    5     6\n1 5.6722 0.0000 0.000 0.00 0.00 0.000\n2 0.0000 5.6722 0.000 0.00 0.00 0.000\n3 0.0000 0.0000 2.557 0.00 0.00 0.000\n4 0.0000 0.0000 0.000 4.97 0.00 0.000\n5 0.0000 0.0000 0.000 0.00 4.97 0.000\n6 0.0000 0.0000 0.000 0.00 0.00 2.557\n  Standard Deviations: 2.3816 2.3816 1.5991 2.2294 2.2294 1.5991 \n\n\nCode\n# summary(lmod4)\n\n\n\n\nCode\n# manual way of extracting the weights and multiplying by sigma for the residual variance components.\n# lmod4$modelStruct$varStruct # The variance structure of the residuals is in this object\nvw <- 1 / varWeights(lmod4$modelStruct$varStruct) # accessor function gives weights, which is inverse of sigma multiplier\n(vw[!duplicated(vw)] * lmod4$sigma)^2 # deduplicate, then multiply by sigma for variance\n\n\n\n\n24.4.3 glmmTMB\n\n\nCode\n## glmmTMB\ntmod4 <- glmmTMB(Yield ~ Env + us(Env + 0 | Name) + diag(Env + 0 | units), # drop name for estimability...\n                 dispformula = ~0,\n                 REML = TRUE,\n                 data=DT_TMB_example)\nsummary(tmod4)\n\n\n Family: gaussian  ( identity )\nFormula:          Yield ~ Env + us(Env + 0 | Name) + diag(Env + 0 | units)\nDispersion:             ~0\nData: DT_TMB_example\n\n     AIC      BIC   logLik deviance df.resid \n   942.9    981.5   -459.5    918.9      176 \n\nRandom effects:\n\nConditional model:\n Groups Name       Variance Std.Dev. Corr      \n Name   EnvCA.2011 15.665   3.958              \n        EnvCA.2012  4.531   2.129    0.73      \n        EnvCA.2013  8.598   2.932    0.55 0.06 \n units  EnvCA.2011  4.970   2.229              \n        EnvCA.2012  5.672   2.382    0.00      \n        EnvCA.2013  2.557   1.599    0.00 0.00 \nNumber of obs: 185, groups:  Name, 41; units, 185\n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  16.3313     0.8174  19.980  < 2e-16 ***\nEnvCA.2012   -5.6959     0.7444  -7.652 1.98e-14 ***\nEnvCA.2013   -6.2711     0.8282  -7.572 3.67e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# tmb_sd <- vcov(tmod4,full=TRUE) %>% diag() %>% `[`(4:12) %>% sqrt()\n# tmb_logsd <- tmod$sdr$par.fixed[2:5]\n# tmb_logsd <- tmb_logsd[c(1, 2, 3, 1, 2, 3, 4, 4, 4)]\n# tmb_sd*2*exp(2*tmb_logsd)\n# \n# sqrt(2*exp(tmod4$sdr$par.fixed * 2))\n# confint(tmod4, component = \"all\", parm = c(1, 2, 3, 4, 5))\n# glmmTMB:::confint.glmmTMB(tmod4,parm=\"theta_\")"
  },
  {
    "objectID": "mixed/mixed_variance.html#multivariate-homogenous-variance-models",
    "href": "mixed/mixed_variance.html#multivariate-homogenous-variance-models",
    "title": "20  Mixed Models (Variance)",
    "section": "24.5 Multivariate, Homogenous variance models",
    "text": "24.5 Multivariate, Homogenous variance models\n\n24.5.1 sommer\n\n\nCode\nans5r <- mmer(cbind(Yield, Weight) ~ Env,\n              random= ~ vsr(Name, Gtc=unsm(2)),\n              rcov= ~ vsr(units, Gtc=diag(2)),\n              data=DT_example, verbose = FALSE)\n\n\nVersion out of date. Please update sommer to the newest version using:\ninstall.packages('sommer') in a new session\n Use the 'dateWarning' argument to disable the warning message.\n\n\nCode\nsummary(ans5r)$varcomp\n\n\n                        VarComp  VarCompSE   Zratio Constraint\nu:Name.Yield-Yield    5.0023799 1.48723681 3.363540   Positive\nu:Name.Yield-Weight   1.4364292 0.33713194 4.260733   Unconstr\nu:Name.Weight-Weight  0.2813217 0.08054357 3.492789   Positive\nu:units.Yield-Yield   7.6133497 0.88506057 8.602066   Positive\nu:units.Weight-Weight 0.3586951 0.04189443 8.561882   Positive"
  },
  {
    "objectID": "network/network_centrality.html#introduction",
    "href": "network/network_centrality.html#introduction",
    "title": "20  Network Centrality",
    "section": "20.1 Introduction",
    "text": "20.1 Introduction\nSeveral types of measuring centrality\n\nDegree\n\nCounting the number of in/out edges. summing by weight\n\nEigenvalue\n\nCaptures the idea that being connected to someone else important means that you yourself are important.\n\nKatz\nPageRank"
  },
  {
    "objectID": "network/network_centrality.html#network-fixtures",
    "href": "network/network_centrality.html#network-fixtures",
    "title": "20  Network Centrality",
    "section": "20.2 Network Fixtures",
    "text": "20.2 Network Fixtures\n\n\nCode\nset.seed(1)\n# ER random graph, 20 nodes, average 5/20 connections, directed\ng_uni <- sample_gnp(20, 5/20, directed=TRUE)\n\n# Simple 1,1, 2, 3 graph\ng_simple_adj_matrix <- matrix(c(0, 1, 1, 1,\n                     1, 0, 0, 1,\n                     1, 0, 0, 0,\n                     1, 1, 0, 0),\n                     nrow = 4, ncol = 4)\n\ng_simple <- graph_from_adjacency_matrix(g_simple_adj_matrix, mode = \"undirected\")\n# g_simple %>% set_vertex_attr(\"name\", value = letters[1:4]) # label with alphabet\n\n# ER random graph, 20 nodes, average 5/20, undirected\n\ng_uni_bi <- sample_gnp(20, 5/20, directed=TRUE)\n\n\n# Simple Directed graph, acyclic\ng_simple_dir_adj_matrix <- matrix(c(0, 1, 1, 0, 1, 0, 0, 0,\n                                    0, 0, 1, 1, 0, 0, 0, 0,\n                                    0, 0, 0, 0, 0, 1, 0, 0,\n                                    0, 0, 0, 0, 1, 0, 1, 0,\n                                    0, 0, 0, 0, 0, 0, 0, 0,\n                                    0, 0, 0, 0, 0, 0, 0, 0,\n                                    0, 0, 0, 0, 0, 0, 0, 0,\n                                    0, 0, 0, 0, 0, 1, 0, 0),\n                                  nrow = 8,\n                                  ncol = 8,\n                                  byrow = TRUE)\n\ng_simple_dir <- graph_from_adjacency_matrix(g_simple_dir_adj_matrix, mode = \"directed\")"
  },
  {
    "objectID": "network/network_centrality.html#eigenvalue-centrality",
    "href": "network/network_centrality.html#eigenvalue-centrality",
    "title": "20  Network Centrality",
    "section": "20.3 Eigenvalue centrality",
    "text": "20.3 Eigenvalue centrality\n\n\nCode\n# The graph we look at\nggraph(g_simple, layout = \"kk\") +\n  geom_edge_fan() +\n  geom_node_label(aes(label = 1:4))\n\n\n\n\n\n\n\nCode\n# Eigenvector centrality through igraph\ncent_eig_igraph <- eigen_centrality(g_simple)$vector\n# Through eigenvector decomp\ncent_eig_manual <- eigen(g_simple_adj_matrix)$vectors[,1] / eigen(g_simple_adj_matrix)$vectors[1,1]\n\n# They are the same!\ncbind(\n  cent_eig_igraph, # Eigencentrality\n  cent_eig_manual # eig centrality calculation\n)\n\n\n     cent_eig_igraph cent_eig_manual\n[1,]       1.0000000       1.0000000\n[2,]       0.8546377       0.8546377\n[3,]       0.4608111       0.4608111\n[4,]       0.8546377       0.8546377\n\n\n\n\nCode\n# Note how\neigen_centrality\n\n\nfunction (graph, directed = FALSE, scale = TRUE, weights = NULL, \n    options = arpack_defaults) \n{\n    if (!is_igraph(graph)) {\n        stop(\"Not a graph object\")\n    }\n    directed <- as.logical(directed)\n    scale <- as.logical(scale)\n    if (is.null(weights) && \"weight\" %in% edge_attr_names(graph)) {\n        weights <- E(graph)$weight\n    }\n    if (!is.null(weights) && any(!is.na(weights))) {\n        weights <- as.numeric(weights)\n    }\n    else {\n        weights <- NULL\n    }\n    options.tmp <- arpack_defaults\n    options.tmp[names(options)] <- options\n    options <- options.tmp\n    on.exit(.Call(C_R_igraph_finalizer))\n    res <- .Call(C_R_igraph_eigenvector_centrality, graph, directed, \n        scale, weights, options)\n    if (igraph_opt(\"add.vertex.names\") && is_named(graph)) {\n        names(res$vector) <- vertex_attr(graph, \"name\", V(graph))\n    }\n    res\n}\n<bytecode: 0x7ff30c39be38>\n<environment: namespace:igraph>\n\n\nCode\neigen_centrality(g_simple_dir)$vector\n\n\n[1] 0.9767991 1.0000000 0.9081894 0.7377802 0.6537287 0.4051718 0.2812982\n[8] 0.1544825\n\n\nCode\neigen(g_simple_dir_adj_matrix)$vectors[,1] / eigen(g_simple_adj_matrix)$vectors[1,1]\n\n\n[1] 1.63498 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000 0.00000"
  },
  {
    "objectID": "network/network_centrality.html#katz-centrality",
    "href": "network/network_centrality.html#katz-centrality",
    "title": "20  Network Centrality",
    "section": "20.4 Katz Centrality",
    "text": "20.4 Katz Centrality\n\n\nCode\nlength(V(g_simple_dir))\n\n\n[1] 8\n\n\nCode\nggraph(g_simple_dir, layout = \"kk\") +\n  geom_edge_fan(arrow = arrow(length = unit(4, \"mm\")),\n                end_cap = circle(3, \"mm\")) +\n  geom_node_label(aes(label = 1:length(V(g_simple_dir))))\n\n\n\n\n\n\n\nCode\n# Calculating Katz Centrality with tidygraph (rank ordered?)\n# Only undirected?\nas_tbl_graph(g_simple) %>% \n  mutate(centrality = centrality_katz())\n\n\n# A tbl_graph: 4 nodes and 4 edges\n#\n# An undirected simple graph with 1 component\n#\n# Node Data: 4 × 1 (active)\n  centrality\n       <dbl>\n1      119. \n2      101. \n3       55.1\n4      101. \n#\n# Edge Data: 4 × 2\n   from    to\n  <int> <int>\n1     1     2\n2     1     3\n3     1     4\n# … with 1 more row\n\n\n\n\nCode\nalpha_centrality(g_simple_dir, alpha = 1)\n\n\n[1] 1 2 4 3 5 6 4 1"
  },
  {
    "objectID": "network/network_centrality.html#pagerank",
    "href": "network/network_centrality.html#pagerank",
    "title": "20  Network Centrality",
    "section": "20.5 PageRank",
    "text": "20.5 PageRank\n\n\nCode\npage_rank(g_simple)$vector\n\n\n[1] 0.3667359 0.2459278 0.1414085 0.2459278"
  },
  {
    "objectID": "network/network_gnar.html#tutorial",
    "href": "network/network_gnar.html#tutorial",
    "title": "21  GNAR",
    "section": "21.1 Tutorial",
    "text": "21.1 Tutorial\n\n\n\n\n\n\n\nCode\n# make_graph(~1-2-3-1, 1-4-5) # make igraph\n# fiveVTS\nfive_gnar <- GNARfit(vts = fiveVTS, net = fiveNet, alphaOrder = 2)\n# alphaOrder gives time lag\n\n\n\n\nCode\nplot(fiveVTS[,1], ylab = \"Node A time series\")\nlines(fitted(five_gnar)[,1], col = 2)\n\n\n\n\n\nThe residuals are examined as follows\n\n\nCode\nlayout(matrix(c(1,2), 2, 1))\npar(mar = c(4.1, 4.1, 1, 2.1), cex.axis = 0.9)\nplot(ts(residuals(five_gnar)[,1]), ylab = \"model residuals\")\nhist(residuals(five_gnar)[,1], main = \"\", xlab = \"model residuals\")\n\n\n\n\n\nShow that GNAR does not have a problem with missing data,\n\n\nCode\nfiveVTS0 <- fiveVTS\n\nfiveVTS0[50:150, 3] <- NA\nnafit <- GNARfit(vts = fiveVTS0, net = fiveNet, alphaOrder = 2, betaOrder = c(1, 1))\nlayout(matrix(c(1, 2), 2, 1))\npar(mar = c(4.1, 4.1, 0.75, 2.1), cex.axis = 0.75)\nplot(ts(fitted(nafit)[, 3]), ylab = \"Node C fitted values\")\npar(mar = c(4.1, 4.1, 0.75, 2.1), cex.axis = 0.75)\nplot(ts(fitted(nafit)[, 4]), ylab = \"Node D fitted values\")"
  },
  {
    "objectID": "next_gen_sequencing/next_gen_sequencing.html#copy-number-variation-window-binning",
    "href": "next_gen_sequencing/next_gen_sequencing.html#copy-number-variation-window-binning",
    "title": "22  Next Generation Sequencing",
    "section": "22.1 Copy Number Variation (window binning)",
    "text": "22.1 Copy Number Variation (window binning)\n\n22.1.1 readDepth\nreadDepth uses a negative binomial distribution to model the counts that fall into each bin."
  },
  {
    "objectID": "next_gen_sequencing/next_gen_sequencing.html#primer-for-microbiomes",
    "href": "next_gen_sequencing/next_gen_sequencing.html#primer-for-microbiomes",
    "title": "22  Next Generation Sequencing",
    "section": "22.2 Primer for Microbiomes",
    "text": "22.2 Primer for Microbiomes\n\n22.2.1 permANOVA\nBeta Diversity with Hagis\n\n\nCode\nhead(P_sojae_survey) # survey sample data\n\n\n   Isolate         Line         Rps Total HR (1) Lesion (2)\n1:       1     Williams susceptible    10      0          0\n2:       1       Harlon      Rps 1a    10      4          0\n3:       1 Harosoy 13xx      Rps 1b     8      0          0\n4:       1     L75-3735      Rps 1c    10     10          0\n5:       1    PI 103091      Rps 1d     9      2          0\n6:       1  Williams 82      Rps 1k    10      0          0\n   Lesion to cotyledon (3) Dead (4) total.susc total.resis perc.susc perc.resis\n1:                       0       10         10           0       100          0\n2:                       0        6          6           4        60         40\n3:                       0        8          8           0       100          0\n4:                       0        0          0          10         0        100\n5:                       1        6          7           2        78         22\n6:                       0       10         10           0       100          0\n\n\nCode\nP_sojae_survey$Isolate <-\n  gsub(pattern = \"MPS17_\",\n       replacement = \"\",\n       x = P_sojae_survey$Isolate)\nP_sojae_survey$Rps <-\n  gsub(pattern = \"Rps \",\n       replacement = \"\",\n       x = P_sojae_survey$Rps)\nhagis_args <- list(\n  x = P_sojae_survey,\n  cutoff = 60,\n  control = \"susceptible\",\n  sample = \"Isolate\",\n  gene = \"Rps\",\n  perc_susc = \"perc.susc\"\n)\nP_sojae_survey.matrix <- do.call(create_binary_matrix, hagis_args)\nP_sojae_survey.matrix.jaccard <-\n  vegdist(P_sojae_survey.matrix, \"jaccard\", na.rm = TRUE)\ngroups <- factor(c(rep(\"Michigan_1\", 11), rep(\"Michigan_2\", 10)))\n\n# calculate beta dispersion for each group, when comparing 2 or more\npathotype.disp <-\n  betadisper(P_sojae_survey.matrix.jaccard, groups)\n\n# tests if centroid distances are significantly different from each other\npathotype.disp.anova <- anova(pathotype.disp) \npathotype.disp.anova\n\n\nAnalysis of Variance Table\n\nResponse: Distances\n          Df   Sum Sq   Mean Sq F value Pr(>F)\nGroups     1 0.008375 0.0083752  0.9672 0.3377\nResiduals 19 0.164523 0.0086591               \n\n\nCode\n# the output gives p-value\nadonis2(P_sojae_survey.matrix.jaccard ~ groups)\n\n\nPermutation test for adonis under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 999\n\nadonis2(formula = P_sojae_survey.matrix.jaccard ~ groups)\n         Df SumOfSqs      R2      F Pr(>F)\ngroups    1  0.11358 0.07869 1.6229  0.185\nResidual 19  1.32976 0.92131              \nTotal    20  1.44335 1.00000              \n\n\n\n\n22.2.2 Corncob\n\n\nCode\ndata(soil_phylo)\n\n\n\n\nCode\nsoil_phylo\n\n\nphyloseq-class experiment-level object\notu_table()   OTU Table:         [ 7770 taxa and 119 samples ]\nsample_data() Sample Data:       [ 119 samples by 5 sample variables ]\ntax_table()   Taxonomy Table:    [ 7770 taxa by 7 taxonomic ranks ]\n\n\nCode\notu_table(soil_phylo)[1:3, 1:3]\n\n\nOTU Table:          [3 taxa and 3 samples]\n                     taxa are rows\n        S009 S204 S112\nOTU.43   350   74  300\nOTU.2   1796 4204 1752\nOTU.187  280  709  426\n\n\nCode\nsample_data(soil_phylo) # covariates\n\n\n     Plants DayAmdmt Amdmt ID Day\nS009      1       01     1  D   0\nS204      1       21     1  D   2\nS112      0       11     1  B   1\nS247      0       22     2  F   2\nS026      0       00     0  A   0\nS023      1       00     0  C   0\nS201      0       20     0  A   2\nS133      1       10     0  C   1\nS103      1       10     0  C   1\nS012      0       01     1  B   0\nS134      1       11     1  D   1\nS108      1       10     0  C   1\nS207      0       21     1  B   2\nS123      1       10     0  C   1\nS202      0       21     1  B   2\nS031      0       00     0  A   0\nS106      0       10     0  A   1\nS233      1       20     0  C   2\nS013      1       00     0  C   0\nS007      0       01     1  B   0\nS139      1       11     1  D   1\nS126      0       10     0  A   1\nS208      1       20     0  C   2\nS122      0       11     1  B   1\nS022      0       01     1  B   0\nS223      1       20     0  C   2\nS113      1       10     0  C   1\nS024      1       01     1  D   0\nS032      0       01     1  B   0\nS218      1       20     0  C   2\nS212      0       21     1  B   2\nS138      1       10     0  C   1\nS117      0       11     1  B   1\nS228      1       20     0  C   2\nS019      1       01     1  D   0\nS104      1       11     1  D   1\nS116      0       10     0  A   1\nS213      1       20     0  C   2\nS144      0       12     2  F   1\nS221      0       20     0  A   2\nS214      1       21     1  D   2\nS043      0       02     2  F   0\nS136      0       10     0  A   1\nS028      1       00     0  C   0\nS203      1       20     0  C   2\nS001      0       00     0  A   0\nS216      0       20     0  A   2\nS109      1       11     1  D   1\nS226      0       20     0  A   2\nS037      0       01     1  B   0\nS042      0       02     2  F   0\nS206      0       20     0  A   2\nS111      0       10     0  A   1\nS101      0       10     0  A   1\nS217      0       21     1  B   2\nS148      0       12     2  F   1\nS039      1       01     1  D   0\nS118      1       10     0  C   1\nS034      1       01     1  D   0\nS006      0       00     0  A   0\nS238      1       20     0  C   2\nS229      1       21     1  D   2\nS046      0       02     2  F   0\nS033      1       00     0  C   0\nS041      0       02     2  F   0\nS132      0       11     1  B   1\nS121      0       10     0  A   1\nS209      1       21     1  D   2\nS245      0       22     2  F   2\nS243      0       22     2  F   2\nS227      0       21     1  B   2\nS107      0       11     1  B   1\nS142      0       12     2  F   1\nS237      0       21     1  B   2\nS224      1       21     1  D   2\nS003      1       00     0  C   0\nS231      0       20     0  A   2\nS127      0       11     1  B   1\nS137      0       11     1  B   1\nS036      0       00     0  A   0\nS241      0       22     2  F   2\nS038      1       00     0  C   0\nS114      1       11     1  D   1\nS147      0       12     2  F   1\nS124      1       11     1  D   1\nS017      0       01     1  B   0\nS143      0       12     2  F   1\nS021      0       00     0  A   0\nS047      0       02     2  F   0\nS011      0       00     0  A   0\nS119      1       11     1  D   1\nS219      1       21     1  D   2\nS027      0       01     1  B   0\nS232      0       21     1  B   2\nS246      0       22     2  F   2\nS131      0       10     0  A   1\nS044      0       02     2  F   0\nS129      1       11     1  D   1\nS002      0       01     1  B   0\nS102      0       11     1  B   1\nS004      1       01     1  D   0\nS234      1       21     1  D   2\nS244      0       22     2  F   2\nS248      0       22     2  F   2\nS128      1       10     0  C   1\nS016      0       00     0  A   0\nS236      0       20     0  A   2\nS029      1       01     1  D   0\nS018      1       00     0  C   0\nS141      0       12     2  F   1\nS146      0       12     2  F   1\nS048      0       02     2  F   0\nS222      0       21     1  B   2\nS045      0       02     2  F   0\nS008      1       00     0  C   0\nS239      1       21     1  D   2\nS145      0       12     2  F   1\nS242      0       22     2  F   2\nS014      1       01     1  D   0\n\n\nCode\ntax_table(soil_phylo)[1:3,] # taxonomic information of \n\n\nTaxonomy Table:     [3 taxa by 7 taxonomic ranks]:\n        Kingdom    Phylum           Class                 Order             \nOTU.43  \"Bacteria\" \"Nitrospirae\"    \"Nitrospira\"          \"Nitrospirales\"   \nOTU.2   \"Bacteria\" \"Proteobacteria\" \"Alphaproteobacteria\" \"Rhizobiales\"     \nOTU.187 \"Bacteria\" \"Acidobacteria\"  \"Acidobacteriia\"      \"Acidobacteriales\"\n        Family              Genus            Species\nOTU.43  \"Nitrospiraceae\"    \"Nitrospira\"     \"\"     \nOTU.2   \"Bradyrhizobiaceae\" \"Bradyrhizobium\" \"\"     \nOTU.187 \"Koribacteraceae\"   \"\"               \"\"     \n\n\n\n\nCode\n# Modeling\ndata(soil_phylum_small)\nsoil <- soil_phylum_small\ncob <- bbdml(formula = OTU.1 ~ 1, \n      phi.formula = ~ 1,\n      data = soil)\nplot(cob, B = 50, total = TRUE, color = \"DayAmdmt\") # bootstrap simulations, total counts scale instead of relative abundance.\n\n\n\n\n\nCode\nsample_data(soil) %>% rownames_to_column(\"sample\") %>% \n  group_by(sample) %>% \n  summarize\n\n\n# A tibble: 6 × 1\n  sample\n  <chr> \n1 S112  \n2 S134  \n3 S139  \n4 S202  \n5 S204  \n6 S207  \n\n\nCode\notu_table(soil) %>% margin.table(margin = 2)\n\n\n  S204   S112   S134   S207   S202   S139   S122   S212   S117   S104   S214 \n194231  66259 160528  92195 177576 154140 144165 182194  42672 114744  80111 \n  S109   S217   S229   S132   S209   S227   S107   S237   S224   S127   S137 \n 67595  50267  79751 159316  66304  70264  89279  58707  71862 102896  39923 \n  S114   S124   S119   S219   S232   S129   S102   S234   S222   S239 \n 32270  80993 107700  80076  58509  20502  53441  61880  65645  25626 \nattr(,\"class\")\n[1] \"otu_table\"\nattr(,\"class\")attr(,\"package\")\n[1] \"phyloseq\"\n\n\nCode\notu_table(soil)[c(\"OTU.1\")]\n\n\nOTU Table:          [1 taxa and 32 samples]\n                     taxa are rows\n       S204  S112  S134  S207  S202  S139  S122  S212  S117  S104  S214  S109\nOTU.1 70300 24858 54847 30271 62920 57219 52490 62570 16760 54818 26890 28420\n       S217  S229  S132  S209  S227  S107  S237  S224  S127  S137  S114  S124\nOTU.1 16811 27760 69870 21679 24939 33999 20473 25134 41171 14948 10803 30108\n       S119  S219  S232 S129  S102  S234  S222 S239\nOTU.1 43700 31658 21926 8408 20578 21069 23182 9449\n\n\n\n\nCode\ncorncob_da <- bbdml(formula = OTU.1 ~ DayAmdmt,\n                    phi.formula = ~ DayAmdmt,\n                    data = soil)\nplot(corncob_da, color = \"DayAmdmt\", total = TRUE, B = 50) # bars are from bootstrap samples, while\n\n\n\n\n\n\n\nCode\n# Multiple Taxa\nda_analysis <- differentialTest(formula = ~ DayAmdmt,\n                                phi.formula = ~ DayAmdmt,\n                                formula_null = ~1,\n                                phi.formula_null = ~ DayAmdmt,\n                                test = \"Wald\", boot = FALSE,\n                                data = soil,\n                                fdr_cutoff = 0.05)\n\n\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.42877    0.06086 -23.478   <2e-16 ***\nDayAmdmt21   0.02974    0.06574   0.452    0.654    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.6767     0.3532 -13.240 1.41e-13 ***\nDayAmdmt21   -1.7872     0.5025  -3.557  0.00136 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -286.69\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.40322    0.02313  -60.68   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.6503     0.3528 -13.183 8.94e-14 ***\nDayAmdmt21   -1.8142     0.5024  -3.611  0.00114 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -286.79\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.44595    0.03604 -12.375 7.18e-13 ***\nDayAmdmt21  -0.16791    0.04067  -4.129 0.000297 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.3077     0.3537 -15.008 6.44e-15 ***\nDayAmdmt21   -1.3518     0.5029  -2.688    0.012 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -286.53\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.59202    0.02141  -27.66   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.6344     0.3803 -12.186 6.23e-13 ***\nDayAmdmt21   -1.9370     0.5857  -3.307  0.00252 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -292.89\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error  t value Pr(>|t|)    \n(Intercept) -3.61543    0.03262 -110.818  < 2e-16 ***\nDayAmdmt21   0.31196    0.04284    7.283 6.28e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -7.77759    0.36661 -21.215   <2e-16 ***\nDayAmdmt21  -0.02873    0.52045  -0.055    0.956    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -225\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.34218    0.03464  -96.49   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.8873     0.4175 -14.102 1.63e-14 ***\nDayAmdmt21   -1.8364     0.6451  -2.847  0.00803 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -238.94\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.91977    0.06024 -31.869   <2e-16 ***\nDayAmdmt21  -0.06460    0.07211  -0.896    0.378    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.0356     0.3544 -14.209 2.51e-14 ***\nDayAmdmt21   -0.8928     0.5021  -1.778   0.0862 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -282.08\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.96479    0.03416  -57.52   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.0352     0.3523 -14.292 1.16e-14 ***\nDayAmdmt21   -0.8634     0.5096  -1.694    0.101    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -282.48\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.7089     0.1618  -47.64   <2e-16 ***\nDayAmdmt21    0.3377     0.2072    1.63    0.114    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.6554     0.3961 -21.852   <2e-16 ***\nDayAmdmt21   -0.1264     0.5627  -0.225    0.824    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -142.93\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.4883     0.1112  -67.35   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.3391     0.4154  -20.07   <2e-16 ***\nDayAmdmt21   -0.5102     0.5545   -0.92    0.365    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -144.19\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.81356    0.06024 -46.706  < 2e-16 ***\nDayAmdmt21   0.23068    0.07101   3.248  0.00301 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.7785     0.3563 -16.218 9.12e-16 ***\nDayAmdmt21   -0.7473     0.5056  -1.478    0.151    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -261.61\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2.6290     0.0409  -64.28   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.1801     0.3967 -13.057 1.14e-13 ***\nDayAmdmt21   -1.2947     0.5870  -2.206   0.0355 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -265.89\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.95534    0.05915 -49.966   <2e-16 ***\nDayAmdmt21   0.12338    0.06727   1.834   0.0773 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.9430     0.3562 -16.684 4.43e-16 ***\nDayAmdmt21   -1.1278     0.5066  -2.226   0.0342 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -253.23\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.85646    0.03088  -92.49   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.6999     0.3702  -15.40 1.71e-15 ***\nDayAmdmt21   -1.3551     0.5336   -2.54   0.0167 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -254.78\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.32235    0.07835 -80.695  < 2e-16 ***\nDayAmdmt21   1.70905    0.18966   9.011  9.1e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.7388     0.3864 -22.616  < 2e-16 ***\nDayAmdmt21    3.3670     0.5363   6.279 8.68e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -202.38\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.9259     0.2205  -22.34   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.7749     0.5303  -9.005 6.73e-10 ***\nDayAmdmt21   -0.7264     0.6390  -1.137    0.265    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -226.7\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.06165    0.03772 -81.157   <2e-16 ***\nDayAmdmt21  -0.03607    0.05019  -0.719    0.478    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.9507     0.3611 -19.249   <2e-16 ***\nDayAmdmt21   -0.2990     0.5105  -0.586    0.563    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -241.34\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.08193    0.02549  -120.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.9506     0.3600 -19.310   <2e-16 ***\nDayAmdmt21   -0.2700     0.5164  -0.523    0.605    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -241.6\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.34777    0.07945 -54.725   <2e-16 ***\nDayAmdmt21  -0.14662    0.09294  -1.578    0.126    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.6782     0.3601 -18.546   <2e-16 ***\nDayAmdmt21   -1.1650     0.5130  -2.271    0.031 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -217.85\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.45481    0.04495  -99.11   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.6758     0.3537 -18.876   <2e-16 ***\nDayAmdmt21   -1.0881     0.5350  -2.034   0.0512 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -219.07\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.12339    0.08148 -50.606  < 2e-16 ***\nDayAmdmt21  -0.30875    0.09587  -3.221  0.00323 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.4055     0.3588 -17.853   <2e-16 ***\nDayAmdmt21   -1.2776     0.5138  -2.486   0.0191 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -223.5\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.35496    0.05918  -73.59   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.2089     0.3677 -16.884   <2e-16 ***\nDayAmdmt21   -1.2655     0.6269  -2.019   0.0529 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -228.08\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.84772    0.03473 -82.001  < 2e-16 ***\nDayAmdmt21   0.20132    0.04202   4.791 4.92e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.9228     0.3582 -19.328   <2e-16 ***\nDayAmdmt21   -0.6013     0.5105  -1.178    0.249    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -244.2\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.68075    0.02922  -91.76   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.9069     0.4184 -14.119 1.58e-14 ***\nDayAmdmt21   -1.5160     0.6470  -2.343   0.0262 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -252.16\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.7649     0.1713 -27.808   <2e-16 ***\nDayAmdmt21   -0.3431     0.2280  -1.505    0.144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.5378     0.3718 -14.894 7.79e-15 ***\nDayAmdmt21   -0.6037     0.5234  -1.153    0.259    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -228.14\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -4.944      0.124  -39.87   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.6535     0.3322 -17.021   <2e-16 ***\nDayAmdmt21   -0.2653     0.5012  -0.529    0.601    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -229.25\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.421950   0.203475 -41.391   <2e-16 ***\nDayAmdmt21   0.005792   0.257610   0.022    0.982    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.9334     0.4816 -18.548   <2e-16 ***\nDayAmdmt21   -0.5671     0.6704  -0.846    0.405    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -120.02\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.4183     0.1248  -67.45   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.9299     0.4571 -19.534   <2e-16 ***\nDayAmdmt21   -0.5727     0.6235  -0.918    0.366    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -120.02\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.9683     0.2160 -27.633   <2e-16 ***\nDayAmdmt21   -0.5946     0.2877  -2.067   0.0481 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.2718     0.3943  -15.91 1.49e-15 ***\nDayAmdmt21   -0.8578     0.5500   -1.56     0.13    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -190.03\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.2581     0.1686  -37.12   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.4594     0.3304 -19.550   <2e-16 ***\nDayAmdmt21   -0.2477     0.5193  -0.477    0.637    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -192.07\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error  t value Pr(>|t|)    \n(Intercept) -4.82762    0.04166 -115.876  < 2e-16 ***\nDayAmdmt21   0.37129    0.05810    6.391 6.43e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.5025     0.3745 -22.701   <2e-16 ***\nDayAmdmt21    0.3339     0.5287   0.632    0.533    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -198.2\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.53318    0.06124  -74.03   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.8361     0.4996 -13.684  3.5e-14 ***\nDayAmdmt21   -1.1889     0.7909  -1.503    0.144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -210.73\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.7877     0.1950 -39.931  < 2e-16 ***\nDayAmdmt21   -1.0579     0.2311  -4.578 8.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.3420     0.4153 -20.087   <2e-16 ***\nDayAmdmt21   -2.3748     0.7183  -3.306   0.0026 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -119.79\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.4975     0.2374  -35.79   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.3883     0.3397  -24.70   <2e-16 ***\nDayAmdmt21   -1.4065     1.0578   -1.33    0.194    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -128.69\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.4897     0.1453 -51.547   <2e-16 ***\nDayAmdmt21   -0.3720     0.1696  -2.194   0.0367 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.6532     0.3922 -22.061   <2e-16 ***\nDayAmdmt21   -1.6629     0.6404  -2.597   0.0148 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -132.12\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -7.75950    0.08932  -86.88   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.7105     0.3569 -24.408   <2e-16 ***\nDayAmdmt21   -1.3719     0.6696  -2.049   0.0496 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -134.43\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.7442     0.1076  -53.37  < 2e-16 ***\nDayAmdmt21   -0.3495     0.1217   -2.87  0.00772 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.4617     0.3704 -20.145  < 2e-16 ***\nDayAmdmt21   -1.7218     0.5560  -3.097  0.00442 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -177.79\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.02424    0.06211  -96.99   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.3705     0.3553  -20.75   <2e-16 ***\nDayAmdmt21   -1.6492     0.6200   -2.66   0.0126 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -181.56\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error  t value Pr(>|t|)    \n(Intercept) -8.25352    0.08151 -101.259  < 2e-16 ***\nDayAmdmt21   0.74375    0.11429    6.508 4.72e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.1494     0.6852 -16.273 8.38e-16 ***\nDayAmdmt21    1.0650     0.8364   1.273    0.213    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -117.53\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.6297     0.0975  -78.26   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.5990     0.4856 -17.707   <2e-16 ***\nDayAmdmt21   -1.4656     0.7549  -1.941    0.062 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -129.47\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -7.71137    0.16867 -45.720   <2e-16 ***\nDayAmdmt21   0.02581    0.23042   0.112    0.912    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.5710     0.4402 -19.470   <2e-16 ***\nDayAmdmt21   -0.1270     0.5954  -0.213    0.833    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -141.06\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -7.697      0.115  -66.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.5571     0.4264 -20.068   <2e-16 ***\nDayAmdmt21   -0.1524     0.5513  -0.276    0.784    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -141.07\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.9544     0.1825  -49.06  < 2e-16 ***\nDayAmdmt21   -1.2187     0.3009   -4.05 0.000368 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.8205     0.5089 -19.299   <2e-16 ***\nDayAmdmt21   -0.8794     0.8056  -1.092    0.284    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -87.12\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.1706     0.2025  -45.28   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.9179     0.4573 -21.687   <2e-16 ***\nDayAmdmt21    1.1583     0.7370   1.572    0.127    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -92.888\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -7.95026    0.07182  -110.7  < 2e-16 ***\nDayAmdmt21   1.34854    0.11054    12.2 1.01e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.0449     0.7011 -15.754  1.9e-15 ***\nDayAmdmt21    2.1581     0.8072   2.674   0.0124 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -136.48\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -6.675      0.089     -75   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.5655     0.3673 -17.876  < 2e-16 ***\nDayAmdmt21   -2.3485     0.5454  -4.306 0.000173 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -157.36\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.1863     0.2707 -33.940   <2e-16 ***\nDayAmdmt21   -0.4446     0.3494  -1.273    0.214    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.1518     0.4545 -20.136   <2e-16 ***\nDayAmdmt21   -1.0233     0.7052  -1.451    0.158    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -93.017\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.4378     0.1821  -51.83   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.3470     0.3778 -24.742   <2e-16 ***\nDayAmdmt21   -0.5742     0.6386  -0.899    0.376    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -93.828\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.6220     0.1058  -81.47   <2e-16 ***\nDayAmdmt21    0.1644     0.1848    0.89    0.381    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.9081     0.6160 -17.708   <2e-16 ***\nDayAmdmt21    1.2503     0.7708   1.622    0.116    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -108.65\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.56622    0.09042  -94.74   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.7993     0.6181 -17.473   <2e-16 ***\nDayAmdmt21    1.0725     0.7483   1.433    0.162    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -109.05\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.0551     0.1469 -61.648   <2e-16 ***\nDayAmdmt21   -0.1178     0.2307  -0.511    0.613    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.5325     0.5402 -19.497   <2e-16 ***\nDayAmdmt21    0.3916     0.7415   0.528    0.602    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -96.038\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.1022     0.1147  -79.37   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.5693     0.5221 -20.246   <2e-16 ***\nDayAmdmt21    0.5074     0.7118   0.713    0.482    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -96.168\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.8230     0.4705 -25.127   <2e-16 ***\nDayAmdmt21   -0.9028     0.9276  -0.973    0.339    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.1635     0.9544 -11.697 2.72e-12 ***\nDayAmdmt21    0.2931     1.6942   0.173    0.864    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -25.731\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -12.0059     0.4686  -25.62   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.2808     0.8769 -12.864 1.64e-13 ***\nDayAmdmt21    1.4251     1.5629   0.912    0.369    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -26.149\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.8184     0.1208 -64.734  < 2e-16 ***\nDayAmdmt21    0.9065     0.1749   5.184 1.68e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.4473     0.4093 -23.084   <2e-16 ***\nDayAmdmt21    1.0930     0.5698   1.918   0.0653 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -144.67\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.1585     0.1663  -43.04   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.8226     0.5567 -14.052 1.79e-14 ***\nDayAmdmt21   -0.5711     0.7507  -0.761    0.453    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -154.37\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.8840     0.1288 -68.981   <2e-16 ***\nDayAmdmt21    0.1337     0.1474   0.907    0.372    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.6505     0.5479 -19.439   <2e-16 ***\nDayAmdmt21   -2.8542     4.3331  -0.659    0.515    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -93.567\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.78215    0.06541  -134.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.5243     0.5624 -18.712   <2e-16 ***\nDayAmdmt21   -2.8615     3.8419  -0.745    0.462    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -93.975\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.4509     0.1040 -81.261  < 2e-16 ***\nDayAmdmt21   -0.4499     0.1623  -2.772  0.00978 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.6731     0.5452 -19.578   <2e-16 ***\nDayAmdmt21   -0.1065     0.8830  -0.121    0.905    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -102.11\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.6004     0.1116  -77.07   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.6636     0.5328 -20.013   <2e-16 ***\nDayAmdmt21    0.7366     0.9486   0.777    0.444    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -105.53\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.2528     0.3916 -28.737   <2e-16 ***\nDayAmdmt21   -0.2756     0.5536  -0.498    0.623    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.8557     0.8145 -13.328 1.21e-13 ***\nDayAmdmt21   -0.7310     1.3597  -0.538    0.595    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -42.629\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.3889     0.2838  -40.12   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.9587     0.7430 -14.750 5.18e-15 ***\nDayAmdmt21   -0.4141     1.1736  -0.353    0.727    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -42.752\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.49718    0.30983 -37.107   <2e-16 ***\nDayAmdmt21   -0.06755    0.51287  -0.132    0.896    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -12.484      1.725  -7.236 7.08e-08 ***\nDayAmdmt21     1.091      2.017   0.541    0.593    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -38.526\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.5219     0.2497  -46.14   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -12.488      1.696  -7.364 4.11e-08 ***\nDayAmdmt21     1.149      1.949   0.590     0.56    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -38.535\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -13.8290     1.5532  -8.903 1.17e-09 ***\nDayAmdmt21    0.3602     1.7077   0.211    0.834    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  -11.646      4.585  -2.540   0.0169 *\nDayAmdmt21    -6.376     73.166  -0.087   0.9312  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -11.349\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -13.5296     0.6683  -20.24   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -11.030      2.549  -4.327 0.000164 ***\nDayAmdmt21    -6.992     58.830  -0.119 0.906212    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -11.37\n[1] \" -- model --\"\n\n\nWarning in waldt(object): Singular Hessian! Cannot calculate p-values in this\nsetting.\n\n\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -14.12         NA      NA       NA\nDayAmdmt21    -23.41         NA      NA       NA\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -13.8079         NA      NA       NA\nDayAmdmt21    0.5128         NA      NA       NA\n\n\nLog-likelihood: -4.2899\n\n\nWarning in print.bbdml(mod): This model is based on a discriminant taxa.\nYou may see NAs in the model summary because Wald testing is invalid.\nLikelihood ratio testing can be used, but valid standard errors cannot be calculated.\n\n\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -14.124      1.426  -9.906 8.18e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -13.81      19.89  -0.694    0.493\nDayAmdmt21     23.49   10718.95   0.002    0.998\n\n\nLog-likelihood: -4.2899\n\n\nWarning in print.bbdml(mod_null): This model is based on a discriminant taxa.\nYou may see NAs in the model summary because Wald testing is invalid.\nLikelihood ratio testing can be used, but valid standard errors cannot be calculated.\n\n\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.1607     0.3773 -26.928   <2e-16 ***\nDayAmdmt21    0.1108     0.4747   0.234    0.817    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.5102     0.6443 -14.761 9.75e-15 ***\nDayAmdmt21   -0.5470     0.8600  -0.636     0.53    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -73.271\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.0897     0.2298   -43.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.4362     0.5840 -16.159 4.85e-16 ***\nDayAmdmt21   -0.6606     0.7171  -0.921    0.365    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -73.298\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -12.7933     0.5656 -22.621   <2e-16 ***\nDayAmdmt21    0.9582     0.9953   0.963    0.344    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -12.911      2.918  -4.424 0.000134 ***\nDayAmdmt21     3.261      3.135   1.040 0.307174    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -23.19\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -12.4581     0.5089  -24.48   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -12.173      2.847  -4.276 0.000188 ***\nDayAmdmt21     2.055      2.813   0.731 0.470916    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -23.675\n[1] \" -- model --\"\n\n\nWarning in waldt(object): Singular Hessian! Cannot calculate p-values in this\nsetting.\n\n\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -36.02         NA      NA       NA\nDayAmdmt21     22.55         NA      NA       NA\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -8.842         NA      NA       NA\nDayAmdmt21    -9.180         NA      NA       NA\n\n\nLog-likelihood: -5.6685\n\n\nWarning in print.bbdml(mod): This model is based on a discriminant taxa.\nYou may see NAs in the model summary because Wald testing is invalid.\nLikelihood ratio testing can be used, but valid standard errors cannot be calculated.\n\n\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -13.4692     0.7075  -19.04   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -1.392     37.396  -0.037    0.971\nDayAmdmt21   -16.630     43.444  -0.383    0.705\n\n\nLog-likelihood: -5.6694\n\n\nWarning in print.bbdml(mod_null): This model is based on a discriminant taxa.\nYou may see NAs in the model summary because Wald testing is invalid.\nLikelihood ratio testing can be used, but valid standard errors cannot be calculated.\n\n\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)    -28.24  371272.78       0        1\nDayAmdmt21      14.08  371272.78       0        1\n\n\nCoefficients associated with dispersion:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)  6.597e-01  4.258e+05       0        1\nDayAmdmt21  -1.868e+01  4.258e+05       0        1\n\n\nLog-likelihood: -3.0518\n\n\nWarning in print.bbdml(mod): This model is based on a discriminant taxa.\nYou may see NAs in the model summary because Wald testing is invalid.\nLikelihood ratio testing can be used, but valid standard errors cannot be calculated.\n\n\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -14.16       1.00  -14.16 1.48e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -1.491     50.653  -0.029    0.977\nDayAmdmt21   -16.530     55.718  -0.297    0.769\n\n\nLog-likelihood: -3.0523\n\n\nWarning in print.bbdml(mod_null): This model is based on a discriminant taxa.\nYou may see NAs in the model summary because Wald testing is invalid.\nLikelihood ratio testing can be used, but valid standard errors cannot be calculated.\n\n\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.49971    0.33418 -34.412   <2e-16 ***\nDayAmdmt21    0.01679    0.49675   0.034    0.973    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.9873     1.0451 -11.470 4.29e-12 ***\nDayAmdmt21    0.4004     1.5478   0.259    0.798    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -39.541\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -11.492      0.247  -46.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.9815     1.0336 -11.592 2.09e-12 ***\nDayAmdmt21    0.3846     1.4754   0.261    0.796    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -39.541\n\n\nCode\nda_analysis %>% names()\n\n\n [1] \"p\"                    \"p_fdr\"                \"significant_taxa\"    \n [4] \"significant_models\"   \"all_models\"           \"restrictions_DA\"     \n [7] \"restrictions_DV\"      \"discriminant_taxa_DA\" \"discriminant_taxa_DV\"\n[10] \"data\"                 \"full_output\"         \n\n\nCode\nda_analysis$restrictions_DA # differential abundance\n\n\n[1] \"DayAmdmt21\"\nattr(,\"index\")\n[1] 2\n\n\nCode\nda_analysis$restrictions_DV # differential variability (because phi.formula and phi.formula_null are the same)\n\n\ncharacter(0)\n\n\nCode\nplot(da_analysis)\n\n\n\n\n\nCode\nda_analysis$significant_taxa # switch from OTU labels to taxonomy\n\n\n [1] \"OTU.1\"    \"OTU.15\"   \"OTU.8\"    \"OTU.69\"   \"OTU.6\"    \"OTU.59\"  \n [7] \"OTU.695\"  \"OTU.628\"  \"OTU.703\"  \"OTU.908\"  \"OTU.1570\" \"OTU.417\" \n[13] \"OTU.584\"  \"OTU.1672\"\n\n\nCode\notu_to_taxonomy(da_analysis$significant_taxa, data = soil)\n\n\n                      OTU.1                      OTU.15 \n  \"Bacteria_Proteobacteria\" \"Bacteria_Gemmatimonadetes\" \n                      OTU.8                      OTU.69 \n   \"Bacteria_Bacteroidetes\"    \"Bacteria_Cyanobacteria\" \n                      OTU.6                      OTU.59 \n      \"Bacteria_Firmicutes\"   \"Bacteria_Planctomycetes\" \n                    OTU.695                     OTU.628 \n \"Bacteria_Armatimonadetes\"     \"Bacteria_Spirochaetes\" \n                    OTU.703                     OTU.908 \n   \"Bacteria_Elusimicrobia\"             \"Bacteria_BRC1\" \n                   OTU.1570                     OTU.417 \n             \"Bacteria_OP3\"              \"Bacteria_FBP\" \n                    OTU.584                    OTU.1672 \n        \"Bacteria_Chlorobi\"              \"Bacteria_TM6\" \n\n\n\n\nCode\n# Looking at differentially variable taxa\ndv_analysis <- differentialTest(formula = ~ DayAmdmt,\n                                phi.formula = ~ DayAmdmt,\n                                formula_null = ~1,\n                                phi.formula_null = ~ DayAmdmt,\n                                test = \"Wald\", boot = FALSE,\n                                data = soil,\n                                fdr_cutoff = 0.05)\n\n\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.42877    0.06086 -23.478   <2e-16 ***\nDayAmdmt21   0.02974    0.06574   0.452    0.654    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.6767     0.3532 -13.240 1.41e-13 ***\nDayAmdmt21   -1.7872     0.5025  -3.557  0.00136 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -286.69\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.40322    0.02313  -60.68   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.6503     0.3528 -13.183 8.94e-14 ***\nDayAmdmt21   -1.8142     0.5024  -3.611  0.00114 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -286.79\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.44595    0.03604 -12.375 7.18e-13 ***\nDayAmdmt21  -0.16791    0.04067  -4.129 0.000297 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.3077     0.3537 -15.008 6.44e-15 ***\nDayAmdmt21   -1.3518     0.5029  -2.688    0.012 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -286.53\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.59202    0.02141  -27.66   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.6344     0.3803 -12.186 6.23e-13 ***\nDayAmdmt21   -1.9370     0.5857  -3.307  0.00252 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -292.89\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error  t value Pr(>|t|)    \n(Intercept) -3.61543    0.03262 -110.818  < 2e-16 ***\nDayAmdmt21   0.31196    0.04284    7.283 6.28e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -7.77759    0.36661 -21.215   <2e-16 ***\nDayAmdmt21  -0.02873    0.52045  -0.055    0.956    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -225\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.34218    0.03464  -96.49   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.8873     0.4175 -14.102 1.63e-14 ***\nDayAmdmt21   -1.8364     0.6451  -2.847  0.00803 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -238.94\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.91977    0.06024 -31.869   <2e-16 ***\nDayAmdmt21  -0.06460    0.07211  -0.896    0.378    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.0356     0.3544 -14.209 2.51e-14 ***\nDayAmdmt21   -0.8928     0.5021  -1.778   0.0862 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -282.08\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.96479    0.03416  -57.52   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.0352     0.3523 -14.292 1.16e-14 ***\nDayAmdmt21   -0.8634     0.5096  -1.694    0.101    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -282.48\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.7089     0.1618  -47.64   <2e-16 ***\nDayAmdmt21    0.3377     0.2072    1.63    0.114    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.6554     0.3961 -21.852   <2e-16 ***\nDayAmdmt21   -0.1264     0.5627  -0.225    0.824    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -142.93\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.4883     0.1112  -67.35   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.3391     0.4154  -20.07   <2e-16 ***\nDayAmdmt21   -0.5102     0.5545   -0.92    0.365    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -144.19\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.81356    0.06024 -46.706  < 2e-16 ***\nDayAmdmt21   0.23068    0.07101   3.248  0.00301 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.7785     0.3563 -16.218 9.12e-16 ***\nDayAmdmt21   -0.7473     0.5056  -1.478    0.151    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -261.61\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2.6290     0.0409  -64.28   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.1801     0.3967 -13.057 1.14e-13 ***\nDayAmdmt21   -1.2947     0.5870  -2.206   0.0355 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -265.89\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.95534    0.05915 -49.966   <2e-16 ***\nDayAmdmt21   0.12338    0.06727   1.834   0.0773 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.9430     0.3562 -16.684 4.43e-16 ***\nDayAmdmt21   -1.1278     0.5066  -2.226   0.0342 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -253.23\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.85646    0.03088  -92.49   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.6999     0.3702  -15.40 1.71e-15 ***\nDayAmdmt21   -1.3551     0.5336   -2.54   0.0167 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -254.78\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.32235    0.07835 -80.695  < 2e-16 ***\nDayAmdmt21   1.70905    0.18966   9.011  9.1e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.7388     0.3864 -22.616  < 2e-16 ***\nDayAmdmt21    3.3670     0.5363   6.279 8.68e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -202.38\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.9259     0.2205  -22.34   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.7749     0.5303  -9.005 6.73e-10 ***\nDayAmdmt21   -0.7264     0.6390  -1.137    0.265    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -226.7\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.06165    0.03772 -81.157   <2e-16 ***\nDayAmdmt21  -0.03607    0.05019  -0.719    0.478    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.9507     0.3611 -19.249   <2e-16 ***\nDayAmdmt21   -0.2990     0.5105  -0.586    0.563    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -241.34\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.08193    0.02549  -120.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.9506     0.3600 -19.310   <2e-16 ***\nDayAmdmt21   -0.2700     0.5164  -0.523    0.605    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -241.6\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.34777    0.07945 -54.725   <2e-16 ***\nDayAmdmt21  -0.14662    0.09294  -1.578    0.126    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.6782     0.3601 -18.546   <2e-16 ***\nDayAmdmt21   -1.1650     0.5130  -2.271    0.031 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -217.85\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.45481    0.04495  -99.11   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.6758     0.3537 -18.876   <2e-16 ***\nDayAmdmt21   -1.0881     0.5350  -2.034   0.0512 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -219.07\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.12339    0.08148 -50.606  < 2e-16 ***\nDayAmdmt21  -0.30875    0.09587  -3.221  0.00323 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.4055     0.3588 -17.853   <2e-16 ***\nDayAmdmt21   -1.2776     0.5138  -2.486   0.0191 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -223.5\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.35496    0.05918  -73.59   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.2089     0.3677 -16.884   <2e-16 ***\nDayAmdmt21   -1.2655     0.6269  -2.019   0.0529 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -228.08\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.84772    0.03473 -82.001  < 2e-16 ***\nDayAmdmt21   0.20132    0.04202   4.791 4.92e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.9228     0.3582 -19.328   <2e-16 ***\nDayAmdmt21   -0.6013     0.5105  -1.178    0.249    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -244.2\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.68075    0.02922  -91.76   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.9069     0.4184 -14.119 1.58e-14 ***\nDayAmdmt21   -1.5160     0.6470  -2.343   0.0262 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -252.16\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.7649     0.1713 -27.808   <2e-16 ***\nDayAmdmt21   -0.3431     0.2280  -1.505    0.144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.5378     0.3718 -14.894 7.79e-15 ***\nDayAmdmt21   -0.6037     0.5234  -1.153    0.259    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -228.14\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -4.944      0.124  -39.87   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.6535     0.3322 -17.021   <2e-16 ***\nDayAmdmt21   -0.2653     0.5012  -0.529    0.601    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -229.25\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.421950   0.203475 -41.391   <2e-16 ***\nDayAmdmt21   0.005792   0.257610   0.022    0.982    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.9334     0.4816 -18.548   <2e-16 ***\nDayAmdmt21   -0.5671     0.6704  -0.846    0.405    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -120.02\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.4183     0.1248  -67.45   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.9299     0.4571 -19.534   <2e-16 ***\nDayAmdmt21   -0.5727     0.6235  -0.918    0.366    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -120.02\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.9683     0.2160 -27.633   <2e-16 ***\nDayAmdmt21   -0.5946     0.2877  -2.067   0.0481 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.2718     0.3943  -15.91 1.49e-15 ***\nDayAmdmt21   -0.8578     0.5500   -1.56     0.13    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -190.03\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.2581     0.1686  -37.12   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.4594     0.3304 -19.550   <2e-16 ***\nDayAmdmt21   -0.2477     0.5193  -0.477    0.637    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -192.07\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error  t value Pr(>|t|)    \n(Intercept) -4.82762    0.04166 -115.876  < 2e-16 ***\nDayAmdmt21   0.37129    0.05810    6.391 6.43e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.5025     0.3745 -22.701   <2e-16 ***\nDayAmdmt21    0.3339     0.5287   0.632    0.533    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -198.2\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -4.53318    0.06124  -74.03   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.8361     0.4996 -13.684  3.5e-14 ***\nDayAmdmt21   -1.1889     0.7909  -1.503    0.144    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -210.73\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.7877     0.1950 -39.931  < 2e-16 ***\nDayAmdmt21   -1.0579     0.2311  -4.578 8.78e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.3420     0.4153 -20.087   <2e-16 ***\nDayAmdmt21   -2.3748     0.7183  -3.306   0.0026 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -119.79\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.4975     0.2374  -35.79   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.3883     0.3397  -24.70   <2e-16 ***\nDayAmdmt21   -1.4065     1.0578   -1.33    0.194    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -128.69\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.4897     0.1453 -51.547   <2e-16 ***\nDayAmdmt21   -0.3720     0.1696  -2.194   0.0367 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.6532     0.3922 -22.061   <2e-16 ***\nDayAmdmt21   -1.6629     0.6404  -2.597   0.0148 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -132.12\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -7.75950    0.08932  -86.88   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.7105     0.3569 -24.408   <2e-16 ***\nDayAmdmt21   -1.3719     0.6696  -2.049   0.0496 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -134.43\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.7442     0.1076  -53.37  < 2e-16 ***\nDayAmdmt21   -0.3495     0.1217   -2.87  0.00772 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.4617     0.3704 -20.145  < 2e-16 ***\nDayAmdmt21   -1.7218     0.5560  -3.097  0.00442 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -177.79\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -6.02424    0.06211  -96.99   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.3705     0.3553  -20.75   <2e-16 ***\nDayAmdmt21   -1.6492     0.6200   -2.66   0.0126 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -181.56\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error  t value Pr(>|t|)    \n(Intercept) -8.25352    0.08151 -101.259  < 2e-16 ***\nDayAmdmt21   0.74375    0.11429    6.508 4.72e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.1494     0.6852 -16.273 8.38e-16 ***\nDayAmdmt21    1.0650     0.8364   1.273    0.213    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -117.53\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.6297     0.0975  -78.26   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.5990     0.4856 -17.707   <2e-16 ***\nDayAmdmt21   -1.4656     0.7549  -1.941    0.062 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -129.47\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -7.71137    0.16867 -45.720   <2e-16 ***\nDayAmdmt21   0.02581    0.23042   0.112    0.912    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.5710     0.4402 -19.470   <2e-16 ***\nDayAmdmt21   -0.1270     0.5954  -0.213    0.833    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -141.06\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -7.697      0.115  -66.96   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.5571     0.4264 -20.068   <2e-16 ***\nDayAmdmt21   -0.1524     0.5513  -0.276    0.784    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -141.07\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.9544     0.1825  -49.06  < 2e-16 ***\nDayAmdmt21   -1.2187     0.3009   -4.05 0.000368 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.8205     0.5089 -19.299   <2e-16 ***\nDayAmdmt21   -0.8794     0.8056  -1.092    0.284    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -87.12\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.1706     0.2025  -45.28   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.9179     0.4573 -21.687   <2e-16 ***\nDayAmdmt21    1.1583     0.7370   1.572    0.127    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -92.888\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -7.95026    0.07182  -110.7  < 2e-16 ***\nDayAmdmt21   1.34854    0.11054    12.2 1.01e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.0449     0.7011 -15.754  1.9e-15 ***\nDayAmdmt21    2.1581     0.8072   2.674   0.0124 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -136.48\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -6.675      0.089     -75   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -6.5655     0.3673 -17.876  < 2e-16 ***\nDayAmdmt21   -2.3485     0.5454  -4.306 0.000173 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -157.36\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.1863     0.2707 -33.940   <2e-16 ***\nDayAmdmt21   -0.4446     0.3494  -1.273    0.214    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.1518     0.4545 -20.136   <2e-16 ***\nDayAmdmt21   -1.0233     0.7052  -1.451    0.158    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -93.017\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.4378     0.1821  -51.83   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.3470     0.3778 -24.742   <2e-16 ***\nDayAmdmt21   -0.5742     0.6386  -0.899    0.376    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -93.828\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.6220     0.1058  -81.47   <2e-16 ***\nDayAmdmt21    0.1644     0.1848    0.89    0.381    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.9081     0.6160 -17.708   <2e-16 ***\nDayAmdmt21    1.2503     0.7708   1.622    0.116    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -108.65\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.56622    0.09042  -94.74   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.7993     0.6181 -17.473   <2e-16 ***\nDayAmdmt21    1.0725     0.7483   1.433    0.162    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -109.05\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.0551     0.1469 -61.648   <2e-16 ***\nDayAmdmt21   -0.1178     0.2307  -0.511    0.613    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.5325     0.5402 -19.497   <2e-16 ***\nDayAmdmt21    0.3916     0.7415   0.528    0.602    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -96.038\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.1022     0.1147  -79.37   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.5693     0.5221 -20.246   <2e-16 ***\nDayAmdmt21    0.5074     0.7118   0.713    0.482    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -96.168\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.8230     0.4705 -25.127   <2e-16 ***\nDayAmdmt21   -0.9028     0.9276  -0.973    0.339    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.1635     0.9544 -11.697 2.72e-12 ***\nDayAmdmt21    0.2931     1.6942   0.173    0.864    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -25.731\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -12.0059     0.4686  -25.62   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.2808     0.8769 -12.864 1.64e-13 ***\nDayAmdmt21    1.4251     1.5629   0.912    0.369    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -26.149\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.8184     0.1208 -64.734  < 2e-16 ***\nDayAmdmt21    0.9065     0.1749   5.184 1.68e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.4473     0.4093 -23.084   <2e-16 ***\nDayAmdmt21    1.0930     0.5698   1.918   0.0653 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -144.67\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.1585     0.1663  -43.04   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -7.8226     0.5567 -14.052 1.79e-14 ***\nDayAmdmt21   -0.5711     0.7507  -0.761    0.453    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -154.37\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.8840     0.1288 -68.981   <2e-16 ***\nDayAmdmt21    0.1337     0.1474   0.907    0.372    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.6505     0.5479 -19.439   <2e-16 ***\nDayAmdmt21   -2.8542     4.3331  -0.659    0.515    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -93.567\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.78215    0.06541  -134.3   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.5243     0.5624 -18.712   <2e-16 ***\nDayAmdmt21   -2.8615     3.8419  -0.745    0.462    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -93.975\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.4509     0.1040 -81.261  < 2e-16 ***\nDayAmdmt21   -0.4499     0.1623  -2.772  0.00978 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.6731     0.5452 -19.578   <2e-16 ***\nDayAmdmt21   -0.1065     0.8830  -0.121    0.905    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -102.11\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -8.6004     0.1116  -77.07   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.6636     0.5328 -20.013   <2e-16 ***\nDayAmdmt21    0.7366     0.9486   0.777    0.444    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -105.53\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.2528     0.3916 -28.737   <2e-16 ***\nDayAmdmt21   -0.2756     0.5536  -0.498    0.623    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.8557     0.8145 -13.328 1.21e-13 ***\nDayAmdmt21   -0.7310     1.3597  -0.538    0.595    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -42.629\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.3889     0.2838  -40.12   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.9587     0.7430 -14.750 5.18e-15 ***\nDayAmdmt21   -0.4141     1.1736  -0.353    0.727    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -42.752\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.49718    0.30983 -37.107   <2e-16 ***\nDayAmdmt21   -0.06755    0.51287  -0.132    0.896    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -12.484      1.725  -7.236 7.08e-08 ***\nDayAmdmt21     1.091      2.017   0.541    0.593    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -38.526\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.5219     0.2497  -46.14   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -12.488      1.696  -7.364 4.11e-08 ***\nDayAmdmt21     1.149      1.949   0.590     0.56    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -38.535\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -13.8290     1.5532  -8.903 1.17e-09 ***\nDayAmdmt21    0.3602     1.7077   0.211    0.834    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  -11.646      4.585  -2.540   0.0169 *\nDayAmdmt21    -6.376     73.166  -0.087   0.9312  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -11.349\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -13.5296     0.6683  -20.24   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -11.030      2.549  -4.327 0.000164 ***\nDayAmdmt21    -6.992     58.830  -0.119 0.906212    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -11.37\n[1] \" -- model --\"\n\n\nWarning in waldt(object): Singular Hessian! Cannot calculate p-values in this\nsetting.\n\n\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -14.12         NA      NA       NA\nDayAmdmt21    -23.41         NA      NA       NA\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -13.8079         NA      NA       NA\nDayAmdmt21    0.5128         NA      NA       NA\n\n\nLog-likelihood: -4.2899\n\n\nWarning in print.bbdml(mod): This model is based on a discriminant taxa.\nYou may see NAs in the model summary because Wald testing is invalid.\nLikelihood ratio testing can be used, but valid standard errors cannot be calculated.\n\n\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -14.124      1.426  -9.906 8.18e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -13.81      19.89  -0.694    0.493\nDayAmdmt21     23.49   10718.95   0.002    0.998\n\n\nLog-likelihood: -4.2899\n\n\nWarning in print.bbdml(mod_null): This model is based on a discriminant taxa.\nYou may see NAs in the model summary because Wald testing is invalid.\nLikelihood ratio testing can be used, but valid standard errors cannot be calculated.\n\n\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.1607     0.3773 -26.928   <2e-16 ***\nDayAmdmt21    0.1108     0.4747   0.234    0.817    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.5102     0.6443 -14.761 9.75e-15 ***\nDayAmdmt21   -0.5470     0.8600  -0.636     0.53    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -73.271\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -10.0897     0.2298   -43.9   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -9.4362     0.5840 -16.159 4.85e-16 ***\nDayAmdmt21   -0.6606     0.7171  -0.921    0.365    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -73.298\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -12.7933     0.5656 -22.621   <2e-16 ***\nDayAmdmt21    0.9582     0.9953   0.963    0.344    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -12.911      2.918  -4.424 0.000134 ***\nDayAmdmt21     3.261      3.135   1.040 0.307174    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -23.19\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -12.4581     0.5089  -24.48   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -12.173      2.847  -4.276 0.000188 ***\nDayAmdmt21     2.055      2.813   0.731 0.470916    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -23.675\n[1] \" -- model --\"\n\n\nWarning in waldt(object): Singular Hessian! Cannot calculate p-values in this\nsetting.\n\n\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -36.02         NA      NA       NA\nDayAmdmt21     22.55         NA      NA       NA\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -8.842         NA      NA       NA\nDayAmdmt21    -9.180         NA      NA       NA\n\n\nLog-likelihood: -5.6685\n\n\nWarning in print.bbdml(mod): This model is based on a discriminant taxa.\nYou may see NAs in the model summary because Wald testing is invalid.\nLikelihood ratio testing can be used, but valid standard errors cannot be calculated.\n\n\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -13.4692     0.7075  -19.04   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -1.392     37.396  -0.037    0.971\nDayAmdmt21   -16.630     43.444  -0.383    0.705\n\n\nLog-likelihood: -5.6694\n\n\nWarning in print.bbdml(mod_null): This model is based on a discriminant taxa.\nYou may see NAs in the model summary because Wald testing is invalid.\nLikelihood ratio testing can be used, but valid standard errors cannot be calculated.\n\n\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)    -28.24  371272.78       0        1\nDayAmdmt21      14.08  371272.78       0        1\n\n\nCoefficients associated with dispersion:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept)  6.597e-01  4.258e+05       0        1\nDayAmdmt21  -1.868e+01  4.258e+05       0        1\n\n\nLog-likelihood: -3.0518\n\n\nWarning in print.bbdml(mod): This model is based on a discriminant taxa.\nYou may see NAs in the model summary because Wald testing is invalid.\nLikelihood ratio testing can be used, but valid standard errors cannot be calculated.\n\n\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -14.16       1.00  -14.16 1.48e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)   -1.491     50.653  -0.029    0.977\nDayAmdmt21   -16.530     55.718  -0.297    0.769\n\n\nLog-likelihood: -3.0523\n\n\nWarning in print.bbdml(mod_null): This model is based on a discriminant taxa.\nYou may see NAs in the model summary because Wald testing is invalid.\nLikelihood ratio testing can be used, but valid standard errors cannot be calculated.\n\n\n[1] \" -- model --\"\n\nCall:\nbbdml(formula = formula_i, phi.formula = phi.formula, data = data_i, \n    link = link, phi.link = phi.link, inits = inits)\n\n\nCoefficients associated with abundance:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.49971    0.33418 -34.412   <2e-16 ***\nDayAmdmt21    0.01679    0.49675   0.034    0.973    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.9873     1.0451 -11.470 4.29e-12 ***\nDayAmdmt21    0.4004     1.5478   0.259    0.798    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -39.541\n[1] \" -- null model --\"\n\nCall:\nbbdml(formula = formula_null_i, phi.formula = phi.formula_null, \n    data = data_i, link = link, phi.link = phi.link, inits = inits_null)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -11.492      0.247  -46.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -11.9815     1.0336 -11.592 2.09e-12 ***\nDayAmdmt21    0.3846     1.4754   0.261    0.796    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -39.541\n\n\nCode\nabbreviate(c(\"a\", \"aa\", \"aaaaaaaaaaaaaaaaaaaaa\", \"bbbbbbbbbbbbb\", \"aasdlfjas;ldkfja;sdixckvjlsdjfiosjdlk\")) # can be useful for plotting long names\n\n\n                                    a                                    aa \n                                  \"a\"                                  \"aa\" \n                aaaaaaaaaaaaaaaaaaaaa                         bbbbbbbbbbbbb \n                               \"aaaa\"                                \"bbbb\" \naasdlfjas;ldkfja;sdixckvjlsdjfiosjdlk \n                               \"as;;\" \n\n\n\n\nCode\n# model selection\nlrtest(mod_null = cob, mod = corncob_da)\n\n\n[1] 4.550571e-05\n\n\n\n\nCode\nsummary(corncob_da)\n\n\n\nCall:\nbbdml(formula = OTU.1 ~ DayAmdmt, phi.formula = ~DayAmdmt, data = soil)\n\n\nCoefficients associated with abundance:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.44595    0.03604 -12.375 7.18e-13 ***\nDayAmdmt21  -0.16791    0.04067  -4.129 0.000297 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCoefficients associated with dispersion:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -5.3077     0.3537 -15.008 6.44e-15 ***\nDayAmdmt21   -1.3518     0.5029  -2.688    0.012 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLog-likelihood: -286.53"
  },
  {
    "objectID": "observable/observable.html#observable",
    "href": "observable/observable.html#observable",
    "title": "23  Observable Testing",
    "section": "23.1 Observable",
    "text": "23.1 Observable\n\n\nCode\ndata = FileAttachment(\"penguins.csv\").csv({ typed: true })\nviewof bill_length_min = Inputs.range(\n  [32, 50], \n  {value: 35, step: 1, label: \"Bill length (min):\"}\n)\nviewof islands = Inputs.checkbox(\n  [\"Torgersen\", \"Biscoe\", \"Dream\"], \n  { value: [\"Torgersen\", \"Biscoe\"], \n    label: \"Islands:\"\n  }\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfiltered = data.filter(function(penguin) {\n  return bill_length_min < penguin.bill_length_mm &&\n         islands.includes(penguin.island);\n})\n\n\n\n\n\n\n\n\n\nCode\nfiltered\n\n\n\n\n\n\n\n\n\nCode\nPlot.plot({\n  marks:[\n    Plot.dot(filtered, {x: \"body_mass_g\", y: \"flipper_length_mm\", fill: \"species\"})\n    ],\n  color: {legend: true},\n  marginLeft: 100,\n})"
  },
  {
    "objectID": "optimization/optimization.html#convex-optimization",
    "href": "optimization/optimization.html#convex-optimization",
    "title": "24  Optimization",
    "section": "24.1 Convex Optimization",
    "text": "24.1 Convex Optimization\nA convex optimization problem has the form:\n\n\\begin{aligned}\n\\min \\quad & f_0(x) \\\\\n\\text{s.t.} \\quad & f_i (x) \\leq 0\\qquad i = 1 \\dots n \\\\\n&a_i^Tx = b_i \\qquad i = 1 \\dots p\n\\end{aligned}\n\n\nobjective function f_0(x) is convex\ninequality constraints f_i(x) \\leq 0 are convex\nequality constraints are affine\n\nSome useful compositions that are preserve convexity\n\ncomposition of convex w/ affine function\n\ng(x) = f(Ax + b)\n\nweighted sums (w_i \\geq 0) of convex functions f_i\n\ng(x) = w_1f_1(x) + w_2f_2(x) + \\dots w_nf_n(x)\n\n\nNotable counter examples, if f_1, f_2 are convex,\n\nf_1 - f_2 may NOT be convex\n\n0 - x^2\n\nf_1 \\times f_2 may NOT be convex\n\nx * x^2\n\nf_1 / f_2 may NOT be convex\n\n\\frac{x^{3/2}}{x}\n\n\n\n24.1.1 CVXR\nA program for principled convex optimization\n\nDefine the variables\n\n\nBetahat <- Variable(p) where p is the length of the vector\n\n\nDefine objective function (objective <- Minimize(...))\n\n\nMinimize(...)\nMaximize(...)\n\n\nDefine the problem\nSolve\n\nUseful functions for debugging the program\n\nis_dcp(problem) - check if problem follows the DCP rules\nis_dgp(problem) - check if problem follows geometric programming rules\n\n\n\nCode\n# Tutorial\nset.seed(123)\n\nn <- 100\np <- 10\nbeta <- -4:5   # beta is just -4 through 5.\n\nX <- matrix(rnorm(n * p), nrow=n)\ncolnames(X) <- paste0(\"beta_\", beta)\nY <- X %*% beta + rnorm(n)\n\n\n\n\nCode\nbetahat <- Variable(p)\nobjective <- Minimize(sum((Y-X %*% betahat)^2))\nproblem <- Problem(objective, \n                   constraints = list(betahat >= 0)) # list constraints, betahat >= 0 \nresult <- solve(problem)\n\n\n\n\nCode\nresult$getValue(betahat) %>% zapsmall()\n\n\n          [,1]\n [1,] 0.000000\n [2,] 0.000000\n [3,] 0.000000\n [4,] 0.000000\n [5,] 1.237449\n [6,] 0.623466\n [7,] 2.123066\n [8,] 2.803564\n [9,] 4.444802\n[10,] 5.207352\n\n\n\nExamples\n\nLargest EigenvalueMinimize \\lambda_1\n\n\nLet A \\in \\mathbb{R}^{pxp} be a symmetric, nonegative matrix. Then our target value is:\n\n\nCode\nset.seed(1)\n# Symmetric Matrix\np <- 10 # matrix is p x p\nA <- matrix(abs(rnorm(p^2)), ncol = p) # non negative matrix\nsA <- forceSymmetric(A, uplo = \"U\") %>% as.matrix()\n\nsA_eig <- eigen(sA)\nsA_eig$values[1] # largest eigenvalue\n\n\n[1] 7.649905\n\n\nWe show 3 ways to use CVXR to solve for the largest eigenvalue, starting with the simplest\n\nobjective is pf_eigenvalue\nperron frobenius, geometric programming\nperron frobenius, convex programming\n\n\n\nCode\n# w/ CVXR\n# using objective \nX <- Variable(p,p, pos = TRUE)\nobjective <- Minimize(pf_eigenvalue(X))\nconstraints <- list(X == sA)\nproblem <- Problem(objective, constraints)\nresults <- solve(problem, gp = TRUE)\n\nc(results$value, sA_eig$values[1])\n\n\n[1] 7.649905 7.649905\n\n\n\n\nCode\n# w/ CVXR\n# spectral radius w/ geometric rules, A is known\n\nlambda <- Variable(1, name = \"lambda\", pos = TRUE)\nu <- Variable(p, pos = TRUE)\nobjective <- Minimize(lambda)\nconstraints <- list()\nfor (i in 1:p) {\n  constraints <- c(constraints, sA[i,, drop = F] %*% u / (lambda * u[i]) <= 1)\n}\nproblem <- Problem(objective,constraints)\nresults <- solve(problem, gp = TRUE)\n\nc(results$getValue(lambda), sA_eig$values[1]) # the same\n\n\n[1] 7.649905 7.649905\n\n\nIn order to transform the geometric program of minimizing largest eigenvalue, we note that the inequalities can be written\n\n\\begin{aligned}\n\\frac{\\sum_j A_{ij}u_j}{\\lambda u_i} &\\leq 1 \\qquad \\text{for } i = 1 \\dots n \\\\\n\\log(\\sum_j \\exp(u_j + \\log(A_{ij}) - \\log(\\lambda) - u_i)) & \\leq 0\n\\end{aligned}\n\n\n\nCode\n# w/ CVXR\n# manually transforming geometric to convex program\nlambda <- Variable(1, name = \"lambda\", pos = TRUE)\nu <- Variable(p, pos = TRUE)\nobjective <- Minimize(lambda)\nconstraints <- list()\nfor ( i in 1:p) {\n constraints <- c(constraints, \n                  log_sum_exp(u + log(sA[i,]) - log(lambda) - u[i]) <= 0)\n}\nproblem <- Problem(objective, constraints)\nresults <- solve(problem)\n# sum(vec + vec - scalar - scalar)\n# curvature(log_sum_exp(u + log(sA[1,]) - log(lambda) - u[1]))\n\nc(results$getValue(lambda), sA_eig$values[1]) # the same! nice!\n\n\n[1] 7.649905 7.649905\n\n\n\n\nIn addition to just finding the maximum eigenvalue (spectral radius) of a fixed matrix, we can also let the matrix be an optimization variable and minimize the spectral radius of the matrix! Thus, if entries of the matrix A(x) are a function of x, and we have some constraints f(x) \\leq 1, we can figure out those entries."
  },
  {
    "objectID": "optimization/optimization.html#deoptim",
    "href": "optimization/optimization.html#deoptim",
    "title": "24  Optimization",
    "section": "24.2 DEoptim",
    "text": "24.2 DEoptim\nDifferential Evolution Optimization, this is a derivative free way of optimization, and seems to work quite well, at least for rosenbrock function!\n\n24.2.1 Examples\n\nFirst Use\n\n\nCode\n# Rosenbrock Function for optimization\n# global minimum f(x) = 0, at x = (1, 1).\nRosenbrock <- function(x){\n    x1 <- x[1]\n    x2 <- x[2]\n    100 * (x2 - x1^2)^2 + (1 - x1)^2\n}\n\n# specify the grid for replicability\nlower <- c(-10,-10)\nupper <- -lower\n\n# Optimize over space\nset.seed(1234)\nDEoptim(Rosenbrock, lower, upper) # basic invocation\n\n# invocation with control aspects\noutDEoptim <- DEoptim(Rosenbrock,\n                      lower,\n                      upper,\n                      DEoptim.control(NP = 80,\n                                      itermax = 400, # number of procedure iterations\n                                      F = 1.2,\n                                      CR = 0.7)) \n\n\n\n\nCode\noutDEoptim$optim$bestmem # the final parameters\noutDEoptim$optim$bestval # the final value\n\nsprintf(\"The found parameters are %.2f %.2f\\n The minimum objective is: %.2f\\n DEoptim did %.2f iterations.\", \n  outDEoptim$optim$bestmem[[1]], outDEoptim$optim$bestmem[[2]], # parameters\n  outDEoptim$optim$bestval, # final value\n  outDEoptim$optim$iter) # final value\n\n\nplot(outDEoptim)\n\n\n\n\n\n\n\n\npar1 par2 \n   1    1 \n[1] 3.78756e-25\n[1] \"The found parameters are 1.00 1.00\\n The minimum objective is: 0.00\\n DEoptim did 400.00 iterations.\""
  },
  {
    "objectID": "ordination/ordination.html#overview",
    "href": "ordination/ordination.html#overview",
    "title": "25  Ordination",
    "section": "25.1 Overview",
    "text": "25.1 Overview\nMethods to Distinguish\n\nCorrespondance Analysis (CA)\n\nunconstrained, compare to PCA\n\nRedundancy Analysis (RDA)\nCanonical correspondance Analysis (CCA)\nCanonical Correlation Analysis (CCorA)\nDiscriminant Analysis (DA)\nPCA\nFactor Analysis\nNMDS\nMDS\n\nGeneral Packages\n\nPCA::\n\nSoftware functions\n\nprcomp - PCA\nprincomp - PCA\ncmdscale() - mds\nfactanal() - factor analysis\nlabdsv::pca\nade4::"
  },
  {
    "objectID": "ordination/ordination.html#organizing-types-of-ecological-data",
    "href": "ordination/ordination.html#organizing-types-of-ecological-data",
    "title": "25  Ordination",
    "section": "25.2 Organizing Types of Ecological Data",
    "text": "25.2 Organizing Types of Ecological Data\nAn ecological data matrix generally has the format of a main “response” matrix, with counts of species (columns) at each site (rows). We will generally have another data frame with the covariates describing each site, called the “environmental covariate” matrix and “species covariates” matrix."
  },
  {
    "objectID": "ordination/ordination.html#pca",
    "href": "ordination/ordination.html#pca",
    "title": "25  Ordination",
    "section": "25.3 PCA",
    "text": "25.3 PCA\nThere are a few ways to do a PCA in R. In order of preferred use,\n\nprcomp\nsvd\nprincomp\neigen\n\nWe’ll use USArrests example to show the similarity between all the program functionality\n\n\nCode\nUSArrests %>% head() # This is what the data looks like\n\n\n           Murder Assault UrbanPop Rape\nAlabama      13.2     236       58 21.2\nAlaska       10.0     263       48 44.5\nArizona       8.1     294       80 31.0\nArkansas      8.8     190       50 19.5\nCalifornia    9.0     276       91 40.6\nColorado      7.9     204       78 38.7\n\n\n\nprcompeigenprincompsvd\n\n\nprcomp uses svd under the hood, and is generally the main workhorse for doing principal components analysis.\n\n\nCode\nus_pr <- prcomp(USArrests, scale = TRUE)\nus_pr$sdev^2 # the variances\n\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\n\nCode\nus_pr$rotation\n\n\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995  0.4181809 -0.3412327  0.64922780\nAssault  -0.5831836  0.1879856 -0.2681484 -0.74340748\nUrbanPop -0.2781909 -0.8728062 -0.3780158  0.13387773\nRape     -0.5434321 -0.1673186  0.8177779  0.08902432\n\n\n\n\neigen is the standard diagonalization technique, with the variance explained extracted in\n\n\nCode\nus_Y <- cov(USArrests %>% scale(center = TRUE, scale = TRUE))\nus_Y <- cor(USArrests) # the same as cov of centered matrix\n\nus_eig <- eigen(us_Y)\n# variances\nus_eig$values\n\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\n\nCode\nus_eig$vectors # loadings\n\n\n           [,1]       [,2]       [,3]        [,4]\n[1,] -0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] -0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] -0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] -0.5434321 -0.1673186  0.8177779  0.08902432\n\n\n\n\nuses eigen under the hood, mostly here for compatibility reasons\n\n\nCode\n# correlation version\nus_prin <- princomp(USArrests, cor = TRUE)\nus_prin$sdev^2\n\n\n   Comp.1    Comp.2    Comp.3    Comp.4 \n2.4802416 0.9897652 0.3565632 0.1734301 \n\n\n\n\nCode\n# loadings\nus_prin$loadings\n\n\n\nLoadings:\n         Comp.1 Comp.2 Comp.3 Comp.4\nMurder    0.536  0.418  0.341  0.649\nAssault   0.583  0.188  0.268 -0.743\nUrbanPop  0.278 -0.873  0.378  0.134\nRape      0.543 -0.167 -0.818       \n\n               Comp.1 Comp.2 Comp.3 Comp.4\nSS loadings      1.00   1.00   1.00   1.00\nProportion Var   0.25   0.25   0.25   0.25\nCumulative Var   0.25   0.50   0.75   1.00\n\n\n\n\nCode\n# these scores should be the same, can't figure out why they're slightly different?\nus_prin$scores %>% head()\n\n\n               Comp.1     Comp.2      Comp.3       Comp.4\nAlabama     0.9855659  1.1333924  0.44426879  0.156267145\nAlaska      1.9501378  1.0732133 -2.04000333 -0.438583440\nArizona     1.7631635 -0.7459568 -0.05478082 -0.834652924\nArkansas   -0.1414203  1.1197968 -0.11457369 -0.182810896\nCalifornia  2.5239801 -1.5429340 -0.59855680 -0.341996478\nColorado    1.5145629 -0.9875551 -1.09500699  0.001464887\n\n\nCode\nscale(USArrests) %*% us_eig$vectors %>% head()\n\n\n                 [,1]       [,2]        [,3]         [,4]\nAlabama    -0.9756604  1.1220012 -0.43980366  0.154696581\nAlaska     -1.9305379  1.0624269  2.01950027 -0.434175454\nArizona    -1.7454429 -0.7384595  0.05423025 -0.826264240\nArkansas    0.1399989  1.1085423  0.11342217 -0.180973554\nCalifornia -2.4986128 -1.5274267  0.59254100 -0.338559240\nColorado   -1.4993407 -0.9776297  1.08400162  0.001450164\n\n\n\n\nCode\n# the scale variables\ncbind(us_prin$center,\n      attr(scale(USArrests), \"scaled:center\"))\n\n\n            [,1]    [,2]\nMurder     7.788   7.788\nAssault  170.760 170.760\nUrbanPop  65.540  65.540\nRape      21.232  21.232\n\n\nCode\nn <- nrow(USArrests)\ncbind(us_prin$scale,\n      attr(scale(USArrests), \"scaled:scale\") * sqrt((n-1) / n))\n\n\n              [,1]      [,2]\nMurder    4.311735  4.311735\nAssault  82.500075 82.500075\nUrbanPop 14.329285 14.329285\nRape      9.272248  9.272248\n\n\n\n\nCode\n# covariance version\nus_prin_cov <- princomp(USArrests) # uses factor of 1/n for variance calculations\n\n# the svd equivalent\nn <- nrow(USArrests)\nus_svd_cov <- svd(cov(USArrests) * (n-1) / n)\n\ncbind(us_prin_cov$sdev^2,\n      us_svd_cov$d)\n\n\n              [,1]        [,2]\nComp.1 6870.892554 6870.892554\nComp.2  197.952519  197.952519\nComp.3   41.270398   41.270398\nComp.4    6.040961    6.040961\n\n\nonly works for when n > p, “R”-mode PCA refers to the respondents (rows) and “Q”-mode refers to the questions (columns). difference is just interpretation. SVD is prefered\n\n\n\n\\begin{aligned}\n\\Sigma = UDV' \\\\\nU'\\Sigma = DV'\n\\end{aligned}\n\n\n\nCode\nus_Y <- cov(USArrests %>% scale(center = TRUE, scale = TRUE))\nus_Y <- cor(USArrests) # the same as cov of centered matrix\n\nus_svd <- svd(us_Y)\n\n# UDV'\n# us_svd$u %*% diag(us_svd$d) %*% t(us_svd$v)\n\n\nus_svd$d # the variances\n\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\n\nCode\nus_svd$v # the loadings\n\n\n           [,1]       [,2]       [,3]        [,4]\n[1,] -0.5358995  0.4181809 -0.3412327  0.64922780\n[2,] -0.5831836  0.1879856 -0.2681484 -0.74340748\n[3,] -0.2781909 -0.8728062 -0.3780158  0.13387773\n[4,] -0.5434321 -0.1673186  0.8177779  0.08902432"
  },
  {
    "objectID": "ordination/ordination.html#correspondence-analysis-ca",
    "href": "ordination/ordination.html#correspondence-analysis-ca",
    "title": "25  Ordination",
    "section": "25.4 Correspondence Analysis (CA)",
    "text": "25.4 Correspondence Analysis (CA)\n\nca::ca()\nFactoMineR::CA()\nade4::dudi.coa()\namap::afc()\nMASS::corresp()\n\n\n\nCode\nauthor_ca <- ca(author)\nplot(author_ca)"
  },
  {
    "objectID": "ordination/ordination.html#multivariate-regression",
    "href": "ordination/ordination.html#multivariate-regression",
    "title": "25  Ordination",
    "section": "25.5 Multivariate Regression",
    "text": "25.5 Multivariate Regression\nlm can handle multivariate regressions\n\n25.5.1 Estimation\n\n\\begin{aligned}\n\\hat B = (X'X)^{-1}X'Y\n\\end{aligned}\n We can also formulate the answers w/ the centered X matrix \n\\begin{aligned}\n\\mathbf{\\hat B_1} &= (\\mathbf{X_c}'\\mathbf{X_c})^{-1}\\mathbf{X_c}'\\mathbf{Y} \\\\\n\\mathbf{\\hat \\beta_0} &=  \\mathbf{\\bar y} - \\mathbf{\\bar x}'\\mathbf{\\hat B_1}\n\\end{aligned}\n\n\n\nCode\niris_lm <- lm(cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)~Species, data = iris)\n\n\n\n\nCode\n# Useful for manual calculations\nY <- with(iris, cbind(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width))\nX <- model.matrix(~Species, data = iris)\nX1 <- X[,-1] # drop the intercept\nXc <- X1 %>% scale(scale = F)\nSYY <- cov(Y)\nSYX <- cov(Y, X1)\nSXX <- cov(X1)\n\n\n\n\nCode\n# Manual calculation of B\nB <- solve(crossprod(X)) %*% t(X) %*% Y # B\n\n# B calculation with centered matrices\nB1 <- solve(crossprod(Xc)) %*% t(Xc) %*% Y # B_1\nB0 <- colMeans(Y) - t(colMeans(X1)) %*% B1\n\n\n# B calculation w/ covariances\nB1_S <- solve(SXX) %*% t(SYX)\nB0_S <- colMeans(Y) -  t(colMeans(X1)) %*% B1_S\n\nlist(lm = coef(iris_lm),\n     manual = B,\n     centered = rbind(B0, B1),\n     covariance = rbind(B0_S, B1_S))\n\n\n$lm\n                  Sepal.Length Sepal.Width Petal.Length Petal.Width\n(Intercept)              5.006       3.428        1.462       0.246\nSpeciesversicolor        0.930      -0.658        2.798       1.080\nSpeciesvirginica         1.582      -0.454        4.090       1.780\n\n$manual\n                  Sepal.Length Sepal.Width Petal.Length Petal.Width\n(Intercept)              5.006       3.428        1.462       0.246\nSpeciesversicolor        0.930      -0.658        2.798       1.080\nSpeciesvirginica         1.582      -0.454        4.090       1.780\n\n$centered\n                  Sepal.Length Sepal.Width Petal.Length Petal.Width\n                         5.006       3.428        1.462       0.246\nSpeciesversicolor        0.930      -0.658        2.798       1.080\nSpeciesvirginica         1.582      -0.454        4.090       1.780\n\n$covariance\n                  Sepal.Length Sepal.Width Petal.Length Petal.Width\n                         5.006       3.428        1.462       0.246\nSpeciesversicolor        0.930      -0.658        2.798       1.080\nSpeciesvirginica         1.582      -0.454        4.090       1.780\n\n\n\n\\begin{aligned}\nR^2 = \\frac{|S_{XY} S_{XX}^{-1} S_{YX}|}{|S_{YY}|}\n\\end{aligned}\n\n\n\nCode\n# TODO: Why is R^2 zero here\n# R^2 from each regression\n# vapply(summary(iris_lm),FUN = function(x) x$r.squared, FUN.VALUE = numeric(1))\n\ndet(SYX %*% solve(SXX) %*% t(SYX)) / det(SYY) # way small, basically 0\n\n\n[1] 4.449697e-31\n\n\n\n\n25.5.2 Hypothesis Testing\nConsider the partitioning, from pg 343 of Rencher Methods of Multivariate Analysis\n\n\\begin{aligned}\n\\mathbf{Y'Y} - n\\mathbf{\\bar y\\bar y'} &= (\\mathbf{Y'Y} + \\mathbf{Y'X'\\hat{B}}) - (\\mathbf{Y'X'\\hat{B}} -  n\\mathbf{\\bar y\\bar y'}) \\\\\n&\\equiv \\mathbf{E} + \\mathbf{H}\n\\end{aligned}\n\nIn hypothesis testing, the matrix E^{-1}H is pretty special, and many of the tests can be expressed in terms of the eigenvalues of this matrix. Hence, let \\lambda_i be the ith largest eigenvalue of E^{-1}H.\n\nWilks Hypothesis Test \\Lambda = \\frac{|E|}{|E + H|} (likelihood ratio test)\n\non the eigenscale: \\Lambda = \\prod_{i=1}^s \\frac{1}{1+\\lambda_i}\nwith covariance: \\Lambda = \\frac{|\\mathbf{S}|}{|\\mathbf{S_{XX}}|\\mathbf{S_{YY}}|}\n\nRoy\n\non the eigenscale \\theta = \\frac{\\lambda_1}{1 + \\lambda_1}\n\nPillai V^{(s)} = \\mathrm{tr}[(H + E)^{-1}H]\n\non the eigenscale: V^{(s)} = \\sum_i^s\\frac{\\lambda_i}{1 + \\lambda_i}\n\nHotelling (generalized T statistic) U^{(s)} = tr(E^{-1}H)\n\non the eigenscale: U^{(s)} = \\sum_i^s \\lambda_i"
  },
  {
    "objectID": "ordination/ordination.html#redundancy-analysis",
    "href": "ordination/ordination.html#redundancy-analysis",
    "title": "25  Ordination",
    "section": "25.6 Redundancy Analysis",
    "text": "25.6 Redundancy Analysis\nRedundancy analysis can be understood as a multivariate regression followed by a PCA on the (covariance) of the fitted values.\nIt seems I’m running into a bunch of singularity issues due to the fact that n < p in these matrices, the standard eigenequation of redundancy analysis is:\n\n\\begin{aligned}\n(S_{YX}S^{-1}_{XX}S'_{YX} - \\lambda_k I)u_k = 0\n\\end{aligned}\n\nThe ecological matrix dune is matrix counts of 30 species (columns) from 20 sites (rows). The covariate matrix dune.env is covariates of the 20 sites (rows), with 5 features of the site (columns):\n\nA1: numeric vector of soil thickness on A1 horizon\nMoisture: ordered factor with levels 1 < 2 < 4 < 5\nType of management\nUse - land-use Hayfield < Hyapastu < Pasture\nManure - ordered factor\n\n\n\nCode\ndata(dune)\ndata(dune.env)\n\ndune %>% head()\n\n\n  Achimill Agrostol Airaprae Alopgeni Anthodor Bellpere Bromhord Chenalbu\n1        1        0        0        0        0        0        0        0\n2        3        0        0        2        0        3        4        0\n3        0        4        0        7        0        2        0        0\n4        0        8        0        2        0        2        3        0\n5        2        0        0        0        4        2        2        0\n6        2        0        0        0        3        0        0        0\n  Cirsarve Comapalu Eleopalu Elymrepe Empenigr Hyporadi Juncarti Juncbufo\n1        0        0        0        4        0        0        0        0\n2        0        0        0        4        0        0        0        0\n3        0        0        0        4        0        0        0        0\n4        2        0        0        4        0        0        0        0\n5        0        0        0        4        0        0        0        0\n6        0        0        0        0        0        0        0        0\n  Lolipere Planlanc Poaprat Poatriv Ranuflam Rumeacet Sagiproc Salirepe\n1        7        0       4       2        0        0        0        0\n2        5        0       4       7        0        0        0        0\n3        6        0       5       6        0        0        0        0\n4        5        0       4       5        0        0        5        0\n5        2        5       2       6        0        5        0        0\n6        6        5       3       4        0        6        0        0\n  Scorautu Trifprat Trifrepe Vicilath Bracruta Callcusp\n1        0        0        0        0        0        0\n2        5        0        5        0        0        0\n3        2        0        2        0        2        0\n4        2        0        1        0        2        0\n5        3        2        2        0        2        0\n6        3        5        5        0        6        0\n\n\nCode\ndune.env %>% head()\n\n\n   A1 Moisture Management      Use Manure\n1 2.8        1         SF Haypastu      4\n2 3.5        1         BF Haypastu      2\n3 4.3        2         SF Haypastu      4\n4 4.2        2         SF Haypastu      4\n5 6.3        1         HF Hayfield      2\n6 4.3        1         HF Haypastu      2\n\n\n\n\nCode\ndune_rda <- rda(dune ~ Manure, dune.env, scale = T)\ndune_rda\n\n\nCall: rda(formula = dune ~ Manure, data = dune.env, scale = T)\n\n              Inertia Proportion Rank\nTotal         30.0000     1.0000     \nConstrained    8.7974     0.2932    4\nUnconstrained 21.2026     0.7068   15\nInertia is correlations \n\nEigenvalues for constrained axes:\n RDA1  RDA2  RDA3  RDA4 \n4.374 2.078 1.449 0.896 \n\nEigenvalues for unconstrained axes:\n  PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10  PC11  PC12  PC13 \n5.133 3.447 2.462 1.924 1.662 1.366 1.357 0.926 0.839 0.585 0.511 0.439 0.308 \n PC14  PC15 \n0.159 0.084 \n\n\nCode\nsummary(dune_rda)\n\n\n\nCall:\nrda(formula = dune ~ Manure, data = dune.env, scale = T) \n\nPartitioning of correlations:\n              Inertia Proportion\nTotal          30.000     1.0000\nConstrained     8.797     0.2932\nUnconstrained  21.203     0.7068\n\nEigenvalues, and their contribution to the correlations \n\nImportance of components:\n                        RDA1    RDA2    RDA3    RDA4    PC1    PC2     PC3\nEigenvalue            4.3740 2.07784 1.44948 0.89609 5.1334 3.4467 2.46206\nProportion Explained  0.1458 0.06926 0.04832 0.02987 0.1711 0.1149 0.08207\nCumulative Proportion 0.1458 0.21506 0.26338 0.29325 0.4644 0.5793 0.66132\n                          PC4     PC5     PC6     PC7     PC8     PC9    PC10\nEigenvalue            1.92357 1.66179 1.36650 1.35737 0.92607 0.83920 0.58545\nProportion Explained  0.06412 0.05539 0.04555 0.04525 0.03087 0.02797 0.01952\nCumulative Proportion 0.72544 0.78083 0.82638 0.87163 0.90250 0.93047 0.94999\n                         PC11    PC12    PC13     PC14     PC15\nEigenvalue            0.51113 0.43886 0.30750 0.158961 0.083959\nProportion Explained  0.01704 0.01463 0.01025 0.005299 0.002799\nCumulative Proportion 0.96702 0.98165 0.99190 0.997201 1.000000\n\nAccumulated constrained eigenvalues\nImportance of components:\n                        RDA1   RDA2   RDA3   RDA4\nEigenvalue            4.3740 2.0778 1.4495 0.8961\nProportion Explained  0.4972 0.2362 0.1648 0.1019\nCumulative Proportion 0.4972 0.7334 0.8981 1.0000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  4.886172 \n\n\nSpecies scores\n\n             RDA1     RDA2      RDA3      RDA4      PC1      PC2\nAchimill  0.26765  0.33281  0.049656  0.003056 -0.49105  0.36224\nAgrostol -0.02671 -0.41434  0.115610 -0.014514  0.65940 -0.11236\nAiraprae -0.41321  0.05069 -0.125661 -0.091859 -0.41815 -0.52440\nAlopgeni  0.34142 -0.21078  0.274032 -0.041120  0.42422 -0.37590\nAnthodor -0.06138  0.31573 -0.003043 -0.028513 -0.62121 -0.03545\nBellpere  0.31718  0.07152 -0.201198 -0.229336 -0.21895  0.18262\nBromhord  0.36283  0.13117 -0.019584  0.009624 -0.29475  0.21019\nChenalbu -0.01231 -0.17847  0.341080  0.138579  0.05555 -0.24663\nCirsarve  0.20685 -0.34222 -0.231882 -0.153871  0.03885 -0.12148\nComapalu -0.42230  0.05180 -0.128423 -0.093878  0.32227  0.28984\nEleopalu -0.37212 -0.15905  0.279029  0.078005  0.51088  0.27431\nElymrepe  0.51896 -0.11325 -0.328887 -0.135082  0.09940 -0.03675\nEmpenigr -0.29065  0.03565 -0.088387 -0.064612 -0.33962 -0.57676\nHyporadi -0.31957  0.10278 -0.182774  0.061050 -0.41390 -0.56049\nJuncarti -0.21820 -0.05029  0.136507  0.289544  0.53378  0.06805\nJuncbufo  0.20091  0.12787  0.221180  0.222888  0.28841 -0.40733\nLolipere  0.59645 -0.16436 -0.190963  0.120042 -0.38788  0.29546\nPlanlanc  0.13923  0.32854  0.138828  0.059107 -0.56352  0.36121\nPoaprat   0.57532 -0.22060 -0.156549  0.225524 -0.32898  0.17668\nPoatriv   0.61056 -0.08547  0.349355 -0.027330 -0.02276 -0.08873\nRanuflam -0.45682 -0.14959  0.255045  0.059903  0.52417  0.17895\nRumeacet  0.33798  0.39609  0.258611 -0.208808 -0.17508  0.14683\nSagiproc  0.20419 -0.09090 -0.056583  0.040345  0.15410 -0.68467\nSalirepe -0.51264  0.06289 -0.155896 -0.113961 -0.01356 -0.12479\nScorautu -0.13605  0.41327 -0.054013  0.046293 -0.48563 -0.23743\nTrifprat  0.23866  0.26819  0.265287 -0.231610 -0.30973  0.27939\nTrifrepe  0.21756  0.48721 -0.033738  0.124932 -0.09282  0.16628\nVicilath  0.04915  0.21898 -0.287987  0.478432 -0.21306  0.10011\nBracruta -0.09738  0.26181  0.009227 -0.013498  0.06513  0.04448\nCallcusp -0.37492 -0.05173  0.073218 -0.006614  0.45127  0.28578\n\n\nSite scores (weighted sums of species scores)\n\n      RDA1    RDA2    RDA3     RDA4      PC1      PC2\n1   0.6084 -1.3919 -1.2828 -0.15944 -0.27047  0.56624\n2   1.5953  0.7411 -1.1559 -0.60367 -0.26494  0.30541\n3   1.2359 -1.4977 -0.5377 -0.66021  0.06306  0.08227\n4   1.4686 -2.5913 -1.8932 -1.64469  0.20741 -0.64851\n5   1.1634  2.0559  0.4468 -2.37998 -0.65117  0.61097\n6   1.0317  3.0282  1.4870 -1.67094 -0.78754  1.02488\n7   1.0569  1.4590  0.8726  0.01950 -1.94387  0.94362\n8  -0.1454 -1.4755  1.1266  1.54932  0.28659 -0.10607\n9   0.8908 -0.6300  0.3403  1.22041  1.43342 -0.91507\n10  1.1827  2.1193 -1.2125  1.52260 -1.07292  1.17204\n11  0.0919  1.0554 -2.2529  3.57753 -0.36050 -0.25697\n12  0.3039 -0.2200  1.6086 -0.09126  1.70365 -1.94126\n13  0.2414 -1.7792  3.1394  1.22768  0.29658 -1.31665\n14 -1.6591 -0.2896  0.3470 -0.39614  1.16871  1.25218\n15 -1.6951 -0.7286  0.5571 -0.06828  1.19948  0.87768\n16 -1.4158 -2.0152  2.1448  0.16275  1.36070  0.47911\n17 -0.9349  0.5539 -0.7611 -0.80973 -1.20578 -0.30418\n18 -0.4582  1.1964 -1.6920  0.54033 -0.86720  0.59227\n19 -2.2192  1.5606 -2.2704 -1.34738 -1.81306 -3.07908\n20 -2.3432 -1.1506  0.9882  0.01161  1.51785  0.66114\n\n\nSite constraints (linear combinations of constraining variables)\n\n       RDA1    RDA2    RDA3    RDA4      PC1      PC2\n1   1.10430 -1.8270 -1.2379 -0.8214 -0.27047  0.56624\n2   1.02356  1.4013  0.5966 -1.1865 -0.26494  0.30541\n3   1.10430 -1.8270 -1.2379 -0.8214  0.06306  0.08227\n4   1.10430 -1.8270 -1.2379 -0.8214  0.20741 -0.64851\n5   1.02356  1.4013  0.5966 -1.1865 -0.65117  0.61097\n6   1.02356  1.4013  0.5966 -1.1865 -0.78754  1.02488\n7  -0.06573 -0.9527  1.8209  0.7398 -1.94387  0.94362\n8  -0.06573 -0.9527  1.8209  0.7398  0.28659 -0.10607\n9   0.72183  0.8482 -1.0417  2.1068  1.43342 -0.91507\n10  0.72183  0.8482 -1.0417  2.1068 -1.07292  1.17204\n11  0.72183  0.8482 -1.0417  2.1068 -0.36050 -0.25697\n12  1.02356  1.4013  0.5966 -1.1865  1.70365 -1.94126\n13 -0.06573 -0.9527  1.8209  0.7398  0.29658 -1.31665\n14 -1.55162  0.1903 -0.4719 -0.3449  1.16871  1.25218\n15 -1.55162  0.1903 -0.4719 -0.3449  1.19948  0.87768\n16 -0.06573 -0.9527  1.8209  0.7398  1.36070  0.47911\n17 -1.55162  0.1903 -0.4719 -0.3449 -1.20578 -0.30418\n18 -1.55162  0.1903 -0.4719 -0.3449 -0.86720  0.59227\n19 -1.55162  0.1903 -0.4719 -0.3449 -1.81306 -3.07908\n20 -1.55162  0.1903 -0.4719 -0.3449  1.51785  0.66114\n\n\nBiplot scores for constraining variables\n\n            RDA1    RDA2    RDA3    RDA4 PC1 PC2\nManure.L  0.7227 -0.6209  0.2738 -0.1315   0   0\nManure.Q -0.5907 -0.4981 -0.5890 -0.2369   0   0\nManure.C  0.5983  0.2084 -0.7428  0.2166   0   0\nManure^4  0.1381  0.4330 -0.1116 -0.8837   0   0\n\n\nCentroids for factor constraints\n\n            RDA1    RDA2    RDA3    RDA4 PC1 PC2\nManure0 -1.55162  0.1903 -0.4719 -0.3449   0   0\nManure1  0.72183  0.8482 -1.0417  2.1068   0   0\nManure2  1.02356  1.4013  0.5966 -1.1865   0   0\nManure3 -0.06573 -0.9527  1.8209  0.7398   0   0\nManure4  1.10430 -1.8270 -1.2379 -0.8214   0   0\n\n\nCode\nscores(dune_rda, scaling = 0, display = \"sp\")\n\n\n                 RDA1        RDA2\nAchimill  0.143458988  0.25880807\nAgrostol -0.014318256 -0.32220859\nAiraprae -0.221477424  0.03941836\nAlopgeni  0.182994436 -0.16391065\nAnthodor -0.032899432  0.24552799\nBellpere  0.170004691  0.05561525\nBromhord  0.194470873  0.10200638\nChenalbu -0.006598932 -0.13878455\nCirsarve  0.110871155 -0.26612997\nComapalu -0.226345641  0.04028481\nEleopalu -0.199452547 -0.12368720\nElymrepe  0.278156602 -0.08806907\nEmpenigr -0.155781754  0.02772590\nHyporadi -0.171282469  0.07992401\nJuncarti -0.116954796 -0.03910985\nJuncbufo  0.107686567  0.09944215\nLolipere  0.319690365 -0.12781833\nPlanlanc  0.074625723  0.25548840\nPoaprat   0.308361280 -0.17154631\nPoatriv   0.327251488 -0.06646981\nRanuflam -0.244850313 -0.11632722\nRumeacet  0.181152477  0.30802177\nSagiproc  0.109442555 -0.07068686\nSalirepe -0.274766547  0.04890272\nScorautu -0.072918338  0.32138155\nTrifprat  0.127920010  0.20855494\nTrifrepe  0.116609271  0.37888351\nVicilath  0.026342810  0.17029063\nBracruta -0.052193007  0.20359595\nCallcusp -0.200951405 -0.04022983\nattr(,\"const\")\n[1] 4.886172\n\n\n\n\nCode\nY <- dune %>% data.matrix()\nX <- model.matrix(~Manure, data = dune.env)[,-1, drop = F]\n# Y <- dune %>% scale(scale = T)\n\nSYY <- cov(Y)\n\nSXX <- cov(X)\nSYX <- cov(Y, X)\nrbind(cbind(SYY, SYX),\n      cbind(t(SYX), SXX))\n\n\n            Achimill    Agrostol    Airaprae    Alopgeni    Anthodor\nAchimill  1.53684211 -2.02105263  0.00000000 -1.20000000  1.32631579\nAgrostol -2.02105263  7.20000000 -0.63157895  3.76842105 -2.65263158\nAiraprae  0.00000000 -0.63157895  0.61842105 -0.47368421  0.77631579\nAlopgeni -1.20000000  3.76842105 -0.47368421  6.90526316 -1.98947368\nAnthodor  1.32631579 -2.65263158  0.77631579 -1.98947368  2.89210526\nBellpere  0.55789474 -0.37894737 -0.17105263  0.03157895  0.12368421\nBromhord  1.26315789 -0.63157895 -0.19736842 -0.68421053  0.64473684\nChenalbu -0.04210526  0.13684211 -0.01315789  0.16842105 -0.05526316\nCirsarve -0.08421053  0.58947368 -0.02631579  0.02105263 -0.11052632\nComapalu -0.16842105  0.33684211 -0.05263158 -0.37894737 -0.22105263\nEleopalu -1.05263158  3.57894737 -0.32894737  0.36842105 -1.38157895\nElymrepe  0.16842105  0.18947368 -0.34210526  0.80000000 -0.59473684\nEmpenigr -0.08421053 -0.25263158  0.28947368 -0.18947368  0.31052632\nHyporadi -0.16842105 -1.13684211  0.88157895 -0.85263158  0.97631579\nJuncarti -0.75789474  1.98947368 -0.23684211  0.61052632 -0.99473684\nJuncbufo -0.33684211  0.62105263 -0.17105263  1.87368421 -0.50789474\nLolipere  1.45263158 -2.80000000 -0.76315789 -0.86315789  0.05789474\nPlanlanc  1.32631579 -3.28421053 -0.13157895 -2.46315789  1.98421053\nPoaprat   0.71578947 -1.32631579 -0.52631579  0.34736842 -0.28421053\nPoatriv   0.97894737  0.98947368 -0.82894737  3.82105263 -0.21842105\nRanuflam -0.58947368  1.81052632 -0.18421053  0.14736842 -0.77368421\nRumeacet  0.71578947 -1.53684211 -0.23684211 -0.54736842  1.32105263\nSagiproc -0.84210526  1.68421053  0.21052632  1.68421053 -0.47368421\nSalirepe -0.46315789 -0.07368421  0.32894737 -1.04210526  0.02368421\nScorautu  0.30526316 -2.29473684  0.44736842 -1.16842105  0.75263158\nTrifprat  0.56842105 -1.13684211 -0.11842105 -0.85263158  0.92368421\nTrifrepe  1.02105263 -1.56842105 -0.30263158 -0.29473684  0.50789474\nVicilath  0.04210526 -0.50526316 -0.05263158 -0.37894737 -0.01052632\nBracruta -0.58947368  0.02105263 -0.17105263 -0.32631579 -0.07631579\nCallcusp -0.42105263  1.47368421 -0.13157895 -0.31578947 -0.55263158\nManure.L  0.00000000  0.38280203 -0.14563121  0.63245553 -0.21220547\nManure.Q -0.23068865  0.22224882  0.13011403 -0.34321970 -0.10057463\nManure.C  0.11650497 -0.24965350 -0.06241337 -0.06657427  0.02080446\nManure^4  0.10694151 -0.24533640  0.02359004 -0.05032542  0.13053155\n            Bellpere    Bromhord     Chenalbu     Cirsarve    Comapalu\nAchimill  0.55789474  1.26315789 -0.042105263 -0.084210526 -0.16842105\nAgrostol -0.37894737 -0.63157895  0.136842105  0.589473684  0.33684211\nAiraprae -0.17105263 -0.19736842 -0.013157895 -0.026315789 -0.05263158\nAlopgeni  0.03157895 -0.68421053  0.168421053  0.021052632 -0.37894737\nAnthodor  0.12368421  0.64473684 -0.055263158 -0.110526316 -0.22105263\nBellpere  1.08157895  1.06578947 -0.034210526  0.142105263 -0.13684211\nBromhord  1.06578947  1.98684211 -0.039473684  0.236842105 -0.15789474\nChenalbu -0.03421053 -0.03947368  0.050000000 -0.005263158 -0.01052632\nCirsarve  0.14210526  0.23684211 -0.005263158  0.200000000 -0.02105263\nComapalu -0.13684211 -0.15789474 -0.010526316 -0.021052632  0.37894737\nEleopalu -0.85526316 -0.98684211 -0.065789474 -0.131578947  0.68421053\nElymrepe  1.00526316  0.86842105 -0.068421053  0.284210526 -0.27368421\nEmpenigr -0.06842105 -0.07894737 -0.005263158 -0.010526316 -0.02105263\nHyporadi -0.30789474 -0.35526316 -0.023684211 -0.047368421 -0.09473684\nJuncarti -0.61578947 -0.71052632 -0.047368421 -0.094736842  0.12631579\nJuncbufo -0.44473684 -0.30263158  0.123684211 -0.068421053 -0.13684211\nLolipere  1.01578947  1.65789474 -0.152631579  0.221052632 -0.61052632\nPlanlanc  0.26842105  0.65789474 -0.068421053 -0.136842105 -0.27368421\nPoaprat   0.88421053  1.05263158 -0.021052632  0.168421053 -0.50526316\nPoatriv   1.16052632  1.77631579  0.307894737  0.194736842 -0.66315789\nRanuflam -0.47894737 -0.55263158  0.068421053 -0.073684211  0.27368421\nRumeacet -0.08947368  0.13157895 -0.047368421 -0.094736842 -0.18947368\nSagiproc -0.15789474  0.00000000  0.052631579  0.421052632 -0.21052632\nSalirepe -0.06052632 -0.43421053 -0.028947368 -0.057894737 -0.11578947\nScorautu  0.52105263  0.50000000 -0.036842105 -0.073684211 -0.14736842\nTrifprat -0.09736842  0.06578947 -0.023684211 -0.047368421 -0.09473684\nTrifrepe  0.55000000  1.03947368 -0.018421053 -0.142105263  0.24210526\nVicilath  0.07368421  0.05263158 -0.010526316 -0.021052632 -0.04210526\nBracruta -0.20263158 -0.77631579 -0.128947368 -0.047368421 -0.09473684\nCallcusp -0.34210526 -0.39473684 -0.026315789 -0.052631579  0.31578947\nManure.L  0.08737872  0.12898764  0.020804458  0.074896050 -0.11650497\nManure.Q -0.02742944 -0.20044593 -0.016176338  0.052045610  0.10409122\nManure.C  0.15395299  0.17891834 -0.029126242  0.041608917 -0.04993070\nManure^4  0.15569425  0.07077012 -0.026735377  0.009436015  0.01887203\n            Eleopalu    Elymrepe     Empenigr    Hyporadi     Juncarti\nAchimill -1.05263158  0.16842105 -0.084210526 -0.16842105 -0.757894737\nAgrostol  3.57894737  0.18947368 -0.252631579 -1.13684211  1.989473684\nAiraprae -0.32894737 -0.34210526  0.289473684  0.88157895 -0.236842105\nAlopgeni  0.36842105  0.80000000 -0.189473684 -0.85263158  0.610526316\nAnthodor -1.38157895 -0.59473684  0.310526316  0.97631579 -0.994736842\nBellpere -0.85526316  1.00526316 -0.068421053 -0.30789474 -0.615789474\nBromhord -0.98684211  0.86842105 -0.078947368 -0.35526316 -0.710526316\nChenalbu -0.06578947 -0.06842105 -0.005263158 -0.02368421 -0.047368421\nCirsarve -0.13157895  0.28421053 -0.010526316 -0.04736842 -0.094736842\nComapalu  0.68421053 -0.27368421 -0.021052632 -0.09473684  0.126315789\nEleopalu  5.56578947 -1.71052632 -0.131578947 -0.59210526  2.552631579\nElymrepe -1.71052632  4.32631579 -0.136842105 -0.61578947  0.031578947\nEmpenigr -0.13157895 -0.13684211  0.200000000  0.47894737 -0.094736842\nHyporadi -0.59210526 -0.61578947  0.478947368  1.52368421 -0.426315789\nJuncarti  2.55263158  0.03157895 -0.094736842 -0.42631579  2.621052632\nJuncbufo -0.85526316  0.37368421 -0.068421053 -0.30789474  0.226315789\nLolipere -2.97368421  1.92631579 -0.305263158 -0.63684211 -1.484210526\nPlanlanc -1.71052632 -0.72631579 -0.136842105 -0.08947368 -1.231578947\nPoaprat  -2.31578947  1.97894737 -0.252631579 -0.61052632 -0.589473684\nPoatriv  -2.46052632  2.74210526 -0.331578947 -1.49210526 -0.773684211\nRanuflam  2.13157895 -0.95789474 -0.073684211 -0.33157895  1.231578947\nRumeacet -1.18421053  0.45263158 -0.094736842 -0.42631579 -0.431578947\nSagiproc -0.89473684  0.31578947  0.210526316  0.52631579 -0.105263158\nSalirepe  0.32894737 -0.75263158  0.257894737  0.52894737  0.531578947\nScorautu -1.55263158 -0.53684211  0.347368421  1.03684211 -0.768421053\nTrifprat -0.59210526 -0.19473684 -0.047368421 -0.21315789 -0.426315789\nTrifrepe -1.14473684 -0.16315789 -0.036842105 -0.27105263 -1.015789474\nVicilath -0.26315789 -0.27368421 -0.021052632  0.11578947 -0.189473684\nBracruta  0.77631579 -1.45789474  0.057894737  0.05000000  0.626315789\nCallcusp  2.07894737 -0.68421053 -0.052631579 -0.23684211  0.631578947\nManure.L -0.12898764  0.40776738 -0.058252483 -0.22884904 -0.108183183\nManure.Q  0.14418041 -0.02672612  0.052045610  0.14980696  0.004219914\nManure.C -0.51178967  0.50762878 -0.024965350 -0.01248267 -0.141470316\nManure^4 -0.25949042  0.18557497  0.009436015 -0.02044470 -0.261063091\n            Juncbufo    Lolipere    Planlanc     Poaprat     Poatriv\nAchimill -0.33684211  1.45263158  1.32631579  0.71578947  0.97894737\nAgrostol  0.62105263 -2.80000000 -3.28421053 -1.32631579  0.98947368\nAiraprae -0.17105263 -0.76315789 -0.13157895 -0.52631579 -0.82894737\nAlopgeni  1.87368421 -0.86315789 -2.46315789  0.34736842  3.82105263\nAnthodor -0.50789474  0.05789474  1.98421053 -0.28421053 -0.21842105\nBellpere -0.44473684  1.01578947  0.26842105  0.88421053  1.16052632\nBromhord -0.30263158  1.65789474  0.65789474  1.05263158  1.77631579\nChenalbu  0.12368421 -0.15263158 -0.06842105 -0.02105263  0.30789474\nCirsarve -0.06842105  0.22105263 -0.13684211  0.16842105  0.19473684\nComapalu -0.13684211 -0.61052632 -0.27368421 -0.50526316 -0.66315789\nEleopalu -0.85526316 -2.97368421 -1.71052632 -2.31578947 -2.46052632\nElymrepe  0.37368421  1.92631579 -0.72631579  1.97894737  2.74210526\nEmpenigr -0.06842105 -0.30526316 -0.13684211 -0.25263158 -0.33157895\nHyporadi -0.30789474 -0.63684211 -0.08947368 -0.61052632 -1.49210526\nJuncarti  0.22631579 -1.48421053 -1.23157895 -0.58947368 -0.77368421\nJuncbufo  1.92368421 -0.93157895 -0.36315789 -0.06315789  1.68684211\nLolipere -0.93157895  7.98947368  2.08421053  4.56842105  2.27894737\nPlanlanc -0.36315789  2.08421053  3.80000000  0.92631579  0.26842105\nPoaprat  -0.06315789  4.56842105  0.92631579  3.41052632  2.56842105\nPoatriv   1.68684211  2.27894737  0.26842105  2.56842105  7.92368421\nRanuflam -0.16315789 -1.71578947 -0.95789474 -1.13684211 -0.74210526\nRumeacet  0.54210526  0.83157895  2.45263158  0.25263158  1.59473684\nSagiproc  0.89473684 -0.36842105 -1.05263158  0.00000000  0.73684211\nSalirepe -0.37631579 -1.36315789 -0.27894737 -0.91578947 -1.82368421\nScorautu -0.37368421  0.60000000  0.93684211  0.44210526 -0.42631579\nTrifprat -0.09736842  1.04736842  1.75263158  0.28421053  0.71842105\nTrifrepe  0.18157895  1.40526316  0.88947368  0.74736842  1.20789474\nVicilath -0.13684211  0.54736842  0.35789474  0.28421053 -0.45263158\nBracruta -0.20263158 -0.21578947  1.17368421 -0.61052632 -1.91315789\nCallcusp -0.34210526 -1.52631579 -0.68421053 -1.26315789 -1.34210526\nManure.L  0.07073516  0.69070802 -0.07489605  0.46601987  0.87794814\nManure.Q -0.26655792 -0.27710771 -0.35025289 -0.18567623 -0.76591445\nManure.C  0.02080446  0.67406445  0.05825248  0.41608917  0.11234407\nManure^4 -0.09593282 -0.10379617  0.09121481 -0.18242963  0.04560741\n            Ranuflam    Rumeacet    Sagiproc    Salirepe    Scorautu\nAchimill -0.58947368  0.71578947 -0.84210526 -0.46315789  0.30526316\nAgrostol  1.81052632 -1.53684211  1.68421053 -0.07368421 -2.29473684\nAiraprae -0.18421053 -0.23684211  0.21052632  0.32894737  0.44736842\nAlopgeni  0.14736842 -0.54736842  1.68421053 -1.04210526 -1.16842105\nAnthodor -0.77368421  1.32105263 -0.47368421  0.02368421  0.75263158\nBellpere -0.47894737 -0.08947368 -0.15789474 -0.06052632  0.52105263\nBromhord -0.55263158  0.13157895  0.00000000 -0.43421053  0.50000000\nChenalbu  0.06842105 -0.04736842  0.05263158 -0.02894737 -0.03684211\nCirsarve -0.07368421 -0.09473684  0.42105263 -0.05789474 -0.07368421\nComapalu  0.27368421 -0.18947368 -0.21052632 -0.11578947 -0.14736842\nEleopalu  2.13157895 -1.18421053 -0.89473684  0.32894737 -1.55263158\nElymrepe -0.95789474  0.45263158  0.31578947 -0.75263158 -0.53684211\nEmpenigr -0.07368421 -0.09473684  0.21052632  0.25789474  0.34736842\nHyporadi -0.33157895 -0.42631579  0.52631579  0.52894737  1.03684211\nJuncarti  1.23157895 -0.43157895 -0.10526316  0.53157895 -0.76842105\nJuncbufo -0.16315789  0.54210526  0.89473684 -0.37631579 -0.37368421\nLolipere -1.71578947  0.83157895 -0.36842105 -1.36315789  0.60000000\nPlanlanc -0.95789474  2.45263158 -1.05263158 -0.27894737  0.93684211\nPoaprat  -1.13684211  0.25263158  0.00000000 -0.91578947  0.44210526\nPoatriv  -0.74210526  1.59473684  0.73684211 -1.82368421 -0.42631579\nRanuflam  1.37894737 -0.66315789 -0.31578947  0.64736842 -0.62105263\nRumeacet -0.66315789  3.25263158 -0.31578947 -0.52105263  0.07368421\nSagiproc -0.31578947 -0.31578947  2.42105263 -0.10526316  0.31578947\nSalirepe  0.64736842 -0.52105263 -0.10526316  1.94473684  0.70000000\nScorautu -0.62105263  0.07368421  0.31578947  0.70000000  2.43157895\nTrifprat -0.33157895  1.99473684 -0.47368421 -0.26052632  0.14210526\nTrifrepe -0.57368421  0.82631579 -0.21052632 -0.72894737  1.16315789\nVicilath -0.14736842 -0.18947368  0.00000000  0.04210526  0.37894737\nBracruta  0.08947368  1.04736842  0.10526316  1.05526316  0.72105263\nCallcusp  1.00000000 -0.47368421 -0.52631579  0.50000000 -0.68421053\nManure.L -0.10818318  0.09153962  0.14979210 -0.32038866 -0.30790598\nManure.Q  0.11112441 -0.47403704 -0.04219914  0.28625086 -0.08580493\nManure.C -0.27461885  0.04160892  0.11650497 -0.13730942  0.04160892\nManure^4 -0.12266820  0.33655121 -0.03145338  0.05189808  0.09750549\n            Trifprat    Trifrepe    Vicilath    Bracruta    Callcusp\nAchimill  0.56842105  1.02105263  0.04210526 -0.58947368 -0.42105263\nAgrostol -1.13684211 -1.56842105 -0.50526316  0.02105263  1.47368421\nAiraprae -0.11842105 -0.30263158 -0.05263158 -0.17105263 -0.13157895\nAlopgeni -0.85263158 -0.29473684 -0.37894737 -0.32631579 -0.31578947\nAnthodor  0.92368421  0.50789474 -0.01052632 -0.07631579 -0.55263158\nBellpere -0.09736842  0.55000000  0.07368421 -0.20263158 -0.34210526\nBromhord  0.06578947  1.03947368  0.05263158 -0.77631579 -0.39473684\nChenalbu -0.02368421 -0.01842105 -0.01052632 -0.12894737 -0.02631579\nCirsarve -0.04736842 -0.14210526 -0.02105263 -0.04736842 -0.05263158\nComapalu -0.09473684  0.24210526 -0.04210526 -0.09473684  0.31578947\nEleopalu -0.59210526 -1.14473684 -0.26315789  0.77631579  2.07894737\nElymrepe -0.19473684 -0.16315789 -0.27368421 -1.45789474 -0.68421053\nEmpenigr -0.04736842 -0.03684211 -0.02105263  0.05789474 -0.05263158\nHyporadi -0.21315789 -0.27105263  0.11578947  0.05000000 -0.23684211\nJuncarti -0.42631579 -1.01578947 -0.18947368  0.62631579  0.63157895\nJuncbufo -0.09736842  0.18157895 -0.13684211 -0.20263158 -0.34210526\nLolipere  1.04736842  1.40526316  0.54736842 -0.21578947 -1.52631579\nPlanlanc  1.75263158  0.88947368  0.35789474  1.17368421 -0.68421053\nPoaprat   0.28421053  0.74736842  0.28421053 -0.61052632 -1.26315789\nPoatriv   0.71842105  1.20789474 -0.45263158 -1.91315789 -1.34210526\nRanuflam -0.33157895 -0.57368421 -0.14736842  0.08947368  1.00000000\nRumeacet  1.99473684  0.82631579 -0.18947368  1.04736842 -0.47368421\nSagiproc -0.47368421 -0.21052632  0.00000000  0.10526316 -0.52631579\nSalirepe -0.26052632 -0.72894737  0.04210526  1.05526316  0.50000000\nScorautu  0.14210526  1.16315789  0.37894737  0.72105263 -0.68421053\nTrifprat  1.52368421  0.62368421 -0.09473684  0.83947368 -0.23684211\nTrifrepe  0.62368421  3.60789474  0.24210526 -0.16578947  0.02631579\nVicilath -0.09473684  0.24210526  0.27368421  0.32631579 -0.10526316\nBracruta  0.83947368 -0.16578947  0.32631579  3.62894737 -0.02631579\nCallcusp -0.23684211  0.02631579 -0.10526316 -0.02631579  1.52631579\nManure.L  0.07073516 -0.17059656 -0.06657427 -0.22884904 -0.14147032\nManure.Q -0.24405171 -0.38049561 -0.02250621 -0.07525514  0.13363062\nManure.C -0.02912624  0.26213617  0.09986140 -0.01248267 -0.17475745\nManure^4  0.19972899  0.12738621 -0.07548812  0.10536884 -0.04718008\n            Manure.L     Manure.Q    Manure.C     Manure^4\nAchimill  0.00000000 -0.230688651  0.11650497  0.106941507\nAgrostol  0.38280203  0.222248822 -0.24965350 -0.245336399\nAiraprae -0.14563121  0.130114026 -0.06241337  0.023590038\nAlopgeni  0.63245553 -0.343219700 -0.06657427 -0.050325415\nAnthodor -0.21220547 -0.100574625  0.02080446  0.130531545\nBellpere  0.08737872 -0.027429443  0.15395299  0.155694253\nBromhord  0.12898764 -0.200445931  0.17891834  0.070770115\nChenalbu  0.02080446 -0.016176338 -0.02912624 -0.026735377\nCirsarve  0.07489605  0.052045610  0.04160892  0.009436015\nComapalu -0.11650497  0.104091221 -0.04993070  0.018872031\nEleopalu -0.12898764  0.144180407 -0.51178967 -0.259490422\nElymrepe  0.40776738 -0.026726124  0.50762878  0.185574968\nEmpenigr -0.05825248  0.052045610 -0.02496535  0.009436015\nHyporadi -0.22884904  0.149806959 -0.01248267 -0.020444700\nJuncarti -0.10818318  0.004219914 -0.14147032 -0.261063091\nJuncbufo  0.07073516 -0.266557923  0.02080446 -0.095932823\nLolipere  0.69070802 -0.277107709  0.67406445 -0.103796169\nPlanlanc -0.07489605 -0.350252891  0.05825248  0.091214815\nPoaprat   0.46601987 -0.185676231  0.41608917 -0.182429630\nPoatriv   0.87794814 -0.765914454  0.11234407  0.045607407\nRanuflam -0.10818318  0.111124411 -0.27461885 -0.122668199\nRumeacet  0.09153962 -0.474037045  0.04160892  0.336551214\nSagiproc  0.14979210 -0.042199143  0.11650497 -0.031453384\nSalirepe -0.32038866  0.286250856 -0.13730942  0.051898084\nScorautu -0.30790598 -0.085804925  0.04160892  0.097505492\nTrifprat  0.07073516 -0.244051713 -0.02912624  0.199728991\nTrifrepe -0.17059656 -0.380495610  0.26213617  0.127386207\nVicilath -0.06657427 -0.022506210  0.09986140 -0.075488123\nBracruta -0.22884904 -0.075255139 -0.01248267  0.105368838\nCallcusp -0.14147032  0.133630621 -0.17475745 -0.047180077\nManure.L  0.21973684 -0.054490209  0.01447368 -0.017406259\nManure.Q -0.05449021  0.220112782 -0.01445659 -0.004623449\nManure.C  0.01447368 -0.014456586  0.18815789  0.012433042\nManure^4 -0.01740626 -0.004623449  0.01243304  0.198308271\n\n\nCode\nsolve(SXX) %*% t(SYX)\n\n\n           Achimill   Agrostol    Airaprae      Alopgeni   Anthodor   Bellpere\nManure.L -0.2635231  2.1081851 -0.52704628  2.687936e+00 -1.1067972 0.42163702\nManure.Q -1.0690450  1.4253933  0.44543540 -9.354143e-01 -0.7126966 0.04454354\nManure.C  0.5270463 -1.3176157 -0.26352314 -6.324555e-01  0.1054093 0.73786479\nManure^4  0.4581710 -0.9362624  0.09960238 -3.330669e-16  0.5378529 0.77689860\n           Bromhord    Chenalbu   Cirsarve    Comapalu   Eleopalu  Elymrepe\nManure.L  0.3689324  0.07905694 0.42163702 -0.42163702 -0.4216370 1.8973666\nManure.Q -0.7572402 -0.06681531 0.35634832  0.35634832  0.3563483 0.5345225\nManure.C  0.8432740 -0.15811388 0.21081851 -0.21081851 -2.5825268 2.5298221\nManure^4  0.3187276 -0.11952286 0.07968191  0.07968191 -1.1753081 0.9561829\n            Empenigr    Hyporadi   Juncarti    Juncbufo   Lolipere   Planlanc\nManure.L -0.21081851 -0.94868330 -0.6061032 -0.02635231  2.7933453 -0.7642171\nManure.Q  0.17817416  0.44543540 -0.2004459 -1.22494736 -0.3563483 -1.7594698\nManure.C -0.10540926  0.05270463 -0.6324555  0.05270463  3.3730962  0.2108185\nManure^4  0.03984095 -0.17928429 -1.3346719 -0.51793240 -0.4980119  0.3386481\n            Poaprat    Poatriv   Ranuflam    Rumeacet      Sagiproc   Salirepe\nManure.L  1.8446620  3.3730962 -0.3689324  0.02635231  6.324555e-01 -1.1595018\nManure.Q -0.2672612 -2.6280689  0.3118048 -2.11581817  2.862294e-17  0.9799579\nManure.C  2.1081851  0.1054093 -1.3703203 -0.05270463  5.797509e-01 -0.5797509\nManure^4 -0.8964215  0.4581710 -0.5577734  1.65339958 -1.394433e-01  0.2191252\n           Scorautu   Trifprat   Trifrepe   Vicilath    Bracruta   Callcusp\nManure.L -1.5811388  0.1581139 -1.3176157 -0.4216370 -1.15950181 -0.5006940\nManure.Q -0.7572402 -1.0690450 -1.9599158 -0.1781742 -0.62360956  0.4231636\nManure.C  0.2635231 -0.3162278  1.3176157  0.5797509 -0.05270463 -0.8432740\nManure^4  0.3187276  1.0159443  0.3984095 -0.4581710  0.41833001 -0.2191252\n\n\nCode\nMASS::ginv(SYY) %*% SYX %*% solve(SXX) %*% t(SYX)\n\n\n           Achimill     Agrostol      Airaprae    Alopgeni      Anthodor\n [1,]  0.2553953498 -0.566056529 -0.0235758370 -0.13981122  0.2551672789\n [2,] -0.1074363815  0.024360688  0.0637690028 -0.30226489 -0.0461112640\n [3,] -0.1214943762  0.209655866  0.0545456770 -0.02283602 -0.0560152651\n [4,]  0.0640146357 -0.079583389 -0.0124734308  0.07883042  0.0451045991\n [5,] -0.1039710613  0.134614007  0.0039384821 -0.11960332 -0.1183144244\n [6,] -0.0184676690  0.002904284  0.0318721309 -0.04076162  0.0105736897\n [7,]  0.0318202098 -0.048724852  0.0035897108  0.02593328  0.0403841403\n [8,]  0.0376557980 -0.049163835 -0.0304950062  0.02732831  0.0051449980\n [9,] -0.0509812305  0.110404842  0.0252212451  0.03383952 -0.0240734622\n[10,] -0.2282120388  0.233493944  0.1289118125 -0.31290670 -0.1043618829\n[11,] -0.1012833657  0.520324402 -0.0735999234  0.54673924 -0.1834585296\n[12,]  0.1437322923 -0.360923216 -0.0358658814 -0.16014347  0.0874978053\n[13,] -0.1970132883  0.498917002  0.0028048509  0.21345652 -0.2152180220\n[14,]  0.1057187199 -0.275612309 -0.0385606760 -0.14992543  0.0724014984\n[15,]  0.1113474086 -0.370466365  0.0094440451 -0.28259646  0.1542952501\n[16,] -0.0903053521  0.140381926  0.0018080492 -0.07043703 -0.0757854227\n[17,]  0.0084200109  0.078111056 -0.0644943106  0.14668270 -0.0984861886\n[18,] -0.0098685971  0.011903857 -0.0149895025 -0.02697927 -0.0112497733\n[19,] -0.2737838417  0.603854863  0.0306993509  0.13987639 -0.2417579557\n[20,]  0.0243777994  0.206718477 -0.1012100876  0.40888099 -0.0908741304\n[21,]  0.1540613834 -0.423795166 -0.0052992300 -0.22741065  0.1548409393\n[22,]  0.1005360742 -0.118572197 -0.0099550274  0.13901614  0.0886664335\n[23,]  0.1197986243  0.004695917 -0.1025564047  0.38208331  0.0002410711\n[24,] -0.1948759296  0.395877466  0.0562787477  0.06232530 -0.1505493822\n[25,]  0.0365253607 -0.184770793  0.0669592116 -0.17934896  0.1473583024\n[26,] -0.0005287557  0.036793534  0.0419322840  0.08722355  0.0548908856\n[27,]  0.0836586170 -0.295209984  0.0008778118 -0.24275628  0.0953610752\n[28,]  0.2937258190 -0.635473366 -0.1087337290 -0.17494521  0.1871042127\n[29,]  0.1091492320 -0.252440145 -0.0405547260 -0.09277425  0.0620311476\n[30,]  0.0627656759 -0.031196339 -0.0529689377  0.13540857  0.0126866088\n           Bellpere    Bromhord      Chenalbu      Cirsarve      Comapalu\n [1,]  0.1066389046  0.16969782 -0.0233736630 -5.612587e-02 -0.0188606696\n [2,] -0.0081768062 -0.07774045 -0.0168079240  3.039537e-02  0.0510152022\n [3,] -0.0781725441 -0.13194554  0.0111258838 -4.859728e-05  0.0436365416\n [4,]  0.0818280895  0.05663079 -0.0081331552  7.883839e-03 -0.0099787447\n [5,] -0.0293746945 -0.03509799 -0.0035438118  3.813908e-02  0.0031507857\n [6,]  0.0512532397 -0.02048223 -0.0103802903  1.841040e-02  0.0254977047\n [7,]  0.0201802113  0.01043034 -0.0012869958 -8.512555e-03  0.0028717686\n [8,] -0.0065850366  0.04335374  0.0025231700 -8.166415e-03 -0.0243960050\n [9,] -0.0107933679 -0.05544800  0.0037396306  6.715452e-03  0.0201769961\n[10,] -0.0006866104 -0.19206745 -0.0187148971  6.168804e-02  0.1031294500\n[11,] -0.1609908621 -0.07242155  0.0565555827 -1.228949e-02 -0.0588799387\n[12,]  0.1655502353  0.17188010 -0.0357891538  2.639086e-02 -0.0286927051\n[13,] -0.0883484190 -0.12777517  0.0250712682  4.314111e-02  0.0022438807\n[14,]  0.0008193771  0.10038884 -0.0087522546 -2.591118e-02 -0.0308485408\n[15,] -0.0445636953  0.04416416 -0.0082627619 -6.322997e-02  0.0075552361\n[16,] -0.1591843509 -0.08211772  0.0207502574 -2.470054e-02  0.0014464394\n[17,]  0.0960137062  0.09770660 -0.0081011792  5.902605e-02 -0.0515954484\n[18,] -0.1055703384 -0.02087131  0.0151954335 -3.321245e-02 -0.0119916020\n[19,] -0.2279002390 -0.22919344  0.0431296463  9.626670e-03  0.0245594807\n[20,] -0.0664920506  0.05449735  0.0336845149 -1.240035e-02 -0.0809680701\n[21,]  0.1059553120  0.12482006 -0.0293440464 -1.427726e-02 -0.0042393840\n[22,]  0.1102098320  0.06867339 -0.0086623024 -1.527845e-04 -0.0079640219\n[23,]  0.0446607343  0.13210292  0.0149888396 -7.020265e-03 -0.0820451237\n[24,] -0.0388642699 -0.15437692  0.0090179725  4.641590e-02  0.0450229982\n[25,] -0.0528405250 -0.06578443 -0.0003480582 -6.096378e-02  0.0535673693\n[26,]  0.0253068894 -0.05219525  0.0019997033 -1.020180e-02  0.0335458272\n[27,]  0.0187884809  0.06378334 -0.0167164303 -2.129817e-02  0.0007022494\n[28,]  0.0793592669  0.27925305 -0.0223763607 -4.771774e-02 -0.0869869832\n[29,]  0.0610146297  0.11906068 -0.0151446040 -3.095174e-03 -0.0324437808\n[30,] -0.0361414584  0.05434923  0.0136927581 -2.696732e-02 -0.0423751502\n          Eleopalu    Elymrepe      Empenigr      Hyporadi      Juncarti\n [1,] -0.341781132  0.07519371 -0.0094303348  0.0003047969 -0.1299998753\n [2,] -0.035895681  0.07260390  0.0255076011  0.1188422550  0.0307524380\n [3,]  0.275329366 -0.22006249  0.0218182708  0.0335488686  0.0686149760\n [4,] -0.130028783  0.11865299 -0.0049893723 -0.0476982999 -0.1348658832\n [5,] -0.032285688  0.13331539  0.0015753928  0.0598325134  0.0893444692\n [6,] -0.041695943  0.06178040  0.0127488523  0.0079240317 -0.1014349519\n [7,] -0.006110701 -0.02385700  0.0014358843 -0.0164121004 -0.0468587666\n [8,] -0.049008976  0.02850341 -0.0121980025 -0.0124154186  0.0355243619\n [9,]  0.110450804 -0.07030168  0.0100884980 -0.0044716547 -0.0180756386\n[10,]  0.110591948  0.02539494  0.0515647250  0.1349555167 -0.0415697836\n[11,]  0.487307191 -0.31267331 -0.0294399694 -0.1514551174  0.1960187367\n[12,] -0.522721138  0.45156392 -0.0143463525  0.0139556069 -0.1724006291\n[13,]  0.308147830 -0.07787846  0.0011219404 -0.0357310592  0.1001099675\n[14,] -0.205284813  0.08483012 -0.0154242704  0.0398661875  0.0724515392\n[15,] -0.074598625 -0.14625354  0.0037776181  0.0904620155  0.1098630346\n[16,]  0.253704017 -0.24444300  0.0007232197  0.0600817168  0.2628839669\n[17,] -0.264899358  0.39667394 -0.0257977242 -0.0642186766 -0.0948535731\n[18,]  0.143372496 -0.18522602 -0.0059958010  0.0269347729  0.1812228840\n[19,]  0.597374069 -0.38983560  0.0122797404  0.0197631542  0.2984547415\n[20,]  0.141067951 -0.06377296 -0.0404840350 -0.1327220897  0.1120415473\n[21,] -0.365906555  0.20463873 -0.0021196920  0.0437918382 -0.1104057266\n[22,] -0.129830700  0.09273213 -0.0039820110 -0.0778991963 -0.2024974707\n[23,] -0.086780577  0.10516386 -0.0410225619 -0.1523451611 -0.0561894786\n[24,]  0.254540414 -0.06796508  0.0225114991  0.0075557008 -0.0005530375\n[25,]  0.169917252 -0.34080897  0.0267836846  0.0632345663  0.0302898289\n[26,]  0.133020379 -0.14724051  0.0167729136 -0.0358161094 -0.1163394931\n[27,] -0.198314853  0.06773019  0.0003511247  0.0711327153  0.0240214823\n[28,] -0.551224024  0.29681862 -0.0434934916  0.0148184039  0.0252295035\n[29,] -0.287177535  0.20920265 -0.0162218904  0.0105738888 -0.0280878338\n[30,]  0.026593859 -0.06331856 -0.0211875751 -0.0483592731  0.0732872734\n           Juncbufo     Lolipere    Planlanc     Poaprat      Poatriv\n [1,]  0.1057616263  0.008558510  0.34950876 -0.06653161  0.136649984\n [2,] -0.1241409594  0.030223005 -0.17965527  0.03462634 -0.526514502\n [3,] -0.0756635723 -0.336877568 -0.13242722 -0.20505235 -0.210195935\n [4,] -0.0201085454  0.021209131  0.05872486 -0.03916482  0.177920292\n [5,] -0.0383192959  0.296756892 -0.17391845  0.25074413 -0.257035270\n [6,] -0.0945909179 -0.100798199 -0.05335908 -0.10973435 -0.091876218\n [7,] -0.0035446639 -0.100091241  0.04725591 -0.09098313  0.062728804\n [8,]  0.0665055948  0.144955224  0.05629003  0.11442231  0.098637323\n [9,] -0.0564897399 -0.176218181 -0.06448754 -0.12762112 -0.037605003\n[10,] -0.2887016395 -0.260076175 -0.36143852 -0.19036490 -0.724617129\n[11,]  0.1391021108 -0.089851120 -0.05318107  0.03090082  0.587423364\n[12,] -0.0002524638  0.510200921  0.10358593  0.28326804  0.008890313\n[13,] -0.0628351423 -0.011175563 -0.26300182  0.05045616  0.031784391\n[14,]  0.1233128927  0.300694741  0.14766120  0.22989982 -0.029435192\n[15,]  0.1138695220 -0.066333593  0.20160786 -0.02104419 -0.224979696\n[16,]  0.0844066332 -0.019398749 -0.05790319  0.09119570 -0.192410486\n[17,] -0.0062104082  0.561958662 -0.07140820  0.36733699  0.245256928\n[18,]  0.1069615552 -0.004334212  0.04260848  0.06567677 -0.036824702\n[19,] -0.0197575486 -0.258068488 -0.29573499 -0.04605761 -0.171418441\n[20,]  0.1782231626  0.225581902  0.07376989  0.21078417  0.590708908\n[21,]  0.0240363053  0.182113977  0.17245036  0.07053629 -0.090475882\n[22,] -0.0284681716 -0.107701398  0.11176613 -0.15812587  0.287461337\n[23,]  0.1442038250  0.262011044  0.16306925  0.16489642  0.670355281\n[24,] -0.1637576475 -0.219554631 -0.27828107 -0.13985264 -0.186979353\n[25,] -0.0045076136 -0.564873006  0.11931727 -0.39816673 -0.234704170\n[26,] -0.0802769897 -0.442962964  0.01528465 -0.35767070  0.066863795\n[27,]  0.0491293373  0.142719425  0.10926682  0.09855637 -0.195454056\n[28,]  0.2632035321  0.689356233  0.38300656  0.47359456  0.209231759\n[29,]  0.0718390801  0.351402838  0.11792649  0.22903203  0.047587286\n[30,]  0.1306050777  0.100790582  0.11925359  0.09777863  0.270025926\n          Ranuflam      Rumeacet      Sagiproc     Salirepe     Scorautu\n [1,] -0.177963317  0.4702159920 -0.0253248804 -0.051866841  0.284751637\n [2,]  0.001182860 -0.3433642059  0.0069167956  0.140291806 -0.006028267\n [3,]  0.154028386 -0.1578416346 -0.0541835431  0.120000489 -0.066296154\n [4,] -0.068756421  0.2087067662  0.0002501493 -0.027441548  0.015467644\n [5,] -0.014961300 -0.3763347167  0.0629353132  0.008664661 -0.085534823\n [6,] -0.011286332  0.0319515965 -0.0190613577  0.070118688 -0.008802771\n [7,] -0.001978437  0.1234398621 -0.0213852293  0.007897364  0.027869892\n [8,] -0.033652990  0.0161844344  0.0252693839 -0.067089014  0.017696792\n [9,]  0.062791775 -0.0143440716 -0.0283079943  0.055486739 -0.045499875\n[10,]  0.093969518 -0.4399423281 -0.0384195598  0.283605987 -0.102359933\n[11,]  0.221573619 -0.0309958749  0.0076628535 -0.161919831 -0.246137545\n[12,] -0.272120333  0.1204626564  0.0799943079 -0.078904939  0.114679619\n[13,]  0.154915370 -0.3182903382  0.0216292737  0.006170672 -0.254624747\n[14,] -0.114210609  0.0005530383  0.0441507932 -0.084833487  0.133843373\n[15,] -0.034466099  0.0547356041 -0.0302762256  0.020776899  0.231810626\n[16,]  0.127394423 -0.3171468167  0.0029898538  0.003977708 -0.025724232\n[17,] -0.151797972 -0.0562466717  0.1107593192 -0.141887483 -0.119485408\n[18,]  0.067189397 -0.1172748600 -0.0002587139 -0.032976906  0.026997596\n[19,]  0.307896840 -0.5079921929 -0.0204009089  0.067538572 -0.249296836\n[20,]  0.040170949  0.0835592753  0.0528117183 -0.222662193 -0.121529053\n[21,] -0.184543047  0.1852674262  0.0145076544 -0.011658306  0.194698633\n[22,] -0.067901858  0.3740458170 -0.0261608470 -0.021901060  0.036751000\n[23,] -0.074157210  0.3182743772  0.0501304807 -0.225624090 -0.045601071\n[24,]  0.144153832 -0.2599104871 -0.0229686218  0.123813245 -0.193067768\n[25,]  0.105046389  0.1577578415 -0.1163934675  0.147310265  0.166501684\n[26,]  0.079089875  0.2324709033 -0.0826218249  0.092251025  0.003323269\n[27,] -0.098894083 -0.0033077363  0.0131270341  0.001931186  0.153464576\n[28,] -0.308232131  0.2331473236  0.1010453126 -0.239214204  0.280824374\n[29,] -0.155755185  0.0637152345  0.0549129146 -0.089220397  0.097095929\n[30,] -0.002593752  0.1011526392  0.0177126662 -0.116531663  0.017588793\n          Trifprat    Trifrepe     Vicilath     Bracruta     Callcusp\n [1,]  0.226266681  0.50859821  0.045251286  0.221869003 -0.103127161\n [2,] -0.207272971 -0.10969118  0.057102277 -0.036293359  0.038852832\n [3,] -0.057658190 -0.24144320 -0.053313483 -0.033883638  0.109741599\n [4,]  0.125533082  0.05802814 -0.047847931  0.030435457 -0.041862269\n [5,] -0.233253273 -0.11411843  0.082265654 -0.109119476 -0.005117560\n [6,]  0.032972183 -0.06958753 -0.048671001  0.007469219  0.013480112\n [7,]  0.077515994  0.03040178 -0.031438601  0.036430276  0.001164608\n [8,] -0.006618172  0.09247054  0.039317384  0.001283563 -0.035123499\n [9,]  0.015135341 -0.13428480 -0.054627848 -0.016900701  0.046528635\n[10,] -0.219578741 -0.39959663 -0.042499169 -0.082382359  0.124331846\n[11,]  0.031131797 -0.30718199 -0.087342822 -0.156571013  0.066626855\n[12,]  0.016547534  0.31979456  0.089078585  0.052268725 -0.157579695\n[13,] -0.140390083 -0.42492134 -0.058925805 -0.188869891  0.079140596\n[14,] -0.053603688  0.30953764  0.133064566  0.060573535 -0.080241710\n[15,] -0.015295927  0.34243233  0.117749337  0.150616999 -0.011566622\n[16,] -0.193779197 -0.06743968  0.086687282 -0.050386512  0.064782041\n[17,] -0.047442008 -0.01245744  0.026211175 -0.115298235 -0.114595573\n[18,] -0.083106722  0.05268835  0.068882214  0.001287262  0.024600997\n[19,] -0.244445043 -0.50719132 -0.028684035 -0.191117162  0.172368030\n[20,]  0.053117944 -0.01586147 -0.006783968 -0.087981182 -0.040640578\n[21,]  0.060898187  0.35438239  0.075756294  0.128523811 -0.095451061\n[22,]  0.232518793  0.07750171 -0.097934242  0.069003255 -0.039923945\n[23,]  0.181863708  0.12665144 -0.033660573 -0.017510318 -0.098612448\n[24,] -0.098126287 -0.42878141 -0.095596069 -0.127750116  0.105844164\n[25,]  0.101240224  0.08916129 -0.032370653  0.155663994  0.092698722\n[26,]  0.176841845 -0.10780464 -0.133395504  0.056379886  0.064704308\n[27,] -0.045850981  0.26249777  0.105031231  0.084983690 -0.048920354\n[28,]  0.026986930  0.71095260  0.228821691  0.147023719 -0.219356303\n[29,] -0.007782301  0.26760134  0.092914813  0.042752042 -0.102210428\n[30,]  0.045842430  0.11950788  0.028102072  0.010342426 -0.033078238\n\n\nCode\neigen(SYX %*% solve(SXX) %*% t(SYX))\n\n\neigen() decomposition\n$values\n [1]  1.600287e+01  6.682976e+00  4.642312e+00  1.488510e+00  3.806000e-16\n [6]  2.128217e-16  1.727072e-16  1.565431e-16  1.542270e-16  1.045176e-16\n[11]  4.970024e-17  3.005714e-17  2.380640e-17  1.438291e-17  1.312568e-17\n[16]  1.156744e-17  2.485848e-18 -1.482933e-18 -1.873723e-17 -2.332531e-17\n[21] -2.466665e-17 -2.566925e-17 -2.657632e-17 -2.689641e-17 -2.779984e-17\n[26] -3.741185e-17 -1.050057e-16 -4.770834e-16 -6.836576e-16 -1.458364e-15\n\n$vectors\n              [,1]         [,2]          [,3]         [,4]         [,5]\n [1,]  0.078499203 -0.190098521 -0.0578356605  0.030357936  0.000000000\n [2,]  0.020745498  0.494096511 -0.0854105136 -0.082696325  0.335824642\n [3,] -0.092692686 -0.006605534  0.0434602254 -0.068346651  0.209145539\n [4,]  0.273835684  0.225703434 -0.3314948277 -0.091947783 -0.135932846\n [5,] -0.047715666 -0.224257060 -0.0365217243 -0.012298240  0.197336418\n [6,]  0.086761253 -0.067412384  0.0952235304 -0.227808859  0.172607766\n [7,]  0.135601810 -0.107937694  0.0049912566  0.019614923  0.015941101\n [8,]  0.001305205  0.022271431 -0.0353166532  0.030602270 -0.041874850\n [9,]  0.029967290  0.052463796  0.0616766687 -0.078719611  0.063318299\n[10,] -0.074154149 -0.005284427  0.0347681804 -0.054677321 -0.068171225\n[11,] -0.225338530  0.250082790 -0.3108032522  0.189525944 -0.016485761\n[12,]  0.301922680 -0.004926784  0.3632586906 -0.316945028 -0.167795192\n[13,] -0.037077075 -0.002642214  0.0173840902 -0.027338660  0.076609555\n[14,] -0.115574800 -0.043671418  0.1080021112  0.064473402 -0.144284243\n[15,] -0.092243408  0.077804931 -0.0920566656  0.438848005  0.026978680\n[16,]  0.074682973 -0.065709457 -0.1564050290  0.310777025 -0.139296055\n[17,]  0.481385630  0.082356094  0.3320446423  0.243104698  0.385029277\n[18,]  0.055835792 -0.269067570 -0.1769930392  0.158761460  0.232287230\n[19,]  0.308330860  0.108409171  0.1986377215  0.334878747  0.046629429\n[20,]  0.493545704  0.063385602 -0.4824457467 -0.032314562 -0.016853261\n[21,] -0.140477071  0.123059735 -0.1423635585  0.074258977  0.275143891\n[22,]  0.147353422 -0.319322012 -0.2997385650 -0.276327671  0.308520661\n[23,]  0.092680382  0.039215280  0.0591794312  0.042367737 -0.141636920\n[24,] -0.203923910 -0.014532174  0.0956124960 -0.150362633  0.317967833\n[25,] -0.081638343 -0.265832975  0.0008115454  0.100162038  0.005176438\n[26,]  0.072202712 -0.144840864 -0.2003736762 -0.223261027 -0.245170447\n[27,]  0.083154377 -0.414572189 -0.0188407248  0.266554784  0.077168991\n[28,]  0.002753903 -0.052956613  0.0794287385  0.226568741 -0.164007700\n[29,] -0.068697986 -0.201712273 -0.0451960018  0.007979922  0.088200758\n[30,] -0.125854147  0.057566547 -0.0451056440 -0.003878502  0.269949072\n             [,6]         [,7]          [,8]         [,9]        [,10]\n [1,]  0.00000000  0.000000000  8.231313e-01  0.000000000  0.000000000\n [2,] -0.09766017 -0.300923629 -2.089830e-01 -0.384861711  0.066178459\n [3,]  0.06486321  0.333907051 -2.801822e-01  0.228087264 -0.046949418\n [4,]  0.22205026  0.447165197 -1.775495e-02 -0.082314518  0.041991269\n [5,] -0.05668447  0.086167548 -1.386088e-01  0.347029919 -0.252213236\n [6,] -0.12355990  0.020087418  1.863344e-02 -0.130045657 -0.107629135\n [7,]  0.09338832  0.007255415 -1.279206e-01  0.035536577  0.268288029\n [8,] -0.08177657 -0.016624396  2.443895e-03  0.002634663 -0.131041675\n [9,]  0.23404932 -0.022414543  3.919045e-02  0.022549594  0.150860948\n[10,]  0.07032585  0.183779354  9.220795e-03 -0.303171721  0.007039689\n[11,]  0.13079255  0.114295669  1.473336e-01 -0.081495322 -0.127097789\n[12,] -0.14584105  0.040996028 -5.981748e-02  0.198499109 -0.089088807\n[13,]  0.08840488  0.042737779  4.344869e-03  0.013606383  0.041428707\n[14,]  0.12777092  0.117348502  1.507858e-02 -0.123316500  0.357638148\n[15,]  0.09560102 -0.055303618 -1.897387e-02  0.086198237 -0.175882884\n[16,] -0.26387860  0.115510553 -1.197500e-01 -0.143561163 -0.407685625\n[17,]  0.19071125  0.269874497  1.113709e-01 -0.155129046 -0.150403563\n[18,] -0.11179190  0.029902429 -1.086502e-01  0.161411856  0.244427642\n[19,]  0.22225810 -0.413722908  6.400584e-03  0.116627779 -0.027764969\n[20,] -0.12496521  0.091837351 -2.590705e-02  0.196842147  0.131227751\n[21,] -0.16593448 -0.133027412  1.083823e-01  0.342284284  0.008163118\n[22,]  0.06672601 -0.273657753  1.016487e-02 -0.214723242 -0.208762435\n[23,] -0.34578633  0.018048205 -5.937529e-05 -0.103159809 -0.097102440\n[24,] -0.10709605  0.204731356  1.413765e-01 -0.152104462 -0.205393262\n[25,]  0.52091915 -0.034314497 -1.721444e-01 -0.007838417 -0.178951081\n[26,]  0.24385466 -0.261306656 -5.782003e-02 -0.096038964 -0.133574026\n[27,] -0.23728936 -0.025331436 -1.220686e-01 -0.336316928  0.331817274\n[28,] -0.17121369  0.055847089 -6.923996e-02 -0.107675454 -0.023041623\n[29,]  0.08999629  0.201019462 -7.133340e-02 -0.183969386 -0.046171249\n[30,]  0.02444308  0.083883539  8.754340e-02  0.072324468  0.308855429\n            [,11]        [,12]        [,13]         [,14]        [,15]\n [1,]  0.00000000  0.000000000  0.000000000  0.000000e+00  0.000000000\n [2,] -0.08824454 -0.004830507 -0.020605911  6.241243e-03  0.034042600\n [3,]  0.18498496  0.002605067  0.141907459 -7.613082e-05 -0.079361172\n [4,] -0.20987165 -0.031333487 -0.302011201 -6.132515e-04 -0.232256836\n [5,] -0.29563108 -0.091693438  0.090901257  5.537830e-02  0.094733334\n [6,]  0.07623648  0.054596079 -0.112701325 -7.305111e-02  0.023766499\n [7,] -0.29905481 -0.056460155  0.163572152 -4.009418e-02  0.021103412\n [8,]  0.01108195  0.163162478 -0.031166677  4.336037e-01 -0.055405640\n [9,]  0.10509488  0.010831448 -0.123988960 -3.180720e-01  0.300666735\n[10,]  0.06193173  0.276620072 -0.081040050  4.168754e-01  0.048813032\n[11,] -0.24839684  0.069407184  0.165926673  3.333252e-02  0.373654151\n[12,] -0.15010983 -0.013685382 -0.153485369 -2.837017e-02  0.334991031\n[13,]  0.02247303  0.241515739 -0.004675891 -9.066823e-02  0.052047685\n[14,] -0.19827262 -0.108501996 -0.197986638  1.091992e-01  0.094056476\n[15,]  0.26229249 -0.376275089 -0.162284452 -1.106881e-01 -0.002143177\n[16,] -0.04304737 -0.099571762 -0.208906624 -1.952501e-01  0.122880874\n[17,]  0.09348664  0.016973449  0.139407447 -1.693887e-02 -0.253383878\n[18,] -0.07517187  0.103365831 -0.576396387  1.079748e-01 -0.059500417\n[19,] -0.31996107  0.101919701 -0.131014584  8.463239e-02  0.178938366\n[20,]  0.11231690  0.052158153  0.339826984  6.327628e-02  0.283480398\n[21,] -0.20151770  0.351721348  0.026663404  1.300641e-02 -0.323274959\n[22,]  0.18342971  0.021214147 -0.183548377  4.223566e-02  0.134316766\n[23,]  0.07317302 -0.019239295 -0.013517342  2.463020e-01  0.071891088\n[24,] -0.40605428 -0.135501127  0.075339485 -3.872399e-02  0.126172472\n[25,]  0.08806321  0.156330157  0.128623479  2.760888e-01  0.177115512\n[26,] -0.17686880 -0.004502999  0.110966958 -2.655288e-01 -0.358782298\n[27,] -0.12517721 -0.162943625  0.310786755  2.497048e-02 -0.042463667\n[28,]  0.07471837  0.640449569  0.039873444 -3.647963e-01  0.065157397\n[29,] -0.16227786  0.161916411 -0.067055341 -2.655039e-01  0.084382669\n[30,]  0.24719262 -0.013317198 -0.034635054 -1.280444e-01  0.239254624\n             [,16]        [,17]        [,18]        [,19]        [,20]\n [1,]  0.000000000  0.000000000  0.000000000  0.000000000  0.000000000\n [2,] -0.017309223 -0.092260969 -0.011930356 -0.044030728  0.007528000\n [3,]  0.013660911 -0.068256137  0.001803999  0.100531269 -0.045370801\n [4,] -0.034791423 -0.029356495 -0.014088006 -0.114173947  0.032476315\n [5,] -0.199846420 -0.060969452  0.002099377 -0.350932513  0.134437864\n [6,]  0.039121436  0.280840169 -0.088481897  0.107018972 -0.463137961\n [7,] -0.049597011  0.588824810  0.145851725  0.058827957 -0.021719619\n [8,] -0.103660379 -0.018583554  0.666683158  0.168476792  0.280590504\n [9,]  0.546922368 -0.187841137  0.315077000 -0.120981298  0.211113270\n[10,] -0.013647932  0.043577349 -0.123154784  0.335509621  0.075991593\n[11,] -0.083723707  0.155933349 -0.049537010 -0.046273904 -0.169257573\n[12,] -0.072810532 -0.085095477 -0.006328523  0.345625996 -0.111723192\n[13,] -0.139264108 -0.039200824  0.551961442 -0.168741164 -0.547537784\n[14,]  0.004904684 -0.202860304 -0.028124664 -0.093338358 -0.250741696\n[15,] -0.257925291 -0.180601612  0.059406090  0.308607086 -0.181720380\n[16,]  0.337386227  0.271934339  0.132703838  0.103617664 -0.017710818\n[17,] -0.001272996 -0.005971555  0.012370626 -0.072329996 -0.008191735\n[18,] -0.043249534 -0.088742135 -0.049339723 -0.040033060 -0.053281886\n[19,] -0.090977930 -0.037643988 -0.075845504  0.075879938  0.060268069\n[20,]  0.023121338 -0.210552242 -0.049417918  0.131393149 -0.063672258\n[21,]  0.389233809 -0.032769347 -0.083404477  0.258739614 -0.156572519\n[22,] -0.120251477  0.079942795 -0.026804850 -0.140365329  0.064474801\n[23,]  0.140771326 -0.266774018 -0.070490342 -0.239760136 -0.209477230\n[24,] -0.050617978 -0.300846262  0.054281814  0.107069485  0.012949766\n[25,]  0.250149435 -0.077776749 -0.155066620 -0.001351224 -0.183174376\n[26,] -0.118754362 -0.255214081  0.094013682  0.236271793 -0.133312381\n[27,]  0.091190949 -0.138926404  0.098520637  0.100467426 -0.042964943\n[28,] -0.300688586 -0.106440837 -0.093084972 -0.161967959  0.086116622\n[29,]  0.027733244 -0.097818391 -0.069350121  0.275656681  0.215261723\n[30,] -0.238814438  0.064350668  0.010945556  0.250820914  0.085678294\n              [,21]        [,22]       [,23]         [,24]         [,25]\n [1,]  0.0000000000  0.000000000  0.00000000  0.0000000000  0.000000e+00\n [2,] -0.0076967754 -0.043377685  0.08291436  0.0064476228  3.812788e-03\n [3,] -0.0028236350  0.101839999  0.05938124  0.0001660026  8.716313e-02\n [4,]  0.0314712680 -0.073296995 -0.13371895  0.1229985035  4.956358e-02\n [5,]  0.1823218811 -0.251110654  0.03139608 -0.1018221716  1.278619e-01\n [6,] -0.1913724060 -0.215592972 -0.24847109  0.3210632726  4.250863e-02\n [7,]  0.1831037974  0.528510580 -0.15278934  0.0253434087  1.348047e-02\n [8,] -0.0296655064 -0.018898633  0.06392478  0.3668686224  2.245424e-02\n [9,] -0.0916567992  0.161312909 -0.03223900 -0.0580824778  1.412814e-01\n[10,]  0.0555597829  0.014534535 -0.12738487 -0.4874194974  1.939517e-01\n[11,] -0.2329328247  0.107573927  0.46268698  0.0295278566 -8.379928e-02\n[12,] -0.0583454378  0.073363262  0.39053897 -0.0429761813  5.915721e-02\n[13,]  0.0919883699 -0.147055422 -0.09343120 -0.3698530626 -1.839813e-01\n[14,]  0.3846381407 -0.107106974  0.15889744  0.1589007006  3.933817e-01\n[15,]  0.0577165079  0.259017064 -0.10780060 -0.1148391442  1.285184e-01\n[16,]  0.1350639936 -0.196365923  0.00563935  0.0193648104  1.438756e-01\n[17,]  0.0297259655  0.054937098  0.28565014  0.0134136787  1.073566e-02\n[18,] -0.4056775540  0.176269647  0.10961422 -0.0015258475 -1.734550e-01\n[19,]  0.0564031219 -0.174143983 -0.27681860 -0.0092643329 -6.553237e-02\n[20,] -0.0172195695 -0.052485237 -0.19529159 -0.0497273051  5.555880e-02\n[21,]  0.2504243901  0.050348295  0.11750394 -0.0060646449  1.670237e-01\n[22,]  0.2563547751  0.178828592  0.15842041 -0.0816874814  2.174145e-01\n[23,]  0.3628930633  0.382388472 -0.04604928  0.1748322548 -3.699784e-01\n[24,] -0.1586279268  0.234675890 -0.39476705  0.0895846522  1.310747e-01\n[25,] -0.0749985540  0.008114536 -0.06873104  0.2990666813 -3.766832e-02\n[26,]  0.0008737574  0.063465682  0.13912202  0.1248488340  2.843909e-05\n[27,] -0.1596844715 -0.168081262  0.12811874 -0.0337896573  7.494226e-02\n[28,] -0.0451514002  0.174052227 -0.07211700  0.1629742417  2.899819e-01\n[29,]  0.2835483322 -0.092323260  0.02446074 -0.0724051929 -5.601578e-01\n[30,]  0.2824110321 -0.192566805  0.02302493  0.3617407566 -7.388923e-02\n             [,26]         [,27]       [,28]        [,29]        [,30]\n [1,]  0.000000000  0.000000e+00  0.00000000  0.000000000 -0.525251131\n [2,] -0.008107564  8.372435e-03 -0.16589615  0.163552721 -0.498598994\n [3,] -0.023160047 -5.945670e-02  0.28810133 -0.543546803 -0.459276994\n [4,]  0.032382035 -8.315153e-02  0.39956151  0.248496949 -0.037398846\n [5,]  0.103285432  4.321236e-01 -0.05390966  0.219651748 -0.139874065\n [6,] -0.233204781  4.428690e-01  0.08583803 -0.040286425  0.042913426\n [7,]  0.011471409  5.808606e-02 -0.09495563  0.076241125 -0.140552231\n [8,] -0.205861308  7.967616e-02 -0.01815272 -0.021584959  0.001621931\n [9,]  0.039292650  3.435501e-01  0.10622223  0.018623249  0.035566093\n[10,]  0.255651423  3.091409e-01 -0.02187001 -0.006995588 -0.001708299\n[11,] -0.010227129  1.527373e-01  0.15119928 -0.150685522  0.151879243\n[12,]  0.010158537 -1.411732e-01  0.10332650  0.249281054 -0.105152523\n[13,]  0.089085409 -1.903132e-01  0.01269987  0.052076053 -0.001270264\n[14,] -0.251404607  3.111220e-03 -0.26612710 -0.226177779  0.013996918\n[15,] -0.210045873  1.674758e-01  0.06974956  0.250462175 -0.036178774\n[16,]  0.260676480 -1.741982e-01 -0.17958535 -0.134842571 -0.117535734\n[17,]  0.061281013  6.248894e-02 -0.21421532 -0.010547168  0.194157874\n[18,]  0.137483661  3.803749e-02 -0.23537427 -0.088463452 -0.035877632\n[19,]  0.048581532 -1.373928e-02  0.28499313 -0.334644760  0.014358290\n[20,] -0.129076441 -8.216548e-05 -0.28413311 -0.076760403  0.061475538\n[21,] -0.054950307 -3.526583e-02  0.11899905  0.166964709  0.124283516\n[22,] -0.120256732 -1.895218e-01  0.21680567 -0.097738125  0.170553899\n[23,]  0.196188254  1.984850e-01  0.16666334 -0.048203685 -0.004502206\n[24,]  0.101718094 -2.422489e-01 -0.08858201 -0.056034461  0.177118312\n[25,]  0.075409794 -2.021581e-01 -0.09647907  0.325942254 -0.180061942\n[26,]  0.334112420  2.433536e-01 -0.12940253 -0.184743040 -0.018240013\n[27,]  0.029110833  7.128233e-04  0.40183503  0.137567888 -0.011346247\n[28,] -0.034913566 -1.227053e-03  0.03510637  0.082319125 -0.084580643\n[29,] -0.402918063  6.731143e-02 -0.07531817  0.003587172 -0.043613597\n[30,]  0.500741835 -3.756857e-03  0.04843076  0.071349833  0.102289958\n\n\n\n\nCode\nXc <- model.matrix(~Manure, data = dune.env)[,-1, drop = F] %>% scale(scale = FALSE)\n\nmod <- lm(Y ~ Xc)\ncoef(mod)[-1, ]\n\n\n             Achimill   Agrostol    Airaprae      Alopgeni   Anthodor\nXcManure.L -0.2635231  2.1081851 -0.52704628  2.687936e+00 -1.1067972\nXcManure.Q -1.0690450  1.4253933  0.44543540 -9.354143e-01 -0.7126966\nXcManure.C  0.5270463 -1.3176157 -0.26352314 -6.324555e-01  0.1054093\nXcManure^4  0.4581710 -0.9362624  0.09960238 -4.031093e-16  0.5378529\n             Bellpere   Bromhord    Chenalbu   Cirsarve    Comapalu   Eleopalu\nXcManure.L 0.42163702  0.3689324  0.07905694 0.42163702 -0.42163702 -0.4216370\nXcManure.Q 0.04454354 -0.7572402 -0.06681531 0.35634832  0.35634832  0.3563483\nXcManure.C 0.73786479  0.8432740 -0.15811388 0.21081851 -0.21081851 -2.5825268\nXcManure^4 0.77689860  0.3187276 -0.11952286 0.07968191  0.07968191 -1.1753081\n            Elymrepe    Empenigr    Hyporadi   Juncarti    Juncbufo   Lolipere\nXcManure.L 1.8973666 -0.21081851 -0.94868330 -0.6061032 -0.02635231  2.7933453\nXcManure.Q 0.5345225  0.17817416  0.44543540 -0.2004459 -1.22494736 -0.3563483\nXcManure.C 2.5298221 -0.10540926  0.05270463 -0.6324555  0.05270463  3.3730962\nXcManure^4 0.9561829  0.03984095 -0.17928429 -1.3346719 -0.51793240 -0.4980119\n             Planlanc    Poaprat    Poatriv   Ranuflam    Rumeacet\nXcManure.L -0.7642171  1.8446620  3.3730962 -0.3689324  0.02635231\nXcManure.Q -1.7594698 -0.2672612 -2.6280689  0.3118048 -2.11581817\nXcManure.C  0.2108185  2.1081851  0.1054093 -1.3703203 -0.05270463\nXcManure^4  0.3386481 -0.8964215  0.4581710 -0.5577734  1.65339958\n                Sagiproc   Salirepe   Scorautu   Trifprat   Trifrepe   Vicilath\nXcManure.L  6.324555e-01 -1.1595018 -1.5811388  0.1581139 -1.3176157 -0.4216370\nXcManure.Q -4.903159e-17  0.9799579 -0.7572402 -1.0690450 -1.9599158 -0.1781742\nXcManure.C  5.797509e-01 -0.5797509  0.2635231 -0.3162278  1.3176157  0.5797509\nXcManure^4 -1.394433e-01  0.2191252  0.3187276  1.0159443  0.3984095 -0.4581710\n              Bracruta   Callcusp\nXcManure.L -1.15950181 -0.5006940\nXcManure.Q -0.62360956  0.4231636\nXcManure.C -0.05270463 -0.8432740\nXcManure^4  0.41833001 -0.2191252\n\n\nCode\nsolve(t(Xc) %*% Xc) %*% t(Xc) %*% Y # Bhat\n\n\n           Achimill   Agrostol    Airaprae      Alopgeni   Anthodor   Bellpere\nManure.L -0.2635231  2.1081851 -0.52704628  2.687936e+00 -1.1067972 0.42163702\nManure.Q -1.0690450  1.4253933  0.44543540 -9.354143e-01 -0.7126966 0.04454354\nManure.C  0.5270463 -1.3176157 -0.26352314 -6.324555e-01  0.1054093 0.73786479\nManure^4  0.4581710 -0.9362624  0.09960238 -4.440892e-16  0.5378529 0.77689860\n           Bromhord    Chenalbu   Cirsarve    Comapalu   Eleopalu  Elymrepe\nManure.L  0.3689324  0.07905694 0.42163702 -0.42163702 -0.4216370 1.8973666\nManure.Q -0.7572402 -0.06681531 0.35634832  0.35634832  0.3563483 0.5345225\nManure.C  0.8432740 -0.15811388 0.21081851 -0.21081851 -2.5825268 2.5298221\nManure^4  0.3187276 -0.11952286 0.07968191  0.07968191 -1.1753081 0.9561829\n            Empenigr    Hyporadi   Juncarti    Juncbufo   Lolipere   Planlanc\nManure.L -0.21081851 -0.94868330 -0.6061032 -0.02635231  2.7933453 -0.7642171\nManure.Q  0.17817416  0.44543540 -0.2004459 -1.22494736 -0.3563483 -1.7594698\nManure.C -0.10540926  0.05270463 -0.6324555  0.05270463  3.3730962  0.2108185\nManure^4  0.03984095 -0.17928429 -1.3346719 -0.51793240 -0.4980119  0.3386481\n            Poaprat    Poatriv   Ranuflam    Rumeacet      Sagiproc   Salirepe\nManure.L  1.8446620  3.3730962 -0.3689324  0.02635231  6.324555e-01 -1.1595018\nManure.Q -0.2672612 -2.6280689  0.3118048 -2.11581817 -1.665335e-16  0.9799579\nManure.C  2.1081851  0.1054093 -1.3703203 -0.05270463  5.797509e-01 -0.5797509\nManure^4 -0.8964215  0.4581710 -0.5577734  1.65339958 -1.394433e-01  0.2191252\n           Scorautu   Trifprat   Trifrepe   Vicilath    Bracruta   Callcusp\nManure.L -1.5811388  0.1581139 -1.3176157 -0.4216370 -1.15950181 -0.5006940\nManure.Q -0.7572402 -1.0690450 -1.9599158 -0.1781742 -0.62360956  0.4231636\nManure.C  0.2635231 -0.3162278  1.3176157  0.5797509 -0.05270463 -0.8432740\nManure^4  0.3187276  1.0159443  0.3984095 -0.4581710  0.41833001 -0.2191252\n\n\n\n\nCode\ncancor(dune %>% data.matrix(), X)$xcoef[,1:2]\n\n\n                 [,1]        [,2]\nAchimill -0.313536924 -0.28268745\nAgrostol  0.026502657 -0.07312482\nAiraprae  0.419611515  0.43845223\nAlopgeni -0.097441053  0.05326747\nAnthodor  0.176390066  0.05548151\nBellpere  0.336914597 -0.05130062\nBromhord -0.063821149  0.14871992\nChenalbu -0.014424177  0.63065915\nCirsarve -0.189542024  0.07194240\nComapalu  0.026524473 -0.09771221\nEleopalu -0.019909717  0.09172470\nElymrepe -0.012242976  0.05459715\nEmpenigr -0.508709898 -0.12618018\nHyporadi -0.173510861 -0.28118945\nJuncarti  0.006636572 -0.03057490\nJuncbufo  0.138041170  0.01371598\nLolipere  0.216978887  0.14583359\nPlanlanc -0.148478263  0.01126204\nPoaprat  -0.194310491 -0.11462565\n\n\nCode\ndata(nutrimouse)\nX=as.matrix(nutrimouse$gene[,1:10])\nY=as.matrix(nutrimouse$lipid)\nres.cc = cc(X, Y)\n\nres.cc\n\n\n$cor\n [1] 0.9906993 0.9848735 0.9388864 0.9191073 0.8149742 0.7234679 0.6413248\n [8] 0.6057535 0.5469842 0.3607641\n\n$names\n$names$Xnames\n [1] \"X36b4\" \"ACAT1\" \"ACAT2\" \"ACBP\"  \"ACC1\"  \"ACC2\"  \"ACOTH\" \"ADISP\" \"ADSS1\"\n[10] \"ALDH3\"\n\n$names$Ynames\n [1] \"C14.0\"    \"C16.0\"    \"C18.0\"    \"C16.1n.9\" \"C16.1n.7\" \"C18.1n.9\"\n [7] \"C18.1n.7\" \"C20.1n.9\" \"C20.3n.9\" \"C18.2n.6\" \"C18.3n.6\" \"C20.2n.6\"\n[13] \"C20.3n.6\" \"C20.4n.6\" \"C22.4n.6\" \"C22.5n.6\" \"C18.3n.3\" \"C20.3n.3\"\n[19] \"C20.5n.3\" \"C22.5n.3\" \"C22.6n.3\"\n\n$names$ind.names\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\" \"11\" \"12\" \"13\" \"14\" \"15\"\n[16] \"16\" \"17\" \"18\" \"19\" \"20\" \"21\" \"22\" \"23\" \"24\" \"25\" \"26\" \"27\" \"28\" \"29\" \"30\"\n[31] \"31\" \"32\" \"33\" \"34\" \"35\" \"36\" \"37\" \"38\" \"39\" \"40\"\n\n\n$xcoef\n            [,1]        [,2]        [,3]         [,4]        [,5]        [,6]\nX36b4 -3.3748490   0.5817180  -7.3528185  -2.75182972  -4.9593593  -1.2664888\nACAT1  1.3694057   4.1938419  -2.1951887  13.24670653  -7.5278797   5.8494391\nACAT2  0.7236782   3.9928812  -3.3376630  -3.88120599   0.2721882   1.3012874\nACBP   0.5927870  -6.8394169  -2.3324859   8.39766198   1.6286876  -6.7585683\nACC1  -4.3801037   0.2876008 -12.4601621   0.06732809 -10.6656173  11.1335523\nACC2   6.2221772   6.3953635   5.8302868   6.38871606   5.9636507  -3.4410507\nACOTH  2.2727330   5.7611549  12.5677715  -0.14230685  -2.8360377  -5.2129368\nADISP -4.6789126 -15.2268608   0.9922763 -12.92907435  19.0493396 -13.2687852\nADSS1 -6.5906498  -1.1136249  -3.1095779   0.32327939  -6.3302874  -6.5385794\nALDH3  8.6967435   1.4525302   6.7672495  -9.52990178  -3.1463163   0.7954847\n             [,7]       [,8]        [,9]       [,10]\nX36b4   6.3851965  13.159138  -4.2742659   4.4254371\nACAT1 -13.4240634   3.626869  -1.0896599  24.6456754\nACAT2  -1.9046485  -3.179304  -0.6021537   1.2924147\nACBP   -5.8552684   5.704047   5.3662421  -8.1207642\nACC1    2.8192415 -18.135016  -9.6545142  -4.3084105\nACC2    3.7490920   3.765147  -3.5499524  -1.0737214\nACOTH  -4.5231792   7.195705   2.2929182 -17.4098041\nADISP  -3.0669710  -6.181938 -12.9466045 -10.0293360\nADSS1   3.8113885  -4.131253  17.8846096  11.9375988\nALDH3   0.6880137  -2.103283  -0.7149102   0.6075842\n\n$ycoef\n             [,1]      [,2]      [,3]       [,4]         [,5]     [,6]\nC14.0    18.25835  9.531869  8.951338 -14.480970  -5.83576334 19.90462\nC16.0    18.39114  8.012748  8.054978 -15.317120  -2.45444921 21.44894\nC18.0    18.48383  7.761822  8.499797 -15.391899  -2.11398881 21.18489\nC16.1n.9 17.47713  5.438128 12.606433 -14.858517  -0.06442915 20.95551\nC16.1n.7 18.49964  8.146145  8.272064 -15.149249  -1.66908125 22.21911\nC18.1n.9 18.55467  8.335579  8.233188 -14.956342  -2.73010007 21.41944\nC18.1n.7 18.17675  7.754032  7.317378 -15.903060  -2.60918722 20.94234\nC20.1n.9 17.48909 10.199644  7.607889 -18.499439   6.57046104 20.87297\nC20.3n.9 18.95541  8.971971  9.106700 -13.449666  -2.53414184 21.44272\nC18.2n.6 18.24963  8.135332  7.912258 -15.571984  -2.56209042 21.45624\nC18.3n.6 18.25139  8.421602  7.833877 -16.088806  -2.62392210 21.41660\nC20.2n.6 20.46932  9.020131  9.674874  -8.291072  -6.02579329 25.13346\nC20.3n.6 20.19757  8.541453  9.423868 -19.559058  -3.49492074 22.27412\nC20.4n.6 18.21923  8.347795  7.682024 -15.083926  -2.65850088 21.51743\nC22.4n.6 22.36806  4.990955 17.969106   1.749075   0.76459980 16.93832\nC22.5n.6 18.34621  8.477176  8.293508 -16.402786  -3.95616647 22.01979\nC18.3n.3 18.52855  8.152770  8.487307 -15.381268  -2.43280758 21.23006\nC20.3n.3 11.89230 10.722131 -6.257866 -11.755138 -16.66337673 24.61401\nC20.5n.3 18.79358  8.270673  8.835837 -15.231939  -2.35666510 21.44424\nC22.5n.3 18.62810  6.987050  8.006177 -13.646691  -2.71383635 22.25207\nC22.6n.3 18.34196  8.407695  7.997997 -14.851378  -2.81437680 21.33650\n               [,7]       [,8]      [,9]     [,10]\nC14.0     9.1601637 -24.041827 -26.23586 -14.50703\nC16.0     9.9146866 -23.380585 -24.83538 -11.99405\nC18.0     9.7993892 -24.019798 -24.55970 -12.11391\nC16.1n.9 15.0082477 -30.273137 -17.25814 -19.91616\nC16.1n.7  9.8445895 -23.163750 -23.84281 -12.09438\nC18.1n.9  9.5666731 -22.918388 -24.60547 -12.04711\nC18.1n.7  9.8069681 -23.298547 -25.66651 -12.49599\nC20.1n.9  9.1086056 -25.290340 -21.97497 -10.23310\nC20.3n.9  9.5409090 -22.443193 -24.24194 -14.00538\nC18.2n.6  9.8618092 -23.588501 -25.04873 -12.36980\nC18.3n.6  9.5235508 -24.070609 -24.48174 -12.20438\nC20.2n.6 10.7987975 -17.966001 -20.39902 -16.31464\nC20.3n.6 11.7807005 -28.370651 -23.02181 -14.87258\nC20.4n.6  9.8929260 -22.612358 -24.85997 -12.68596\nC22.4n.6 -0.1530453  -3.263162 -14.35357  -6.93795\nC22.5n.6 10.7328297 -27.082963 -25.38803 -11.37265\nC18.3n.3  9.9928397 -22.770669 -24.42168 -12.10238\nC20.3n.3  1.1504999 -48.489272 -40.66735 -25.20117\nC20.5n.3  9.7601761 -22.108820 -24.10900 -12.48905\nC22.5n.3 10.8456290 -21.482942 -25.00679 -12.21762\nC22.6n.3  9.3775041 -22.986270 -24.45812 -12.56643\n\n$scores\n$scores$xscores\n           [,1]        [,2]         [,3]         [,4]         [,5]        [,6]\n1   1.242097873 -1.32073181  0.544253142 -0.860104127 -0.018440941  0.23142614\n2   0.488885161 -2.03191478 -0.324266139 -0.942704614 -0.933256190 -0.73236181\n3   0.852497081 -1.03663862  0.450651837  0.350804793 -0.343689251  1.88817244\n4   0.637431091  0.42777568 -0.374687983 -1.533260933 -1.139478415  0.67440950\n5   0.776070104  0.35818618 -1.562988367 -1.196454567  1.353144113 -0.21001974\n6   1.230157511  0.36148325 -0.322829820  1.409782773  0.347972649 -0.43459660\n7   0.936014623 -1.17036368  1.486553053 -0.109308656 -0.003854736 -0.04018232\n8   0.898609399 -1.37319950 -0.256329394  0.480651678  0.588055605  0.86135868\n9   0.131416238 -0.06527504 -1.216621765 -1.789277200 -2.580911585  0.79150609\n10  1.886288324  1.08431527  1.214228134  1.665907911 -0.261155806 -0.63730384\n11  0.143333122  0.07576795 -1.153324654  0.381147641 -1.832474170 -0.10591115\n12  0.746625813  0.55430165 -0.296479818  0.031536889 -0.496646319  1.50096970\n13  0.395614197 -0.94421571  0.876187355  1.336556648  0.577829229 -0.17871056\n14  0.204331012  0.24914989 -0.923313916 -1.823237156  2.094749089  0.51616967\n15  0.323757446 -1.67764237  0.273147797 -0.574736868 -0.217214233 -1.42857216\n16  0.783604596 -1.34473506  1.754273317 -0.588838760  0.554349037  0.35212221\n17  2.220809544  1.51611465 -1.287091459  0.380397701  0.900889514  0.99690094\n18  0.286101603 -0.26218451 -1.187893737 -0.092064457 -1.405371822 -0.67066965\n19  1.567270752  0.45952330 -1.156034159  0.893256111  0.250552821 -1.06836371\n20  0.966726471  0.08323253 -0.034238448 -0.048077666  0.075688432 -0.46721678\n21 -0.638325852  1.44107265 -0.044625139 -1.038069985  0.175444801  0.50729169\n22 -0.640105898  0.90324542  1.042548131  0.532806895  0.574540819  0.95169360\n23 -1.517954028 -1.44013601 -0.041627742  1.278190796  0.404342854  1.42142946\n24 -1.395124563  0.65732217 -1.403992890  1.652596817 -1.086919193  1.39401799\n25 -1.169503077 -0.05134300  1.015086381  0.467168272  0.125985374  0.57492665\n26 -0.477543084  1.10598117 -0.231142500  1.111037147 -1.964618819 -1.54068661\n27 -0.900318888  0.78728718 -0.164472557 -1.195013930  1.842642503 -1.38162550\n28 -0.782121571  0.85993056  1.426412687  0.013334403 -0.803327338 -0.08301912\n29 -0.810435533  0.24829565 -0.545395544  0.466016988  0.477469132  0.69126979\n30 -0.051437054  1.19915554  2.240475547 -1.066965646 -0.156819224 -0.08496980\n31 -0.008879629  1.75763044  0.007043267 -0.426718452  1.122398322 -0.06886190\n32 -0.850543921 -1.33717389 -1.321900064 -0.243632000  0.009663020 -2.31046649\n33 -0.916579082  0.69654110 -0.473208176  0.076866469 -0.214817475  0.81511874\n34 -0.361617579 -0.74245988  0.613300266 -0.009051483  0.699157048  0.64226849\n35 -1.345172670 -0.12693017 -0.528614794  0.108874298  1.001390927  0.71966887\n36  0.075913011  1.07037674  0.929762382  1.380233335  0.272991555 -2.42627415\n37 -0.696948680 -0.23193204  1.054580567  0.698735377 -0.503572563  0.47073255\n38 -1.346820162 -0.70266598 -1.171564141  0.248882752  1.407491242 -0.88188356\n39 -1.221785259  0.87390174  1.491279730 -2.242638129 -1.066630502 -0.70688858\n40 -1.662338443 -0.91104866 -0.397140384  0.815368936  0.172450497 -0.54286920\n          [,7]        [,8]        [,9]       [,10]\n1  -0.97959852  1.50861709 -1.10072038 -1.25756470\n2   0.74939610 -0.75597689  1.42730289  0.57690057\n3   2.04973079  0.65540962 -0.05356618  0.24045064\n4   0.20773968 -0.61340194  0.83487896  0.08111406\n5   1.20280794 -1.41344690 -1.57938882 -0.44378089\n6   1.40452663 -0.81293272 -1.61612789 -2.16308814\n7  -0.04666510 -1.59042281 -0.28492748  0.60938931\n8   0.41346119  0.16447806 -0.29597254  0.33076809\n9   0.54339660 -0.95253688 -0.59974377 -0.54715378\n10  0.21921625 -0.62208854  0.75587234  0.86554339\n11 -2.63608112  1.01784476  0.47040555 -1.56361125\n12 -0.88681053 -0.63171857  0.35434299  0.25922915\n13  0.20541480 -0.70414816 -0.10517271  0.74469533\n14 -0.79188208  1.32384827  0.39188321  0.98842227\n15  0.16817207  0.24869145  0.69565247 -0.50559803\n16 -2.26415766 -0.37003624  0.40104356 -0.36020822\n17  0.03502314  0.40946440 -0.20634763  1.17385967\n18 -0.37331285  0.06978614  0.16885528  2.16460584\n19 -0.51754379  1.65622543 -0.62137146  0.40034605\n20  0.98979401  1.76010637  2.37498615 -0.33728324\n21  0.88295566 -0.94114942  0.27552401 -0.47028840\n22  0.59040276  0.62110604 -0.31953870 -0.70584427\n23 -0.05877332  0.30910838  0.34074446 -0.34189845\n24  0.99449645  1.88237776  0.11371867 -0.45460594\n25 -0.96973378  0.36540098  1.29105270  1.13235417\n26 -0.60180598 -0.84844339 -1.01340287  0.31140743\n27  0.38156641 -0.13581838  0.84582662  0.46226470\n28 -0.92293924  0.60329144 -1.23013687  0.15433178\n29 -0.31585311  0.43418558  1.27969413 -0.04399685\n30  1.79323373  0.60661806  0.87534112 -1.74908117\n31 -1.81932001 -1.04836431  0.41363043 -0.37005630\n32  0.32760258  0.07911568 -0.51996373 -0.10614929\n33 -0.59685816 -2.46738188  1.35040730 -0.91396696\n34 -0.96858161  0.29056614 -2.52620952 -0.97830727\n35 -0.09634101 -0.41215435 -1.59314556  2.31380235\n36  0.12297728  0.24744285  0.18815764  0.12638715\n37  0.19833350 -1.26151231 -0.40076943  0.70079878\n38 -0.37463657 -0.26678537  0.41576363 -2.14679106\n39  0.56688895  1.74490458 -1.51736991  1.08112299\n40  1.17375792 -0.15027005  0.31879136  0.74148051\n\n$scores$yscores\n          [,1]        [,2]        [,3]         [,4]         [,5]         [,6]\n1   0.88058160 -1.08759878  0.93074368 -0.814137063  0.051109311  0.058894635\n2   0.39212221 -1.87930485 -0.02594305 -0.683840429 -0.544531474  0.008329497\n3   0.86722797 -0.84072982  0.32680696 -0.287274359 -0.463647252  0.137443781\n4   0.65334586  0.39730476 -0.73302308 -1.415780849 -1.834507671  0.211879309\n5   0.94124398  0.33667861 -0.68466653 -1.673677963  1.075015903  0.455329592\n6   1.35799979  0.47360197 -0.90033271  0.957248937  0.610506810  0.160323505\n7   0.94922090 -1.10021516  0.77863830 -0.048867508 -0.457833110  0.930564363\n8   1.04360601 -1.55348601  0.38301713  0.596256687  0.205254230  1.122761946\n9   0.07139738 -0.39568978 -1.24825582 -1.007469213 -1.862087037  0.500574825\n10  2.04683860  1.19578076  1.28101528  2.082455972  0.304215542 -1.419810759\n11  0.32744222  0.40605949 -1.19459227 -0.004675656 -2.303209465 -0.498169660\n12  0.55280697  0.46270515 -0.77330108 -0.511759680  0.702626571  0.451476645\n13  0.27702903 -1.38785022  0.74741777  1.021680734 -0.299753063 -0.748810014\n14  0.31414081  0.11121727 -1.61907264 -1.438493422  0.738061864  0.255291543\n15  0.54283889 -1.48535908  0.45377015  0.490119181 -0.238455659 -0.628443581\n16  1.00426659 -1.56905041  1.54176199 -1.041637276  1.243989865  0.211600190\n17  2.15846186  1.48479626 -1.07442137  0.688707000  1.041556562  0.933809430\n18  0.24001129 -0.25465497 -1.07599213 -0.672144442 -1.252887633  0.291354668\n19  1.38329395  0.33509395 -0.73498471  1.220967271 -0.339273685 -1.358192378\n20  0.90252773  0.19536142 -0.30451080 -0.198288495  1.333730422  0.404274152\n21 -0.66915749  1.63788570 -0.22632545 -0.440373397  0.856724454 -0.767761540\n22 -0.49453085  0.81651854  0.56594679  0.999285261 -0.044901496  1.133143966\n23 -1.43527457 -1.33479821  0.38540012  0.993991374  0.552325720  0.444186770\n24 -1.23233440  0.46149144 -1.19540470  1.332933590 -0.908094898  1.456761287\n25 -1.10542312 -0.05993872  1.06367375  0.498697704 -0.113392579  0.948971591\n26 -0.49769459  1.12841464 -0.42758334  0.866568189 -1.786036052  0.300979693\n27 -0.95034745  0.87881227 -0.09141902 -1.654412098  1.835482622 -1.062324126\n28 -0.67377660  0.77856752  1.66965981  0.450016485  0.248784679 -0.505274042\n29 -0.99776124  0.24385229 -0.50677994  0.533525171  0.240479825  1.353577313\n30 -0.13442897  1.17818334  2.37560714 -1.190837873 -1.290275684  0.488381809\n31  0.08772510  1.75257614  0.42198148 -0.738391486  1.132333783  0.496853392\n32 -0.72266800 -1.40065764 -1.54059849 -0.459125834  0.007735647 -3.501405663\n33 -1.00624670  0.56082261  0.04200532  0.375557837 -0.430404896 -0.158617826\n34 -0.56906710 -0.75616376  0.33557170  0.319778720  1.052769825  1.117698422\n35 -1.33456761  0.31440243 -0.62109474  0.510224971  0.523564880  0.451885771\n36 -0.09377257  0.92462323  0.85076020  1.234934748 -0.163257827 -2.430910315\n37 -0.81471603  0.02243947  1.11368552  0.280096661 -1.155206125  0.014212074\n38 -1.43904844 -0.72972576 -1.30751487  0.398687569  1.128624628 -0.710687121\n39 -1.13492533  0.75766199  1.58638763 -2.419972029 -0.506626616 -0.838195625\n40 -1.68838770 -1.01962808 -0.56803400  0.849425009  1.109489080  0.288042483\n          [,7]        [,8]        [,9]        [,10]\n1  -1.53995973 -0.15447820 -0.42664839 -0.989221222\n2   2.45703607 -1.49023079  0.49889539  0.618975240\n3   1.32250182 -0.73809880  0.41114858  1.140893454\n4  -0.93132184 -0.17076608  0.61436782 -0.114434529\n5   0.67643110 -0.39499161  0.66330901 -0.793839151\n6   1.84062569 -0.99570897 -2.07516951 -1.948210453\n7  -1.10329625 -0.80461144 -1.25912076 -0.588117887\n8  -1.31200982  0.12510422 -1.25568830  0.514658493\n9  -0.26683427 -0.10962038 -0.15659950 -0.994577099\n10 -0.07436346 -0.10952466 -0.06384920  1.564622615\n11 -1.98995133  1.43212645  0.95598770  0.495052022\n12  0.18765230  0.81679340  0.79669689  0.009818485\n13  0.62287374 -0.15434827  1.94721017  1.654336546\n14  0.24801123  0.17790617 -1.22170432  0.812373848\n15  0.96173354  0.47258478  1.29849749  0.540818850\n16 -1.12302957  0.35685053  0.18758320 -1.047094938\n17  0.25601954  0.59891309 -0.71694452  1.143173451\n18 -1.01703551 -1.34259430  0.47124638  0.149464300\n19  0.09133063  1.52363165 -0.84402066 -0.895361390\n20  0.28219400  1.62824466  2.43105501 -0.365901160\n21 -0.37044147 -1.28021579  0.12399324 -0.655271306\n22  0.35792277 -1.56837825  2.10069915 -0.640192274\n23 -0.29976778  0.49358893 -0.72876475 -1.051584715\n24  0.82858941  1.22385904 -0.45339907  0.317290482\n25 -1.52307846  0.25528586  0.77821060 -0.542905923\n26 -0.46059180 -0.65220095 -0.93336063  1.408425820\n27 -0.96638009 -0.14739104 -0.21048762  2.104123051\n28 -0.66449907 -0.32083544 -1.17597507 -0.658512402\n29  1.21075203 -0.04886203  0.35570970 -1.504634983\n30  1.47158730  2.45512369 -0.10367470 -0.985966024\n31 -0.50986091 -1.60377757  0.70230133 -0.431190278\n32  0.53338267  0.21063788 -0.65253718 -1.022298432\n33  0.17469937 -1.83394305  0.24353470  0.472252366\n34 -0.53473490  0.57317125 -1.44959477  1.771938109\n35  0.21316512  1.60846701  0.37960842 -0.150912894\n36 -0.53915966 -0.09518024  0.55588883 -1.316877971\n37  0.83191812 -1.13994023 -1.03934633  0.311853803\n38 -0.98209275 -0.34986333  0.32841456 -0.081982986\n39  1.32825743  0.96437878 -1.11551179  1.147346897\n40  0.31172478  0.58889406  0.03803886  0.601670184\n\n$scores$corr.X.xscores\n              [,1]        [,2]         [,3]        [,4]         [,5]       [,6]\nX36b4 -0.004147031  0.08031376 -0.353255619 -0.18064944 -0.451029389 -0.5042562\nACAT1 -0.110978348  0.03510453  0.210524357  0.18082040 -0.377085546 -0.4004143\nACAT2  0.182625401  0.58092100 -0.557951872 -0.27536350  0.253264714 -0.1545253\nACBP   0.659884074 -0.25288482 -0.522813743  0.17044864 -0.136178641 -0.3123814\nACC1   0.064321033  0.01285737 -0.123007814  0.25110857 -0.574330101 -0.4549204\nACC2   0.341290505  0.33996445 -0.073881720  0.35872252 -0.002544151 -0.6266599\nACOTH -0.477384718  0.38623899  0.351440855 -0.03660321 -0.419783843 -0.4037806\nADISP -0.253999692 -0.09055100  0.133074374 -0.09811440 -0.088339865 -0.7337710\nADSS1 -0.162858311  0.15997475  0.042335605  0.04736904 -0.353223326 -0.7949483\nALDH3  0.763922125 -0.30656876  0.007984654 -0.20486051 -0.418491770 -0.2802920\n             [,7]        [,8]        [,9]       [,10]\nX36b4  0.20656867  0.42360261 -0.38961482  0.06706312\nACAT1 -0.51578663 -0.07432948 -0.44212367  0.37399967\nACAT2 -0.34755735 -0.02003586  0.16458395 -0.07574233\nACBP  -0.16453026  0.03018004  0.13804019 -0.18722291\nACC1   0.12241104 -0.41997845 -0.42118219 -0.10928220\nACC2   0.32954402 -0.22838644 -0.27394109  0.07274718\nACOTH -0.28747785 -0.01294899 -0.20498554 -0.18435097\nADISP -0.18332076 -0.23377890 -0.49165156  0.15409523\nADSS1  0.16877345 -0.33776826  0.09817094  0.18689607\nALDH3  0.04790734 -0.09475924 -0.07911627  0.09578203\n\n$scores$corr.Y.xscores\n                [,1]        [,2]         [,3]        [,4]        [,5]\nC14.0     0.05333084  0.45305154  0.001487133  0.12555031  0.17853070\nC16.0     0.67338005 -0.26028283 -0.383532693  0.04492735 -0.04507824\nC18.0     0.39533047 -0.59181698 -0.095708619 -0.23807581  0.04170962\nC16.1n.9 -0.33973442  0.42624031 -0.013866519  0.27160247  0.03230013\nC16.1n.7  0.13377548  0.47673007  0.070499634  0.08424460  0.13797394\nC18.1n.9 -0.02997286  0.59288769 -0.081682899  0.26280024  0.05125117\nC18.1n.7  0.21661212  0.30971789 -0.124824225  0.22777070  0.14756562\nC20.1n.9 -0.33928472  0.16011564  0.256950319  0.23800696  0.48181573\nC20.3n.9  0.59729064  0.29574709 -0.159416112  0.36005168  0.14810355\nC18.2n.6 -0.69041443 -0.15563794  0.067368871  0.01649435  0.01358603\nC18.3n.6  0.33266085  0.21596205 -0.171662122  0.17215492  0.20383610\nC20.2n.6 -0.52364179 -0.26055738  0.085677821  0.24618542  0.12480741\nC20.3n.6  0.42919631 -0.44070812 -0.227383638 -0.14168614  0.02050834\nC20.4n.6  0.23616279 -0.44214807 -0.116208264 -0.06443796  0.15338564\nC22.4n.6 -0.17755786 -0.43255539  0.113938305  0.10044344  0.16729927\nC22.5n.6  0.02899953 -0.46462426  0.080688616  0.17290098  0.09541784\nC18.3n.3 -0.13349184  0.10450305  0.608932687 -0.21429905 -0.16430534\nC20.3n.3  0.02428397 -0.08706868  0.613952709 -0.13623160 -0.11348915\nC20.5n.3  0.14139317 -0.32126411  0.290622047 -0.21266570 -0.11272433\nC22.5n.3  0.15442043 -0.02937458 -0.017963314 -0.03496373 -0.08657896\nC22.6n.3  0.04207631 -0.18013533 -0.337283537 -0.23128859 -0.25101632\n                [,6]        [,7]          [,8]        [,9]        [,10]\nC14.0    -0.34662255  0.02200318 -0.1503402847 -0.04424750 -0.094411045\nC16.0    -0.05666512 -0.10146443 -0.1018933886  0.01843506 -0.011849787\nC18.0     0.16377993 -0.06412795  0.0564436825  0.07831380  0.011654204\nC16.1n.9 -0.32250420  0.10693734 -0.1470819666  0.01660255 -0.025045609\nC16.1n.7 -0.33146341  0.01434930 -0.1321177019 -0.05057969 -0.095232819\nC18.1n.9 -0.31841098  0.07289152 -0.1182043511 -0.02349495  0.021107156\nC18.1n.7 -0.44190883  0.07705318 -0.1154821096 -0.04032313 -0.058896136\nC20.1n.9  0.12295179  0.05717429 -0.0560752366 -0.08552798  0.005236849\nC20.3n.9 -0.11312682  0.15175008  0.0317989765 -0.15602903 -0.022040978\nC18.2n.6  0.29813154 -0.02163425  0.0825582766  0.06477093  0.081078565\nC18.3n.6  0.12381261  0.12591605 -0.0009332623 -0.11656948  0.022721799\nC20.2n.6  0.21498824  0.08201372  0.0395169927  0.11896403  0.073224298\nC20.3n.6  0.13869640  0.08092787  0.0130705678  0.23931907  0.069712209\nC20.4n.6  0.04129411  0.23837182  0.0109297925  0.20934451  0.104383680\nC22.4n.6  0.09066014  0.12492195 -0.0080338731  0.16357939  0.122251906\nC22.5n.6  0.01713283  0.25920483 -0.0510644079  0.16325701  0.126967657\nC18.3n.3  0.02817369  0.05135326  0.0925313424 -0.18887951 -0.041094593\nC20.3n.3  0.07608099 -0.08464070  0.0098514416 -0.20485180 -0.060366976\nC20.5n.3  0.16999587 -0.27836844  0.0570984660 -0.12359405 -0.096937647\nC22.5n.3  0.28771416 -0.05127361  0.0879868139 -0.20577994 -0.098715292\nC22.6n.3  0.20908287 -0.23098548  0.0936791430  0.05456911 -0.062714649\n\n$scores$corr.X.yscores\n             [,1]        [,2]         [,3]        [,4]         [,5]       [,6]\nX36b4 -0.00410846  0.07909890 -0.331666884 -0.16603622 -0.367577298 -0.3648132\nACAT1 -0.10994617  0.03457353  0.197658448  0.16619336 -0.307314977 -0.2896869\nACAT2  0.18092685  0.57213372 -0.523853404 -0.25308861  0.206404198 -0.1117941\nACBP   0.65374666 -0.24905956 -0.490862694  0.15666059 -0.110982074 -0.2259979\nACC1   0.06372280  0.01266288 -0.115490359  0.23079572 -0.468064193 -0.3291203\nACC2   0.33811625  0.33482199 -0.069366540  0.32970450 -0.002073418 -0.4533683\nACOTH -0.47294469  0.38039656  0.329963027 -0.03364228 -0.342112986 -0.2921223\nADISP -0.25163731 -0.08918129  0.124941715 -0.09017766 -0.071994708 -0.5308597\nADSS1 -0.16134361  0.15755490  0.039748322  0.04353724 -0.287867884 -0.5751196\nALDH3  0.75681708 -0.30193146  0.007496683 -0.18828880 -0.341059980 -0.2027823\n             [,7]         [,8]        [,9]       [,10]\nX36b4  0.13247761  0.256598740 -0.21311316  0.02419397\nACAT1 -0.33078675 -0.045025339 -0.24183467  0.13492567\nACAT2 -0.22289715 -0.012136793  0.09002483 -0.02732512\nACBP  -0.10551733  0.018281662  0.07550581 -0.06754331\nACC1   0.07850524 -0.254403398 -0.23038002 -0.03942510\nACC2   0.21134475 -0.138345875 -0.14984145  0.02624457\nACOTH -0.18436667 -0.007843896 -0.11212386 -0.06650722\nADISP -0.11756815 -0.141612378 -0.26892565  0.05559203\nADSS1  0.10823860 -0.204604290  0.05369796  0.06742540\nALDH3  0.03072416 -0.057400738 -0.04327535  0.03455472\n\n$scores$corr.Y.yscores\n                [,1]        [,2]         [,3]        [,4]        [,5]\nC14.0     0.05383151  0.46000986  0.001583933  0.13660027  0.21906301\nC16.0     0.67970178 -0.26428045 -0.408497458  0.04888151 -0.05531248\nC18.0     0.39904186 -0.60090657 -0.101938448 -0.25902939  0.05117907\nC16.1n.9 -0.34292387  0.43278685 -0.014769114  0.29550681  0.03963332\nC16.1n.7  0.13503138  0.48405207  0.075088570  0.09165915  0.16929855\nC18.1n.9 -0.03025424  0.60199372 -0.086999772  0.28592987  0.06288686\nC18.1n.7  0.21864569  0.31447478 -0.132949236  0.24781731  0.18106785\nC20.1n.9 -0.34246994  0.16257482  0.273675632  0.25895448  0.59120368\nC20.3n.9  0.60289804  0.30028941 -0.169792765  0.39174063  0.18172791\nC18.2n.6 -0.69689608 -0.15802835  0.071754020  0.01794605  0.01667051\nC18.3n.6  0.33578389  0.21927897 -0.182835888  0.18730666  0.25011358\nC20.2n.6 -0.52855777 -0.26455923  0.091254729  0.26785275  0.15314278\nC20.3n.6  0.43322563 -0.44747686 -0.242184408 -0.15415625  0.02516441\nC20.4n.6  0.23837990 -0.44893893 -0.123772448 -0.07010929  0.18820920\nC22.4n.6 -0.17922479 -0.43919892  0.121354734  0.10928369  0.20528169\nC22.5n.6  0.02927178 -0.47176032  0.085940769  0.18811838  0.11708081\nC18.3n.3 -0.13474507  0.10610809  0.648569103 -0.23315998 -0.20160804\nC20.3n.3  0.02451195 -0.08840595  0.653915887 -0.14822164 -0.13925491\nC20.5n.3  0.14272058 -0.32619834  0.309539108 -0.23138288 -0.13831645\nC22.5n.3  0.15587014 -0.02982574 -0.019132575 -0.03804097 -0.10623521\nC22.6n.3  0.04247133 -0.18290199 -0.359237869 -0.25164481 -0.30800525\n                [,6]        [,7]         [,8]        [,9]       [,10]\nC14.0    -0.47911255  0.03430895 -0.248187253 -0.08089356 -0.26169742\nC16.0    -0.07832430 -0.15821067 -0.168209341  0.03370309 -0.03284636\nC18.0     0.22638175 -0.09999293  0.093179300  0.14317379  0.03230422\nC16.1n.9 -0.44577541  0.16674443 -0.242808302  0.03035289 -0.06942378\nC16.1n.7 -0.45815911  0.02237446 -0.218104745 -0.09247011 -0.26397530\nC18.1n.9 -0.44011763  0.11365772 -0.195136076 -0.04295362  0.05850680\nC18.1n.7 -0.61082023  0.12014689 -0.190642100 -0.07371899 -0.16325386\nC20.1n.9  0.16994781  0.08915028 -0.092571056 -0.15636278  0.01451599\nC20.3n.9 -0.15636744  0.23661970  0.052494916 -0.28525326 -0.06109526\nC18.2n.6  0.41208676 -0.03373368  0.136290229  0.11841463  0.22474120\nC18.3n.6  0.17113767  0.19633741 -0.001540664 -0.21311306  0.06298242\nC20.2n.6  0.29716348  0.12788172  0.065236100  0.21749079  0.20297000\nC20.3n.6  0.19171051  0.12618859  0.021577372  0.43752463  0.19323487\nC20.4n.6  0.05707801  0.37168657  0.018043302  0.38272495  0.28934051\nC22.4n.6  0.12531328  0.19478733 -0.013262612  0.29905686  0.33886935\nC22.5n.6  0.02368153  0.40417092 -0.084298996  0.29846749  0.35194091\nC18.3n.3  0.03894256  0.08007371  0.152754132 -0.34531070 -0.11390986\nC20.3n.3  0.10516153 -0.13197791  0.016263121 -0.37451134 -0.16733087\nC20.5n.3  0.23497361 -0.43405221  0.094260241 -0.22595541 -0.26870090\nC22.5n.3  0.39768753 -0.07994953  0.145251858 -0.37620818 -0.27362834\nC22.6n.3  0.28900089 -0.36016926  0.154648963  0.09976359 -0.17383837\n\n\n\n\nCode\nY <- dune %>% scale(scale = T)\ndune_lm <- lm(Y ~ Manure, data = dune.env)\nYhat <- fitted(dune_lm)\n\n# residuals (unconstrained)\nS <- cov(resid(dune_lm))\nS_eig <- eigen(S)\nS_eig$values # eigenvalues for unconstrained axes\n\n\n [1]  5.133449e+00  3.446741e+00  2.462058e+00  1.923570e+00  1.661788e+00\n [6]  1.366496e+00  1.357365e+00  9.260721e-01  8.392027e-01  5.854538e-01\n[11]  5.111340e-01  4.388610e-01  3.075023e-01  1.589608e-01  8.395939e-02\n[16]  2.120562e-16 -1.559267e-17 -3.575405e-17 -4.416045e-17 -5.139537e-17\n[21] -7.019758e-17 -7.985329e-17 -8.798157e-17 -1.033332e-16 -1.363653e-16\n[26] -1.422024e-16 -1.513624e-16 -1.756357e-16 -1.934723e-16 -3.721498e-16\n\n\nCode\n# fitted values\nSfit <- cov(Yhat)\nSfit_eig <- eigen(Sfit)\nSfit_eig$values # eigenvalues for constrained axes\n\n\n [1]  4.373976e+00  2.077844e+00  1.449479e+00  8.960858e-01  3.084491e-16\n [6]  1.955883e-16  7.326783e-17  1.839472e-17  1.790275e-17  1.588353e-17\n[11]  1.523642e-17  1.484430e-17  6.817158e-18  6.228401e-18  4.529189e-18\n[16] -3.943651e-19 -3.623882e-18 -7.955388e-18 -1.503661e-17 -1.561647e-17\n[21] -1.875475e-17 -1.885194e-17 -1.938641e-17 -2.595745e-17 -3.112896e-17\n[26] -3.401692e-17 -3.457048e-17 -5.677008e-17 -1.169537e-16 -3.316104e-16\n\n\nCode\n# Species Scores\ncbind(\n  scores(dune_rda, scaling = 0, display = \"sp\"), # rda scores\n  Sfit_eig$vectors[,1:2]) # eigen scores\n\n\n                 RDA1        RDA2                         \nAchimill  0.143458988  0.25880807  0.143458988 -0.25880807\nAgrostol -0.014318256 -0.32220859 -0.014318256  0.32220859\nAiraprae -0.221477424  0.03941836 -0.221477424 -0.03941836\nAlopgeni  0.182994436 -0.16391065  0.182994436  0.16391065\nAnthodor -0.032899432  0.24552799 -0.032899432 -0.24552799\nBellpere  0.170004691  0.05561525  0.170004691 -0.05561525\nBromhord  0.194470873  0.10200638  0.194470873 -0.10200638\nChenalbu -0.006598932 -0.13878455 -0.006598932  0.13878455\nCirsarve  0.110871155 -0.26612997  0.110871155  0.26612997\nComapalu -0.226345641  0.04028481 -0.226345641 -0.04028481\nEleopalu -0.199452547 -0.12368720 -0.199452547  0.12368720\nElymrepe  0.278156602 -0.08806907  0.278156602  0.08806907\nEmpenigr -0.155781754  0.02772590 -0.155781754 -0.02772590\nHyporadi -0.171282469  0.07992401 -0.171282469 -0.07992401\nJuncarti -0.116954796 -0.03910985 -0.116954796  0.03910985\nJuncbufo  0.107686567  0.09944215  0.107686567 -0.09944215\nLolipere  0.319690365 -0.12781833  0.319690365  0.12781833\nPlanlanc  0.074625723  0.25548840  0.074625723 -0.25548840\nPoaprat   0.308361280 -0.17154631  0.308361280  0.17154631\nPoatriv   0.327251488 -0.06646981  0.327251488  0.06646981\nRanuflam -0.244850313 -0.11632722 -0.244850313  0.11632722\nRumeacet  0.181152477  0.30802177  0.181152477 -0.30802177\nSagiproc  0.109442555 -0.07068686  0.109442555  0.07068686\nSalirepe -0.274766547  0.04890272 -0.274766547 -0.04890272\nScorautu -0.072918338  0.32138155 -0.072918338 -0.32138155\nTrifprat  0.127920010  0.20855494  0.127920010 -0.20855494\nTrifrepe  0.116609271  0.37888351  0.116609271 -0.37888351\nVicilath  0.026342810  0.17029063  0.026342810 -0.17029063\nBracruta -0.052193007  0.20359595 -0.052193007 -0.20359595\nCallcusp -0.200951405 -0.04022983 -0.200951405  0.04022983\n\n\nCode\n# Site Scores (order is the same)\ndune_site_scores <- (Y %*% Sfit_eig$vectors)[,1]\n\n# I don't know yet how they get scaling factor, but details described in vignette(\"decision-vegan\"), section 3\n# dune_site_scores / scores(dune_rda, scaling = 0, display = \"wa\")[,\"RDA1\"] \ncbind(\n  scores(dune_rda, scaling = 0, display = \"wa\")[,\"RDA1\"] %>% order(), # rda ordering\n  dune_site_scores %>% order()) # my ordering\n\n\n      [,1] [,2]\n [1,]   20   20\n [2,]   19   19\n [3,]   15   15\n [4,]   14   14\n [5,]   16   16\n [6,]   17   17\n [7,]   18   18\n [8,]    8    8\n [9,]   11   11\n[10,]   13   13\n[11,]   12   12\n[12,]    1    1\n[13,]    9    9\n[14,]    6    6\n[15,]    7    7\n[16,]    5    5\n[17,]   10   10\n[18,]    3    3\n[19,]    4    4\n[20,]    2    2\n\n\nCode\n# factor biplot\nscores(dune_rda, scaling = 1, display = \"bp\")[,1] %>% order()\n\n\n[1] 2 4 3 1\n\n\nCode\n(coef(dune_lm)[-1,])[,1] %>% na.omit()\n\n\n  Manure.L   Manure.Q   Manure.C   Manure^4 \n-0.2125711 -0.8623456  0.4251421  0.3695838 \n\n\nFor general scaling, see source code in vegan:::scores.cca\n\nif (\"sites\" %in% take) {\nwa <- cbind(xCCAwa, xCAu)[, choices, drop = FALSE] if (scaling) { scal <- list(slam, 1, sqrt(slam))[[abs(scaling)]] wa <- sweep(wa, 2, scal, “”) if (scaling < 0) { scal <- sqrt(1/(1 - slam^2)) wa <- sweep(wa, 2, scal, ””) } } sol$sites <- wa }\n\n\n\nCode\n# The relevant functions for figuring out what's going\n# vegan:::rda.default\n# vegan:::ordConstrained\n# vegan:::initPCA\n# vegan:::ordConstrain"
  },
  {
    "objectID": "ordination/ordination.html#factor-analysis",
    "href": "ordination/ordination.html#factor-analysis",
    "title": "25  Ordination",
    "section": "25.7 Factor Analysis",
    "text": "25.7 Factor Analysis\nBasic theory, say we start by looking at the observations with matrix X (n people by p observations). The assumption is we can explain those The premise is that you can break down the covariance matrix into a systematic component (loadings) and an error.\n\\begin{equation}\nX - M =\n\\end{equation}\n\n25.7.1 Exploratory Factor Analysis\n\n\\hat \\Sigma = \\underbrace{\\hat \\Lambda\\hat \\Lambda^T}_{communality} + \\underbrace{\\hat \\Psi}_{uniqueness}\n\n\nExample\nIntroduction with practical example\n\n\nCode\nfood <- read.csv(\"https://userpage.fu-berlin.de/soga/300/30100_data_sets/food-texture.csv\", row.names = \"X\")\n\n# built in factor analysis\n(food.fa <- factanal(food, factors = 2)) \n\n\n\nCall:\nfactanal(x = food, factors = 2)\n\nUniquenesses:\n     Oil  Density   Crispy Fracture Hardness \n   0.334    0.156    0.042    0.256    0.407 \n\nLoadings:\n         Factor1 Factor2\nOil      -0.816         \nDensity   0.919         \nCrispy   -0.745   0.635 \nFracture  0.645  -0.573 \nHardness          0.764 \n\n               Factor1 Factor2\nSS loadings      2.490   1.316\nProportion Var   0.498   0.263\nCumulative Var   0.498   0.761\n\nTest of the hypothesis that 2 factors are sufficient.\nThe chi square statistic is 0.27 on 1 degree of freedom.\nThe p-value is 0.603 \n\n\nCode\nfood.fa$uniquenesses\n\n\n      Oil   Density    Crispy  Fracture  Hardness \n0.3338599 0.1555255 0.0422238 0.2560235 0.4069459 \n\n\nCode\nsum(food.fa$uniquenesses^2)\n\n\n[1] 0.3685864\n\n\n\nR doesn’t print loadings lower than .1 (to help spot common chunks)\nHigher levels of uniqueness mean that the factors do not account well for its variance."
  },
  {
    "objectID": "parallel/parallel.html#parallel-packages",
    "href": "parallel/parallel.html#parallel-packages",
    "title": "26  Parallel Computing in R",
    "section": "26.1 Parallel Packages",
    "text": "26.1 Parallel Packages\n\nforeach\ndoparallel\n\n\n26.1.1 parallel\n\n\n26.1.2 furrr\nfurrr is meant to be an easy way to parallelize things when you already use purrr. It uses parallel as a backend."
  },
  {
    "objectID": "plant_breeding/plant_breeding.html",
    "href": "plant_breeding/plant_breeding.html",
    "title": "27  Plant Breeding",
    "section": "",
    "text": "28 Plant Breeding Introduction\nThe motivation for many plant breeding programs is to identify a particular variety/strain/genotype\nAMMI stands for Additive Main Effects and Multiplicative Interaction models and have been used to analyze main effects and genotype by environment interactions in multilocation variety trials.\nConsider the anova of genotype (G) the environment (E) and their interaction (GE),\n\\begin{aligned}\nY_{ijk} = G_i + E_j + GE_{ij} + \\varepsilon_{ijk}\n\\end{aligned}\nThere are a few downsides to this ANOVA, in that GE is not sufficiently explored. AMMI breaks down the GE interaction into orthogonal components and tests each of the orthogonal components with svd, that is,\n\\begin{aligned}\nY_{ijk} = G_i + E_j + \\sum_{n=1}^{h}u_{ni}s_nv_{in} + \\varepsilon_{ijk}\n\\end{aligned}\n where s_n is the n’th singular value, u_{ni} is the nth left singular vector, v_{in} is the n^{th} right singular vector.\nGGE stands for “Genotype + Genotype:Environment” biplot analysis. There is additional specification for “block” or grouping structures among different environments (locations or years typically),"
  },
  {
    "objectID": "plant_breeding/plant_breeding.html#terminology",
    "href": "plant_breeding/plant_breeding.html#terminology",
    "title": "27  Plant Breeding",
    "section": "28.1 Terminology",
    "text": "28.1 Terminology\nThere are several names loosely related to genotype in plant breeding:\n\nvariety - normally used to divide plant types\ncultivar - used similarly to variety\nspecies - taxonomic categorization which are reproductively isolated from one another. Often broader than variety.\nstrain - informal taxonomic division, generally used as a subdivision of species\ngenotype - used commonly when referencing specifically genomic distinguished characteristics.\naccession - accession numbers are assigned like an ID code by people that maintain or curate a set of plants or crops. The analogy is that it’s a call number for their library. For example, the USDA Agricultural Research Station has a library of crops that they maintain, and their system for assigning accession numbers is detailed in a directive. One of their goals is to help facilitate the discovery of better plants, you can read the full mission statement here and list of plants they have curated in their library."
  },
  {
    "objectID": "plant_breeding/plant_breeding.html#additional-resources",
    "href": "plant_breeding/plant_breeding.html#additional-resources",
    "title": "27  Plant Breeding",
    "section": "28.2 Additional Resources",
    "text": "28.2 Additional Resources\n\nA great glossary of plant germplasm terms"
  },
  {
    "objectID": "plant_breeding/plant_breeding.html#example-and-breakdown-of-ammi",
    "href": "plant_breeding/plant_breeding.html#example-and-breakdown-of-ammi",
    "title": "27  Plant Breeding",
    "section": "29.1 Example and Breakdown of AMMI",
    "text": "29.1 Example and Breakdown of AMMI\nThe example from the documetation is plrv, which we will use and replicate results for\n\n\nCode\ndata(plrv)\nhead(plrv)\n\n\n  Genotype Locality Rep WeightPlant WeightPlot    Yield\n1   102.18     Ayac   1   0.5100000       5.10 18.88889\n2   104.22     Ayac   1   0.3450000       2.76 12.77778\n3   121.31     Ayac   1   0.5425000       4.34 20.09259\n4   141.28     Ayac   1   0.9888889       8.90 36.62551\n5   157.26     Ayac   1   0.6250000       5.00 23.14815\n6    163.9     Ayac   1   0.5120000       2.56 18.96296\n\n\n\n29.1.1 ANOVA\nanova with rep nested in location.\n\n\nCode\n# AMMI\nplrv <- plrv %>% mutate(Rep = factor(Rep))\nmod_ammi <- with(plrv, AMMI(Locality, Genotype, Rep, Yield, console = FALSE))\nmod_ammi$ANOVA\n\n\nAnalysis of Variance Table\n\nResponse: Y\n           Df Sum Sq Mean Sq  F value    Pr(>F)    \nENV         5 122284 24456.9 257.0382  9.08e-12 ***\nREP(ENV)   12   1142    95.1   2.5694  0.002889 ** \nGEN        27  17533   649.4  17.5359 < 2.2e-16 ***\nENV:GEN   135  23762   176.0   4.7531 < 2.2e-16 ***\nResiduals 324  11998    37.0                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\n# The same anova table by lm\n# mod_plrv <- lm(Yield~Genotype*Locality +  Rep:Locality, data = plrv) # same\nmod_plrv <- lm(Yield~Genotype*Locality +  Rep %in% Locality, data = plrv)\nanova(mod_plrv)\n\n\nAnalysis of Variance Table\n\nResponse: Yield\n                   Df Sum Sq Mean Sq  F value    Pr(>F)    \nGenotype           27  17533   649.4  17.5359 < 2.2e-16 ***\nLocality            5 122284 24456.9 660.4343 < 2.2e-16 ***\nGenotype:Locality 135  23762   176.0   4.7531 < 2.2e-16 ***\nLocality:Rep       12   1142    95.1   2.5694  0.002889 ** \nResiduals         324  11998    37.0                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "plant_breeding/plant_breeding.html#genxenv",
    "href": "plant_breeding/plant_breeding.html#genxenv",
    "title": "27  Plant Breeding",
    "section": "29.2 genXenv",
    "text": "29.2 genXenv\nThese are residuals from the regression of only gen and env main effects.\n\n\nCode\n# AMMI version\nmod_ammi$genXenv %>% head()\n\n\n        ENV\nGEN             Ayac     Hyo-02     LM-02      LM-03       SR-02      SR-03\n  102.18   5.5726162 -12.491822  1.742525  -2.707044   2.9173487  4.9663762\n  104.22  -2.8712076   7.168410  3.933622  -4.035837   0.4788158 -4.6738028\n  121.31   0.3255230  -3.866684  4.318281  10.436613 -11.8834384  0.6697043\n  141.28  -0.9451837   5.645482 -9.780664  14.646310  -4.8033711 -4.7625741\n  157.26 -10.3149711 -10.624168  4.233636  16.868361   2.7171021 -2.8799609\n  163.9    3.0874931  -6.941672  3.496379 -12.553327   7.0168816  5.8942454\n\n\n\n\nCode\n# by hand\nmean_plrv <- plrv %>% group_by(Genotype, Locality) %>% \n  summarize(Yield = mean(Yield), .groups = \"drop_last\") %>% \n  add_column(residuals = lm(Yield~Genotype + Locality, data = .)$residuals)\n\nplrv_int <- mean_plrv %>% pivot_wider(id_cols = \"Genotype\", names_from = \"Locality\", values_from = \"residuals\")\nplrv_int %>% head()\n\n\n# A tibble: 6 × 7\n# Groups:   Genotype [6]\n  Genotype    Ayac `Hyo-02` `LM-02` `LM-03` `SR-02` `SR-03`\n  <fct>      <dbl>    <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 102.18     5.57    -12.5     1.74   -2.71   2.92    4.97 \n2 104.22    -2.87      7.17    3.93   -4.04   0.479  -4.67 \n3 121.31     0.326    -3.87    4.32   10.4  -11.9     0.670\n4 141.28    -0.945     5.65   -9.78   14.6   -4.80   -4.76 \n5 157.26   -10.3     -10.6     4.23   16.9    2.72   -2.88 \n6 163.9      3.09     -6.94    3.50  -12.6    7.02    5.89"
  },
  {
    "objectID": "plant_breeding/plant_breeding.html#svd",
    "href": "plant_breeding/plant_breeding.html#svd",
    "title": "27  Plant Breeding",
    "section": "29.3 SVD",
    "text": "29.3 SVD\n\n\nCode\n# AMMI version\nmod_ammi$analysis\n\n\n    percent  acum Df     Sum.Sq   Mean.Sq F.value   Pr.F\nPC1    56.3  56.3 31 13368.5954 431.24501   11.65 0.0000\nPC2    27.1  83.3 29  6427.5799 221.64069    5.99 0.0000\nPC3     9.4  92.7 27  2241.9398  83.03481    2.24 0.0005\nPC4     4.3  97.1 25  1027.5785  41.10314    1.11 0.3286\nPC5     2.9 100.0 23   696.1012  30.26527    0.82 0.7059\n\n\n\n\nCode\n# by hand\nplrv_mat <- plrv_int %>% data.matrix() %>% `[`(,-1)\n\ns <- svd(plrv_mat)\nU <- s$u\nL <- s$d\nV <- s$v\n\n# sum of squares for each orthogonal component is simply sum(singular values^2). We multiply by 3 because we averaged over 3 reps\nSS <- (L^2) * 3\nscales::number(SS, big.mark = \"\", accuracy = .001)\n\n\n[1] \"13368.595\" \"6427.580\"  \"2241.940\"  \"1027.579\"  \"696.101\"   \"0.000\"    \n\n\nthe sum of squares, is because the frobenius norm is invariant to unitary changes, thus, the SVD, U and V matrices don’t affect the norm, and we can just calculate frobenius norm of the singular values. See proof on site facts of unitary invariant norms\n\n\nCode\n# total is the same as SS of the GE in the anova table\nSSTotal <- sum(SS, na.rm = TRUE)\npercent <- (1/SSTotal) * SS * 100\n\n# percent is just the ratio to total.\nscales::number(percent, accuracy = .01)\n\n\n[1] \"56.26\" \"27.05\" \"9.44\"  \"4.32\"  \"2.93\"  \"0.00\" \n\n\nThe formula for degrees of freedom is\n\n\\begin{aligned}\ndf_n = i + j - 1 - 2m\n\\end{aligned}\n\nwhere i is number of genotypes, j is number of environments, and m is how many components you choose.\nnot sure the derivation, but details should be in:\n\nZobel, R. W., Wright, M.J., and Gauch, H. G. (1988). Statistical analysis of a yield trial. Agronomy Journal, 388-393.\n\n\n\nCode\n# df in table \n28 + 6 - 1 - 2*(1:5)\n\n\n[1] 31 29 27 25 23\n\n\n\n\nCode\n# mean squares of residual from anova table\nSSres <- 37.0 # df = 31\n\n# F = MS / MSres, df from the tables and formulas\n1 - pf(431.24501 / 37, 31, 31)\n\n\n[1] 4.109519e-10"
  },
  {
    "objectID": "plant_breeding/plant_breeding.html#biplots",
    "href": "plant_breeding/plant_breeding.html#biplots",
    "title": "27  Plant Breeding",
    "section": "29.4 Biplots",
    "text": "29.4 Biplots\nbiplots are just principal components."
  },
  {
    "objectID": "plant_breeding/plant_breeding.html#crossa.wheat-example",
    "href": "plant_breeding/plant_breeding.html#crossa.wheat-example",
    "title": "27  Plant Breeding",
    "section": "30.1 crossa.wheat example",
    "text": "30.1 crossa.wheat example\nWe use the crossa wheat example to see how the method works. see ?crossa.wheat for details.\n\n\nCode\ndata(crossa.wheat)\n\n\nWarning in data(crossa.wheat): data set 'crossa.wheat' not found\n\n\n\n\nCode\nlibrary(agridat)\ndata(crossa.wheat)\nm1 <- gge(crossa.wheat, yield ~ gen*loc , env.group=locgroup, scale=FALSE, ggb = TRUE)\nplot(m1)\n\n\n\n\n\nCode\nbiplot(m1)\n\n\n\n\n\nPlot explanations:\n\nThe proportion explained plot - is just the ratio of singular values squared\nRaster - is showing the heatmap of probably the residuals after regression on G + G:E i think?\nbiplot - first two principal components with vectors for each environment."
  },
  {
    "objectID": "plant_breeding/plant_breeding.html#plrv-example-reproduction",
    "href": "plant_breeding/plant_breeding.html#plrv-example-reproduction",
    "title": "27  Plant Breeding",
    "section": "30.2 plrv example (reproduction)",
    "text": "30.2 plrv example (reproduction)\nWe use the plrv example to try and recreate the components of the GGE analysis\n\n\nCode\nm2 <- gge(plrv, formula = Yield~Genotype*Locality, center=TRUE, scale = TRUE)\nm2 %>% names()\n\n\n [1] \"x\"          \"x.orig\"     \"genCoord\"   \"locCoord\"   \"blockCoord\"\n [6] \"gen.group\"  \"env.group\"  \"ggb\"        \"genMeans\"   \"mosdat\"    \n[11] \"R2\"         \"center\"     \"scale\"      \"method\"     \"pctMiss\"   \n[16] \"maxPCs\"    \n\n\n\n\nCode\n# x.orig is the means grouped by genotype and locality, first dimension is genotype\nx.orig <- plrv %>% group_by(Genotype, Locality) %>% \n  summarize(mean_yield = mean(Yield), .groups = \"drop_last\") %>% \n  pivot_wider(id_cols = \"Genotype\", names_from = \"Locality\", values_from = \"mean_yield\") %>% \n  data.matrix() %>% \n  `[`(,-1)\n\n# gge$x is center scaled matrix\nx <- x.orig %>% scale()\n\n\n\n\nCode\n# genMeans is just average across all the environments\ngenMeans <- rowMeans(x, na.rm = TRUE)\n# genMeans %*% t(rep(1, ncol(x)))"
  },
  {
    "objectID": "social_network/social_network.html",
    "href": "social_network/social_network.html",
    "title": "28  Social Network Analysis",
    "section": "",
    "text": "29 Visualizing Networks\nSection follows Dai Shizuka’s Intro to MRQAP\nIt seems that netlm is simply vectorizing the upper triangle of the matrix and running lm. In this case, they have 1 response and 1 covariate matrix. There is some additional output for the coefficient distribution.\nFor the additional “testing of null hypothesis” version, see the paper references, but it involves first regressing on other covariates to “take out the effect”, Now we use the residuals in place of the original covariate and shuffle them for the permutation test. It’s unclear that there is any structure to the permutations.\nAnalagous to classic mixture models, and used to fit stochastic block models\n\\begin{aligned}\n\\mathbb{P}_\\theta (Y = y) = \\frac{1}{\\kappa}\\exp \\left\\{\\sum_{q,r} \\theta_{qr}L_{qr}(y)\\right\\}\n\\end{aligned}\n where L_{qr}(y) is the number of edges in the observed graph y connecting pairs of vertices of classes between q and r.\nAlgorithmically, the EM algorithm is a natural choice here.\nUnfortunately, couldn’t get mixer to work because it errored out in installation.\nLoosely speaking, latent network models extract the eigenspace of relational data, essentially the eigen decomposition of the symmetric adjacency matrix. We follow Kolacyzk’s statistical analysis of network data in this section.\nBelow are the 3 latent variable spaces based on"
  },
  {
    "objectID": "social_network/social_network.html#introduction",
    "href": "social_network/social_network.html#introduction",
    "title": "28  Social Network Analysis",
    "section": "30.1 Introduction",
    "text": "30.1 Introduction\nStands for Simulation Investigation for Empirical Network Analysis, named after the city in which the work for this started.\nProgram for statistical analysis of network data, with the focus on social networks. There’s some interesting context for this\nThe basic model fit"
  },
  {
    "objectID": "social_network/social_network.html#resources",
    "href": "social_network/social_network.html#resources",
    "title": "28  Social Network Analysis",
    "section": "30.2 Resources",
    "text": "30.2 Resources\n\nInformational Website - the home website for the SIENNA Project. The website also contains many helpful example scripts and datasets.\nManual for R - This is the place to get started with SIENA modeling, with a longer, practical introduction to using their software.\nDuke 2018 Workshop Materials - There’s some powerpoints with details calculations of how each objective function and evaluation function actually plays out for change in the dyad.\n\nLinks to all the resources and lectures - the entire workshop materials listed on the website."
  },
  {
    "objectID": "social_network/social_network.html#model-specification",
    "href": "social_network/social_network.html#model-specification",
    "title": "28  Social Network Analysis",
    "section": "30.3 Model Specification",
    "text": "30.3 Model Specification\nSee Section 5 of Manual for R\nThere are 4 types of functions in the model specification:\n\nRate functions - how often do we get the opportunity to change around the specification?\nEvaluation functions - determines the probability of changing their ties. Can be thought of evaluating “satisfaction” for each actor’s local environment, actors will change to increase their total satisfaction.\nCreation function - probability of changes only in the creation of new ties.\nMaintenance function - probability of changes only in the maintenance and termination of existing ties.\n\nMost of the time we’re only using the first 2, rate and evaluation, with most of the focus on the evaluation function"
  },
  {
    "objectID": "social_network/social_network.html#meta-analysis-in-rsienna",
    "href": "social_network/social_network.html#meta-analysis-in-rsienna",
    "title": "28  Social Network Analysis",
    "section": "30.4 Meta Analysis in Rsienna",
    "text": "30.4 Meta Analysis in Rsienna\nThe idea is to fit SABM’s to each network, and combine the parameter estimates intelligently from all the studies. We could also consider combining the network into one large network and add structural zeros for nodes from different networks, this would also work, but apparently it’s very slow running as a large diagonal block matrix in RSienna."
  },
  {
    "objectID": "social_network/social_network.html#examples",
    "href": "social_network/social_network.html#examples",
    "title": "28  Social Network Analysis",
    "section": "30.5 Examples",
    "text": "30.5 Examples\n\n30.5.1 PH819 Lab (single network, two timepoints)\n\n\nCode\nfriend.data.w1 <- as.matrix(read.table(\"data/class5809.txt\"))\nfriend.data.w2 <- as.matrix(read.table(\"data/class5809_t2.txt\"))\nattr5809<-read.table(\"data/attr5809.txt\",header=TRUE,stringsAsFactors=FALSE)\n\nfor (i in 1:nrow(attr5809)) {if (attr5809[i,50] > 98) {friend.data.w2[i,]=9}}\nfriend.data.w1[friend.data.w1 %in% c(6,9)] <- NA\nfriend.data.w2[friend.data.w2 %in% c(6,9)] <- NA\n\nalcfrq<-cbind(attr5809[,45],attr5809[,48])\nalcfrq[alcfrq %in% c(4,5,6)] <- 4\nalcfrq[alcfrq %in% c(99)] <- NA\n\n\n\n\nCode\n# Visualize the networks that we're working with\npar(mfrow = c(1, 2), oma = c(0, 0, 0, 0), mar = c(0, 0, 1, 0))\n\nkcoord <- gplot.layout.fruchtermanreingold(friend.data.w1, layout.par = NULL)\ngplot(friend.data.w1, coord = kcoord, main = \"timepoint 1\", vertex.cex = alcfrq[,1], object.scale = .005)\ngplot(friend.data.w2, coord = kcoord, main = \"timepoint 2\", vertex.cex = alcfrq[,2], object.scale = .005)\n\n\n\n\n\n\n\nCode\n# create the network (response) and the behavior attributes for each timepoint\nfriendties <- sienaNet(array(c(friend.data.w1, friend.data.w2),\n                             dim=c(nrow(attr5809), nrow(attr5809), 2)))\nalcdrinkbeh <- sienaNet(alcfrq, type=\"behavior\")\n\n# create data covariates\nah_pvt <- coCovar(attr5809[,36]) # denotes constant Covariates\nage <- coCovar(attr5809[,38]) \nmale <- coCovar(attr5809[,40])\nparentdrkfrq <- coCovar(attr5809[,53])\nparentdrkfiv <- coCovar(attr5809[,54])\ntobacco <- coCovar(attr5809[,56])\n\n\n\n\nCode\n# create the data object for siena\ndata5809 <- sienaDataCreate(friendties,\n                            ah_pvt, age, male, parentdrkfrq,\n                            parentdrkfiv,tobacco,alcdrinkbeh)\neff5809 <- getEffects(data5809)\n\n# prints out a txtfile with more information\nprint01Report(data5809, modelname = 'class5809_init')\n\n\n\n\nCode\n# Fit the null model, and display the summary without covariance\n# creates output file \"class5809_run1.txt\"\nmodel5809 <- sienaModelCreate(useStdInits = FALSE, \n                              projname = 'class5809_run1')\nans5809 <- siena07(model5809, data=data5809, effects=eff5809, \n                   batch=FALSE, verbose=FALSE)\n\n\n\n\nCode\nans5809 # display results from fitting\n\n\nEstimates, standard errors and convergence t-ratios\n\n                                          Estimate   Standard   Convergence \n                                                       Error      t-ratio   \nNetwork Dynamics \n  1. rate basic rate parameter friendties  8.5126  ( 0.5487   )   -0.0096   \n  2. eval outdegree (density)             -2.7250  ( 0.0444   )   -0.0058   \n  3. eval reciprocity                      2.5915  ( 0.1137   )   -0.0510   \n\nBehavior Dynamics\n  4. rate rate alcdrinkbeh period 1        5.7149  ( 0.9284   )   -0.0260   \n  5. eval alcdrinkbeh linear shape        -0.3687  ( 0.0561   )   -0.0001   \n  6. eval alcdrinkbeh quadratic shape      0.1546  ( 0.0331   )    0.0059   \n\nOverall maximum convergence ratio:    0.0788 \n\n\nTotal of 2669 iteration steps.\n\n\n\n\nCode\n# Wald-style p-values can be calculated by\n\n1 - pnorm(abs(ans5809$theta / ans5809$se))\n\n\n[1] 0.000000e+00 0.000000e+00 0.000000e+00 3.738017e-10 2.470590e-11\n[6] 1.457415e-06\n\n\n\n\n30.5.2 PH819 Lab Meta analysis (4 networks, 2 timepoints each)\nThere’s a massive amount of code for this, and we need to run SABM for all 4 models before we can actually do a meta analysis\n\n\nCode\n# Network 1: class 7\nfriend.data.w1 <- as.matrix(read.table(\"data/class7.txt\"))\nfriend.data.w2 <- as.matrix(read.table(\"data/class7_t2.txt\"))\nattr7<-read.table(\"data/attr7.txt\",header=TRUE,stringsAsFactors=FALSE)\nfor (i in 1:nrow(attr7)) {if (attr7[i,48] > 98) {friend.data.w2[i,]=9}}\nfriend.data.w1[friend.data.w1 %in% c(6,9)] <- NA\nfriend.data.w2[friend.data.w2 %in% c(6,9)] <- NA\nalcdrinkbeh<-cbind(attr7[,45],attr7[,48])\nalcdrinkbeh[alcdrinkbeh %in% c(99)] <- NA\nfriendties <- sienaNet(array(c(friend.data.w1,friend.data.w2),dim=c(nrow(attr7),nrow(attr7),2)))\nalcdrinkbeh <- sienaNet(alcdrinkbeh,type=\"behavior\")\nah_pvt <- coCovar(attr7[,36])\nage <- coCovar(attr7[,38])\nmale <- coCovar(attr7[,40])\nparentdrkfrq <- coCovar(attr7[,53])\ntobacco <- coCovar(attr7[,56])\ndataset.7 <- sienaDataCreate(friendties,ah_pvt,age,male,parentdrkfrq,tobacco,alcdrinkbeh)\neffects.7 <- getEffects(dataset.7)\neffectsDocumentation(effects.7)\n\neffects.7 <- includeEffects(effects.7,transTrip,type=\"eval\")\neffects.7 <- includeEffects(effects.7,cycle3,type=\"eval\")\neffects.7 <- includeEffects(effects.7,simX,type=\"eval\",interaction1=\"age\") #age similarity\neffects.7 <- includeEffects(effects.7,sameX,type=\"eval\",interaction1=\"male\") #same gender\neffects.7 <- includeEffects(effects.7,simX,type=\"eval\",interaction1=\"ah_pvt\") #similar scholastic aptitude\neffects.7 <- includeEffects(effects.7,simX,type=\"eval\",interaction1=\"alcdrinkbeh\") #similar alcoholic use\neffects.7 <- includeEffects(effects.7,name=\"alcdrinkbeh\",maxAlt,interaction1=\"friendties\") #average exposure\n# effects.7 <- includeEffects(effects.7,name=\"alcdrinkbeh\",avRecAlt,interaction1=\"friendties\") #average exposure\neffects.7 <- includeEffects(effects.7,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"age\") #age effect\neffects.7 <- includeEffects(effects.7,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"parentdrkfrq\") #parent drinking effect\neffects.7 <- includeEffects(effects.7,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"tobacco\") #tobacco effect\neffects.7 <- includeEffects(effects.7,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"ah_pvt\") #ah_pvt effect\n\nmodelall <- sienaModelCreate(useStdInits = FALSE, projname = 'AlcoholBehavior')\n\nans.7 <- siena07(modelall, data=dataset.7, effects=effects.7, batch=FALSE)\n\n\n\n\nCode\n# Network 2: class 5809\nfriend.data.w1 <- as.matrix(read.table(\"data/class5809.txt\"))\nfriend.data.w2 <- as.matrix(read.table(\"data/class5809_t2.txt\"))\nattr5809<-read.table(\"data/attr5809.txt\",header=TRUE,stringsAsFactors=FALSE)\nfor (i in 1:nrow(attr5809)) {if (attr5809[i,48] > 98) {friend.data.w2[i,]=9}}\nfriend.data.w1[friend.data.w1 %in% c(6,9)] <- NA\nfriend.data.w2[friend.data.w2 %in% c(6,9)] <- NA\nalcdrinkbeh<-cbind(attr5809[,45],attr5809[,48])\nalcdrinkbeh[alcdrinkbeh %in% c(99)] <- NA\nfriendties <- sienaNet(array(c(friend.data.w1,friend.data.w2),dim=c(nrow(attr5809),nrow(attr5809),2)))\nalcdrinkbeh <- sienaNet(alcdrinkbeh,type=\"behavior\")\nah_pvt <- coCovar(attr5809[,36])\n\nage <- coCovar(attr5809[,38])\nmale <- coCovar(attr5809[,40])\nparentdrkfrq <- coCovar(attr5809[,53])\ntobacco <- coCovar(attr5809[,56])\ndataset.5809 <- sienaDataCreate(friendties,ah_pvt,age,male,parentdrkfrq,tobacco,alcdrinkbeh)\neffects.5809 <- getEffects(dataset.5809)\n\neffects.5809 <- includeEffects(effects.5809,transTrip,type=\"eval\")\neffects.5809 <- includeEffects(effects.5809,cycle3,type=\"eval\")\neffects.5809 <- includeEffects(effects.5809,simX,type=\"eval\",interaction1=\"age\") #age similarity\neffects.5809 <- includeEffects(effects.5809,sameX,type=\"eval\",interaction1=\"male\") #same gender\neffects.5809 <- includeEffects(effects.5809,simX,type=\"eval\",interaction1=\"ah_pvt\") #similar scholastic aptitude\neffects.5809 <- includeEffects(effects.5809,simX,type=\"eval\",interaction1=\"alcdrinkbeh\") #similar alcoholic use\neffects.5809 <- includeEffects(effects.5809,name=\"alcdrinkbeh\",maxAlt,interaction1=\"friendties\") #average exposure\n# effects.5809 <- includeEffects(effects.5809,name=\"alcdrinkbeh\",avRecAlt,interaction1=\"friendties\") #average exposure\neffects.5809 <- includeEffects(effects.5809,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"age\") #age effect\neffects.5809 <- includeEffects(effects.5809,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"parentdrkfrq\") #parent drinking effect\neffects.5809 <- includeEffects(effects.5809,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"tobacco\") #tobacco effect\neffects.5809 <- includeEffects(effects.5809,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"ah_pvt\") #ah_pvt effect\n\nmodelall <- sienaModelCreate(useStdInits = FALSE, projname = 'AlcoholBehavior')\n\nans.5809 <- siena07(modelall, data=dataset.5809, effects=effects.5809, batch=FALSE)\n\n\n\n\nCode\n# Network 3: class 8\nfriend.data.w1 <- as.matrix(read.table(\"data/class8.txt\"))\nfriend.data.w2 <- as.matrix(read.table(\"data/class8_t2.txt\"))\nattr8<-read.table(\"data/attr8.txt\",header=TRUE,stringsAsFactors=FALSE)\nfor (i in 1:nrow(attr8)) {if (attr8[i,48] > 98) {friend.data.w2[i,]=9}}\nfriend.data.w1[friend.data.w1 %in% c(6,9)] <- NA\nfriend.data.w2[friend.data.w2 %in% c(6,9)] <- NA\n\nalcdrinkbeh<-cbind(attr8[,45],attr8[,48])\nalcdrinkbeh[alcdrinkbeh %in% c(99)] <- NA\nfriendties <- sienaNet(array(c(friend.data.w1,friend.data.w2),dim=c(nrow(attr8),nrow(attr8),2)))\nalcdrinkbeh <- sienaNet(alcdrinkbeh,type=\"behavior\")\nah_pvt <- coCovar(attr8[,36])\nage <- coCovar(attr8[,38])\nmale <- coCovar(attr8[,40])\nparentdrkfrq <- coCovar(attr8[,53])\ntobacco <- coCovar(attr8[,56])\ndataset.8 <- sienaDataCreate(friendties,ah_pvt,age,male,parentdrkfrq,tobacco,alcdrinkbeh)\neffects.8 <- getEffects(dataset.8)\neffects.8 <- includeEffects(effects.8,transTrip,type=\"eval\")\neffects.8 <- includeEffects(effects.8,cycle3,type=\"eval\")\neffects.8 <- includeEffects(effects.8,simX,type=\"eval\",interaction1=\"age\") #age similarity\neffects.8 <- includeEffects(effects.8,sameX,type=\"eval\",interaction1=\"male\") #same gender\neffects.8 <- includeEffects(effects.8,simX,type=\"eval\",interaction1=\"ah_pvt\") #similar scholastic aptitude\neffects.8 <- includeEffects(effects.8,simX,type=\"eval\",interaction1=\"alcdrinkbeh\") #similar alcoholic use\neffects.8 <- includeEffects(effects.8,name=\"alcdrinkbeh\",maxAlt,interaction1=\"friendties\") #average exposure\n# effects.8 <- includeEffects(effects.8,name=\"alcdrinkbeh\",avRecAlt,interaction1=\"friendties\") #average exposure\neffects.8 <- includeEffects(effects.8,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"age\") #age effect\neffects.8 <- includeEffects(effects.8,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"parentdrkfrq\") #parent drinking effect\neffects.8 <- includeEffects(effects.8,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"tobacco\") #tobacco effect\neffects.8 <- includeEffects(effects.8,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"ah_pvt\") #ah_pvt effect\n\nmodelall <- sienaModelCreate(useStdInits = FALSE, projname = 'AlcoholBehavior')\n\nans.8 <- siena07(modelall, data=dataset.8, effects=effects.8, batch=FALSE)\n\n\n\n\nCode\n# Network 4: class 88\nfriend.data.w1 <- as.matrix(read.table(\"data/class88.txt\"))\nfriend.data.w2 <- as.matrix(read.table(\"data/class88_t2.txt\"))\nattr88<-read.table(\"data/attr88.txt\",header=TRUE,stringsAsFactors=FALSE)\nfor (i in 1:nrow(attr88)) {if (attr88[i,48] > 98) {friend.data.w2[i,]=9}}\nfriend.data.w1[friend.data.w1 %in% c(6,9)] <- NA\nfriend.data.w2[friend.data.w2 %in% c(6,9)] <- NA\nalcdrinkbeh<-cbind(attr88[,45],attr88[,48])\nalcdrinkbeh[alcdrinkbeh %in% c(99)] <- NA\nfriendties <- sienaNet(array(c(friend.data.w1,friend.data.w2),dim=c(nrow(attr88),nrow(attr88),2)))\nalcdrinkbeh <- sienaNet(alcdrinkbeh,type=\"behavior\")\nah_pvt <- coCovar(attr88[,36])\nage <- coCovar(attr88[,38])\nmale <- coCovar(attr88[,40])\nparentdrkfrq <- coCovar(attr88[,53])\ntobacco <- coCovar(attr88[,56])\ndataset.88 <- sienaDataCreate(friendties,ah_pvt,age,male,parentdrkfrq,tobacco,alcdrinkbeh)\neffects.88 <- getEffects(dataset.88)\n\neffects.88 <- includeEffects(effects.88,transTrip,type=\"eval\")\neffects.88 <- includeEffects(effects.88,cycle3,type=\"eval\")\neffects.88 <- includeEffects(effects.88,simX,type=\"eval\",interaction1=\"age\") #age similarity\neffects.88 <- includeEffects(effects.88,sameX,type=\"eval\",interaction1=\"male\") #same gender\neffects.88 <- includeEffects(effects.88,simX,type=\"eval\",interaction1=\"ah_pvt\") #similar scholastic aptitude\neffects.88 <- includeEffects(effects.88,simX,type=\"eval\",interaction1=\"alcdrinkbeh\") #similar alcoholic use\neffects.88 <- includeEffects(effects.88,name=\"alcdrinkbeh\",maxAlt,interaction1=\"friendties\") #average exposure\n# effects.88 <- includeEffects(effects.88,name=\"alcdrinkbeh\",avRecAlt,interaction1=\"friendties\") #average exposure\neffects.88 <- includeEffects(effects.88,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"age\") #age effect\neffects.88 <- includeEffects(effects.88,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"parentdrkfrq\") #parent drinking effect\neffects.88 <- includeEffects(effects.88,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"tobacco\") #tobacco effect\neffects.88 <- includeEffects(effects.88,name=\"alcdrinkbeh\",effFrom,type=\"eval\",interaction1=\"ah_pvt\") #ah_pvt effect\n\nmodelall <- sienaModelCreate(useStdInits = FALSE, projname = 'AlcoholBehavior')\n\nans.88 <- siena07(modelall, data=dataset.88, effects=effects.88, batch=FALSE)\n\n\nthe actual meta analysis is done by the function siena08(), based on Siena fits.\n\nMust have the same covariates fit in the networks. The output of the file is pretty self explanatory.\nThere’s an automatic warning/filtering if the standard error of any of the variable estimates is >5 because this is a sign of instability, and need to model the networks again.\nIf you have fit warning, the advice is to check the output simulations and goodness-of-fit parameters to make sure we have a good fit\n\n\n\nCode\nmeta <- siena08(ans.5809, ans.88, ans.7, ans.8)\n\n\nWarning in iwlsm.default(x, y, weights, method = method, wt.method =\nwt.method, : iwlsm failed to converge in 20 steps\n\n\nCode\nsummary(meta)\n\n\n================================= SIENA08 ================================================\nMultilevel use of Siena algorithms according to Snijders & Baerveldt (2003) with extension\n==========================================================================================\n\nNumber of projects in the list is 4.\nThe names of these projects are :\nproject1: <AlcoholBehavior(ans.5809)>\nproject2: <AlcoholBehavior(ans.88)>\nproject3: <AlcoholBehavior(ans.7)>\nproject4: <AlcoholBehavior(ans.8)>\n\nOptions for running Siena08:\n-> Parameters are excluded from the meta-analysis when their standard\n   error exceeds an upper bound of 5.\n-> Extra output requested\n\nThe RSiena Version of the first fit object is 1.3.14.\n\nObject <AlcoholBehavior(ans.5809)> contains estimates of 17 parameters.\nThe number of valid score tests found was 17.\nObject <AlcoholBehavior(ans.7)> contains estimates of 17 parameters.\nThe number of valid score tests found was 17.\nObject <AlcoholBehavior(ans.8)> contains estimates of 17 parameters.\nThe number of valid score tests found was 17.\nObject <AlcoholBehavior(ans.88)> contains estimates of 17 parameters.\nThe number of valid score tests found was 17.\n\nA total of 17 parameters in 4 projects :\n 1. rate : basic rate parameter friendties\n 2. eval : outdegree (density)\n 3. eval : reciprocity\n 4. eval : transitive triplets\n 5. eval : 3-cycles\n 6. eval : ah_pvt similarity\n 7. eval : age similarity\n 8. eval : same male\n 9. eval : alcdrinkbeh similarity\n10. rate : rate alcdrinkbeh period 1\n11. eval : alcdrinkbeh linear shape\n12. eval : alcdrinkbeh quadratic shape\n13. eval : alcdrinkbeh max. alter\n14. eval : alcdrinkbeh: effect from ah_pvt\n15. eval : alcdrinkbeh: effect from age\n16. eval : alcdrinkbeh: effect from parentdrkfrq\n17. eval : alcdrinkbeh: effect from tobacco\n\nThe projects contain the parameters as follows (1=present, 0=absent):\n\nProject  |    1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17\n---------+-----------------------------------------------------\n   1     |    1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n   2     |    1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n   3     |    1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n   4     |    1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n\nProject AlcoholBehavior(ans.5809) \npar. 1 estimate       9.8292 (s.e.       0.5833 ) (conv_t   -0.0144 )\n par. 2 estimate      -3.1471 (s.e.       0.0839 ) (conv_t    0.0098 )\n par. 3 estimate       2.3902 (s.e.       0.1302 ) (conv_t    0.0045 )\n par. 4 estimate       0.6644 (s.e.       0.0612 ) (conv_t    0.0027 )\n par. 5 estimate      -0.4685 (s.e.       0.1267 ) (conv_t    0.0183 )\n par. 6 estimate       0.6474 (s.e.       0.2360 ) (conv_t   -0.0576 )\n par. 7 estimate      -0.0497 (s.e.       0.1852 ) (conv_t    0.0193 )\n par. 8 estimate       0.2841 (s.e.       0.0721 ) (conv_t    0.0165 )\n par. 9 estimate       1.6232 (s.e.       0.7060 ) (conv_t    0.0291 )\n par. 10 estimate       6.2508 (s.e.       1.0278 ) (conv_t   -0.0418 )\npar. 11 estimate      -0.3628 (s.e.       0.0930 ) (conv_t    0.0965 )\npar. 12 estimate       0.0045 (s.e.       0.0223 ) (conv_t   -0.0025 )\npar. 13 estimate       0.0133 (s.e.       0.0740 ) (conv_t    0.0252 )\npar. 14 estimate      -0.0026 (s.e.       0.0051 ) (conv_t    0.1028 )\npar. 15 estimate      -0.0362 (s.e.       0.1053 ) (conv_t   -0.0091 )\npar. 16 estimate       0.0130 (s.e.       0.0421 ) (conv_t   -0.0356 )\npar. 17 estimate       0.4565 (s.e.       0.1662 ) (conv_t   -0.0228 )\n\nMaximal absolute convergence t-statistic =     0.1028 \n\nProject AlcoholBehavior(ans.7) \npar. 1 estimate       8.0083 (s.e.       0.6869 ) (conv_t    0.0567 )\n par. 2 estimate      -2.9324 (s.e.       0.1028 ) (conv_t    0.0195 )\n par. 3 estimate       2.4027 (s.e.       0.1719 ) (conv_t    0.0725 )\n par. 4 estimate       0.7559 (s.e.       0.0878 ) (conv_t   -0.0255 )\n par. 5 estimate      -0.7416 (s.e.       0.1987 ) (conv_t   -0.0204 )\n par. 6 estimate       0.9396 (s.e.       0.3295 ) (conv_t   -0.0089 )\n par. 7 estimate       1.2576 (s.e.       0.2569 ) (conv_t    0.0308 )\n par. 8 estimate       0.3966 (s.e.       0.1017 ) (conv_t    0.0038 )\n par. 9 estimate       0.4264 (s.e.       0.3829 ) (conv_t   -0.0051 )\n par. 10 estimate       5.1388 (s.e.       1.2683 ) (conv_t    0.0269 )\npar. 11 estimate      -0.3858 (s.e.       0.1158 ) (conv_t    0.0550 )\npar. 12 estimate       0.0332 (s.e.       0.0346 ) (conv_t    0.0540 )\npar. 13 estimate      -0.1452 (s.e.       0.1205 ) (conv_t    0.0258 )\npar. 14 estimate       0.0097 (s.e.       0.0088 ) (conv_t   -0.0302 )\npar. 15 estimate      -0.0004 (s.e.       0.0549 ) (conv_t    0.0142 )\npar. 16 estimate       0.1426 (s.e.       0.0994 ) (conv_t    0.0132 )\npar. 17 estimate       0.3567 (s.e.       0.2080 ) (conv_t    0.0045 )\n\nMaximal absolute convergence t-statistic =     0.0725 \n\nProject AlcoholBehavior(ans.8) \npar. 1 estimate       6.5754 (s.e.       0.6509 ) (conv_t    0.0354 )\n par. 2 estimate      -2.4235 (s.e.       0.1275 ) (conv_t    0.0368 )\n par. 3 estimate       1.6312 (s.e.       0.2647 ) (conv_t   -0.0154 )\n par. 4 estimate       0.6344 (s.e.       0.0816 ) (conv_t   -0.0014 )\n par. 5 estimate      -0.4651 (s.e.       0.1770 ) (conv_t   -0.0199 )\n par. 6 estimate       0.7482 (s.e.       0.3947 ) (conv_t    0.0306 )\n par. 7 estimate       1.3274 (s.e.       0.3425 ) (conv_t    0.0113 )\n par. 8 estimate       0.2883 (s.e.       0.1199 ) (conv_t   -0.0067 )\n par. 9 estimate       0.8932 (s.e.       0.5473 ) (conv_t   -0.0251 )\n par. 10 estimate       4.7190 (s.e.       1.9662 ) (conv_t   -0.0388 )\npar. 11 estimate      -0.6328 (s.e.       0.2101 ) (conv_t   -0.0179 )\npar. 12 estimate       0.0507 (s.e.       0.0520 ) (conv_t   -0.0806 )\npar. 13 estimate       0.0674 (s.e.       0.1825 ) (conv_t   -0.0648 )\npar. 14 estimate       0.0198 (s.e.       0.0141 ) (conv_t   -0.0684 )\npar. 15 estimate       0.0759 (s.e.       0.0701 ) (conv_t   -0.0364 )\npar. 16 estimate       0.0674 (s.e.       0.1173 ) (conv_t    0.0413 )\npar. 17 estimate       0.0612 (s.e.       0.2558 ) (conv_t   -0.0055 )\n\nMaximal absolute convergence t-statistic =     0.0806 \n\nProject AlcoholBehavior(ans.88) \npar. 1 estimate       8.5888 (s.e.       0.9511 ) (conv_t    0.0581 )\n par. 2 estimate      -2.6220 (s.e.       0.1316 ) (conv_t    0.0660 )\n par. 3 estimate       1.6849 (s.e.       0.2560 ) (conv_t    0.0602 )\n par. 4 estimate       0.4889 (s.e.       0.0696 ) (conv_t    0.0932 )\n par. 5 estimate      -0.3216 (s.e.       0.1768 ) (conv_t    0.0907 )\n par. 6 estimate       0.9429 (s.e.       0.3701 ) (conv_t    0.0450 )\n par. 7 estimate       1.7286 (s.e.       0.4198 ) (conv_t    0.0564 )\n par. 8 estimate       0.0924 (s.e.       0.1292 ) (conv_t    0.0600 )\n par. 9 estimate       0.6717 (s.e.       0.5422 ) (conv_t    0.0553 )\n par. 10 estimate       1.8970 (s.e.       0.7808 ) (conv_t    0.0573 )\npar. 11 estimate      -1.1139 (s.e.       0.4904 ) (conv_t    0.0281 )\npar. 12 estimate       0.5294 (s.e.       0.5378 ) (conv_t    0.0504 )\npar. 13 estimate       0.3070 (s.e.       0.4469 ) (conv_t    0.0356 )\npar. 14 estimate      -0.0385 (s.e.       0.0324 ) (conv_t   -0.0657 )\npar. 15 estimate       0.0798 (s.e.       0.1718 ) (conv_t   -0.0079 )\npar. 16 estimate      -0.1119 (s.e.       0.3499 ) (conv_t    0.0128 )\npar. 17 estimate      -0.0197 (s.e.       0.7183 ) (conv_t    0.0150 )\n\nMaximal absolute convergence t-statistic =     0.0932 \n\n\n=============================\nResults of the meta-analysis:\n=============================\n\nUpper bound used for standard error is     5.00.\n\n--------------------------------------------------------------------------------\nParameter 1: rate : basic rate parameter friendties\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate       9.8292 (standard error        0.58)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate       8.5888 (standard error        0.95)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate       8.0083 (standard error        0.69)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate       6.5754 (standard error        0.65)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =     -0.2, two-sided p = 0.917\n\nTest that all parameters are 0 : \nchi-squared = 603.4541, d.f. = 4, p < 0.001\n\nEstimated mean parameter   8.2532 (s.e.   0.6983), two-sided p = 0.001\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error    1.3965\nTest that variance of parameter is 0 :\nChi-squared =   14.1632 (d.f. = 3), p = 0.003\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter   8.2551 (s.e.   0.6316), two-sided p < 0.001\n0.95 level confidence interval [  6.6765 ,  9.8245 ]\nEstimated standard deviation    1.0425 \n0.95 level confidence interval [   0.293 ,   2.926 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =  630.4575 (d.f. = 8), p < 0.001\nCombination of left one-sided p-values:\nChi-squared =         0 (d.f. = 8), p = 1.000\n\n--------------------------------------------------------------------------------\nParameter 2: eval : outdegree (density)\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate      -3.1471 (standard error        0.08)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate      -2.6220 (standard error        0.13)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate      -2.9324 (standard error        0.10)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate      -2.4235 (standard error        0.13)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =      0.8, two-sided p = 0.333\n\nTest that all parameters are 0 : \nchi-squared = 2979.318, d.f. = 4, p < 0.001\n\nEstimated mean parameter  -2.7919 (s.e.   0.1614), two-sided p < 0.001\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error    0.3227\nTest that variance of parameter is 0 :\nChi-squared =   27.0656 (d.f. = 3), p < 0.001\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter  -2.7954 (s.e.   0.1405), two-sided p < 0.001\n0.95 level confidence interval [ -3.1427 , -2.4306 ]\nEstimated standard deviation    0.2577 \n0.95 level confidence interval [  0.1166 ,  0.6775 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =         0 (d.f. = 8), p = 1.000\nCombination of left one-sided p-values:\nChi-squared =  3012.509 (d.f. = 8), p < 0.001\n\n--------------------------------------------------------------------------------\nParameter 3: eval : reciprocity\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate       2.3902 (standard error        0.13)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate       1.6849 (standard error        0.26)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate       2.4027 (standard error        0.17)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate       1.6312 (standard error        0.26)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =     -0.8, two-sided p = 0.333\n\nTest that all parameters are 0 : \nchi-squared = 613.8601, d.f. = 4, p < 0.001\n\nEstimated mean parameter   2.0724 (s.e.   0.2122), two-sided p = 0.002\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error    0.4244\nTest that variance of parameter is 0 :\nChi-squared =   12.1538 (d.f. = 3), p = 0.007\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter   2.0898 (s.e.   0.1793), two-sided p = 0.001\n0.95 level confidence interval [  1.5865 ,  2.5149 ]\nEstimated standard deviation     0.295 \n0.95 level confidence interval [       0 ,  0.8727 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =  639.8221 (d.f. = 8), p < 0.001\nCombination of left one-sided p-values:\nChi-squared =         0 (d.f. = 8), p = 1.000\n\n--------------------------------------------------------------------------------\nParameter 4: eval : transitive triplets\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate       0.6644 (standard error        0.06)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate       0.4889 (standard error        0.07)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate       0.7559 (standard error        0.09)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate       0.6344 (standard error        0.08)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =      0.4, two-sided p = 0.750\n\nTest that all parameters are 0 : \nchi-squared = 301.7613, d.f. = 4, p < 0.001\n\nEstimated mean parameter   0.6304 (s.e.   0.0549), two-sided p = 0.001\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error    0.1098\nTest that variance of parameter is 0 :\nChi-squared =    6.4731 (d.f. = 3), p = 0.091\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter   0.6288 (s.e.   0.0467), two-sided p < 0.001\n0.95 level confidence interval [  0.5127 ,   0.753 ]\nEstimated standard deviation    0.0571 \n0.95 level confidence interval [       0 ,    0.22 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =   326.301 (d.f. = 8), p < 0.001\nCombination of left one-sided p-values:\nChi-squared =         0 (d.f. = 8), p = 1.000\n\n--------------------------------------------------------------------------------\nParameter 5: eval : 3-cycles\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate      -0.4685 (standard error        0.13)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate      -0.3216 (standard error        0.18)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate      -0.7416 (standard error        0.20)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate      -0.4651 (standard error        0.18)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =     -0.4, two-sided p = 0.750\n\nTest that all parameters are 0 : \nchi-squared =  37.8092, d.f. = 4, p < 0.001\n\nEstimated mean parameter  -0.4824 (s.e.   0.0768), two-sided p = 0.008\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error    0.1536\nTest that variance of parameter is 0 :\nChi-squared =    2.5521 (d.f. = 3), p = 0.466\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter  -0.4824 (s.e.   0.0812), two-sided p = 0.010\n0.95 level confidence interval [ -0.6633 , -0.3136 ]\nEstimated standard deviation  < 0.0001 \n0.95 level confidence interval [       0 ,  0.2927 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =    0.0792 (d.f. = 8), p = 1.000\nCombination of left one-sided p-values:\nChi-squared =   54.4054 (d.f. = 8), p < 0.001\n\n--------------------------------------------------------------------------------\nParameter 6: eval : ah_pvt similarity\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate       0.6474 (standard error        0.24)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate       0.9429 (standard error        0.37)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate       0.9396 (standard error        0.33)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate       0.7482 (standard error        0.39)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =      0.4, two-sided p = 0.750\n\nTest that all parameters are 0 : \nchi-squared =  25.7413, d.f. = 4, p < 0.001\n\nEstimated mean parameter   0.7818 (s.e.    0.081), two-sided p = 0.002\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error     0.162\nTest that variance of parameter is 0 :\nChi-squared =    0.7508 (d.f. = 3), p = 0.861\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter   0.7818 (s.e.   0.1564), two-sided p = 0.015\n0.95 level confidence interval [  0.4751 ,  1.1097 ]\nEstimated standard deviation  < 0.0001 \n0.95 level confidence interval [       0 ,  0.4542 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =   41.3675 (d.f. = 8), p < 0.001\nCombination of left one-sided p-values:\nChi-squared =    0.0802 (d.f. = 8), p = 1.000\n\n--------------------------------------------------------------------------------\nParameter 7: eval : age similarity\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate      -0.0497 (standard error        0.19)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate       1.7286 (standard error        0.42)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate       1.2576 (standard error        0.26)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate       1.3274 (standard error        0.34)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =        1, two-sided p = 0.083\n\nTest that all parameters are 0 : \nchi-squared =  56.0198, d.f. = 4, p < 0.001\n\nEstimated mean parameter   1.0169 (s.e.   0.3941), two-sided p = 0.082\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error    0.7882\nTest that variance of parameter is 0 :\nChi-squared =   30.3829 (d.f. = 3), p < 0.001\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter   1.0043 (s.e.   0.3517), two-sided p = 0.065\n0.95 level confidence interval [  0.1597 ,  1.9231 ]\nEstimated standard deviation    0.6341 \n0.95 level confidence interval [  0.2956 ,  1.6637 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =    71.477 (d.f. = 8), p < 0.001\nCombination of left one-sided p-values:\nChi-squared =    1.8621 (d.f. = 8), p = 0.985\n\n--------------------------------------------------------------------------------\nParameter 8: eval : same male\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate       0.2841 (standard error        0.07)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate       0.0924 (standard error        0.13)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate       0.3966 (standard error        0.10)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate       0.2883 (standard error        0.12)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =     -0.4, two-sided p = 0.750\n\nTest that all parameters are 0 : \nchi-squared =  36.9989, d.f. = 4, p < 0.001\n\nEstimated mean parameter   0.2808 (s.e.   0.0565), two-sided p = 0.016\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error    0.1131\nTest that variance of parameter is 0 :\nChi-squared =    3.4271 (d.f. = 3), p = 0.330\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter   0.2833 (s.e.   0.0489), two-sided p = 0.010\n0.95 level confidence interval [  0.1603 ,  0.3909 ]\nEstimated standard deviation  < 0.0001 \n0.95 level confidence interval [       0 ,  0.2035 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =   52.5821 (d.f. = 8), p < 0.001\nCombination of left one-sided p-values:\nChi-squared =    0.5578 (d.f. = 8), p = 1.000\n\n--------------------------------------------------------------------------------\nParameter 9: eval : alcdrinkbeh similarity\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate       1.6232 (standard error        0.71)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate       0.6717 (standard error        0.54)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate       0.4264 (standard error        0.38)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate       0.8932 (standard error        0.55)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =        1, two-sided p = 0.083\n\nTest that all parameters are 0 : \nchi-squared =  10.7244, d.f. = 4, p = 0.013\n\nEstimated mean parameter   0.7344 (s.e.   0.2316), two-sided p = 0.050\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error    0.4631\nTest that variance of parameter is 0 :\nChi-squared =    2.3295 (d.f. = 3), p = 0.507\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter   0.7344 (s.e.   0.2535), two-sided p = 0.063\n0.95 level confidence interval [  0.2367 ,  1.3593 ]\nEstimated standard deviation  < 0.0001 \n0.95 level confidence interval [       0 ,  0.9042 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =   23.5003 (d.f. = 8), p = 0.003\nCombination of left one-sided p-values:\nChi-squared =    0.6398 (d.f. = 8), p = 1.000\n\n--------------------------------------------------------------------------------\nParameter 10: rate : rate alcdrinkbeh period 1\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate       6.2508 (standard error        1.03)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate       1.8970 (standard error        0.78)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate       5.1388 (standard error        1.27)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate       4.7190 (standard error        1.97)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =      0.2, two-sided p = 0.917\n\nTest that all parameters are 0 : \nchi-squared =  65.0635, d.f. = 4, p < 0.001\n\nEstimated mean parameter   4.3543 (s.e.   1.0244), two-sided p = 0.024\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error    2.0488\nTest that variance of parameter is 0 :\nChi-squared =   12.9376 (d.f. = 3), p = 0.005\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter   4.3377 (s.e.   0.9766), two-sided p = 0.021\n0.95 level confidence interval [  2.0426 ,  6.8312 ]\nEstimated standard deviation    1.5289 \n0.95 level confidence interval [  0.4061 ,  4.3436 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =   83.0214 (d.f. = 8), p < 0.001\nCombination of left one-sided p-values:\nChi-squared =    0.0317 (d.f. = 8), p = 1.000\n\n--------------------------------------------------------------------------------\nParameter 11: eval : alcdrinkbeh linear shape\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate      -0.3628 (standard error        0.09)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate      -1.1139 (standard error        0.49)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate      -0.3858 (standard error        0.12)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate      -0.6328 (standard error        0.21)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =       -1, two-sided p = 0.083\n\nTest that all parameters are 0 : \nchi-squared =  40.5536, d.f. = 4, p < 0.001\n\nEstimated mean parameter  -0.4526 (s.e.   0.1063), two-sided p = 0.024\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error    0.2125\nTest that variance of parameter is 0 :\nChi-squared =    3.4839 (d.f. = 3), p = 0.323\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter  -0.4132 (s.e.   0.0679), two-sided p = 0.009\n0.95 level confidence interval [ -0.6298 , -0.2801 ]\nEstimated standard deviation  < 0.0001 \n0.95 level confidence interval [       0 ,  0.2877 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =    0.0268 (d.f. = 8), p = 1.000\nCombination of left one-sided p-values:\nChi-squared =    57.608 (d.f. = 8), p < 0.001\n\n--------------------------------------------------------------------------------\nParameter 12: eval : alcdrinkbeh quadratic shape\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate       0.0045 (standard error        0.02)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate       0.5294 (standard error        0.54)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate       0.0332 (standard error        0.03)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate       0.0507 (standard error        0.05)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =        1, two-sided p = 0.083\n\nTest that all parameters are 0 : \nchi-squared =   2.8786, d.f. = 4, p = 0.411\n\nEstimated mean parameter   0.0178 (s.e.   0.0165), two-sided p = 0.360\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error     0.033\nTest that variance of parameter is 0 :\nChi-squared =    1.8578 (d.f. = 3), p = 0.602\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter   0.0178 (s.e.   0.0176), two-sided p = 0.387\n0.95 level confidence interval [ -0.0175 ,  0.0656 ]\nEstimated standard deviation  < 0.0001 \n0.95 level confidence interval [       0 ,   0.064 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =   12.5311 (d.f. = 8), p = 0.129\nCombination of left one-sided p-values:\nChi-squared =    2.1752 (d.f. = 8), p = 0.975\n\n--------------------------------------------------------------------------------\nParameter 13: eval : alcdrinkbeh max. alter\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate       0.0133 (standard error        0.07)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate       0.3070 (standard error        0.45)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate      -0.1452 (standard error        0.12)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate       0.0674 (standard error        0.18)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =      0.8, two-sided p = 0.333\n\nTest that all parameters are 0 : \nchi-squared =   2.0926, d.f. = 4, p = 0.553\n\nEstimated mean parameter   -0.014 (s.e.   0.0582), two-sided p = 0.826\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error    0.1164\nTest that variance of parameter is 0 :\nChi-squared =    2.0368 (d.f. = 3), p = 0.565\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter   -0.014 (s.e.   0.0591), two-sided p = 0.828\n0.95 level confidence interval [ -0.1557 ,  0.1289 ]\nEstimated standard deviation  < 0.0001 \n0.95 level confidence interval [       0 ,  0.2326 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =    6.8084 (d.f. = 8), p = 0.557\nCombination of left one-sided p-values:\nChi-squared =    6.9042 (d.f. = 8), p = 0.547\n\n--------------------------------------------------------------------------------\nParameter 14: eval : alcdrinkbeh: effect from ah_pvt\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate      -0.0026 (standard error        0.01)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate      -0.0385 (standard error        0.03)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate       0.0097 (standard error        0.01)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate       0.0198 (standard error        0.01)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =     -0.2, two-sided p = 0.917\n\nTest that all parameters are 0 : \nchi-squared =   4.8712, d.f. = 4, p = 0.181\n\nEstimated mean parameter   0.0038 (s.e.   0.0082), two-sided p = 0.675\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error    0.0165\nTest that variance of parameter is 0 :\nChi-squared =    4.7381 (d.f. = 3), p = 0.192\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter   0.0015 (s.e.   0.0042), two-sided p = 0.738\n0.95 level confidence interval [ -0.0092 ,  0.0167 ]\nEstimated standard deviation  < 0.0001 \n0.95 level confidence interval [       0 ,  0.0238 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =   10.0287 (d.f. = 8), p = 0.263\nCombination of left one-sided p-values:\nChi-squared =    7.1317 (d.f. = 8), p = 0.522\n\n--------------------------------------------------------------------------------\nParameter 15: eval : alcdrinkbeh: effect from age\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate      -0.0362 (standard error        0.11)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate       0.0798 (standard error        0.17)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate      -0.0004 (standard error        0.05)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate       0.0759 (standard error        0.07)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =      0.4, two-sided p = 0.750\n\nTest that all parameters are 0 : \nchi-squared =   1.5047, d.f. = 4, p = 0.681\n\nEstimated mean parameter   0.0223 (s.e.   0.0266), two-sided p = 0.463\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error    0.0532\nTest that variance of parameter is 0 :\nChi-squared =    1.1763 (d.f. = 3), p = 0.759\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter   0.0223 (s.e.   0.0389), two-sided p = 0.607\n0.95 level confidence interval [ -0.0593 ,  0.1071 ]\nEstimated standard deviation  < 0.0001 \n0.95 level confidence interval [       0 ,  0.1275 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =    8.4925 (d.f. = 8), p = 0.387\nCombination of left one-sided p-values:\nChi-squared =    4.4877 (d.f. = 8), p = 0.811\n\n--------------------------------------------------------------------------------\nParameter 16: eval : alcdrinkbeh: effect from parentdrkfrq\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate       0.0130 (standard error        0.04)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate      -0.1119 (standard error        0.35)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate       0.1426 (standard error        0.10)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate       0.0674 (standard error        0.12)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =     -0.4, two-sided p = 0.750\n\nTest that all parameters are 0 : \nchi-squared =   2.5853, d.f. = 4, p = 0.460\n\nEstimated mean parameter   0.0367 (s.e.   0.0377), two-sided p = 0.402\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error    0.0754\nTest that variance of parameter is 0 :\nChi-squared =    1.6974 (d.f. = 3), p = 0.638\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter   0.0345 (s.e.   0.0366), two-sided p = 0.416\n0.95 level confidence interval [ -0.0427 ,  0.1488 ]\nEstimated standard deviation  < 0.0001 \n0.95 level confidence interval [       0 ,  0.1532 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =   10.5681 (d.f. = 8), p = 0.227\nCombination of left one-sided p-values:\nChi-squared =     3.738 (d.f. = 8), p = 0.880\n\n--------------------------------------------------------------------------------\nParameter 17: eval : alcdrinkbeh: effect from tobacco\n--------------------------------------------------------------------------------\nData set 1, AlcoholBehavior(ans.5809) :  Estimate       0.4565 (standard error        0.17)\nData set 2, AlcoholBehavior(ans.88)   :  Estimate      -0.0197 (standard error        0.72)\nData set 3, AlcoholBehavior(ans.7)    :  Estimate       0.3567 (standard error        0.21)\nData set 4, AlcoholBehavior(ans.8)    :  Estimate       0.0612 (standard error        0.26)\n\n 4 datasets used.\n\nIWLS modification of Snijders-Baerveldt (2003) method  of combining estimates\n-----------------------------------------------------------------------------\nThis method assumes that true parameters and standard errors are uncorrelated.\nThis can be checked by the plot method and the test below.\n\nTest that estimates and standard errors are uncorrelated: \nSpearman's rank correlation rho =       -1, two-sided p = 0.083\n\nTest that all parameters are 0 : \nchi-squared =  10.5466, d.f. = 4, p = 0.014\n\nEstimated mean parameter   0.3354 (s.e.    0.099), two-sided p = 0.043\nbased on IWLS modification of Snijders & Baerveldt (2003). \n\nResidual standard error     0.198\nTest that variance of parameter is 0 :\nChi-squared =    1.9346 (d.f. = 3), p = 0.586\nbased on IWLS modification of Snijders & Baerveldt (2003).\n\nEstimates and confidence intervals under normality assumptions\n------------------------------------------------------- -------\nEstimated mean parameter   0.3354 (s.e.   0.1143), two-sided p = 0.061\n0.95 level confidence interval [  0.0379 ,  0.5654 ]\nEstimated standard deviation  < 0.0001 \n0.95 level confidence interval [       0 ,   0.424 ]\n\nFisher's combination of one-sided tests\n----------------------------------------\nCombination of right one-sided p-values:\nChi-squared =   21.0486 (d.f. = 8), p = 0.007\nCombination of left one-sided p-values:\nChi-squared =    2.5647 (d.f. = 8), p = 0.959"
  },
  {
    "objectID": "social_network/social_network.html#lazega-example",
    "href": "social_network/social_network.html#lazega-example",
    "title": "28  Social Network Analysis",
    "section": "31.1 Lazega example",
    "text": "31.1 Lazega example\nFrom Statistical Analysis of Networks with R\n\n\nCode\n# igraph example\ndata(lazega)\nplot(lazega, layout = layout_with_kk, vertex.label = V(lazega),\n     vertex.size = 10)\n\n\n\n\n\nThe statnet representation of\n\n\nCode\n# making statnet version of network data\nA <- get.adjacency(lazega)\n\n\nas(<dsCMatrix>, \"dgCMatrix\") is deprecated since Matrix 1.5-0; do as(., \"generalMatrix\") instead\n\n\nCode\nv.attrs <- get.data.frame(lazega, what= \"vertices\")\nlazega.s <- as.network(as.matrix(A), directed = FALSE)\nnetwork::set.vertex.attribute(lazega.s, \"office\", v.attrs$Office)\nnetwork::set.vertex.attribute(lazega.s, \"practice\", v.attrs$Practice)\nnetwork::set.vertex.attribute(lazega.s, \"gender\", v.attrs$Gender)\nnetwork::set.vertex.attribute(lazega.s, \"seniority\", v.attrs$Seniority)\n\n\n\n\nCode\n# summary statistics\n## Degrees\nergm_deg <- formula(lazega.s ~ degree(1:8) + triangles) \nsummary(ergm_deg)\n\n\n degree1  degree2  degree3  degree4  degree5  degree6  degree7  degree8 \n       3        2        4        2        4        4        1        1 \ntriangle \n     120 \n\n\nCode\ndegree_distribution(lazega) * vcount(lazega)\n\n\n [1] 2 3 2 4 2 4 4 1 1 5 1 1 2 3 0 1\n\n\nCode\ndegreedist(lazega.s) # statnet\n\n\n degree0  degree1  degree2  degree3  degree4  degree5  degree6  degree7 \n       2        3        2        4        2        4        4        1 \n degree8  degree9 degree10 degree11 degree12 degree13 degree15 \n       1        5        1        1        2        3        1 \n\n\n\n\nCode\n# mixing matrix - how many ties are between each for each attribute\nmixingmatrix(lazega.s, \"gender\")\n\n\n   1  2\n1 98 16\n2 16  1\n\n\nNote:  Marginal totals can be misleading for undirected mixing matrices.\n\n\n\n\nCode\n# Bernoulli ERGM\nergm_bern_form <- formula(lazega.s ~ edges)\nsummary(ergm_bern_form)\n\n\nedges \n  115 \n\n\nCode\nergm_bern <- ergm(ergm_bern_form)\nsummary(ergm_bern)\n\n\nCall:\nergm(formula = ergm_bern_form)\n\nMaximum Likelihood Results:\n\n      Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges  -1.4992     0.1031      0  -14.54   <1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 873.4  on 630  degrees of freedom\n Residual Deviance: 598.8  on 629  degrees of freedom\n \nAIC: 600.8  BIC: 605.2  (Smaller is better. MC Std. Err. = 0)\n\n\n\n\nCode\nexp(-1.499) / (1 + exp(-1.499 )) # this is also the density of the network\n\n\n[1] 0.1825747\n\n\nThe interpretation of this “edges” network means, that if we were to try to estimate the probability of the existence of an edge by just the number of edges in the model, we should expect a uniform distribution of the edges\n\n\nCode\n# Simulating from ergm\nsim_bern <- simulate(ergm_bern, burnin = 1e+6, seed = 9)\nplot(degreedist(sim_bern))\n\n\n degree3  degree4  degree5  degree6  degree7  degree8  degree9 degree10 \n       2        8        1        5        6        9        1        2 \ndegree11 \n       2 \n\n\n\n\n\nCode\ndegreedist(lazega.s)\n\n\n degree0  degree1  degree2  degree3  degree4  degree5  degree6  degree7 \n       2        3        2        4        2        4        4        1 \n degree8  degree9 degree10 degree11 degree12 degree13 degree15 \n       1        5        1        1        2        3        1 \n\n\n\n\nCode\n# simulate with triangles\nergm_tri_form <- formula(lazega.s ~ triangle)\nsummary(ergm_tri_form)\n\n\ntriangle \n     120 \n\n\nCode\nergm_tri <- ergm(ergm_tri_form)\n\n\nWarning: 'glpk' selected as the solver, but package 'Rglpk' is not available;\nfalling back to 'lpSolveAPI'. This should be fine unless the sample size and/or\nthe number of parameters is very big.\n\n\nCode\nsim_tri <- simulate(ergm_tri)\nplot(sim_tri)\n\n\n\n\n\nCode\nsummary(ergm_tri)\n\n\nCall:\nergm(formula = ergm_tri_form)\n\nMonte Carlo Maximum Likelihood Results:\n\n         Estimate Std. Error MCMC % z value Pr(>|z|)    \ntriangle -0.38428    0.05762      0  -6.669   <1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 873.4  on 630  degrees of freedom\n Residual Deviance: 733.1  on 629  degrees of freedom\n \nAIC: 735.1  BIC: 739.5  (Smaller is better. MC Std. Err. = 1.988)\n\n\nCode\nexp(-0.38937) / (1 + exp(-0.38937)) # p = .403\n\n\n[1] 0.403869\n\n\n.403 should loosely translate to the density of triangles, but I don’t know how.\n\n\nCode\n# Complex with k stars\nergm_complex <- formula(lazega.s ~ edges + kstar(2) + kstar(3) + triangle)\nsummary(ergm_complex)\n\n\n   edges   kstar2   kstar3 triangle \n     115      926     2681      120 \n\n\nCode\n# alternating kstar statistic\nergm_geometric <- formula(lazega.s ~ edges + gwesp(1, fixed = TRUE))\nsummary(ergm_geometric)\n\n\n        edges gwesp.fixed.1 \n     115.0000      213.1753 \n\n\n\n\nCode\nergm_lazega <- formula(lazega.s ~ edges +\n                         gwesp(log(3), fixed = TRUE) +\n                         nodemain(\"seniority\") +\n                         nodemain(\"practice\") + \n                         match(\"practice\") + # dyads that link same practice\n                         match(\"gender\") + # dyads that link same gender\n                         match(\"office\"))\nsummary(ergm_lazega)\n\n\n                       edges gwesp.fixed.1.09861228866811 \n                    115.0000                     222.9108 \n           nodecov.seniority             nodecov.practice \n                   4687.0000                     359.0000 \n          nodematch.practice             nodematch.gender \n                     72.0000                      99.0000 \n            nodematch.office \n                     85.0000 \n\n\nTypes of structural components\n\n“geometrically weighted degree count”\n“Alternating sums of k-triangles”\n“Alternating k-star statistic”\n\n\n\nCode\n# ERGM fitting\nergm_lazega_fit <- ergm(ergm_lazega)\nanova(ergm_lazega_fit) # overall fit based on deviance statistic i guess\n\n\nAnalysis of Deviance Table\n\nModel 1: lazega.s ~ edges + gwesp(log(3), fixed = TRUE) + nodemain(\"seniority\") + \n    nodemain(\"practice\") + match(\"practice\") + match(\"gender\") + \n    match(\"office\")\n         Df Deviance Resid. Df Resid. Dev Pr(>|Chisq|)    \nNULL                       630     873.37                 \nModel 1:  7    413.8       623     459.57    < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nsummary(ergm_lazega_fit)\n\n\nCall:\nergm(formula = ergm_lazega)\n\nMonte Carlo Maximum Likelihood Results:\n\n                             Estimate Std. Error MCMC % z value Pr(>|z|)    \nedges                        -6.97916    0.68461      0 -10.194  < 1e-04 ***\ngwesp.fixed.1.09861228866811  0.59367    0.09150      0   6.488  < 1e-04 ***\nnodecov.seniority             0.02431    0.00635      0   3.828 0.000129 ***\nnodecov.practice              0.39029    0.11452      0   3.408 0.000655 ***\nnodematch.practice            0.76953    0.19477      0   3.951  < 1e-04 ***\nnodematch.gender              0.72433    0.25253      0   2.868 0.004127 ** \nnodematch.office              1.16433    0.19854      0   5.864  < 1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n     Null Deviance: 873.4  on 630  degrees of freedom\n Residual Deviance: 459.6  on 623  degrees of freedom\n \nAIC: 473.6  BIC: 504.7  (Smaller is better. MC Std. Err. = 0.3342)\n\n\n\n\nCode\n# goodness of fit\nergm_lazega_gof <- gof(ergm_lazega_fit)\npar(mfrow = c(2, 2))\nplot(ergm_lazega_gof)\n\n\n\n\n\nThe way goodness of fit is currently conducted is by generating a bunch of network statistics and seeing if they fit the simulated distribution."
  },
  {
    "objectID": "social_network/social_network.html#resources-1",
    "href": "social_network/social_network.html#resources-1",
    "title": "28  Social Network Analysis",
    "section": "32.1 Resources",
    "text": "32.1 Resources\n\nIntro to Network Regression"
  },
  {
    "objectID": "social_network/social_network.html#qap-with-correlation",
    "href": "social_network/social_network.html#qap-with-correlation",
    "title": "28  Social Network Analysis",
    "section": "32.2 QAP with correlation",
    "text": "32.2 QAP with correlation\nMatch results from “QAP Partialling as a test of spuriousness, Krackhardt (1987)”.\n\n\nCode\nm1 <- matrix(c(0, 1, 0, 0, 0,\n               1, 0, 1, 0, 0,\n               0, 1, 0, 1, 1,\n               0, 0, 1, 0, 1,\n               0, 0, 1, 1, 0), nrow = 5, byrow = T)\nm2 <- matrix(c(0, 0, 1, 1, 1,\n               0, 0, 0, 1, 1,\n               1, 0, 0, 0, 1, \n               1, 1, 0, 0, 0,\n               1, 1, 1, 0, 0), nrow = 5, byrow = T)\n\ncor(c(m1[upper.tri(m1)]), c(m2[upper.tri(m2)])) # matches paper\n\n\n[1] -0.8164966"
  },
  {
    "objectID": "spatial/carbayes.html",
    "href": "spatial/carbayes.html",
    "title": "29  CARBayes",
    "section": "",
    "text": "30 A simple example\nWe’ll use the lollipop graph, and suppose that we observe some random process on the graph."
  },
  {
    "objectID": "spatial/carbayes.html#cressie-parameterization",
    "href": "spatial/carbayes.html#cressie-parameterization",
    "title": "29  CARBayes",
    "section": "30.1 Cressie parameterization",
    "text": "30.1 Cressie parameterization\nCressie’s parameterization gives another parameter to control the degree of spatial averaging in the\nThe variable is \\rho, which will take values between 0 < \\rho 1. As \\rho \\rightarrow 0, then the spatial variability disappears.\n\n\\begin{aligned}\n\\phi_i | \\phi_{-i} \\sim N(0, (I-\\rho C)^{-1}D)\n\\end{aligned}\n\nLet\n\n\\begin{aligned}\nY = \\mu + \\phi\n\\end{aligned}\n\n\n\nCode\n# CAR(rho, sigma2)\ncressieCAR <- function(nsamples = 10, W, rho = .5, sigma2=1) {\n  DW <- diag(rowSums(W))\n  Q <- (DW - rho * W) / sigma2\n  print(solve(Q))\n  mvtnorm::rmvnorm(nsamples, sigma = solve(Q))\n}\n\nlibrary(tidyverse)\ncressie_sim <- data.frame(rho = c(0, .1, .4, .8, .9)) %>% \n  rowwise() %>% \n  mutate(x = list(data.frame(cressieCAR(10, W, rho, sigma2 = 2))))\n\n\n     [,1] [,2]      [,3] [,4]\n[1,]    1    0 0.0000000    0\n[2,]    0    1 0.0000000    0\n[3,]    0    0 0.6666667    0\n[4,]    0    0 0.0000000    2\n            [,1]        [,2]       [,3]        [,4]\n[1,] 1.004365710 0.051984758 0.03532945 0.003532945\n[2,] 0.051984758 1.004365710 0.03532945 0.003532945\n[3,] 0.035329447 0.035329447 0.67125949 0.067125949\n[4,] 0.003532945 0.003532945 0.06712595 2.006712595\n           [,1]       [,2]      [,3]       [,4]\n[1,] 1.08901515 0.25568182 0.1893939 0.07575758\n[2,] 0.25568182 1.08901515 0.1893939 0.07575758\n[3,] 0.18939394 0.18939394 0.7575758 0.30303030\n[4,] 0.07575758 0.07575758 0.3030303 2.12121212\n          [,1]      [,2]     [,3]      [,4]\n[1,] 1.8777614 1.1634757 1.030928 0.8247423\n[2,] 1.1634757 1.8777614 1.030928 0.8247423\n[3,] 1.0309278 1.0309278 1.546392 1.2371134\n[4,] 0.8247423 0.8247423 1.237113 2.9896907\n         [,1]     [,2]     [,3]     [,4]\n[1,] 3.120493 2.430838 2.281369 2.053232\n[2,] 2.430838 3.120493 2.281369 2.053232\n[3,] 2.281369 2.281369 2.788340 2.509506\n[4,] 2.053232 2.053232 2.509506 4.258555\n\n\nCode\ncressie_sim  %>% unnest_wider(x) %>% unnest(X1:X4) %>% \n  pivot_longer(X1:X4) %>% \n  ggplot(aes(name, value)) +\n  geom_boxplot()\n\n\n\n\n\nCode\nW <- make_star(5, mode = \"undirected\") %>% as_adj()\n\nQcar(W) # not symmetric when the weight matrix is not symmetric\n\n\n5 x 5 sparse Matrix of class \"dgCMatrix\"\n                   \n[1,]  4 -1 -1 -1 -1\n[2,] -1  1  .  .  .\n[3,] -1  .  1  .  .\n[4,] -1  .  .  1  .\n[5,] -1  .  .  .  1"
  },
  {
    "objectID": "spatial/carbayes.html#conclusions-about-besag-model",
    "href": "spatial/carbayes.html#conclusions-about-besag-model",
    "title": "29  CARBayes",
    "section": "30.2 Conclusions about Besag model",
    "text": "30.2 Conclusions about Besag model\nI don’t think using the graph laplacian for spatial random effects is very powerful, using a laplacian matrix for the normal distribution does not induce a very strong effect, and also presupposes the spatial dependencies in the model. I think that it’s not very economical use of the parameters in the model. There’s not nearly the degree of spatial averaging that you’d expect"
  },
  {
    "objectID": "spatial/carbayes.html#simulations-of-car-models",
    "href": "spatial/carbayes.html#simulations-of-car-models",
    "title": "29  CARBayes",
    "section": "30.3 Simulations of CAR models",
    "text": "30.3 Simulations of CAR models\nHere we use libraries to visualize the process, and thus the richness of our modeling space.\n\n\nCode\nlibrary(mclcar)\n\n\n\n\nCode\nset.seed(33)\nn.torus <- 10\nrho <- 0.2\nsigma <- 1.5\nprec <- 1/sigma\nbeta <- c(1, 1)\nXX <- cbind(rep(1, n.torus^2), sample(log(1:n.torus^2)/5))\nmydata1 <- CAR.simTorus(n1 = n.torus, n2 = n.torus, rho = rho, prec = prec)\n\nCAR.simTorus\n\n\nfunction (n1, n2, rho, prec) \n{\n    x <- list(c(2, n2), rep(1, 2))\n    Wx <- circulant.spam(x, n = n2)\n    Ix <- diag.spam(1, n1)\n    W <- kronecker(Ix, Wx) + kronecker(Wx, Ix)\n    ew <- eigen(W, only.values = TRUE)$values\n    min <- 1/min(ew)\n    max <- 1/max(ew)\n    if (rho < min | rho > max) \n        stop(paste(\"rho should be within\", min, max))\n    I <- diag.spam(1, n1 * n2)\n    Q <- as.spam(prec * (I - rho * W))\n    cholR <- chol.spam(Q, pivot = \"MMD\", memory = list(nnzcolindices = 6.25 * \n        n1 * n2))\n    X <- backsolve(cholR, rnorm(n1 * n2))\n    W <- as.matrix(W)\n    result <- list(W = W, X = X)\n    return(result)\n}\n<bytecode: 0x7fda99e945c0>\n<environment: namespace:mclcar>\n\n\nCode\nWmat <- mydata1$W\n\nmydata2 <- CAR.simWmat(rho = .2, prec = prec, W = Wmat)\n\n\n\n\nCode\ntmp <- graph_from_adjacency_matrix(Wmat, mode = \"undirected\")\nplot(tmp, layout = layout.grid(tmp))"
  },
  {
    "objectID": "spatial/carbayes.html#bym-model",
    "href": "spatial/carbayes.html#bym-model",
    "title": "29  CARBayes",
    "section": "30.4 BYM Model",
    "text": "30.4 BYM Model\nTo better fit the data, we split up the spatial effects into spatial components and independent errors for each of the locations. Now it’s more like a mixed model, where we can add some level of independent noise to each observation"
  },
  {
    "objectID": "spatial/spatial.html",
    "href": "spatial/spatial.html",
    "title": "32  Spatial Statistics",
    "section": "",
    "text": "33 Spatial Classes\nHas slots for bbox which are the spatial extents, and a CRS object for the coordinate reference system.\nThis exposition follows Spatio-Temporal Statistics With R (Wikle, Zammit-Mangion, Cressie)\nOne formulation is\n\\begin{aligned}\n(I - B)(Z - \\mu) = \\varepsilon\n\\end{aligned}\nwhere:\nif we assume that \\varepsilon \\overset{\\mathrm{iid}}{\\sim} N(0, \\Sigma), then it’s a gaussian SAR model.\nIf so, then we have"
  },
  {
    "objectID": "spatial/spatial.html#spatial-points",
    "href": "spatial/spatial.html#spatial-points",
    "title": "32  Spatial Statistics",
    "section": "33.1 Spatial Points",
    "text": "33.1 Spatial Points\nClass SpatialPoints adds a coords slot to Spatial\n\n\nCode\ngetClass(\"SpatialPoints\")\n\n\nClass \"SpatialPoints\" [package \"sp\"]\n\nSlots:\n                                          \nName:       coords        bbox proj4string\nClass:      matrix      matrix         CRS\n\nExtends: \"Spatial\", \"SpatialVector\"\n\nKnown Subclasses: \nClass \"SpatialPointsDataFrame\", directly\nClass \"SpatialPixels\", directly\nClass \"SpatialPixelsDataFrame\", by class \"SpatialPixels\", distance 2\n\n\n\n\nCode\n# example instantiation\ncran <- read.table(\"data/CRAN051001a.txt\", header = TRUE)\ncran_mat <- cbind(cran$long, cran$lat)\n\nllcrs <- CRS(\"+proj=longlat +ellps=WGS84\")\n\n\nWarning in showSRID(uprojargs, format = \"PROJ\", multiline = \"NO\", prefer_proj\n= prefer_proj): Discarded datum Unknown based on WGS84 ellipsoid in Proj4\ndefinition\n\n\nCode\ncran_sp <- SpatialPoints(cran_mat, proj4string = llcrs)\nsummary(cran_sp)\n\n\nObject of class SpatialPoints\nCoordinates:\n                 min      max\ncoords.x1 -122.95000 153.0333\ncoords.x2  -37.81667  57.0500\nIs projected: FALSE \nproj4string : [+proj=longlat +ellps=WGS84 +no_defs]\nNumber of points: 54\n\n\n\n\nCode\n# accessor methods\ncoordinates(cran_sp)[which(cran$loc == \"Brazil\"),] # index coordinates\n\n\n     coords.x1 coords.x2\n[1,] -49.26667 -25.41667\n[2,] -42.86667 -20.75000\n[3,] -43.20000 -22.90000\n[4,] -47.63333 -22.71667\n[5,] -46.63333 -23.53333\n\n\nCode\nbbox(cran_sp) # bounding box\n\n\n                 min      max\ncoords.x1 -122.95000 153.0333\ncoords.x2  -37.81667  57.0500\n\n\nCode\nsummary(cran_sp[which(cran$loc == \"Brazil\"), ]) # can index class directly, new bbox.\n\n\nObject of class SpatialPoints\nCoordinates:\n                min       max\ncoords.x1 -49.26667 -42.86667\ncoords.x2 -25.41667 -20.75000\nIs projected: FALSE \nproj4string : [+proj=longlat +ellps=WGS84 +no_defs]\nNumber of points: 5"
  },
  {
    "objectID": "spatial/spatial.html#spatialpointsdataframe",
    "href": "spatial/spatial.html#spatialpointsdataframe",
    "title": "32  Spatial Statistics",
    "section": "33.2 SpatialPointsDataFrame",
    "text": "33.2 SpatialPointsDataFrame\nUsed for matching coordinates with other covariates, like name of coordinate. Needs to have matching row names. Objects are meant to operate like dataframes.\n\n\nCode\n# matches by rownames\nrow.names(cran_mat) <- 1:nrow(cran)\n# cran %>% row.names() # matches\n\ncran_spdf <- SpatialPointsDataFrame(cran_mat, cran, proj4string = llcrs, match.ID = TRUE)\nsummary(cran_spdf) # meant to\n\n\nObject of class SpatialPointsDataFrame\nCoordinates:\n                 min      max\ncoords.x1 -122.95000 153.0333\ncoords.x2  -37.81667  57.0500\nIs projected: FALSE \nproj4string : [+proj=longlat +ellps=WGS84 +no_defs]\nNumber of points: 54\nData attributes:\n    place              north               east               loc           \n Length:54          Length:54          Length:54          Length:54         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n      long                lat        \n Min.   :-122.9500   Min.   :-37.82  \n 1st Qu.: -47.3833   1st Qu.: 34.52  \n Median :   7.8500   Median : 42.73  \n Mean   :  -0.6617   Mean   : 31.73  \n 3rd Qu.:  16.8333   3rd Qu.: 47.58  \n Max.   : 153.0333   Max.   : 57.05  \n\n\n\n\nCode\n# alternate way, just attach coordinates and proj4string by assignment\ncran_spdf_alt <- cran\n# coordinates(cran_spdf_alt) <- c(\"long\", \"lat\") # can select out columns if already in dataframe\ncoordinates(cran_spdf_alt) <- cran_mat # coordinates as matrix\nproj4string(cran_spdf_alt) <- llcrs # crs\nsummary(cran_spdf_alt) # Turns it into a SpatialPointsDataFrame by assigning coordinate/proj\n\n\nObject of class SpatialPointsDataFrame\nCoordinates:\n                 min      max\ncoords.x1 -122.95000 153.0333\ncoords.x2  -37.81667  57.0500\nIs projected: FALSE \nproj4string : [+proj=longlat +ellps=WGS84 +no_defs]\nNumber of points: 54\nData attributes:\n    place              north               east               loc           \n Length:54          Length:54          Length:54          Length:54         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n      long                lat        \n Min.   :-122.9500   Min.   :-37.82  \n 1st Qu.: -47.3833   1st Qu.: 34.52  \n Median :   7.8500   Median : 42.73  \n Mean   :  -0.6617   Mean   : 31.73  \n 3rd Qu.:  16.8333   3rd Qu.: 47.58  \n Max.   : 153.0333   Max.   : 57.05"
  },
  {
    "objectID": "spatial/spatial.html#spacetime-package",
    "href": "spatial/spatial.html#spacetime-package",
    "title": "32  Spatial Statistics",
    "section": "33.3 spacetime package",
    "text": "33.3 spacetime package\nThe spacetime package provides dataframes for certain types of spatial data.\n - STFDF\n\n\nCode\ngetClass(\"ST\")\n\n\nClass \"ST\" [package \"spacetime\"]\n\nSlots:\n                              \nName:       sp    time endTime\nClass: Spatial     xts POSIXct\n\nKnown Subclasses: \nClass \"STF\", directly\nClass \"STS\", directly\nClass \"STI\", directly\nClass \"STT\", directly\nClass \"STFDF\", by class \"STF\", distance 2\nClass \"STSDF\", by class \"STS\", distance 2\nClass \"STIDF\", by class \"STI\", distance 2\nClass \"STTDF\", by class \"STT\", distance 2"
  },
  {
    "objectID": "spatial/spatial.html#examples",
    "href": "spatial/spatial.html#examples",
    "title": "32  Spatial Statistics",
    "section": "34.1 Examples",
    "text": "34.1 Examples\n\n34.1.1 Glasgow Prices (Spatial CAR)\nThis example is the vignette for CARBayes, another packages that allows spatial modeling of this kind. This example is taken from Th CARBayes Vignette\n\n\nCode\ndata(pricedata)\ndata(GGHB.IZ)\n\npricedata <- pricedata %>% mutate(logprice = log(pricedata$price))\n\nggpairs(data = pricedata, columns = c(8, 3:7))\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nCode\n# merge price \npricedata.sf <- merge(x=GGHB.IZ, y=pricedata, by=\"IZ\", all.x=FALSE)\npricedata.sf <- st_transform(x=pricedata.sf,\n                             crs='+proj=longlat +datum=WGS84 +no_defs')\n\nlibrary(leaflet)\ncolours <- colorNumeric(palette = \"YlOrRd\", domain = pricedata.sf$price)\nmap1 <- leaflet(data=pricedata.sf) %>% \n  addTiles() %>% \n  addPolygons(fillColor = ~colours(price), color = \"\", weight = 1,\n              fillOpacity = .7) %>% \n  addLegend(pal = colours, values = pricedata.sf$price, opacity = 1,\n            title = \"price\") %>% \n  addScaleBar(position=\"bottomleft\")\nmap1\n\n\n\n\n\n\n\nThe Nonspatial modeling approach\n\n\nCode\nglasgow_lm <- lm(logprice~crime+rooms+sales + factor(type) + driveshop, data = pricedata.sf)\nglasgow_lm %>% summary()\n\n\n\nCall:\nlm(formula = logprice ~ crime + rooms + sales + factor(type) + \n    driveshop, data = pricedata.sf)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9717 -0.1510 -0.0012  0.1672  0.8346 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          4.342e+00  1.540e-01  28.203  < 2e-16 ***\ncrime               -2.248e-04  5.541e-05  -4.057 6.55e-05 ***\nrooms                2.119e-01  2.890e-02   7.334 2.79e-12 ***\nsales                2.264e-03  3.544e-04   6.388 7.61e-10 ***\nfactor(type)flat    -2.948e-01  5.988e-02  -4.923 1.51e-06 ***\nfactor(type)semi    -1.768e-01  5.747e-02  -3.077 0.002312 ** \nfactor(type)terrace -3.251e-01  6.921e-02  -4.697 4.26e-06 ***\ndriveshop           -4.848e-02  1.452e-02  -3.340 0.000961 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2273 on 262 degrees of freedom\nMultiple R-squared:  0.6073,    Adjusted R-squared:  0.5968 \nF-statistic: 57.88 on 7 and 262 DF,  p-value: < 2.2e-16\n\n\n\n\nCode\nW.nb <- poly2nb(pricedata.sf, row.names = pricedata.sf$IZ)\nW.list <- nb2listw(W.nb, style = \"B\") # binary coding, just 1 if they are a neighbor\nmoran.mc(x = residuals(glasgow_lm), listw=W.list, nsim = 1000)\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  residuals(glasgow_lm) \nweights: W.list  \nnumber of simulations + 1: 1001 \n\nstatistic = 0.30141, observed rank = 1001, p-value = 0.000999\nalternative hypothesis: greater\n\n\n\n\nSpatial Modeling\nThe models here are no different, but I’m still having a hard time understanding the parameters that are estimate\n\n\nCode\nW <- nb2mat(W.nb, style = \"B\")\nchain1 <- S.CARleroux(logprice~crime+rooms+sales + factor(type) + driveshop, data = pricedata.sf, family=\"gaussian\", W=W, burnin = 100000, n.sample=300000, thin=100)\n\n\nSetting up the model.\nGenerating 2000 post burnin and thinned (if requested) samples.\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\nSummarising results.\nFinished in  67.2 seconds.\n\n\nCode\nchain2 <- S.CARleroux(logprice~crime+rooms+sales + factor(type) + driveshop, data = pricedata.sf, family=\"gaussian\", W=W, burnin = 100000, n.sample=300000, thin=100)\n\n\nSetting up the model.\nGenerating 2000 post burnin and thinned (if requested) samples.\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\nSummarising results.\nFinished in  74.9 seconds.\n\n\nCode\nchain3 <- S.CARleroux(logprice~crime+rooms+sales + factor(type) + driveshop, data = pricedata.sf, family=\"gaussian\", W=W, burnin = 100000, n.sample=300000, thin=100)\n\n\nSetting up the model.\nGenerating 2000 post burnin and thinned (if requested) samples.\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\nSummarising results.\nFinished in  63.8 seconds.\n\n\n\n\n\n34.1.2 Scotland Lip Cancer (Spatial CAR)\nThis section follows the tutorial from Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny\n\n\nCode\ndata(\"scotland\")\n\n# getClass(\"SpatialPolygons\") # has polygons, plotOrder, bbox, proj4string\n# slotNames(map) # getting the value names of the object itself\nmap <- scotland$spatial.polygon\nplot(map) # class SpatialPolygons\n\n\n\n\n\nThe map is in the projection OSGB 1936/British National Grid which has EPSG code 27700. The proj4 string of this projection can be seen in https://spatialreference.org/ref/epsg/27700/proj4/ or can be obtained with R as follows:\n\n\nCode\ncodes <- rgdal::make_EPSG()\ncodes[which(codes$code == \"27700\"), ]\n\n\n      code                           note\n6461 27700 OSGB36 / British National Grid\n                                                                                                                 prj4\n6461 +proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +units=m +no_defs +type=crs\n              prj_method\n6461 Transverse Mercator\n\n\n\n\nCode\n# assign \nproj4string(map) <- \"+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=km +no_defs\"\n\n\nWarning in showSRID(uprojargs, format = \"PROJ\", multiline = \"NO\", prefer_proj =\nprefer_proj): Discarded datum OSGB 1936 in Proj4 definition\n\n\nWarning in proj4string(obj): CRS object has comment, which is lost in output; in tests, see\nhttps://cran.r-project.org/web/packages/sp/vignettes/CRS_warnings.html\n\n\nWarning in ReplProj4string(obj, CRS(value)): A new CRS was assigned to an object with an existing CRS:\n+proj=eqc +lat_ts=0 +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\nwithout reprojecting.\nFor reprojection, use function spTransform\n\n\nCode\n# reproject into WGS84 for leaflet\nmap <- spTransform(map, CRS(\"+proj=longlat +datum=WGS84 +no_defs\"))\n\n\nData preparation\n\ncounty: id of each country\nY: observed number of lip cancer cases in each county\nE: expected number of lip cancer cases in each county\nAFF: proportion of population in agriculture, fishing and forestry\nSIR: standardized incidence ratio\n\n\n\nCode\n# prepare the dataset and calculate SIR\nd <- scotland$data[,c(\"county.names\", \"cases\", \"expected\", \"AFF\")]\nnames(d) <- c(\"county\", \"Y\", \"E\", \"AFF\")\nd$SIR <- d$Y / d$E # observed over expected\n\n# prepare SpatialPolygonsDataFrame, SpatialPolygons + Data basically\nrownames(d) <- d$county # set rownames to county for matching later\n# getClass(\"SpatialPolygonsDataFrame\") # just adds \"data\" slot to SpatialPolygons\nmap <- SpatialPolygonsDataFrame(map, d, match.ID = TRUE) # make Object for leaflet\n\n\n\n\nCode\nl <- leaflet(map) %>% addTiles()\n\n# domain gives possible values to be mapped, returns function\npal <- colorNumeric(palette = \"YlOrRd\", domain = map$SIR)\n\n# Create the tooltip label\nlabels <- sprintf(\"<strong> %s </strong> <br/>\n  Observed: %s <br/> Expected: %s <br/>\n  AFF: %s <br/> SIR: %s\",\n  map$county, map$Y, round(map$E, 2),\n  map$AFF, round(map$SIR, 2)\n) %>%\n  lapply(htmltools::HTML)\n\nl %>%\n  addPolygons(\n    color = \"grey\", weight = 1, # the outlines\n    fillColor = ~ pal(SIR), fillOpacity = 0.5, # how to fill the shapes\n    highlightOptions = highlightOptions(weight = 4), # thicker outline on hover\n    label = labels, # the tooltip label\n    labelOptions = labelOptions( # some general options for the labels\n      style = list(\n        \"font-weight\" = \"normal\",\n        padding = \"3px 8px\"\n      ),\n      textsize = \"15px\", direction = \"auto\"\n    )\n  ) %>%\n  # legend in bottom right\n  addLegend(\n    pal = pal, values = ~SIR, opacity = 0.5,\n    title = \"SIR\", position = \"bottomright\"\n  )\n\n\n\n\n\n\nFor the modeling piece, we model with CAR,\n\n\\begin{aligned}\nY_i &\\sim Poisson(E_i \\theta_i), i = 1,\\dots , n \\\\\n\\log(\\theta_i) &= \\beta_0 + \\beta_1 \\times AFF_i + u_i + v_i\n\\end{aligned}\n - E_i is expected count - \\theta_i is relative risk in area i - AFF_i is the fishing/farming covariate - u_i is spatial component with conditional guassian model, we assum u_i|\\mathbf{u_{-i}} \\sim N(\\bar u_{\\delta_i}, \\frac{\\sigma^2_u}{n_{\\delta_i}}) - v_i is unstructured spatial effect\n\n\nCode\n# create neighborhood matrix from scottish data\n\nnb <- poly2nb(map) # \"nb\" class\n\nhead(nb) # just a list of neighbors by node\n\n\n[[1]]\n[1]  5  9 19\n\n[[2]]\n[1]  7 10\n\n[[3]]\n[1] 12\n\n[[4]]\n[1] 18 20 28\n\n[[5]]\n[1]  1 12 19\n\n[[6]]\n[1] 0\n\n\nCode\nnb # printing shows some summary statistics of the dataset, as well as \"disconnected regions\" with no neighbors\n\n\nNeighbour list object:\nNumber of regions: 56 \nNumber of nonzero links: 234 \nPercentage nonzero weights: 7.461735 \nAverage number of links: 4.178571 \n3 regions with no links:\norkney shetland western.isles\n\n\nCode\n# nb2mat(nb, style=\"B\",zero.policy=T) # convert to an adjacency matrix\n\n\nCan calculate the Moran’s I, allows testing of spatial autocorrelation in the data, defined as\n\n\\begin{equation}\nI = \\frac{n}{\\sum_{i=1}^{n}\\sum_{j=1}^{n}w_{ij}}\n\\frac{\\sum_{i=1}^{n}\\sum_{j=1}^{n}w_{ij}(x_i-\\bar{x})(x_j-\\bar{x})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\n\\end{equation}\n\n\nw_{ij} is weight of neighborhood incidence\nx_i is covariate at area i.\n\n\n\nCode\nset.ZeroPolicyOption(TRUE) # setting globally, in argument it didn't work\n\n\n[1] FALSE\n\n\nCode\nnb_W <- nb2listw(nb, style = \"W\")\nnb_global <- Szero(nb_W) # get global sum of weights\n\n# direct calculation of the statistic\nmoran(x =map$SIR, \n      listw = nb_W,\n      n = length(map$SIR),\n      S0 = nb_global)\n\n\n$I\n[1] 0.5251582\n\n$K\n[1] 5.023736\n\n\nCode\nmoran.plot(map$SIR, nb_W) # upper right and lower left are positively correlated, slope is ~ approximately the moran statistic\n\n\n\n\n\nCode\nmoran.mc(map$SIR, nb_W, 100) # Lots of spatial correlation, randomization test.\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  map$SIR \nweights: nb_W  \nnumber of simulations + 1: 101 \n\nstatistic = 0.49702, observed rank = 101, p-value = 0.009901\nalternative hypothesis: greater\n\n\nNot quite sure how to interpret this plot,\n\n\nCode\n# transfer the neighbors object to INLA\nnb2INLA(\"output/map.adj\", nb) # export to file, in INLA format\ng <- inla.read.graph(filename = \"output/map.adj\") # read the .adj file for INLA\n\n\n\n\nCode\n# indices for random effects u, v\nmap$idareau <- 1:nrow(map@data) # u\nmap$idareav <- 1:nrow(map@data) # v\n\n# f() denotes random effect, \"besag\"\nformula <- Y ~ AFF +\n  f(idareau, model = \"besag\", graph = g, scale.model = TRUE) + # spatial dependence\n  f(idareav, model = \"iid\") # unstructured noise\n\n# fit the formual\nres <- inla(formula, # formula from above\n  family = \"poisson\", # log link\n  data = map@data, # the data file\n  E = E, # \"Known component in the mean for the Poisson likelihoods defined as E exp(eta)\" see docs\n  control.predictor = list(compute = TRUE)) # compute posteriors of predictions\n\n\nWarning in is.null(x) || is.na(x): 'length(x) = 56 > 1' in coercion to\n'logical(1)'\n\nWarning in is.null(x) || is.na(x): 'length(x) = 56 > 1' in coercion to\n'logical(1)'\n\n\nCode\nsummary(res) # output\n\n\nWarning in length(alist[[idx]]) > 0 && !is.null(alist[[idx]]) && !\nis.na(alist[[idx]]): 'length(x) = 4 > 1' in coercion to 'logical(1)'\n\n\n\nCall:\n   c(\"inla(formula = formula, family = \\\"poisson\\\", data = map@data, \", \" \n   E = E, control.predictor = list(compute = TRUE))\") \nTime used:\n    Pre = 1.71, Running = 0.442, Post = 0.0892, Total = 2.24 \n\n\nWarning in length(alist[[idx]]) > 0 && !is.null(alist[[idx]]) && !\nis.na(alist[[idx]]): 'length(x) = 14 > 1' in coercion to 'logical(1)'\n\n\nFixed effects:\n              mean    sd 0.025quant 0.5quant 0.975quant   mode kld\n(Intercept) -0.305 0.120     -0.539   -0.306     -0.068 -0.307   0\nAFF          4.330 1.277      1.744    4.356      6.770  4.408   0\n\n\nWarning in length(alist[[idx]]) > 0 && !is.null(alist[[idx]]) && !\nis.na(alist[[idx]]): 'length(x) = 2 > 1' in coercion to 'logical(1)'\n\n\nRandom effects:\n  Name    Model\n    idareau Besags ICAR model\n   idareav IID model\n\n\nWarning in length(alist[[idx]]) > 0 && !is.null(alist[[idx]]) && !\nis.na(alist[[idx]]): 'length(x) = 12 > 1' in coercion to 'logical(1)'\n\n\nModel hyperparameters:\n                          mean       sd 0.025quant 0.5quant 0.975quant    mode\nPrecision for idareau     4.15     1.45       2.02     3.91       7.63    3.49\nPrecision for idareav 19340.23 19385.83    1347.00 13600.84   70978.11 3679.30\n\n\nWarning in length(alist[[idx]]) > 0 && !is.null(alist[[idx]]) && !\nis.na(alist[[idx]]): 'length(x) = 3 > 1' in coercion to 'logical(1)'\n\n\nExpected number of effective parameters(stdev): 28.55(3.53)\nNumber of equivalent replicates : 1.96 \n\n\nWarning in length(alist[[idx]]) > 0 && !is.null(alist[[idx]]) && !\nis.na(alist[[idx]]): 'length(x) = 2 > 1' in coercion to 'logical(1)'\n\n\nMarginal log-Likelihood:  -189.69 \n\n\nWarning in length(alist[[idx]]) > 0 && !is.null(alist[[idx]]) && !\nis.na(alist[[idx]]): 'length(x) = 392 > 1' in coercion to 'logical(1)'\n\n\nPosterior marginals for the linear predictor and\n the fitted values are computed\n\n\nThe results are pretty standard for interpretation, we can get marginals of the parameters (AFF) with smoothed (spline) marginals\n\n\nCode\n# results has an output of marginals for each of the fixed effects, on some grid\n\n# plot(res$marginals.fixed$AFF)\n# lines(marginal$x, marginal$y)\n\nmarginal <- inla.smarginal(res$marginals.fixed$AFF) # smoothed marginals\nmarginal <- data.frame(marginal)\nggplot(marginal, aes(x = x, y = y)) + geom_line() +\n  labs(x = expression(beta[1]), y = \"Density\") +\n  geom_vline(xintercept = 0, col = \"black\") + theme_bw()\n\n\n\n\n\nEach fitted value for each of the counties can be extracted from the summaries.\n\n\nCode\nhead(res$summary.fitted.values)\n\n\n                        mean        sd 0.025quant 0.5quant 0.975quant     mode\nfitted.Predictor.01 4.964048 1.4515367   2.642604 4.787601   8.294123 4.449065\nfitted.Predictor.02 4.396492 0.6752363   3.172491 4.362145   5.816743 4.294504\nfitted.Predictor.03 3.621166 1.0169791   1.919166 3.523713   5.883808 3.335673\nfitted.Predictor.04 3.082839 0.8950134   1.627130 2.982350   5.112796 2.788997\nfitted.Predictor.05 3.328858 0.7501013   2.042334 3.266257   4.973439 3.144269\nfitted.Predictor.06 2.975477 0.9194586   1.490918 2.869083   5.069922 2.665742\n\n\nfinal version of the leaflet map, with relative risk, and 95% CI for RR.\n\n\nCode\n# Add to map df data\nmap$RR <- res$summary.fitted.values[, \"mean\"]\nmap$LL <- res$summary.fitted.values[, \"0.025quant\"]\nmap$UL <- res$summary.fitted.values[, \"0.975quant\"]\n\n# Leaflet code \npal <- colorNumeric(palette = \"YlOrRd\", domain = map$RR)\n\nlabels <- sprintf(\"<strong> %s </strong> <br/>\n  Observed: %s <br/> Expected: %s <br/>\n  AFF: %s <br/> SIR: %s <br/> RR: %s (%s, %s)\",\n  map$county, map$Y, round(map$E, 2),\n  map$AFF, round(map$SIR, 2), round(map$RR, 2),\n  round(map$LL, 2), round(map$UL, 2)\n) %>% lapply(htmltools::HTML)\n\nlRR <- leaflet(map) %>%\n  addTiles() %>%\n  addPolygons(\n    color = \"grey\", weight = 1, fillColor = ~ pal(RR),\n    fillOpacity = 0.5,\n    highlightOptions = highlightOptions(weight = 4),\n    label = labels,\n    labelOptions = labelOptions(\n      style =\n        list(\n          \"font-weight\" = \"normal\",\n          padding = \"3px 8px\"\n        ),\n      textsize = \"15px\", direction = \"auto\"\n    )\n  ) %>%\n  addLegend(\n    pal = pal, values = ~RR, opacity = 0.5, title = \"RR\",\n    position = \"bottomright\"\n  )\nlRR\n\n\n\n\n\n\n\n\nCode\n# exceedence probabilities, cut offs for action\n# P(\\theta_i > c), probability that a specific area i has relative risk higher than some threshold.\nsapply(res$marginals.fitted.values, FUN = function(marg){1 - inla.pmarginal(q = 2, marginal = marg)})\n\n\nfitted.Predictor.01 fitted.Predictor.02 fitted.Predictor.03 fitted.Predictor.04 \n       9.975116e-01        9.999973e-01        9.664363e-01        9.058851e-01 \nfitted.Predictor.05 fitted.Predictor.06 fitted.Predictor.07 fitted.Predictor.08 \n       9.791544e-01        8.672743e-01        9.778041e-01        4.571506e-01 \nfitted.Predictor.09 fitted.Predictor.10 fitted.Predictor.11 fitted.Predictor.12 \n       5.114627e-01        9.719346e-01        6.525373e-01        9.437729e-01 \nfitted.Predictor.13 fitted.Predictor.14 fitted.Predictor.15 fitted.Predictor.16 \n       7.393550e-01        5.339887e-01        4.733604e-01        6.254751e-01 \nfitted.Predictor.17 fitted.Predictor.18 fitted.Predictor.19 fitted.Predictor.20 \n       5.183790e-01        1.287362e-02        4.842561e-01        8.762968e-02 \nfitted.Predictor.21 fitted.Predictor.22 fitted.Predictor.23 fitted.Predictor.24 \n       3.164637e-02        2.629611e-02        1.326037e-02        7.327491e-05 \nfitted.Predictor.25 fitted.Predictor.26 fitted.Predictor.27 fitted.Predictor.28 \n       3.408706e-03        1.690956e-03        1.856060e-03        2.117133e-03 \nfitted.Predictor.29 fitted.Predictor.30 fitted.Predictor.31 fitted.Predictor.32 \n       1.561128e-03        6.580110e-05        2.389475e-04        2.023057e-01 \nfitted.Predictor.33 fitted.Predictor.34 fitted.Predictor.35 fitted.Predictor.36 \n       1.946921e-03        2.558442e-06        4.684672e-05        1.442569e-04 \nfitted.Predictor.37 fitted.Predictor.38 fitted.Predictor.39 fitted.Predictor.40 \n       5.397963e-05        2.746032e-07        5.103369e-03        1.608706e-05 \nfitted.Predictor.41 fitted.Predictor.42 fitted.Predictor.43 fitted.Predictor.44 \n       9.481904e-11        5.104418e-07        6.558648e-03        2.057684e-10 \nfitted.Predictor.45 fitted.Predictor.46 fitted.Predictor.47 fitted.Predictor.48 \n       0.000000e+00        2.708524e-07        6.464529e-08        1.934923e-08 \nfitted.Predictor.49 fitted.Predictor.50 fitted.Predictor.51 fitted.Predictor.52 \n       0.000000e+00        6.498743e-10        7.150632e-06        3.268866e-06 \nfitted.Predictor.53 fitted.Predictor.54 fitted.Predictor.55 fitted.Predictor.56 \n       1.067678e-07        1.194544e-08        2.783834e-03        5.146667e-04 \n\n\n\n\n34.1.3 Lung Cancer Iowa (spatio-temporal modeling)\nHad to pull the data directly from the archived package, it seems to have been taken off CRAN because the contact author is no longer active. Link to Archive\n\n\nCode\ndohio <- read.csv(\"data/dataohiocomplete.csv\")\nhead(dohio)\n\n\n  county gender race year y     n  NAME\n1      1      1    1 1968 6  8912 Adams\n2      1      1    1 1969 5  9139 Adams\n3      1      1    1 1970 8  9455 Adams\n4      1      1    1 1971 5  9876 Adams\n5      1      1    1 1972 8 10281 Adams\n6      1      1    1 1973 5 10876 Adams\n\n\nCode\nmap <- as_Spatial(st_read(\"data/fe_2007_39_county\", layer=\"fe_2007_39_county\")) # read shape files, not quite, then convert to Spatial for sp methods\n\n\nReading layer `fe_2007_39_county' from data source \n  `/Users/mliou/projects/mywebsite/quarto/spatial/data/fe_2007_39_county' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 11 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -84.8203 ymin: 38.40342 xmax: -80.5182 ymax: 42.32713\nGeodetic CRS:  NAD83\n\n\nCode\nplot(map)\n\n\n\n\n\n\n\nCode\n# observed cases\nd <- aggregate(\n  x = dohio$y,\n  by = list(county = dohio$NAME, year = dohio$year),\n  FUN = sum\n)\nnames(d) <- c(\"county\", \"year\", \"Y\")\nhead(d)\n\n\n     county year  Y\n1     Adams 1968  6\n2     Allen 1968 32\n3   Ashland 1968 15\n4 Ashtabula 1968 27\n5    Athens 1968 12\n6  Auglaize 1968  7\n\n\nCode\n# reorder correctly,\ndohio <- dohio[order(\n  dohio$county,\n  dohio$year,\n  dohio$gender,\n  dohio$race\n), ]\n\n# check sorting\n# dohio[1:20, ]\n\n# 2 race x 2 gender\nn.strata <- 4\nE <- expected( # function needs sorted data\n  population = dohio$n,\n  cases = dohio$y,\n  n.strata = n.strata\n)\nnyears <- length(unique(dohio$year)) # number of years\ncountiesE <- rep(unique(dohio$NAME), # county vector\n                 each = nyears)\n\nncounties <- length(unique(dohio$NAME))\nyearsE <- rep(unique(dohio$year), # rep by counties\n              times = ncounties)\n\n# putting it all together\ndE <- data.frame(county = countiesE, \n                 year = yearsE,\n                 E = E)\nhead(dE)\n\n\n  county year         E\n1  Adams 1968  8.278660\n2  Adams 1969  8.501767\n3  Adams 1970  8.779313\n4  Adams 1971  9.175276\n5  Adams 1972  9.548736\n6  Adams 1973 10.099777\n\n\nCode\n# put it back into the main dataset\nd <- merge(d, dE, by = c(\"county\", \"year\"))\nhead(d)\n\n\n  county year  Y         E\n1  Adams 1968  6  8.278660\n2  Adams 1969  5  8.501767\n3  Adams 1970  9  8.779313\n4  Adams 1971  6  9.175276\n5  Adams 1972 10  9.548736\n6  Adams 1973  7 10.099777\n\n\nCode\n# calc SIR\nd$SIR <- d$Y / d$E\nhead(d)\n\n\n  county year  Y         E       SIR\n1  Adams 1968  6  8.278660 0.7247549\n2  Adams 1969  5  8.501767 0.5881130\n3  Adams 1970  9  8.779313 1.0251372\n4  Adams 1971  6  9.175276 0.6539313\n5  Adams 1972 10  9.548736 1.0472591\n6  Adams 1973  7 10.099777 0.6930846\n\n\nCode\n# reshape into lengthwise file with rows as counties and columns as years\ndw <- reshape(d,\n  timevar = \"year\",\n  idvar = \"county\",\n  direction = \"wide\"\n)\n\nmap <- merge(map, dw, by.x = \"NAME\", by.y = \"county\")\n\nmap_sf <- st_as_sf(map) # into sf for ggplot\nmap_sf <- gather(map_sf, year, SIR, paste0(\"SIR.\", 1968:1988)) # longwise data now\nmap_sf$year <- as.integer(substring(map_sf$year, 5, 8)) # just get the year string\n\n# make plot\nggplot(map_sf) + \n  geom_sf(aes(fill = SIR)) + # map the shapefiles \n  facet_wrap(~year, dir = \"h\", ncol = 7) +\n  ggtitle(\"SIR\") + theme_bw() +\n  theme(\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank()\n  ) +\n  scale_fill_gradient2(\n    midpoint = 1, low = \"blue\", mid = \"white\", high = \"red\"\n  )\n\n\n\n\n\n\n\nCode\nggplot(d, aes(x = year, y = SIR, \n                   group = county, color = county)) +\n  geom_line() + geom_point(size = 2) + theme_bw() +\n  theme(legend.position = \"none\") +\n  gghighlight(county == \"Adams\")\n\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\nTried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: county\n\n\n\n\n\nNow we actually model the data with a spatio-temporal component\n\n\nCode\nnb <- poly2nb(map) # convert to adjmat\nnb2INLA(\"data/map_ohio.adj\", nb)\ng <- inla.read.graph(filename = \"data/map_ohio.adj\")\n\nd$idarea <- as.numeric(as.factor(d$county))\nd$idarea1 <- d$idarea # make copy of the idarea, becuase two terms with spatial random componenet, but in INLA, only one variable can be associated to the `f()` at a time.\nd$idtime <- 1 + d$year - min(d$year) # index years/dates from 1\n\n# for documentation of how this matches with model\n# inla.doc(what = \"bym\")\nformula <- Y ~ f(idarea, model = \"bym\", graph = g) + # spatial component\n  f(idarea1, idtime, model = \"iid\") + # time x area interaction\n  idtime # global trend across time\n\nres <- inla(formula,\n  family = \"poisson\", \n  data = d, \n  E = E,\n  control.predictor = list(compute = TRUE)\n)\n\n\nWarning in is.null(x) || is.na(x): 'length(x) = 176 > 1' in coercion to\n'logical(1)'\n\n\nWarning in is.null(x) || is.na(x): 'length(x) = 88 > 1' in coercion to\n'logical(1)'\n\n\n\n\nCode\nsummary(res)\n\n\nWarning in length(alist[[idx]]) > 0 && !is.null(alist[[idx]]) && !\nis.na(alist[[idx]]): 'length(x) = 4 > 1' in coercion to 'logical(1)'\n\n\n\nCall:\n   c(\"inla(formula = formula, family = \\\"poisson\\\", data = d, E = E, \", \" \n   control.predictor = list(compute = TRUE))\") \nTime used:\n    Pre = 1.88, Running = 1.93, Post = 0.245, Total = 4.06 \n\n\nWarning in length(alist[[idx]]) > 0 && !is.null(alist[[idx]]) && !\nis.na(alist[[idx]]): 'length(x) = 14 > 1' in coercion to 'logical(1)'\n\n\nFixed effects:\n              mean    sd 0.025quant 0.5quant 0.975quant   mode kld\n(Intercept) -0.530 0.024     -0.578   -0.530     -0.483 -0.530   0\nidtime       0.037 0.001      0.035    0.037      0.039  0.037   0\n\n\nWarning in length(alist[[idx]]) > 0 && !is.null(alist[[idx]]) && !\nis.na(alist[[idx]]): 'length(x) = 2 > 1' in coercion to 'logical(1)'\n\n\nRandom effects:\n  Name    Model\n    idarea BYM model\n   idarea1 IID model\n\n\nWarning in length(alist[[idx]]) > 0 && !is.null(alist[[idx]]) && !\nis.na(alist[[idx]]): 'length(x) = 18 > 1' in coercion to 'logical(1)'\n\n\nModel hyperparameters:\n                                             mean       sd 0.025quant 0.5quant\nPrecision for idarea (iid component)        24.63     4.22      17.14    24.38\nPrecision for idarea (spatial component)  1876.15  1840.62     128.96  1334.40\nPrecision for idarea1                    49493.04 17429.28   24246.10 46511.04\n                                         0.975quant     mode\nPrecision for idarea (iid component)          33.67    23.95\nPrecision for idarea (spatial component)    6743.10   353.74\nPrecision for idarea1                      91724.60 41176.03\n\n\nWarning in length(alist[[idx]]) > 0 && !is.null(alist[[idx]]) && !\nis.na(alist[[idx]]): 'length(x) = 3 > 1' in coercion to 'logical(1)'\n\n\nExpected number of effective parameters(stdev): 111.13(4.70)\nNumber of equivalent replicates : 16.63 \n\n\nWarning in length(alist[[idx]]) > 0 && !is.null(alist[[idx]]) && !\nis.na(alist[[idx]]): 'length(x) = 2 > 1' in coercion to 'logical(1)'\n\n\nMarginal log-Likelihood:  -5940.87 \n\n\nWarning in length(alist[[idx]]) > 0 && !is.null(alist[[idx]]) && !\nis.na(alist[[idx]]): 'length(x) = 12936 > 1' in coercion to 'logical(1)'\n\n\nPosterior marginals for the linear predictor and\n the fitted values are computed\n\n\nIt’s hard to interpret these\n\n\nCode\n# plot with RR values\nd$RR <- res$summary.fitted.values[, \"mean\"]\nd$LL <- res$summary.fitted.values[, \"0.025quant\"]\nd$UL <- res$summary.fitted.values[, \"0.975quant\"]\n\nmap_sf <- merge(\n  map_sf, d,\n  by.x = c(\"NAME\", \"year\"),\n  by.y = c(\"county\", \"year\")\n)\n\nggplot(map_sf) + geom_sf(aes(fill = RR)) +\n  facet_wrap(~year, dir = \"h\", ncol = 7) +\n  ggtitle(\"RR\") + theme_bw() +\n  theme(\n    axis.text.x = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks = element_blank()\n  ) +\n  scale_fill_gradient2(\n    midpoint = 1, low = \"blue\", mid = \"white\", high = \"red\"\n  )"
  },
  {
    "objectID": "spatial/spatial.html#examples-1",
    "href": "spatial/spatial.html#examples-1",
    "title": "32  Spatial Statistics",
    "section": "35.1 Examples",
    "text": "35.1 Examples\n\n35.1.1 1D (IDE)\nThis example assumes the Itero-Differential Equation (IDE) is given, and we will show how to simulate from a given IDE. The ID we consider is\n\n\\begin{aligned}\nY_t(s) = \\int_{D_s} m(s, x; \\mathbf{\\theta_p})Y_{t-1}(x)\\,dx + \\eta_t(s), \\quad s,x\\in D_s\n\\end{aligned}\n\n\nY_t(s) is the spatio-temporal process at time t,\nm(s,x;\\mathbf{\\theta_p}) - is called the transition kernel, and governs the dynamics of the process\n\\theta_p are the parameters, normally estimated, but we fix them for now.\n\\eta_t(\\cdot) is a spatial process.\n\n\n\nCode\nset.seed(1)\n\n# create grid for simulation, (between 0,1)\n\nds <- 0.01 # step for spatial grid\ns_grid <- seq(0, 1, by = ds) # spatial grid\nN <- length(s_grid)\n\n\n\n\nCode\nnT <- 201 # number of time points\nt_grid <- 0:(nT-1) # time grid\nst_grid <- expand.grid(s = s_grid, t = t_grid) # space-time grid\n\n\n\n\nCode\nm <- function(x, s,thetap) {\n gamma <- thetap[1]             # amplitude\n l <- thetap[2]                 # length\n offset <- thetap[3]            # offset\n D <- outer(s + offset, x, \"-\") # displacements\n gamma * exp(-D^2/l)            # kernel eval.\n}\n\n\n\n\nCode\n# see how the parameterization differs for a few\nthetap <- list()\nthetap[[1]] <- c(40, 0.0002, 0) # narrow, centered.\nthetap[[2]] <- c(5.75, 0.01, 0) # wider\nthetap[[3]] <- c(8, 0.005, 0.1) # shifted right\nthetap[[4]] <- c(8, 0.005, -0.1) # shifted left\n\n\n\n\nCode\nm_x <- m(s=0.5, x = s_grid,\n         thetap=thetap[[1]]) %>% \n  as.numeric()\ndf <- data.frame(x=s_grid, m = m_x)\n\n# plot the data\nggplot(df) + \n  geom_line(aes(x,m)) +\n  theme_bw() \n\n\n\n\n\n\n\nCode\n# The covariance of the spatial noise\n# Simulate from Multivariate Normal, by cholesky decomp of the covariance\nSigma_eta <- 0.1 * exp( -abs(outer(s_grid, s_grid, '-') / 0.1))\n\nL <- t(chol(Sigma_eta)) # chol() returns upper Cholesky factor\nsim <- L %*% rnorm(nrow(Sigma_eta)) # simulate realization of eta\n\nqplot(s_grid, sim, geom = \"line\") +\n  theme_minimal() +\n  labs(title = \"realization of a random spatial process in 1d\")\n\n\n\n\n\n\n\nCode\nY <- list()\n\n# outer loop does 4 kernels\nfor(i in 1:4) {\n  M <- m(s_grid, s_grid, thetap[[i]]) # create kernel function\n  \n  \n  Y[[i]] <- data.frame(s = s_grid, # initialize grid and process\n                       t = 0,\n                       Y = 0)\n  # for each timepoint\n  for (j in t_grid[-1]) {\n    prev_Y <- filter(Y[[i]], t == j - 1)$Y # get Y at t-1\n    eta <- L %*% rnorm(N) # simulate eta\n    \n    new_Y <- (M %*% prev_Y * ds + eta) %>% as.numeric() # euler approximation, approximating integral with sum\n    \n    Y[[i]] <- rbind(Y[[i]],\n                    data.frame(s = s_grid,\n                               t = j,\n                               Y = new_Y))\n  }\n}\n\n\n\n\nCode\nhovmoller_plot <- function(idx) {\n  ggplot(Y[[idx]]) + \n    geom_tile(aes(s,t,fill = Y)) +\n    scale_x_continuous(expand = c(0,0)) +\n    scale_y_reverse(expand = c(0,0)) + \n    fill_scale(name = \"Y\") +\n    theme_bw() +\n    theme(legend.position = \"bottom\")\n}\n\n\n\n\nCode\np1 <- hovmoller_plot(1)\np2 <- hovmoller_plot(2)\np3 <- hovmoller_plot(3)\np4 <- hovmoller_plot(4)\nplot_grid(p1, p2, p3, p4, nrow = 1)\n\n\n\n\n\nNow we learn how to simulate data. The above graphs show the entire realization of the process model. In reality, we don’t get to observe all that data, instead, we just get little bits of the dynamics. So we create an “Incidence Matrix” \\mathbf{H}_t to account for the data that we actually do observe. i.e., the above was the “process model”, now we implement the “data model”. We also assume we make some measurement errors, that are iid w/ mean zero.\n\n\\begin{aligned}\nZ_t = H_tY_t + \\varepsilon_t\n\\end{aligned}\n\n\n\nCode\nnobs <- 50 # number of observations at each timepoint\nsobs <- sample(s_grid, nobs) # sample points on the grid\n# should be w/ replacment right?\n\n# Initialize incidence, nobs x N matrix\nHt <- matrix(0, nrow = nobs, ncol = N) # the incidence matrix\nfor(i in 1:nobs){\n  idx <- which(sobs[i] == s_grid) # find elements and set to 1\n  Ht[i, idx] <- 1\n}\n\n\n\n\nCode\nz_df <- data.frame() # init data frame to save noisy observations\n\n# for each time point, we get the underlying process, take our observations and add measurement error\nfor (j in 0:(nT-1)) {\n  Yt <- Y[[1]] %>% filter(t == j) %>% pull(Y) # get simulated process\n  zt <- Ht %*% Yt + rnorm(nobs) # map \n  z_df <- rbind(z_df, data.frame(s = sobs, t = j, z = zt))\n}\n\n\n\n\nCode\nggplot(z_df) + geom_point(aes(s, t, colour = z)) + \n  col_scale(name = \"z\") +\n  scale_y_reverse(expand = c(0, 0)) +\n  scale_x_continuous(expand = c(0, 0)) + \n  theme_test() +\n  labs(title = \"Noisy Observation of Latent Process\")\n\n\n\n\n\n\n\n35.1.2 Spatio-Temporal Inference with the IDE Model\nFirst example if simulating from a spatially invariant kernel, this is what k_spat_invariant=1 specificies. The movement here is very simple, it simply drifts away.\n\n\nCode\nSIM1 <- simIDE(T = 10, nobs = 100, k_spat_invariant = 1)\n\n\nas(<ngCMatrix>, \"dgCMatrix\") is deprecated since Matrix 1.5-0; do as(., \"dMatrix\") instead\n\n\n\n\nCode\nstr(SIM1, max.level=1)\n\n\nList of 6\n $ s_df    :'data.frame':   16810 obs. of  4 variables:\n $ z_df    :'data.frame':   1000 obs. of  4 variables:\n $ z_STIDF :Formal class 'STIDF' [package \"spacetime\"] with 4 slots\n $ g_truth :List of 9\n  ..- attr(*, \"class\")= chr [1:2] \"gg\" \"ggplot\"\n $ g_obs   :List of 9\n  ..- attr(*, \"class\")= chr [1:2] \"gg\" \"ggplot\"\n $ IDEmodel:List of 5\n  ..- attr(*, \"class\")= chr \"IDE\"\n\n\n\ns_df - is the space df\nz_df - is the observed noisy data\ng_truth - a ggplot of the true process\ng_obs - a ggplot of the observations\n\n\n\nCode\nprint(SIM1$g_truth)\n\n\n\n\n\nCode\nprint(SIM1$g_obs)\n\n\n\n\n\n\n\nCode\n# returns object of class IDE and initial parameter estimates of \\alpha_t\nIDEmodel <- IDE(f=z~s1+s2,\n                data = SIM1$z_STIDF,\n                dt = as.difftime(1, units = \"days\"),\n                grid_size = 41)\n\n# Fits with deOptim, a derivative free optimzation method (same category as Nelder Mead and Simulated Annealing, but with different principles)\n# Stands for differential evolution\n# function is incredibly intenstive for calculation so the results can also be loaded below - \n# fit_results_sim1 <- fit.IDE(IDEmodel, parallelType = 1)\ndata(\"IDE_Sim1_results\", package = \"STRbook\")\n\n\n\n\nCode\nshow_kernel(fit_results_sim1$IDEmodel) # show the kernel of the function\n\n\nKernel is spatially invariant, plotting it centred on the origin.\n\n\n\n\n\n\n\nCode\n# parameters of the model can be extracted from the object here\n# real values are c(150, .002, -0.1, 0.1)\nfit_results_sim1$IDEmodel$get(\"k\") %>% unlist()\n\n\n         par1          par2          par3          par4 \n152.836345912   0.001977115  -0.101601099   0.100368743 \n\n\n\n\nCode\ncoef(fit_results_sim1$IDEmodel) # true values are c(.2, .2, .2)\n\n\nIntercept        s1        s2 \n0.2073442 0.1966224 0.1907062 \n\n\nNow we extract the eigenvalues of the fitted evolution matrix to understand its long term behavior.\n\n\nCode\nabs_ev <- eigen(fit_results_sim1$IDEmodel$get(\"M\"))$values %>% \n  abs()\n\nabs_ev %>% head() \n\n\n[1] 0.4640858 0.4640858 0.4492270 0.4492270 0.4289470 0.4289470\n\n\nThe largest of these values is greater than 1, thus we conclude that the dynamics of this evolution are stable.\n\n\nCode\nST_grid_df <- predict(fit_results_sim1$IDEmodel) # can also take a new prediction grid, but by default it takes the simulation grid that we fit the data model for.\n\n\n\n\nCode\n# Prediction map\ngpred <- ggplot(ST_grid_df) + # Plot the predictions geom_tile(aes(s1, s2, fill=Ypred)) +\n  geom_tile(aes(s1,s2, fill = Ypred)) +\n  facet_wrap(~t) +\n  fill_scale(name = \"Ypred\", limits = c(-0.1, 1.4)) + \n  coord_fixed(xlim=c(0, 1), ylim = c(0, 1)) +\n  labs(title = \"Predictions\")\n\n# Prediction standard error map\ngpredse <- ggplot(ST_grid_df) + # Plot the prediction s.es\n  geom_tile(aes(s1, s2, fill = Ypredse)) +\n  facet_wrap(~t) +\n  fill_scale(name = \"Ypredse\") +\n  coord_fixed(xlim=c(0, 1), ylim = c(0, 1)) +\n  labs(title = \"Predictions SE\")\n\nplot_grid(gpred, gpredse)\n\n\n\n\n\n\n\n35.1.3 Now we run an example of spatially varying kernel\nspatially varying kerneles are necessary in order to model forms of advection. specified with k_spat_invariant = 0\n\n\nCode\nSIM2 <- simIDE(T = 15, nobs = 1000, k_spat_invariant = 0)\n\n\n\n\nCode\n# the two ggplots of the truth and the simulated observations\nprint(SIM2$g_truth)\n\n\n\n\n\nCode\nprint(SIM2$g_obs)\n\n\n\n\n\nappears to counter clockwise rotate, and then come to standstill in bottom of the domain. The varying advection that generated this field is visualized as follows…\n\n\nCode\nshow_kernel(SIM2$IDEmodel, scale = 0.2) # scale says change the arrow sizes.\n\n\nKernel is spatially variant, plotting displacements\n\n\n\n\n\n\n\nCode\n#  bisquare basis functions, same class as used by FRK.\nmbasis_1 <- auto_basis(manifold = plane(), #fns, on the plane\n                       data = SIM2$z_STIDF, # data\n                       nres = 1, # 1 resolution\n                       type = 'bisquare')\nshow_basis(mbasis_1)\n\n\nNote: show_basis assumes spherical distance functions when plotting\n\n\n\n\n\n\n\nCode\n# now in our kernel, we can assume that amplitude (thetam1) and the scale (thetam2) are both constant\nkernel_basis <- list(thetam1 = constant_basis(),\n                     thetam2 = constant_basis(),\n                     thetam3 = mbasis_1,\n                     thetam4 = mbasis_1)\n\n# specify the model\nIDEmodel <- IDE(f = z ~ s1 + s2 + 1,\n                data = SIM2$z_STIDF,\n                dt = as.difftime(1, units = \"days\"),\n                grid_size = 41,\n                kernel_basis = kernel_basis)\n\n# fit the IDE, computationally intensive so load from cache\n# fit_results_sim2 <- fit.IDE(IDEmodel, parallelType = 1,\n#                            itermax = 400)\ndata(\"IDE_Sim2_results\", package = \"STRbook\")\n\nshow_kernel(fit_results_sim2$IDEmodel)\n\n\nKernel is spatially variant, plotting displacements\n\n\n\n\n\nCode\nST_grid_df2 <- predict(fit_results_sim2$IDEmodel)\n\n\n\n\nCode\ngpred <- ggplot(ST_grid_df2) + # Plot the predictions geom_tile(aes(s1, s2, fill=Ypred)) +\n  geom_tile(aes(s1, s2, fill = Ypred)) +\n  facet_wrap(~t) +\n  fill_scale(name = \"Ypred\", limits = c(-0.1, 1.4)) + \n  coord_fixed(xlim=c(0, 1), ylim = c(0, 1)) +\n  labs(title = \"Predictions\")\n\n\nThese are the predictions that were\n\n\n35.1.4 Sydney Radar Data Set\nAbove we did a simulation example, but here we do a real data example\n\n\nCode\ndata(\"radar_STIDF\", package = \"STRbook\")\n\n\n\nhindcast how many time intervals in to past to estimate\nforecase how many time intervals into the future to estimate.\n\n\n\nCode\nIDEmodel <- IDE(f = z~1,\n                data = radar_STIDF,\n                dt=as.difftime(10, units = \"mins\"),\n                grid_size = 41,\n                forecast = 2,\n                hindcast = 2)\n\n# fitmodel\n# fit_results_radar <- fit.IDE(IDEmodel, parallelType = 1)\ndata(\"IDE_Radar_results\", package = \"STRbook\") # load cached results\nshow_kernel(fit_results_radar$IDEmodel) \n\n\nKernel is spatially invariant, plotting it centred on the origin.\n\n\n\n\n\nshifted off center, so suggestive of predominatly north east direction.\n\n\nCode\nshift_pars <- (fit_results_radar$IDEmodel$get(\"k\") %>% unlist())[3:4]\nprint(shift_pars)\n\n\n     par3      par4 \n-5.488385 -1.860784 \n\n\nWe can make a calculation here, \\sqrt{5.5^2 + 1.9^2} = 5.82 in 10 minutes, 34.91 km per hour.\n\n\nCode\nabs_ev <- eigen(fit_results_radar$IDEmodel$get(\"M\"))$values %>% abs()\nsummary(abs_ev)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008377 0.581449 0.671552 0.615239 0.719497 0.786783 \n\n\nmaximum eigenvalue less than 1 means that its stable, but more persistent than the simulation example.\n\n\nCode\nST_grid_df <- predict(fit_results_radar$IDEmodel)\nradar_df$time <- format(radar_df$t, \"%H:%M\")\nST_grid_df$time <- format(ST_grid_df$t, \"%H:%M\")\n\n\n\n\nCode\n## Add time records with missing data\nradar_df <- rbind.fill(radar_df, data.frame(time = c(\"08:05\", \"08:15\",\n                                           \"10:25\", \"10:35\")))\n\n## Plot of data, with color scale capped to (-20, 60)\ngobs <- ggplot(radar_df) +\n  geom_tile(aes(s1, s2, fill = pmin(pmax(z, -20), 60))) + \n  fill_scale(limits = c(-20, 60), name = \"Z\") + \n  facet_wrap(~time) + \n  coord_fixed() + \n  theme_bw() +\n  labs(title = \"Observed Sydney Data\")\n\n## Plot of predictions with color scale forced to (-20, 60)\ngpred <- ggplot(ST_grid_df) +\n  geom_tile(aes(s1, s2, fill = Ypred)) + facet_wrap(~time) + coord_fixed() + theme_bw() + fill_scale(limits = c(-20, 60), name = \"Ypred\") +\n  labs(title = \"Predicted Data, with 2 time periods before and after\")\n\nplot_grid(gobs, gpred)\n\n\nWarning: Removed 4 rows containing missing values (geom_tile).\n\n\n\n\n\n\n\n35.1.5 Inference with Unknown Evolution Operator\nWe use EOFs are ideal basis functions for exploring the observed signal.\nWe fit two models, a traditional one in time-series framework with vector autoregression and use method of moments.\n\n\nCode\n# trucate at April 1997 in order to forcast 6 months ahead\n\ndata(\"SSTlandmask\", package = \"STRbook\")\ndata(\"SSTlonlat\", package = \"STRbook\")\ndata(\"SSTdata\", package = \"STRbook\")\ndelete_rows <- which(SSTlandmask == 1) # remove land values\nSST_Oct97 <- SSTdata[-delete_rows, 334] # save Oct 1997 SSTs  \nSSTdata <- SSTdata[-delete_rows, 1:328] # until April 1997 , 399 times points normally, but truncate\nSSTlonlat$mask <- SSTlandmask # assign mask to df\n\n\n\n\nCode\n# 2520 x 328 dataframe\nSSTdata %>% head()\n\n\n          V1        V2        V3        V4          V5          V6         V7\n16 0.2296104 0.2515602 0.4510937 0.2989082 -0.28913879 -0.20289230 -0.1392193\n17 0.4862499 0.4507828 0.5332813 0.5499210 -0.22593880 -0.05968857  0.2024994\n18 0.7855454 0.6135120 0.5860958 0.6160946 -0.11890602 -0.01961136  0.3596077\n19 0.9411717 0.7603111 0.7017193 0.5956268 -0.02531242 -0.02273369  0.4096889\n20 1.0239792 0.7840633 0.7537537 0.6158600  0.02359390 -0.06570435  0.3028908\n21 1.0313301 0.8165626 0.8502331 0.6375790  0.09546661 -0.03921890  0.2607803\n           V8          V9         V10         V11       V12       V13\n16 -0.3317967 -0.08953095 0.000623703  0.16234589 0.3113289 0.6296101\n17  0.0696888  0.15523529 0.237970350  0.11281204 0.5346890 0.7362499\n18  0.2849217  0.20914078 0.395938870 -0.06546974 0.5482826 0.7855454\n19  0.3793736  0.20929337 0.427658080 -0.05945587 0.5828152 0.8411713\n20  0.4363270  0.26359367 0.372110370 -0.05234337 0.5239067 0.9239807\n21  0.4079704  0.33273506 0.348985670 -0.01726532 0.4121094 0.9813290\n          V14         V15        V16         V17       V18        V19\n16 -0.1484413 -0.29890633 -0.4010925 -0.18914032 0.1471081 -0.1392174\n17  0.2507839 -0.16671944 -0.1000786  0.02406311 0.5903110  0.3024979\n18  0.7135143 -0.06390381  0.1160946  0.18109322 0.9303894  0.5096092\n19  1.0103111  0.05171967  0.2456264  0.32468605 1.0772667  0.5596886\n20  1.0840626  0.10375214  0.3158608  0.47359467 1.0842972  0.5528908\n21  1.0665646  0.20023537  0.3875771  0.69546700 1.1107807  0.5607834\n           V20        V21        V22         V23        V24        V25\n16 -0.08179855 -0.2395306 -0.1993771 -0.58765602 -0.4886703 -0.3203907\n17  0.06968880  0.1552353  0.1879711 -0.23718643 -0.3653126 -0.2137508\n18  0.18492317  0.4591408  0.4959373 -0.01547051 -0.3517170 -0.3144531\n19  0.22937393  0.6592922  0.6776581  0.14054489 -0.3171864 -0.5088291\n20  0.28632736  0.7135944  0.6721096  0.14765739 -0.2760944 -0.6260185\n21  0.30796814  0.6827354  0.6489849  0.08273506 -0.3378906 -0.6186695\n          V26        V27         V28         V29        V30         V31\n16 -0.4984398 -0.5489063 -0.10109329 -0.53913879 -0.3528919 -0.08921814\n17 -0.3492184 -0.6167183  0.14992332 -0.22593880 -0.2096882 -0.14750099\n18 -0.3864861 -0.6639042  0.21609306  0.08109283 -0.1196098 -0.24039078\n19 -0.4396877 -0.4982815  0.14562798  0.12468910 -0.1227322 -0.24031067\n20 -0.6159363 -0.2462463  0.06586075  0.12359619 -0.2157040 -0.09710693\n21 -0.6834374  0.1502342  0.08757782  0.09546661 -0.2392197  0.11078262\n          V32         V33       V34       V35         V36       V37       V38\n16 0.06820297 -0.33953094 0.2006245 0.7123470 -0.03866959 0.3796101 0.7515602\n17 0.21968651 -0.14476395 0.5379696 0.7128124 -0.31531143 0.5362492 0.9007816\n18 0.13492393 -0.09085846 0.6959381 0.6345291 -0.50171661 0.6355476 0.9135132\n19 0.07937241 -0.04070663 0.6276588 0.5905438 -0.41718483 0.6911717 0.8103104\n20 0.03632736  0.06359291 0.4221096 0.5476570 -0.17609215 0.7239800 0.6840630\n21 0.05797005  0.33273506 0.2989864 0.6327343  0.06211090 0.6813297 0.5665626\n         V39        V40         V41       V42       V43       V44       V45\n16 0.7010937 -0.4510918 -0.23913956 0.7971077 0.7107811 0.7182026 0.9604683\n17 0.7832794 -0.2000790 -0.02593803 0.8903122 0.8024979 0.7696876 1.0552330\n18 0.7860966  0.1160946  0.23109245 0.8303890 0.7096100 0.6849232 0.9091415\n19 0.7517204  0.3456287  0.52468681 0.6272678 0.6096878 0.6793728 0.7592945\n20 0.8037529  0.4158592  0.72359467 0.5342960 0.5528908 0.6863270 0.6135922\n21 0.8002338  0.3875771  0.79546738 0.5607796 0.4607811 0.6579704 0.5327339\n         V46       V47       V48       V49       V50        V51          V52\n16 0.9506245 0.6123447 0.7113304 0.3296108 0.1515617 -0.5489063 -0.001091003\n17 1.1879711 0.8128128 0.6846886 0.4362488 0.1507816 -0.5167198  0.049921036\n18 1.2459373 0.9845295 0.5982838 0.4855461 0.1635151 -0.3139057  0.066095352\n19 1.0776577 0.9905453 0.6328144 0.4911728 0.1603108 -0.1482792  0.145627980\n20 1.0221100 0.9976559 0.7239056 0.5239792 0.1840630 -0.1462479  0.265859600\n21 0.9489842 1.0327358 0.8121109 0.5813293 0.2665634 -0.1497650  0.387578960\n           V53          V54          V55         V56         V57        V58\n16 -0.13913918 -0.102891920 -0.089218140 -0.28179550 -0.03952980 -0.3493748\n17 -0.07593918 -0.009689331  0.002498627 -0.23031044  0.05523491 -0.2120304\n18 -0.06890678 -0.169610980  0.009609222 -0.21507835 -0.04085922 -0.2540627\n19 -0.12531281 -0.272733690  0.009689331 -0.12062645 -0.09070587 -0.2223415\n20 -0.17640495 -0.165702820  0.102891920 -0.11367226 -0.13640785 -0.2778912\n21 -0.05453491  0.060779572  0.310781480 -0.09202957 -0.21726608 -0.2510128\n          V59        V60         V61        V62         V63         V64\n16 -0.8376541 -1.2386703 -0.77038956 -0.9984398 -0.14890671 -0.20109177\n17 -0.7871876 -0.9153118 -0.56375122 -0.8492184  0.08328247  0.04992104\n18 -0.7654705 -0.6017170 -0.41445351 -0.5864868  0.18609428  0.21609497\n19 -0.6094551 -0.2671871 -0.30882835 -0.4896889  0.25172043  0.09562874\n20 -0.3523426  0.0239048 -0.22602081 -0.3659363  0.25375366  0.06586075\n21 -0.1672649  0.3121109 -0.01867104 -0.3334370  0.20023537  0.13757706\n         V65         V66       V67       V68       V69         V70          V71\n16 0.1108608 -0.50289345 0.2607803 0.4182014 0.2604694 -0.34937477  0.212347030\n17 0.3240623 -0.25968933 0.5024986 0.5196876 0.5052338 -0.21203041  0.212812420\n18 0.3310947 -0.06961060 0.4596081 0.5849247 0.6091423 -0.05406189  0.134529110\n19 0.2746868 -0.02273369 0.3096886 0.6793728 0.6092930  0.02765655  0.040544510\n20 0.2235947  0.03429604 0.3028908 0.6863270 0.5635929  0.07210922 -0.002342224\n21 0.2454681  0.16077995 0.4107819 0.6079712 0.4327354  0.09898567  0.082735062\n           V72         V73        V74        V75        V76        V77\n16 -0.03866959 -0.17038918 -0.3984394 -0.1989078 -0.1510925 -0.2891388\n17  0.18468857 -0.06375122 -0.6492176 -0.1167183 -0.3500767 -0.3259392\n18  0.24828339 -0.01445389 -0.8364849 -0.1139050 -0.6339054 -0.4189053\n19  0.18281364 -0.05882835 -0.9896908 -0.2482796 -0.8043728 -0.5253124\n20  0.12390709 -0.12601852 -1.0659370 -0.2962456 -0.7841396 -0.6264038\n21  0.21210861  0.03132820 -0.9334354 -0.3997669 -0.6624222 -0.6045341\n          V78         V79        V80        V81         V82        V83\n16  0.2971077 -0.23921967 -0.4817963 -0.9395313  0.10062408 -0.1876545\n17  0.1403122 -0.04750061 -0.4303112 -0.9447651 -0.01202965 -0.1871872\n18 -0.0696106  0.05960846 -0.5650768 -0.9908581 -0.15406227 -0.2654705\n19 -0.1727333  0.10968781 -0.6706276 -0.9907055 -0.27234268 -0.4594555\n20 -0.2157040  0.00289154 -0.5636711 -0.9864063 -0.22789192 -0.6023426\n21 -0.2392197 -0.33921814 -0.3420296 -0.9172649 -0.15101433 -0.7672653\n          V84         V85           V86        V87         V88         V89\n16 -0.2386704  0.07961082  0.0015602112 -0.1989078  0.19890785  0.01086044\n17 -0.1653118 -0.06374931  0.0007820129 -0.2167206  0.09992027 -0.07593918\n18 -0.1017170 -0.26445389 -0.0864849090 -0.3139057 -0.13390541 -0.21890831\n19 -0.1671848 -0.60882950 -0.2396888700 -0.4982815 -0.30437279 -0.32531166\n20 -0.2260933 -0.97601891 -0.5159378100 -0.6962471 -0.43413925 -0.47640419\n21 -0.3878899 -1.11866950 -0.6334362000 -0.8997669 -0.46242142 -0.55453300\n            V90        V91         V92         V93        V94        V95\n16 -0.252893450 -0.6392193 -0.13179779  0.31046867 -0.1993771 -0.6376553\n17 -0.009689331 -0.5475006  0.06968880  0.15523529 -0.1620293 -0.2371864\n18  0.180389400 -0.6403923  0.08492279  0.05914116 -0.1540623  0.1345291\n19  0.227266310 -0.6903114  0.07937241 -0.04070473 -0.3223419  0.3405437\n20  0.134296420 -0.7971077  0.03632736 -0.13640785 -0.4778900  0.3476582\n21  0.060779572 -0.9392185 -0.04203033 -0.31726456 -0.5010147  0.1327343\n          V96         V97        V98          V99         V100        V101\n16 -0.4386711  0.12961006 0.35155869  0.001092911 -0.001092911  0.01086044\n17 -0.4153118 -0.01375008 0.30078316  0.033281326 -0.100078580 -0.07593918\n18 -0.3517170 -0.21445274 0.11351395 -0.063903809  0.016094208 -0.21890831\n19 -0.3671856 -0.45882797 0.06031227 -0.298280720  0.045627594 -0.32531166\n20 -0.4260941 -0.57601929 0.03406334 -0.496246340 -0.034141541 -0.37640381\n21 -0.4878883 -0.76867104 0.01656151 -0.699766160 -0.112421040 -0.60453415\n         V102       V103       V104       V105       V106       V107\n16 -0.1028919 -0.4892197 -0.3317967 -0.3895321 -0.4493771 -0.4876537\n17 -0.1096878 -0.2475014 -0.2303123 -0.1447659 -0.6620293 -0.3871880\n18 -0.2196102 -0.2403908 -0.5150776 -0.2408600 -0.8540630 -0.5154686\n19 -0.3727322 -0.3903122 -0.7706261 -0.4407063 -1.0223427 -0.6594562\n20 -0.5157051 -0.7471085 -1.0136719 -0.5364075 -1.1278896 -0.7523422\n21 -0.5892181 -1.1392174 -1.1420307 -0.5672646 -1.2010136 -0.7672672\n          V108        V109        V110        V111       V112       V113\n16 -0.08866882 -0.07039070 -0.29843903 -0.24890709 -0.2510929 0.06085968\n17  0.08468819  0.03624916 -0.14921761 -0.11671829 -0.4000778 0.22406006\n18  0.14828491 -0.01445389 -0.03648567 -0.01390266 -0.4839039 0.23109245\n19 -0.06718636 -0.10882950  0.01031113  0.05171967 -0.5543728 0.17468834\n20 -0.37609291 -0.12601852 -0.01593780  0.05375290 -0.4841404 0.17359543\n21 -0.53788948 -0.06867027 -0.13343620  0.05023384 -0.4124222 0.14546585\n           V114      V115        V116       V117        V118      V119\n16 -0.002893448 0.3607807 -0.03179741 -0.5395298 -0.09937668 0.2623463\n17 -0.009689331 0.4024983  0.06968880 -0.3447647 -0.06202888 0.5128136\n18 -0.219610210 0.2596092  0.08492279 -0.2408600 -0.25406265 0.6845303\n19 -0.272733690 0.2096882  0.02937317 -0.1907062 -0.42234230 0.7405453\n20 -0.265703200 0.1528912 -0.06367111 -0.1364078 -0.47789001 0.5976582\n21 -0.139219280 0.1107826 -0.09202957 -0.1172638 -0.40101433 0.4827347\n        V120        V121        V122        V123        V124        V125\n16 0.7113304  0.27961159 0.001560211  0.25109291  0.29890823 -0.03913879\n17 1.0346889  0.53624916 0.100782390  0.13328171  0.29992104  0.02406311\n18 1.0982838  0.58554840 0.163515090 -0.01390266  0.16609383 -0.11890602\n19 1.0828152  0.54117203 0.160310750  0.05171967 -0.00437355 -0.42531204\n20 1.0239048  0.22397995 0.134063720  0.10375214 -0.28413963 -0.57640457\n21 0.7621098 -0.01867104 0.166563030  0.15023422 -0.51242065 -0.65453339\n          V126       V127      V128      V129      V130       V131        V132\n16 -0.15289116 -0.5392189 0.3682022 0.5604687 0.5506229 0.46234512  0.36132812\n17 -0.05968857 -0.6475029 0.4196873 0.6552353 0.9379711 0.61281204  0.18468857\n18  0.03038979 -0.6403923 0.5849228 0.6591415 1.0959358 0.68452835  0.19828415\n19  0.12726784 -0.4403114 0.5793724 0.6592941 1.1776581 0.54054451  0.03281403\n20  0.18429565 -0.0471077 0.3363266 0.5635929 1.0721092 0.29765892 -0.27609444\n21  0.16077995  0.3607826 0.1079712 0.2827339 0.8989849 0.03273583 -0.18788910\n         V133      V134      V135        V136        V137       V138       V139\n16 -0.4703884 0.3015595 0.2010937  0.34890747  0.21086121 -0.1528931  0.2607803\n17 -0.4137516 0.5007820 0.3832817  0.34992027  0.12406158 -0.1596889 -0.1475029\n18 -0.2644539 0.6135139 0.4860954  0.11609459  0.03109169 -0.2696114 -0.6403923\n19 -0.1588287 0.7603092 0.5517197 -0.00437355  0.02468681 -0.3227329 -0.7403107\n20 -0.1260185 0.9340630 0.6037521  0.01585960 -0.07640457 -0.2657032 -0.4471092\n21 -0.2186699 1.0165634 0.6002350  0.23757935  0.04546738 -0.1892204 -0.0392189\n         V140        V141        V142        V143      V144      V145\n16 -0.7817974  0.16046906 -0.49937630  0.03609657 0.7950783 0.8483601\n17 -0.9803104 -0.04476547 -0.41203117 -0.07968712 0.7746887 0.7725010\n18 -1.0650768 -0.24085999 -0.35406303 -0.24671745 0.8395328 0.9817963\n19 -1.0206261 -0.34070778 -0.22234154 -0.53445625 0.7090645 1.1211700\n20 -0.7636719 -0.43640709 -0.07789040 -0.73484230 0.4301567 1.1452293\n21 -0.5920296 -0.26726532  0.09898567 -0.64476585 0.1733589 1.0863323\n          V146        V147        V148      V149        V150        V151\n16  1.06906130  0.77859306 0.637657170 0.9833584  0.33085823 -0.02671814\n17  0.60828018  0.42952919 0.227422710 0.4353123 -0.08218956 -0.30250359\n18  0.39851379  0.40234566 0.208593370 0.3185940 -0.21085930 -0.16664124\n19  0.17281151  0.31171989 0.316875460 0.3934364 -0.16648483  0.02218819\n20  0.01656342  0.10500336 0.229610440 0.3823452 -0.15695190  0.14789200\n21 -0.05843735 -0.05351639 0.005079269 0.2579651 -0.16296959  0.20453262\n          V152        V153       V154       V155        V156       V157\n16  0.12570190 -0.43577957 -0.8331261 -0.7339058 -0.21742058 -0.7816391\n17 -0.16781044 -0.68476486 -0.9507809 -0.9159355 -0.37906075 -0.8950024\n18 -0.09757805 -0.56586075 -0.8340626 -0.8817196 -0.29546738 -0.8582020\n19  0.09687424 -0.30320549 -0.5198422 -0.6694565 -0.03718758 -0.5488319\n20  0.26757812 -0.10640717 -0.3866406 -0.4398441  0.32140541 -0.2497692\n21  0.23421860 -0.08226585 -0.4585152 -0.3460140  0.36960983 -0.2899208\n          V158      V159       V160      V161      V162        V163        V164\n16 -0.30718994 0.1573410 0.05515671 0.3821087 0.6333599  0.91703033  0.61695290\n17 -0.38046646 0.1532784 0.15867233 0.3565579 0.4078121  0.51374817  0.09093666\n18 -0.33023643 0.2698460 0.40234566 0.3273430 0.3666382  0.45085907  0.06742287\n19 -0.20718956 0.2617188 0.58687592 0.2296867 0.2435169  0.09093857 -0.08062744\n20 -0.00718689 0.2062511 0.67585945 0.3248463 0.2292957 -0.14710999 -0.21367264\n21 -0.02343750 0.1427345 0.61632729 0.3792152 0.1582813 -0.12171745 -0.06077957\n        V165        V166       V167        V168       V169       V170\n16 0.7117176  1.38562390  1.0110931 -0.05742073 -0.5178909 -0.1209412\n17 0.2564831  0.51922035  0.5528126 -0.60531235 -1.1012516 -0.3117180\n18 0.3141422  0.10843658  0.1707783 -1.01671600 -1.3957024 -0.6764889\n19 0.2755413 -0.09609222 -0.1082058 -1.14843750 -1.2488289 -0.8209400\n20 0.1223450 -0.07289123 -0.1173420 -1.05734440 -1.0360203 -0.7784386\n21 0.1727352  0.06648445  0.1189823 -0.86164093 -1.0849228 -0.7984371\n         V171       V172       V173        V174       V175       V176\n16 -0.1339092 -0.4660950 -0.1666393 -0.08039475 0.12578011 0.21570206\n17 -0.2142200 -0.5988293 -0.2109375 -0.24968910 0.06374741 0.04093742\n18 -0.4201546 -0.6976547 -0.2576561 -0.20336151 0.23960876 0.02742195\n19 -0.5032806 -0.5843735 -0.2228146 -0.16773415 0.29718781 0.07187271\n20 -0.5249977 -0.5328903 -0.1551533 -0.05320358 0.33039093 0.16257668\n21 -0.4910164 -0.5149231 -0.1007843 -0.12796974 0.20953178 0.11421967\n          V177       V178        V179       V180       V181       V182\n16 -0.22078323 -0.5731258  0.12359619 -0.2949219  0.1958599 -0.1096897\n17 -0.55101585 -0.8070278 -0.11468697 -0.3703117 -0.2112522 -0.5754681\n18 -0.51210785 -0.7140636  0.04577828 -0.3154678 -0.5044517 -0.7539864\n19 -0.30445671 -0.4910946  0.26179314 -0.2834358 -0.6175804 -0.7184391\n20 -0.08765793 -0.3266411  0.32390594 -0.2535934 -0.4972687 -0.4896889\n21 -0.11726379 -0.3822651  0.28148270 -0.1966400 -0.1874199 -0.3184357\n         V183       V184        V185       V186       V187        V188\n16 -0.2151585  0.1864052  0.12085915 -0.2991428 -0.2454681 -0.07679748\n17 -0.5679703 -0.1863270 -0.26968765 -0.8096905 -0.7050018 -0.30031395\n18 -0.7351551 -0.3064079 -0.31140900 -0.7671108 -0.4891434 -0.26632690\n19 -0.7907810 -0.2668724 -0.09656334 -0.3864842 -0.1265602 -0.24062729\n20 -0.5812473 -0.1128921  0.01484489 -0.2807026 -0.1658592 -0.20242310\n21 -0.4510174 -0.2074223 -0.18203354 -0.5467205 -0.4217186 -0.13328171\n         V189        V190       V191        V192      V193      V194       V195\n16  0.2692184  0.36312485 -0.1226540  0.25008011 0.4158592 0.1753082 -0.4701595\n17 -0.1335163 -0.11702919 -0.5834370  0.04843903 0.3225002 0.2020321 -0.2867184\n18 -0.2958603 -0.41156387 -0.7179718 -0.14296722 0.2180481 0.3510132 -0.1589031\n19 -0.4294567 -0.42484093 -0.5457058 -0.15093803 0.2024193 0.4028091 -0.2270317\n20 -0.3414059 -0.17914009 -0.2060928 -0.10359383 0.2714806 0.3728123 -0.3724976\n21 -0.1660156 -0.06351471 -0.1697655 -0.11788940 0.3925800 0.4190617 -0.2622662\n          V196      V197      V198       V199          V200        V201\n16  0.07890701 0.6096096 1.0221043 0.47328186  0.4857006100  0.58547020\n17  0.26742172 0.7765636 0.7503109 0.12499809 -0.0790615080  0.18398285\n18  0.19734573 0.6723423 0.4016399 0.03335762 -0.1675758400  0.01664162\n19 -0.10312271 0.3846874 0.1810169 0.13718987  0.0006217956 -0.07320786\n20 -0.36289024 0.2610950 0.2867947 0.21664047  0.0688266750 -0.10890579\n21 -0.30367279 0.4079666 0.4595299 0.26703262  0.0092182159 -0.18976593\n        V202        V203        V204        V205        V206       V207\n16 0.9256229  0.37484550  0.67757797  0.61710930  0.73031235  0.3635922\n17 0.7967205 -0.08593941  0.05968666  0.24624825  0.32953262 -0.1304684\n18 0.7534370 -0.25172234 -0.23921776  0.06679535 -0.01148605 -0.4939041\n19 0.6614075 -0.29195595 -0.38718796 -0.09132957 -0.30593872 -0.6345291\n20 0.5396099 -0.11484337 -0.43234444 -0.18851852 -0.26968765 -0.6574974\n21 0.4102344  0.05648231 -0.55289078 -0.37616920 -0.16718864 -0.5597649\n         V208       V209        V210       V211       V212        V213\n16  0.3026562  0.7621098  0.22335625 0.12828064 0.67695236  0.49046707\n17 -0.4175777  0.2878094  0.03155899 0.04874802 0.57343674  0.14898491\n18 -0.8276558 -0.0701580 -0.08461189 0.03085899 0.35367203 -0.01460838\n19 -0.8168735 -0.1740627 -0.01273155 0.22218895 0.28437424 -0.01195717\n20 -0.8103924 -0.3301563  0.02929497 0.25163841 0.17507553 -0.03765678\n21 -0.6874218 -0.2782841 -0.07547188 0.06828117 0.09671974 -0.04101562\n         V214       V215       V216       V217        V218       V219\n16  0.2331219  0.1260948  0.5738277  0.1471119 -0.01969147 -0.3151588\n17 -0.1932812 -0.5146885 -0.0465622 -0.3875027 -0.53296852 -0.7954674\n18 -0.3840637 -0.9417210 -0.4767170 -0.7269554 -0.49648476 -0.8614063\n19 -0.1685925 -0.6757069 -0.2846870 -0.5325794 -0.13094139 -0.6257820\n20 -0.1003914 -0.3348427 -0.1773453 -0.4710197 -0.06593704 -0.5387478\n21 -0.1497650 -0.3322658 -0.4166412 -0.5811691 -0.15468788 -0.5660152\n          V220      V221       V222      V223      V224      V225      V226\n16  0.11265945 0.7871094 0.18210983 0.6070309 0.3407040 0.5929680 1.3456230\n17 -0.18507957 0.4715614 0.07031059 0.3962479 0.1459370 0.2514858 1.1279716\n18 -0.29140663 0.3098431 0.12163925 0.4771080 0.1499233 0.1228924 0.9871864\n19 -0.14812088 0.4196873 0.26851654 0.6059380 0.3931236 0.2492924 1.0926571\n20 -0.05539131 0.5448437 0.30429459 0.7591400 0.8350773 0.6285915 1.2883587\n21 -0.22867203 0.4192162 0.22702980 0.7982826 1.0704689 0.8977356 1.4464855\n        V227      V228        V229       V230       V231        V232      V233\n16 0.3835945 0.4400787  0.23710823 -0.4234409 -0.6201553  0.49265671 0.7646084\n17 0.6715622 0.4509373 -0.26375008 -0.9267159 -0.9779682  0.07742119 0.4590607\n18 0.9320297 0.3357830 -0.43195343 -0.8952370 -0.8676548 -0.09015846 0.2360935\n19 1.1880436 0.3053131 -0.28133011 -0.6159382 -0.6045322  0.02437592 0.3471889\n20 1.4201565 0.3601570 -0.25977135 -0.5934372 -0.7362499 -0.07538986 0.3373451\n21 1.5002327 0.6358604 -0.03116989 -0.4821873 -0.7747669 -0.07367325 0.3992157\n          V234        V235        V236         V237       V238        V239\n16  0.36210823 -0.10046959 -0.22554779 -0.773281100 -0.6468754 -0.34015465\n17 -0.03343773 -0.38250160 -0.48406219 -1.066015200 -1.0020313 -0.64093590\n18 -0.10336113 -0.26414108 -0.39007568 -0.840858460 -0.8053150 -0.43421936\n19  0.11226654  0.04843903 -0.01687622 -0.300706860 -0.3085938 -0.03320503\n20  0.11304474  0.13039017  0.27382660  0.001092911 -0.1491413  0.03390503\n21  0.13828087  0.21203232  0.44046974  0.072734833 -0.1610146 -0.08726502\n         V240       V241        V242         V243        V244       V245\n16 -0.4949207 -0.3678913  0.09031105 -0.381408690  0.08140755 0.21586037\n17 -0.9828110 -1.0800018 -0.52671814 -0.732967380 -0.08757973 0.14530945\n18 -1.0229645 -1.3569527 -0.57023621 -0.492654800 -0.01640701 0.04109383\n19 -0.8709374 -1.1675797 -0.16843987 -0.007030487  0.13937759 0.02593803\n20 -0.9298458 -1.0422707  0.01156044  0.127500530  0.16585922 0.16359520\n21 -1.0541401 -0.9186726  0.12406158  0.223983760  0.25757790 0.41796684\n          V246       V247       V248       V249       V250       V251      V252\n16  0.34710693 0.64578056 -0.1067963 -0.1845303  0.2131214 -0.3064060 0.5575790\n17  0.01656151 0.21499825 -0.4565620 -0.6985149 -0.4407787 -1.0421886 0.1659374\n18 -0.32461166 0.03210831 -0.5163288 -0.7821102 -0.6190624 -1.1242218 0.2407837\n19 -0.33648300 0.09718704 -0.3043766 -0.5294571 -0.5198422 -0.8394566 0.4990635\n20 -0.15070343 0.13164139 -0.3011723 -0.3976574 -0.4916420 -0.5148430 0.6739044\n21 -0.13921928 0.08828354 -0.2282810 -0.3597641 -0.5772648 -0.5585155 0.5121098\n        V253        V254       V255       V256        V257         V258\n16 0.6883602  0.52280807  0.1173420 -0.3673420  0.04461098  0.555856700\n17 0.3287487  0.31078148 -0.3504696 -0.6413288  0.05531120  0.230312350\n18 0.2892971  0.30726242 -0.5126534 -0.7239075 -0.05515671  0.002887726\n19 0.3824196  0.26655960 -0.6420307 -0.6206226 -0.09406090 -0.083982468\n20 0.3589802  0.09031296 -0.9037476 -0.6853905 -0.25265503 -0.118204120\n21 0.2575779 -0.02968788 -1.1035175 -0.8199196 -0.34328461 -0.152969360\n        V259         V260        V261        V262       V263       V264\n16 0.3945312  0.385702130  0.74921799  0.65187263 -0.1651535 -0.5974217\n17 0.1187477 -0.029062271  0.26773453  0.37172127 -0.1821861 -0.7328110\n18 0.1196079 -0.096326828  0.13914108  0.29468536 -0.3179703 -0.9342156\n19 0.1959381  0.005624771  0.04554367  0.17140770 -0.4232063 -0.9621849\n20 0.2053909 -0.008672714 -0.13515854 -0.00164032 -0.3598423 -0.9435921\n21 0.1345310 -0.067031860 -0.07351494  0.04273605 -0.3147659 -1.0328903\n         V265         V266       V267        V268        V269         V270\n16 -0.2053909  0.006557465 -0.4001579 -0.07859039  0.63336182  0.638355260\n17 -0.3000011 -0.175468440 -0.7092209 -0.43757820  0.20406151 -0.005937576\n18 -0.4544544 -0.065237045 -0.6789055 -0.42765617 -0.06265831 -0.500860210\n19 -0.4400787  0.131561280 -0.5432816 -0.34187317 -0.23531342 -0.660232540\n20 -0.3622704  0.125312810 -0.7737465 -0.52038956 -0.39390564 -0.603204730\n21 -0.2936726  0.071561813 -1.0147648 -0.67117310 -0.42453384 -0.462970730\n         V271      V272       V273       V274       V275        V276       V277\n16 0.84953117 0.7444515  1.0642185  0.7081222  0.2785950 -0.02117157 0.24335861\n17 0.60374832 0.5446892  0.6114845  0.1242218 -0.4334373 -0.40781212 0.29999924\n18 0.22085762 0.2886734  0.1203918 -0.2240620 -0.7629700 -0.70046616 0.47054482\n19 0.04843903 0.3018723 -0.2057056 -0.5110931 -0.8394566 -0.84718704 0.55867004\n20 0.03413963 0.3513260 -0.3414059 -0.8741417 -0.7823429 -0.87984467 0.35023117\n21 0.04578209 0.2629700 -0.3422661 -0.8860149 -0.5047665 -0.80289078 0.06883049\n        V278        V279         V280         V281       V282      V283\n16 0.1703110 -0.76391029 -0.133592610  0.282110210 0.59835815 1.1932812\n17 0.5270328 -0.33671761 -0.086328506  0.086561203 0.25656319 0.8962479\n18 0.8060112  0.06734657  0.007341385  0.069843292 0.26788902 1.0121078\n19 0.7203102  0.02672005 -0.040622711  0.004686356 0.26101685 1.0909367\n20 0.3765640 -0.29874802 -0.186639790 -0.167654040 0.15929413 1.1091404\n21 0.1015606 -0.49976540 -0.241174700 -0.243284230 0.02453041 1.0557804\n       V284      V285       V286        V287        V288      V289        V290\n16 1.634453 1.1504688 0.60812187  0.07484436 -0.01992035 0.5021095 -0.02719116\n17 1.203436 0.5914841 0.08797073 -0.57593536 -0.61906242 0.2437477 -0.38671684\n18 1.213673 0.6216392 0.12218857 -0.71796989 -0.85421753 0.2405472 -0.51648712\n19 1.271874 0.6980419 0.38390732 -0.40945625 -0.71343613 0.5386696 -0.34093857\n20 1.145077 0.5698433 0.44335938 -0.06484222 -0.51359558 0.7352295 -0.25093651\n21 1.000469 0.4539852 0.40023613  0.03898239 -0.39163971 0.7938290 -0.18718910\n         V291       V292        V293       V294         V295       V296\n16 -0.4151573 -0.4085922  0.10335922 0.79585648  0.913280490 0.58570290\n17 -0.6767178 -0.7700787 -0.34844017 0.48280907  0.487499240 0.18218803\n18 -0.6276531 -0.9126549 -0.44265747 0.25413895  0.223356250 0.06117058\n19 -0.4332790 -0.8231239 -0.18906403 0.14476585  0.028438568 0.13812065\n20 -0.3974953 -0.6878910 -0.05265427 0.17429543  0.006639481 0.19507599\n21 -0.3435135 -0.6411705 -0.18328476 0.06202888 -0.096719742 0.13296890\n          V297        V298        V299        V300        V301        V302\n16  0.23546791 -0.09062576  0.38109589  0.18008041  0.21461105 -0.06093979\n17 -0.09226608  0.02172089  0.21531296 -0.11156464  0.08499908 -0.05421829\n18 -0.11460876  0.03718567  0.08827972 -0.13046646 -0.08820343  0.00601387\n19 -0.03070831 -0.10734367 -0.05195618 -0.08093643 -0.04382896  0.07156181\n20 -0.05515671 -0.17663956  0.10515785 -0.05109405  0.30648041  0.20906258\n21 -0.10726547 -0.19726753  0.10523415 -0.25164032  0.42882919  0.30406380\n          V303        V304      V305       V306       V307      V308      V309\n16  0.29359055  0.29015541 0.7721100 0.75460815 0.70828056 0.8419533 0.4429684\n17  0.05327988  0.02116966 0.5815601 0.66781044 0.70374870 0.7959385 0.7502346\n18 -0.33140373 -0.27640724 0.3110924 0.45788956 0.54585838 0.7011719 1.0791397\n19 -0.54827881 -0.31187248 0.1621876 0.23226547 0.25843811 0.5168724 0.8530445\n20 -0.41749954 -0.19038963 0.2785950 0.20304489 0.10039139 0.5563278 0.6723423\n21 -0.29351616 -0.10367203 0.2454662 0.03703117 0.09703064 0.3904686 0.3277340\n         V310      V311      V312     V313      V314        V315       V316\n16 -0.1281261 0.1448460 0.5113296 1.208361 0.3628101 -0.19390678 0.65640640\n17  0.2329693 0.4465618 0.7159386 1.297499 0.4120312 -0.48546982 0.19867325\n18  0.6484375 1.0332794 1.1445332 1.378048 0.7472629 -0.40265274 0.09484482\n19  0.4576569 1.0342960 1.1665630 1.243673 0.8503094 -0.17453003 0.38437843\n20  0.3858585 1.1189060 1.2851543 1.128979 1.1790619  0.08875275 0.61460876\n21  0.2927342 1.0514832 1.3158588 1.017578 1.2753105 -0.02476311 0.25757790\n         V317      V318      V319      V320       V321       V322        V323\n16 0.59710884 1.1371059 0.6007824 0.7444515 -0.1945305 0.01437187  0.13234520\n17 0.35156059 1.0865612 0.7849960 0.7121887  0.1389847 0.21921730  0.05531120\n18 0.06984329 0.8091392 0.8708592 0.7199230  0.5978908 0.38843536 -0.19922256\n19 0.09718895 0.4385166 0.5584393 0.5481224  0.6417923 0.31515694 -0.35320473\n20 0.28609276 0.3030453 0.3666420 0.4713268  0.6698418 0.49085999 -0.07734299\n21 0.11046600 0.1432781 0.2645302 0.3579712  0.5952339 0.47398567 -0.16976547\n          V324       V325       V326       V327       V328\n16  0.43882942 -0.3291397  0.7328091  0.8723450  0.4501572\n17  0.23093796 -0.7037506  0.2107830  0.5207806  0.2324200\n18 -0.06046677 -1.2319546 -0.6264858 -0.1476555 -0.1301556\n19 -0.04093361 -1.1313286 -0.6496925 -0.2182808 -0.0706253\n20  0.04640579 -0.7122688 -0.5821876 -0.3124962 -0.1203918\n21 -0.22789192 -0.8699207 -0.7609367 -0.5010166 -0.3286724\n\n\nCode\nt(SSTdata) %>% dim()\n\n\n[1]  328 2261\n\n\n\n\nCode\n# construct the EOFs \nZ <- t(SSTdata) # data matrix, rows are times, columns are spatial sampling locations.\nspat_mean <- apply(SSTdata, 1, mean) # average across all time.\nnT <- ncol(SSTdata) # no of time points\n# outer command makes the spatial mean into a matrix\nZspat_detrend <- Z - outer(rep(1, nT), spat_mean) # detrend data (center to spatial mean)\nZt <- 1/sqrt(nT-1)*Zspat_detrend # normalize\nE <- svd(Zt) # SVD\n\n\n\n\nCode\n# number of EOFs to estimate in the data\nn <- 10\n\n# Projected onto the \nTS <- Zspat_detrend %*% E$v[, 1:n]\nsummary(colMeans(TS))\n\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-2.091e-16 -8.031e-17  3.201e-16  3.113e-16  6.916e-16  8.885e-16 \n\n\nThis verifies that the time series have a zero mean.\n\n\nCode\ntau <- 6\nnT <- nrow(TS)\nTStplustau <- TS[-(1:tau), ] # TS with first tau time pts removed \nTSt <- TS[-((nT-5):nT), ] # TS with last tau time pts removed\nCov0 <- crossprod(TS)/nT # cross product\nCovtau <- crossprod(TStplustau, TSt) / (nT - tau)\nC0inv <- solve(Cov0)\nMest <- Covtau %*% C0inv # estimated M, the dynamics\nCeta <- Cov0 - Covtau %*% C0inv %*% t(Covtau) # Covariance matrix\n\n\n\n\nCode\nimage(Mest)\n\n\n\n\n\nCode\nimage(Ceta)\n\n\n\n\n\nThe forecast is given by \\hat\\mu + \\PhiM^2\\alpha_t.\n\n\nCode\nSSTlonlat$pred <- NA\nalpha_forecast <- Mest %*% TS[328, ] # propigate alpha first,\nidx <- which(SSTlonlat$mask == 0)\nSSTlonlat$curr[idx] <- as.numeric(E$v[, 1:n] %*% TS[328, ] + spat_mean)\nSSTlonlat$pred[idx] <- as.numeric(E$v[, 1:n] %*% alpha_forecast + spat_mean) # add to spatial mean for original scale.\n\n# for plotting\nSSTlonlat$obs1[idx]  <- SSTdata[, 328]\nSSTlonlat$obs2[idx]  <- SST_Oct97\n\n\nC <- Mest %*% Cov0 %*% t(Mest) + Ceta # 6 month ahead variances\n\nSSTlonlat$predse[idx] <- sqrt(diag(E$v[, 1:n] %*% C %*% t(E$v[, 1:n])))\n\nSSTlonlat %>% ggplot(aes(lon, lat, fill = pred)) +\n  geom_tile()\n\n\n\n\n\nSee the appendix for proper visualization.\nDSTM_EM is provided with the package STRbook, and runs an EM algorithm that does ML estimation in state-space models.\n\n\nCode\nDSTM_Results <- DSTM_EM(Z = SSTdata,\n                        Cov0 = Cov0,\n                        muinit = matrix(0, n, 1),\n                        M = Mest,\n                        Ceta = Ceta,\n                        sigma2_eps = 0.1,\n                        H = H <- E$v[, 1:n],\n                        itermax = 10,\n                        tol = 1)\n\n\nThe object DSTM_Results has the setimated parameters, and complete-data negative log-likelihood. Estimates of \\alpha_{1,t}, \\alpha_{2,t} and \\alpha_{3,t}\n\n\nCode\npar(mfrow = c(1,3))\nfor(i in 1:3) {\n  plot(DSTM_Results$alpha_smooth[i, ], type = \"l\",\n       xlab = \"t\", ylab = bquote(alpha[.(i)]))\n  lines(TS[, i], lty = \"dashed\", col = 'red')\n}\n\n\n\n\n\n\n\nCode\nimage(as(DSTM_Results$Mest, \"Matrix\"))\n\n\n\n\n\nCode\nimage(as(DSTM_Results$Mest %^% 6, \"Matrix\"))\n\n\n\n\n\nCode\nimage(as(Mest, \"Matrix\"))\n\n\n\n\n\n\n\nCode\nalpha <- DSTM_Results$alpha_smooth[, nT]\nP <- DSTM_Results$Cov0\nfor(t in 1:6) {\n  alpha <- DSTM_Results$Mest %*% alpha\n  P <- DSTM_Results$Mest %*% P %*% t(DSTM_Results$Mest) + DSTM_Results$Ceta\n}"
  },
  {
    "objectID": "spatial/spatial.html#tutorial-from-website",
    "href": "spatial/spatial.html#tutorial-from-website",
    "title": "32  Spatial Statistics",
    "section": "37.1 Tutorial from website",
    "text": "37.1 Tutorial from website\nWe follow along here with the explanation: https://mgimond.github.io/simple_moransI_example/\n\n\nCode\nlibrary(sf)\nlibrary(spdep)\nlibrary(tmap) # ggplot for large maps\n\n\nMoran’s I statistic is the regression of spatial lag values (weighted mean of neighbors), against the value itself.\n\n\nCode\ns <- readRDS(url(\"https://github.com/mgimond/Data/raw/gh-pages/Exercises/nhme.rds\"))\ns\n\n\nSimple feature collection with 26 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 209698.1 ymin: 4729883 xmax: 660529.1 ymax: 5255569\n\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\nProjected CRS: NAD83 / UTM zone 19N\nFirst 10 features:\n\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n           NAME    STATE_NAME POP10_SQMI AVE_HH_SZ AVE_FAM_SZ Income House_year\n1  Androscoggin         Maine      216.6      2.37       2.88  45765       1966\n2    Cumberland         Maine      305.1      2.32       2.90  59560       1972\n3      Kennebec         Maine      128.4      2.32       2.82  46559       1972\n4          Knox         Maine      105.2      2.22       2.75  50515       1971\n5       Lincoln         Maine       72.5      2.24       2.72  50027       1975\n6        Oxford         Maine       26.6      2.35       2.81  40695       1974\n7     Sagadahoc         Maine      131.0      2.32       2.81  55046       1976\n8         Waldo         Maine       51.4      2.33       2.81  43484       1980\n9          York         Maine      194.1      2.40       2.89  56701       1977\n10      Belknap New Hampshire      128.1      2.39       2.84  60782       1977\n                         geometry\n1  MULTIPOLYGON (((420086.1 48...\n2  MULTIPOLYGON (((356518.5 48...\n3  MULTIPOLYGON (((410590 4926...\n4  MULTIPOLYGON (((472230.8 48...\n5  MULTIPOLYGON (((435888.6 48...\n6  MULTIPOLYGON (((339726.8 49...\n7  MULTIPOLYGON (((428755.2 48...\n8  MULTIPOLYGON (((466941.8 49...\n9  MULTIPOLYGON (((341282.3 48...\n10 MULTIPOLYGON (((324460.1 48...\n\n\nCode\ntm_shape(s) + tm_fill(col=\"Income\", style=\"quantile\", n = 8, palette = \"Greens\") + \n  tm_legend(outside=TRUE)\n\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\nCode\nnb <- poly2nb(s, queen = TRUE) # neighbors from poly\n\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\nCode\nlw <- nb2listw(nb, style=\"W\", zero.policy = TRUE) # row standardizing\ninc_lag <- lag.listw(x = lw, var = s$Income) # means from all neighbors\n\nplot(inc_lag ~ s$Income, pch = 16, asp = 1)\nM1 <- lm(inc_lag ~ s$Income)\nabline(M1, col = \"blue\")\n\n\n\n\n\nCode\ncoef(M1)[2]\n\n\n s$Income \n0.6827955 \n\n\nCode\nI <- moran(s$Income, lw, length(nb), Szero(lw))[[1]]\n\nspweights.constants(lw) # constants relating to n, given a listw object\n\n\n$n\n[1] 26\n\n$n1\n[1] 25\n\n$n2\n[1] 24\n\n$n3\n[1] 23\n\n$nn\n[1] 676\n\n$S0\n[1] 26\n\n$S1\n[1] 12.66111\n\n$S2\n[1] 107.1594\n\n\nCode\nnb2mat(nb) # W matrix basically\n\n\n        [,1]      [,2]      [,3]      [,4]      [,5]      [,6]      [,7]\n1  0.0000000 0.2000000 0.2000000 0.0000000 0.0000000 0.2000000 0.2000000\n2  0.2500000 0.0000000 0.0000000 0.0000000 0.0000000 0.2500000 0.2500000\n3  0.1666667 0.0000000 0.0000000 0.0000000 0.1666667 0.0000000 0.1666667\n4  0.0000000 0.0000000 0.0000000 0.0000000 0.5000000 0.0000000 0.0000000\n5  0.0000000 0.0000000 0.2500000 0.2500000 0.0000000 0.0000000 0.2500000\n6  0.1666667 0.1666667 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n7  0.2500000 0.2500000 0.2500000 0.0000000 0.2500000 0.0000000 0.0000000\n8  0.0000000 0.0000000 0.1666667 0.1666667 0.1666667 0.0000000 0.0000000\n9  0.0000000 0.2500000 0.0000000 0.0000000 0.0000000 0.2500000 0.0000000\n10 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n11 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.1666667 0.0000000\n12 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n13 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n14 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n15 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n16 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n17 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n18 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n19 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n20 0.2500000 0.0000000 0.2500000 0.0000000 0.0000000 0.2500000 0.0000000\n21 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n22 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n23 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n24 0.0000000 0.0000000 0.1666667 0.0000000 0.0000000 0.0000000 0.0000000\n25 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n26 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.3333333 0.0000000\n        [,8]      [,9]     [,10]     [,11] [,12]     [,13]     [,14]     [,15]\n1  0.0000000 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.0000000 0.0000000\n2  0.0000000 0.2500000 0.0000000 0.0000000  0.00 0.0000000 0.0000000 0.0000000\n3  0.1666667 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.0000000 0.0000000\n4  0.5000000 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.0000000 0.0000000\n5  0.2500000 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.0000000 0.0000000\n6  0.0000000 0.1666667 0.0000000 0.1666667  0.00 0.0000000 0.0000000 0.0000000\n7  0.0000000 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.0000000 0.0000000\n8  0.0000000 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.0000000 0.0000000\n9  0.0000000 0.0000000 0.0000000 0.2500000  0.00 0.0000000 0.0000000 0.0000000\n10 0.0000000 0.0000000 0.0000000 0.2500000  0.00 0.2500000 0.0000000 0.2500000\n11 0.0000000 0.1666667 0.1666667 0.0000000  0.00 0.1666667 0.0000000 0.0000000\n12 0.0000000 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.5000000 0.0000000\n13 0.0000000 0.0000000 0.2000000 0.2000000  0.00 0.0000000 0.0000000 0.2000000\n14 0.0000000 0.0000000 0.0000000 0.0000000  0.25 0.0000000 0.0000000 0.2500000\n15 0.0000000 0.0000000 0.1666667 0.0000000  0.00 0.1666667 0.1666667 0.0000000\n16 0.0000000 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.3333333 0.3333333\n17 0.0000000 0.2000000 0.2000000 0.2000000  0.00 0.0000000 0.0000000 0.2000000\n18 0.0000000 0.0000000 0.0000000 0.0000000  0.25 0.2500000 0.2500000 0.2500000\n19 0.0000000 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.0000000 0.0000000\n20 0.0000000 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.0000000 0.0000000\n21 0.3333333 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.0000000 0.0000000\n22 0.1666667 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.0000000 0.0000000\n23 0.0000000 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.0000000 0.0000000\n24 0.1666667 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.0000000 0.0000000\n25 0.0000000 0.0000000 0.0000000 0.0000000  0.00 0.0000000 0.0000000 0.0000000\n26 0.0000000 0.0000000 0.0000000 0.3333333  0.00 0.3333333 0.0000000 0.0000000\n       [,16]     [,17]     [,18]     [,19]     [,20]     [,21]     [,22]\n1  0.0000000 0.0000000 0.0000000 0.0000000 0.2000000 0.0000000 0.0000000\n2  0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n3  0.0000000 0.0000000 0.0000000 0.0000000 0.1666667 0.0000000 0.0000000\n4  0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n5  0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n6  0.0000000 0.0000000 0.0000000 0.0000000 0.1666667 0.0000000 0.0000000\n7  0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n8  0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.1666667 0.1666667\n9  0.0000000 0.2500000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n10 0.0000000 0.2500000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n11 0.0000000 0.1666667 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n12 0.0000000 0.0000000 0.5000000 0.0000000 0.0000000 0.0000000 0.0000000\n13 0.0000000 0.0000000 0.2000000 0.0000000 0.0000000 0.0000000 0.0000000\n14 0.2500000 0.0000000 0.2500000 0.0000000 0.0000000 0.0000000 0.0000000\n15 0.1666667 0.1666667 0.1666667 0.0000000 0.0000000 0.0000000 0.0000000\n16 0.0000000 0.3333333 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n17 0.2000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n18 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n19 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.2500000\n20 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n21 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.3333333\n22 0.0000000 0.0000000 0.0000000 0.1666667 0.0000000 0.1666667 0.0000000\n23 0.0000000 0.0000000 0.0000000 0.3333333 0.0000000 0.0000000 0.3333333\n24 0.0000000 0.0000000 0.0000000 0.1666667 0.1666667 0.0000000 0.1666667\n25 0.0000000 0.0000000 0.0000000 0.3333333 0.0000000 0.3333333 0.3333333\n26 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n       [,23]     [,24]     [,25]     [,26]\n1  0.0000000 0.0000000 0.0000000 0.0000000\n2  0.0000000 0.0000000 0.0000000 0.0000000\n3  0.0000000 0.1666667 0.0000000 0.0000000\n4  0.0000000 0.0000000 0.0000000 0.0000000\n5  0.0000000 0.0000000 0.0000000 0.0000000\n6  0.0000000 0.0000000 0.0000000 0.1666667\n7  0.0000000 0.0000000 0.0000000 0.0000000\n8  0.0000000 0.1666667 0.0000000 0.0000000\n9  0.0000000 0.0000000 0.0000000 0.0000000\n10 0.0000000 0.0000000 0.0000000 0.0000000\n11 0.0000000 0.0000000 0.0000000 0.1666667\n12 0.0000000 0.0000000 0.0000000 0.0000000\n13 0.0000000 0.0000000 0.0000000 0.2000000\n14 0.0000000 0.0000000 0.0000000 0.0000000\n15 0.0000000 0.0000000 0.0000000 0.0000000\n16 0.0000000 0.0000000 0.0000000 0.0000000\n17 0.0000000 0.0000000 0.0000000 0.0000000\n18 0.0000000 0.0000000 0.0000000 0.0000000\n19 0.2500000 0.2500000 0.2500000 0.0000000\n20 0.0000000 0.2500000 0.0000000 0.0000000\n21 0.0000000 0.0000000 0.3333333 0.0000000\n22 0.1666667 0.1666667 0.1666667 0.0000000\n23 0.0000000 0.3333333 0.0000000 0.0000000\n24 0.1666667 0.0000000 0.0000000 0.0000000\n25 0.0000000 0.0000000 0.0000000 0.0000000\n26 0.0000000 0.0000000 0.0000000 0.0000000\nattr(,\"call\")\nnb2mat(neighbours = nb)\n\n\n\n\\begin{aligned}\nI = \\frac{Z'WZ}{Z'Z}\n\\end{aligned}\n\nshould be large for spatially dependent Z.\nTo understand this as a regression let Y = WZ, Y gives the weighted sums of spatial neighbors (the spatial lags). Now I = \\hat\\beta = (Z'Z)^{-1}Z'Y when we regress the spatial lags against our observed stochastic realization, so moran’s I statistic is just the slope of that regression.\nThe properties of this are a little involved yet, see formula’s for the variance here Spatial Autocorrelation, Michael Goodchild"
  },
  {
    "objectID": "spatial/spatial.html#manually",
    "href": "spatial/spatial.html#manually",
    "title": "32  Spatial Statistics",
    "section": "37.2 Manually",
    "text": "37.2 Manually\n\n\nCode\nlibrary(igraph)\nn <- 20\ng <- make_lattice(c(20, 20))\nA <- as_adj(g) # Adj matr\nj <- cbind(rep(1, n^2))\nAj <- (A %*% j) # out degree, \"rowSums\"\nW <- Diagonal(x = 1/ Aj[,1]) %*% A # weighted W matrix, manually\nlw_W <- mat2listw(W) # convert to listw object\n\n\n\n\nCode\n# create various spatial dependencies...\nset.seed(1)\n\n# z <- rbinom(n^2, 1, .2)\n# z <- c(rbinom(n^2 / 2, 1, .8), rbinom(n^2 / 2, 1, .4)) # half one prob, another half different prob\n# z <- c(rep(1, n^2 / 2), rep(0, n^2 /2 )) # half yes half no\nz <- rbinom(n^2, 1, seq(0, 1, length.out = n^2)) # getting larger by number\nz_c <- scale(z, center = T, scale = F)  # need to center to match results\nZ <- cbind(z)\nZ_c <- cbind(z_c) # center to match results\n\n\n\n\nCode\n# by I statistic\nI_man <- (t(Z_c) %*% W %*% Z_c) / crossprod(Z_c)\n# I_man\n\n# by regression\nlag_W <- lag.listw(lw_W, z_c)\n\nI_lm <- lm(lag_W ~ Z_c)\nsummary(I_lm)\n\n\n\nCall:\nlm(formula = lag_W ~ Z_c)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.70366 -0.31453  0.04634  0.29634  0.68547 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.001458   0.015686   0.093    0.926    \nZ_c         0.389129   0.031382  12.400   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3137 on 398 degrees of freedom\nMultiple R-squared:  0.2787,    Adjusted R-squared:  0.2769 \nF-statistic: 153.8 on 1 and 398 DF,  p-value: < 2.2e-16\n\n\nCode\n# coef(I_lm)[2]\n\nI_mor <- moran(z, lw_W, length(lw_W$neighbours), S0 = Szero(lw_W)) # centered version of it\n# I_mor$I\nI_mor$K\n\n\n[1] 1.002502\n\n\nCode\nsum(z_c^4) / sum(z_c^2)\n\n\n[1] 0.2504687\n\n\n\n\nCode\n# Testing\nmoran.test(z_c, lw_W, randomisation = FALSE)\n\n\n\n    Moran I test under normality\n\ndata:  z_c  \nweights: lw_W    \n\nMoran I statistic standard deviate = 10.778, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.389128622      -0.002506266       0.001320440 \n\n\nCode\n(.389128622 + 0.002506266) / sqrt(0.001320440)\n\n\n[1] 10.7776\n\n\n\n\nCode\n# vec -> matrix\nmicrobenchmark(times = 100L,\n               \"matrix\" = matrix(z, ncol = 20, nrow = 20), # way way way faster\n               \"Matrix\" = Matrix(z, ncol = 20, nrow = 20))\n\n\nUnit: microseconds\n   expr     min       lq     mean   median      uq     max neval cld\n matrix   1.374   1.7905   2.7016   2.5175   3.204  13.690   100  a \n Matrix 136.993 145.0210 158.1759 151.1345 162.420 365.092   100   b\n\n\nCode\n# vec -> colvec\nmicrobenchmark(times = 100L,\n               \"cbind\" = cbind(z), # similar to matrix\n               \"t\" = t(t(z)),\n               \"Matrix\" = Matrix(z, ncol = 1),\n               \"matrix\" = matrix(z, ncol = 1)) # similar to matrix\n\n\nUnit: microseconds\n   expr    min      lq      mean  median      uq     max neval cld\n  cbind  1.271  1.6330   2.37272  2.0890  2.7195  14.904   100 a  \n      t  9.927 10.8595  18.01326 12.8850 13.9445 474.566   100  b \n Matrix 85.349 89.8715 100.48169 91.5965 97.5855 254.838   100   c\n matrix  1.837  2.1590   2.87080  2.6650  3.3345   8.222   100 a  \n\n\nCode\n# dividing by\nlibrary(microbenchmark)\nmicrobenchmark(times = 100L,\n               \"1\" = A / Aj[row(A)], # slowwww\n               \"2\" = Diagonal(x = 1/ Aj[,1]) %*% A) # multiply by diag matrix\n\n\nUnit: microseconds\n expr      min       lq      mean    median        uq      max neval cld\n    1 3566.482 3680.574 3856.2672 3769.8430 3899.7695 7124.158   100   b\n    2  244.293  252.105  310.9439  290.8145  363.7815  506.892   100  a"
  },
  {
    "objectID": "stochastic/stochastic.html#gamblers-ruin",
    "href": "stochastic/stochastic.html#gamblers-ruin",
    "title": "31  Stochastic",
    "section": "31.1 Gamblers Ruin",
    "text": "31.1 Gamblers Ruin\nThe random walk we’re simulating is a symmetric random walk from some initial location, and calculating the expected hitting time of crossing some upper or lower boundary\n\n\n\n\n\nCode\nset.seed(20)\nk <- 10\nrw <- rw_sim(a, b, p, k, return_path = TRUE)\n\nqplot(0:(length(rw) - 1), rw, geom = \"line\", xlab = \"time\", ylab = \"state\", linetype = \"random walk path\") + \n  geom_hline(yintercept = b, color = \"tomato1\") + \n  geom_hline(yintercept = a, color = \"tomato1\") +\n  annotate(\"point\", x = 0, y = 10, size = 2, color = \"lightblue\") +\n  annotate(\"label\", label = \"k - initial state\", x = 3, y = 9, fill = \"lightblue\", size = 2) +\n  annotate(\"point\", x = 44, y = 0, size = 2, color = \"lightblue\") +\n  annotate(\"label\", label = \"T - hitting time\", x = 41, y = 1, fill = \"lightblue\", size = 2) +\n  scale_linetype_manual(values = c(3)) + \n  labs(linetype = \"\",\n       title = \"Hitting Times of Random Walk\") + \n  lims(y = c(a, b)) + \n  theme_minimal() + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\nBy theory, for the symmetric random walk, (see Durrett)\n\n\\begin{aligned}\nE[T|X_0 = k] = (k - a)(b - k)\n\\end{aligned}\n\n\n\nCode\nhitting_times <- replicate(10000,\n                           rw_sim(a,b,p, k))\nmean(hitting_times)\n\n\n[1] 101.0178"
  },
  {
    "objectID": "stochastic/stochastic.html#stochastic-processes",
    "href": "stochastic/stochastic.html#stochastic-processes",
    "title": "31  Stochastic",
    "section": "31.2 Stochastic Processes",
    "text": "31.2 Stochastic Processes\nA great overview of Differential Equations in R is covered by\n\n“sde” - stochastic differential equations\n“Sim.DiffProc”- simulate diffusion processes\n“ReacTran” - functions for generating finite differences on a grid\n\n\n31.2.1 Wiener Process\nThe most basic wiener process takes the form, which describes Brownian motion. This differential equation models 1 dimensional diffusion, and to see this, we can imagine the probability distribution over time. Each of the sample paths are a random walk with gaussian increment with proportional\n\ndx = dW\n\n\nx is the position, which is a function of time\ndW is the Wiener Noise, gaussian distribution with\n\n\n\nCode\nx0 <- 0 # initial position\nt0 <- 0 # initial time\ndt <- .01\nnt <- 100 # how many time steps to take, \ndx <- rnorm(nt, 0, dt) # sample steps\nx <- cumsum(c(x0, dx)) # sample path\n\nrwiener <- function(x0 = 0, t0 = 0, dt = .01, nt = 100) {\n  dx <- rnorm(nt, 0, sqrt(dt)) # sample steps\n  cumsum(c(x0, dx)) # sample path\n}\n\nset.seed(1)\nwiener_paths <- replicate(2000, rwiener())\nwiener_ts <- ts(wiener_paths, start = t0, deltat = dt)\nts.plot(wiener_ts, col = rgb(0,0,0,alpha = .03), ylim = c(-4, 4), main = \"Wiener Sample Paths\")\n\n\n\n\n\nInstead of the sample path approach, if we instead think about the probability density of our position as a function of time, we can see that the probability function becomes more diffuse over time. Thus, it’s not surprising that we can describe the diffusion with a PDE through the Fokker-Plank Equation.\n\n\nCode\n# code to create animations\nx <- seq(-3, 3, by = .01)\n# animation of density\nsaveGIF({\n    for (i in 2:100)  {\n      hist(wiener_ts[i,], xlim = c(-3, 3), ylim = c(0, 10),\n           freq = F,\n           breaks = 40,\n           main = paste0(\"Density of Sample Paths, Time = \", i*dt),\n           xlab = \"x\")\n      lines(x, dnorm(x, sd = sqrt(dt * i)), col = \"red\")\n      legend(\"topright\", legend = c(\"theoretical density\"), col = 2, lty = 1, bty = \"n\")\n    }\n}, movie.name = \"test.gif\", loop = T, interval = .01)\n\n# along the sample paths\nsaveGIF({\n    for (i in 2:100)  {\n      ts.plot(wiener_ts, col = rgb(0,0,0,alpha = .03), ylim = c(-4, 4), main = \"Wiener Sample Paths\")\n      abline(v = i * dt, col = \"red\")\n    }\n}, movie.name = \"sample.gif\", loop = T, interval = .01)\n\n\n\n\n\n\n\n\n\n\n\n\nThe associated Fokker Plank equation associated with this stochastic differential equation, is simply the heat equation.\n\n\\frac{\\partial}{\\partial t} P(x, t) =\\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}P(x,t)\n\n\nInitial condition: P(x, 0) = \\delta(x)\n\\delta(x) is the delta-dirac function, which has infinite mass at 0.\nBoundary Condition: P(a, t) = P(b, t) = 0\n\nsince our simulation has to occur on some bounded region [a,b], we just set the simulation to have absorbing boundaries.\n\n\nSolving the PDE with initial value conditions analytically, we find that the solution to this eigenvalue problem is\n\n\\begin{aligned}\nP(x, t) = \\frac{1}{\\sqrt{2\\pi t}}\\exp \\left(\\frac{x^2}{2t}\\right)\n\\end{aligned}\n\nWe can also calculate the solution by finite differencing. We can solve the PDE and show that our solutions match the rate given by the SDE formulation of the PDE. We use code from the vignette of the ReacTran R package. ReacTran package uses the method of lines for solving PDE’s, in which we set up a discretized grid, and solve the ODE as a vector\n\n\nCode\n# ReacTran uses the\nN <- 601\nxgrid <- setup.grid.1D(x.up = -3, x.down = 3, N = N) # grid of values\nx <- xgrid$x.mid\nD.coeff <- .5 # diffusion coefficient from solving FP\n\n# defines the diffusion (the derivative with respect to time)\n# Since our function has no time dependence, we only need to calculate the derivatives for the next step\nDiffusion <- function (t, Y, parms){\n  tran <- tran.1D(C = Y, C.up = 0, C.down = 0, # dirchlet boundary conditions, set to 0\n                D = D.coeff, dx = xgrid)\n  list(dY = tran$dC)\n}\n\n# Set initial condition of the differential equation, we approximate the \nYini <- c(rep(0, 300), 100, rep(0, 300)) # very tall initial mass\ntt <- seq(t0, dt * nt, dt) #times to simulate\n\n# solve heat equation\nout <- ode.1D(y = Yini, times = tt, func = Diffusion, parms = NULL, dimens = N)\n\n# library(tidyverse)\ncolorBreaks = 10^(seq(-2, 2, length.out = 255)) # different\nplot(raster(t(out[,-1]), xmn =0, xmx = 1, ymn = -3, ymx = 3),\n     asp = NA,\n     breaks = colorBreaks,\n     col = rev(hcl.colors(length(colorBreaks) - 1L, palette = \"Rocket\")), legend = FALSE,\n     xlab = \"time\", ylab = \"x\", interpolate = TRUE)\n\n\n\n\n\n\n\n31.2.2 Ornstein-Uhlenbeck Process\nAdding a drift term to the stochastic equation, gives the stochastic equation a mean. The negative in front of the drift implies that it will always regress to 0, because it’s a function of x (kind of like a spring constant). The \\theta parameter controls how strong the restoring force is.\n\ndx = -\\theta x\\, dt + \\sigma \\, dW\n\n\nx is position, which is a function of time.\ndW is the wiener process.\n\\theta is the rate of return to the mean (0)\nVariance of\n\nThe following shows the effect of the parameters \\theta = 3 and \\sigma = .5 with initial state x_0 = 5. We also show the process with a different initial state x_0 = -3 to show the restoring effect to the mean.\n\n\nCode\n# OU function\nou_paths <- function(npaths = 1, theta = 3, sigma = .5, x0 = 5, nt = 100, t0 = 0, dt = .01) {\n  sde_path_ou <- function() {\n    x <- vector(mode = \"numeric\", length = nt + 1)\n    x[1] <- x0\n    wiener_noise <- rnorm(nt, sd = sqrt(dt))\n    for (i in 1:nt) {\n      dx <- -theta * x[i] * dt + sigma * wiener_noise[i]\n      x[i+1] <- x[i] + dx\n    }\n    return(x)\n  }\n  ts(replicate(npaths, sde_path_ou()), start = t0, deltat = dt)\n}\n\ntheta <- 3\nsig <- .5\ny0 <- 5\nnt <- 100\ndt <- .01\nt0 <- 0\n\nset.seed(1)\nou_ts <- ou_paths(npaths = 500, theta = theta, sigma = sig, x0 = y0, nt = nt, t0 = t0, dt = dt)\n\nts.plot(ou_ts, col = rgb(0,0,0,alpha = .05)) # plot\n\ntt <- seq(0, 1, .01)\ntheory_mean <- y0*exp(-theta*tt)\ntheory_var <- sig^2 / 2 / theta * (1 - exp(-2 * theta * tt))\n\nlines(tt, theory_mean, col = \"red\")\nlines(tt, theory_mean + 2 * sqrt(theory_var), lty = 2, col = \"red\")\nlines(tt, theory_mean - 2 * sqrt(theory_var), lty = 2, col = \"red\")\nlegend(\"topright\", legend = c(\"mean\", \"\\u00B1 2sd\"), col = 2, lty = 1:2)\n\n\n\n\n\nBecause of this restoring property to the mean, a closely related stochastic process is known as the Vasicek model (commonly used to model interest rates), which adds another parameter to control what the mean is.\n\ndx = \\theta(\\mu - x)\\, dt + \\sigma \\, dW\n\n\nreduces to the 0 mean OU process when \\mu = 0.\nThe long term variance of this model is also\n\nThis process is important because it’s the continuous time analogue of a discrete time AR1 process.\nSimilarly, the associated Fokker-Plank Equation for this SDE is:\n\n\\begin{aligned}\n\\frac{\\partial P(x, t)}{\\partial t} = -\\mu \\frac{\\partial P(x, t)}{\\partial x} + \\frac{\\sigma^2}{2}\\frac{\\partial^2P(x, t)}{\\partial x^2}\n\\end{aligned}\n\n\n\nCode\n# simulation parameters\nt0 <- 0                       # time start\ndt <- .01                     # time step \ntn <- 1                       # time end\nnt <- tn/dt                   # number of time steps\ntgrid <- seq(t0, nt * dt, dt) # time grid\n\nx0 <- 6                       # space start\nxn <- -2                    # space end\nnx <- 800                     # number of grid points\nxgrid <- setup.grid.1D(x.up = x0, x.down = xn, N = nx) # space grid\nx <- xgrid$x.int\n\nsig <- .5            # Parameters from OU simulation\nmu <- -3             # Parameters from OU simulation  \nD_coef <- sig^2 / 2  # Diffusion function\n\ny0 <- c(rep(0, 100), 100, rep(0, 699))                 # initial condition\n\n# advection-diffusion (method of lines)\nadvec <- function(t, Y, parms) {\n  trans <- tran.1D(C = Y, D = D_coef, v = mu*x, C.up = 0, C.down = 0, dx = xgrid)\n  return(list(dY = trans$dC))\n}\n\n# solve advec/diffusion equation\nout <- ode.1D(y = y0, times = tgrid, func = advec, parms = NULL, dimens = nx)\n\n# plot solution\ncolorBreaks = 10^(seq(-3, 3, length.out = 255)) # different to capture more drift in lower parameters\nplot(raster(t(out[,-1]), xmn = 0, xmx = 1, ymn = xn, ymx = x0),\n     asp = NA,\n     breaks = colorBreaks,\n     col = rev(hcl.colors(length(colorBreaks) - 1L, palette = \"Rocket\")), legend = FALSE,\n     xlab = \"time\", ylab = \"x\", interpolate = TRUE)\n\n\n\n\n\nCode\n# rcOU(n=1, Dt=0.1, x0=1, theta=c(0,2,1))\n\n\n\n\n\n\n\n\nODE for Mean/Variance of OU process\n\n\n\nWe can get a First Order ODE characterization of the mean and variance for the Ornstein-Uhlenbeck process.\n\\begin{aligned}\n\\frac{d\\langle x \\rangle}{dt} = -\\theta \\langle x \\rangle \\\\\n\\langle x\\rangle = \\langle x_0 \\rangle e^{-\\theta t}\n\\end{aligned}\n\n\\frac{dV}{dt} = -2 \\theta V + \\sigma^2\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\ndx = -\\theta x\\, dt + \\sigma\\, dW\n First we note the expression for the derivative of variance, and calculate the necessary components.\n\n\\begin{align*}\n\\frac{dV}{dt} &= \\frac{d\\langle x^2\\rangle}{dt} - \\frac{d \\langle x\\rangle^2}{dt} \\\\\n&= \\frac{d\\langle x^2\\rangle}{dt} - 2 \\langle x\\rangle \\frac{d \\langle x\\rangle}{dt}\n\\end{align*}\n\nHence, we need to evaluate the mean differentials to the second order, from\n\n\\begin{align}\nd\\langle x\\rangle &= -\\theta \\langle x\\rangle \\, dt\\\\\ndx^2 &= (x + dx)^2 - x^2 \\\\\n&=2x\\,dx + (dx)^2 \\\\\n&= (-2\\theta x^2 + \\sigma^2)\\,dt + \\sigma \\, dW \\\\\nd\\langle x^2\\rangle &= (-2\\theta \\langle x^2\\rangle + \\sigma^2)\\, dt\n\\end{align}\n\nWe’ve used that the rules of Ito’s calculus, that dt\\,dW = 0, (dW)^2 = dt, (dt)^2 =0. Plugging in the values and simplifying gives us the desired result.\n\n\n\n\n\n\n\nCode\ntheta <- 3\nsig <- .5\n\n# simplest first order ode\nou_mean <- function(t, y, parms) {\n  dy <- -theta * y\n  return(list(dy))\n}\n\n\nx0 <- 5 # initial mean\ntt <- seq(0, 1, by = .01)\nout_mean <- ode(x0, tt, ou_mean, parms = NULL)\n\ncbind(out_mean[,2],\n      5 * exp(-3* tt)) |> head() # matches\n\n\n         [,1]     [,2]\n[1,] 5.000000 5.000000\n[2,] 4.852224 4.852228\n[3,] 4.708817 4.708823\n[4,] 4.569653 4.569656\n[5,] 4.434600 4.434602\n[6,] 4.303538 4.303540\n\n\nCode\n# function coding differential equation \nou_var <- function(t, v, parms) {\n  dv <- -2 * theta * v + sig^2\n  return(list(dv))\n}\n\ny0 <- 0 # initial variance\ntt <- seq(0, 1, by = .01)\n\nout <- ode(y0, tt, ou_var, parms = NULL)\n\ncbind(out[,2],\n      sig^2 / 2 / theta * (1 - exp(-2 * theta * tt))) |> head() # matches!\n\n\n            [,1]        [,2]\n[1,] 0.000000000 0.000000000\n[2,] 0.002426880 0.002426478\n[3,] 0.004712408 0.004711648\n[4,] 0.006864444 0.006863741\n[5,] 0.008891073 0.008890506\n[6,] 0.010799760 0.010799241\n\n\n\n\n31.2.3 General Linear SDE\n\ndx = -\\gamma x \\, dt + g x\\, dW"
  },
  {
    "objectID": "stochastic/stochastic.html#simulating-stochastic-differential-equations",
    "href": "stochastic/stochastic.html#simulating-stochastic-differential-equations",
    "title": "31  Stochastic",
    "section": "31.3 Simulating Stochastic Differential Equations",
    "text": "31.3 Simulating Stochastic Differential Equations\nFollowing the package vignette\n\nrsde1d - random deviates from a sde object\ndsde1d() - approximate density from sde object\nsnssde2d() - simulates 2d sde\n\nWe simulate from the example\n\n\\begin{aligned}\ndX = \\frac{1}{2}\\theta^2Xdt + \\theta X dW\n\\end{aligned}\n\nSolving this for the steady state mean, then we get\n\n\\begin{aligned}\nE[X| t= 1, x_0 = 10, \\theta = .5] = \\exp{\\left(\\frac{1}{2}\\theta^2t\\right)}\n\\end{aligned}\n\n\n\nCode\nlibrary(sde)\nlibrary(Sim.DiffProc)\nset.seed(1234, kind = \"L'Ecuyer-CMRG\")\ntheta <- .5\nf <- expression((.5*theta^2*x))\ng <- expression(theta*x)\nmod1 <- snssde1d(drift = f, \n                 diffusion = g, # for the random gaussian portion\n                 x0=10, # initial conditions\n                 t0=0,\n                 T = 5,\n                 M = 1000, # number of sample solutions to simulate\n                 type = \"ito\", # type of integral\n                 method = \"euler\") # method\n\nmod2 <- snssde1d(drift = f, diffusion = g, x0=10, M = 1000, type = \"str\")\n\n# many summary and confidence intervals.\nsummary(mod1, at = .3) # by monte carlo of fitting the stochastic paths of the pandemic\n\n\n\n    Monte-Carlo Statistics for X(t) at time t = 0.3\n                             \nMean             1.017744e+01\nVariance         8.046738e+00\nMedian           9.705629e+00\nMode             8.953720e+00\nFirst quartile   8.137266e+00\nThird quartile   1.178582e+01\nMinimum          3.875000e+00\nMaximum          2.458763e+01\nSkewness         9.007665e-01\nKurtosis         4.327431e+00\nCoef-variation   2.787220e-01\n3th-order moment 2.056089e+01\n4th-order moment 2.802011e+02\n5th-order moment 2.130882e+03\n6th-order moment 2.519499e+04\n\n\nCode\nsummary(mod1)\n\n\n\n    Monte-Carlo Statistics for X(t) at time t = 5\n                             \nMean             1.852923e+01\nVariance         8.618737e+02\nMedian           9.895477e+00\nMode             4.667985e+00\nFirst quartile   4.490471e+00\nThird quartile   2.092506e+01\nMinimum          3.555266e-01\nMaximum          3.234119e+02\nSkewness         5.208753e+00\nKurtosis         4.095205e+01\nCoef-variation   1.584399e+00\n3th-order moment 1.317951e+05\n4th-order moment 3.042026e+07\n5th-order moment 7.738945e+09\n6th-order moment 2.080372e+12\n\n\n\n\nCode\n# sample from the conditional distribution, conditional on the initial value.\nrsde1d(mod1, at = 1)\n\n\n   [1]  8.735682  7.344977  8.438110 11.141480  5.586668  6.867419  7.721280\n   [8] 12.731186 10.360972  6.018939 14.066648 17.632072 11.276746 11.415946\n  [15]  8.265950 14.989736 14.369225  4.820591  5.271232 20.415143 17.979233\n  [22] 13.230907  6.887687  2.187285 12.242744 14.272089 20.569979  7.562125\n  [29] 12.919931 19.054617 18.087574  9.257144  9.943448 15.041281 12.334913\n  [36] 18.685543 14.704401  4.301899 16.709410 14.305020  6.789316 13.940393\n  [43]  6.961607 11.836772  4.334405 10.145721  5.925220  5.398389 15.377760\n  [50] 14.162652  9.727295  9.029960  8.911256  6.229414 15.443105  9.591912\n  [57]  2.272097  6.588119  4.347615 12.435848 14.882340 14.506381 14.865681\n  [64] 19.749515  5.319737  6.020032  7.552928 13.851179  6.560008 11.221533\n  [71] 23.895195  8.199682 13.339344  8.177069 10.998979 10.382818  6.421800\n  [78]  5.117034 11.057268 13.411960 21.883071  7.458649  2.764852  7.795483\n  [85]  3.494987  8.436737 29.733549 10.116049 11.144152  9.677250  5.530168\n  [92]  4.957671 13.428547  4.674463  9.916626  9.569839  7.427121  8.724480\n  [99] 13.887375 37.832214  4.827476  7.409082  7.590002  6.832259  5.701437\n [106] 10.939493 10.508211 10.489977  9.168765  5.442206 16.367188 12.729222\n [113]  8.394352  6.296143  6.000235 12.853669 15.592119  3.305159 19.224247\n [120] 10.097026 44.081143  5.302471  6.619034 21.294119  9.883666  7.660163\n [127] 16.111134  8.640803  9.070033  8.521571 14.957914  9.737643 14.314386\n [134]  8.301395 16.850106  4.723184 10.570269  8.751662  6.431607  6.979234\n [141]  5.511512 26.072577 15.973050 13.956336 22.281831  7.980447  5.167372\n [148] 13.633486 10.902735  4.777038  5.237959  8.998896 14.379111 16.796432\n [155] 17.796592  4.844048 15.062977  7.677406  8.445849 12.292614 10.925302\n [162]  4.170871 11.204972 12.258030  6.415237  6.684313  5.495029  6.850544\n [169] 10.913465 49.550683  7.601754  9.877510 11.417959 11.645556 31.897234\n [176] 18.124257  7.917117  9.229859 23.545624  5.907581  6.367854 16.060443\n [183] 16.906608 15.458326  7.382460 12.498490  6.763430 18.322312 12.067221\n [190]  6.080208 10.249380 18.303086 18.264506  8.728580  3.651273 11.783608\n [197]  5.060811 12.424674  8.349002  8.469587  7.752336 16.294506  7.827614\n [204]  7.571563 10.349443 12.190055  6.931615  6.369010  8.259565  9.588349\n [211] 12.020285  7.922763 14.949491 30.392037  5.264710 10.551347  3.384744\n [218] 20.320983 21.856545 11.647066  8.165454 29.861374  6.168673 19.204258\n [225]  8.919032 15.066448  6.130486  6.481084  8.632471  6.292030 40.124115\n [232]  9.637588  4.706044 17.181973 11.534985 19.559957 12.331954 20.305912\n [239] 18.928723  8.605554 23.624724 16.527202 14.367721 10.695400 12.257499\n [246] 10.192400  8.192270  7.672497 11.238924 19.316097 15.986567 15.611011\n [253] 17.519380 29.201258 11.289080  9.692351  8.599260  8.392094  5.994662\n [260]  4.420677 25.570475  3.042475 15.181913 13.223439  8.824830 17.427158\n [267] 10.109894 10.619604  9.990138 12.708607  5.451930 16.366812 17.749570\n [274] 11.251404 22.060901 13.359091  4.418347 18.234523  5.836354 10.997174\n [281]  2.883649 15.985710 18.634768 33.745729 11.946528  9.440531 16.152732\n [288] 14.880520  9.168743  8.631228 12.753438  8.075738  9.693134  8.342836\n [295] 20.006189  8.524874  8.663646  7.597066  4.401652 10.961263 15.295185\n [302] 12.127061 10.941505  7.115255  6.663204  5.789734  9.589647 22.846088\n [309] 10.453168  9.980825  3.215731  6.744712  7.847761 14.446800  8.132224\n [316]  4.591336 11.428826 15.692374  6.720083  8.102107 12.109067 19.693051\n [323] 13.334106  3.227853 21.016287 21.537173 20.175474  4.583812  7.857081\n [330]  6.969925 32.054799 10.944720  8.072071  6.843841  7.302450 10.995340\n [337]  9.145603 11.263114 17.169122 14.213967 16.979699 13.092382  7.531280\n [344] 30.619403 13.971973  9.822643  6.179722  8.576703 10.338145  6.566865\n [351]  7.620894 10.133430 15.163833 13.274279  6.832292  6.943379 18.555874\n [358] 12.687174 10.018740  6.257596 11.429321 13.087985  2.259171 20.971742\n [365]  6.334067 10.045572  8.205755 11.711326  9.118618  6.676735  8.117085\n [372] 10.928763 11.507133 15.253922 14.932853 10.061278  6.164131  6.546635\n [379]  8.625697 12.149510 11.037470  9.079694 16.082104  8.482924 14.318134\n [386] 15.781915 10.291881 21.851704 13.871237  4.276425  6.207424  8.029599\n [393] 11.960281  5.559591 17.291110  9.677869 11.317548 14.330349  7.810183\n [400] 11.656946 16.664891  8.091306  6.034429 10.601989 16.624960 14.969227\n [407] 13.382674  6.649358 13.085946 14.187247  9.598343 15.411206 13.089092\n [414] 14.619234 10.976456  4.399503 12.225032 16.272124  9.255786 13.420766\n [421] 18.237756  7.717608 18.250837  5.484751  6.033969 14.973058 11.576019\n [428] 10.466406 12.671159  8.899241  6.791583 13.422994  7.357089  9.806762\n [435]  6.560399 12.105947  7.946314 33.023914  5.394390 13.579767  6.886571\n [442] 14.622790  6.475099 12.496643  8.585582 20.881496 20.479550 17.481984\n [449]  9.297564  3.450331  3.797576  8.111555  6.171378  7.966105  7.236737\n [456]  5.465096  8.596575  7.817410 11.538488  8.766524  4.827839  4.502377\n [463]  8.901147  6.473632  6.663416 10.265154  5.874232  9.001177  5.715018\n [470]  7.121910  4.041778  8.754861  6.287263  8.973394 18.717558 26.161071\n [477] 12.311441  7.612512 10.106153  7.446709  7.541342 13.020324  9.146652\n [484]  6.766166  7.457560  8.290926 16.938588  4.856854  7.589751  7.947044\n [491]  7.950814 10.750785 10.159217 13.429467  7.771240 30.121005 12.172068\n [498]  7.950629 10.462018 18.088553 16.483523 12.694791 14.671843  4.005193\n [505] 10.586292  5.729691 16.966844 14.672584 13.066228  4.742985 12.670527\n [512] 14.901270 18.209804  7.604993 18.141755  7.688447  8.227291  8.361436\n [519]  7.597488 12.209817  6.828570 35.243991 13.958377 10.593089 13.057116\n [526]  7.254756  4.808694  5.429336  6.925283 22.640658  9.531312  9.404048\n [533]  8.583732 14.259011 13.305016 15.772552  7.110868  3.864934 34.554205\n [540]  6.916409  5.647479  9.822804  9.394861 11.844615  5.138353 19.474611\n [547] 10.302989 16.462268  8.716621  6.470711 20.181255  6.802601 15.097106\n [554]  8.314611 13.665932  4.746037 11.280094  6.382058 14.532618  4.592776\n [561]  9.385172  6.616017  4.370788 15.934445  9.220422  8.052510  6.414038\n [568]  9.319051 10.461963  9.163073 18.131839 11.405959  5.050081 14.878276\n [575]  9.440306  7.242909  9.685178 12.001159  7.626404  5.841647  9.401417\n [582] 10.913486  8.137871  8.250435  6.241282 20.503743  6.835440  4.075568\n [589] 13.317741  8.908230  9.607514  8.781462 11.559201  8.523228  6.189690\n [596]  5.004807  8.544644  6.622334  8.725181  4.556923  6.967527  9.825650\n [603]  6.800849  3.373310 11.958287  5.057759  3.275936  7.252035  7.505889\n [610]  4.961906 15.925863 10.456583  7.333704 13.069035  9.165401  8.437423\n [617] 24.225879 10.719532  9.378575 13.554836  6.170304 11.845080 12.074063\n [624] 11.107898 17.352267  6.122797  9.149488  8.901404 14.308107 11.750440\n [631] 34.384038 10.906011 28.107689  6.623738 17.509469  8.522182  5.067133\n [638] 11.250448  6.978229 13.314177 14.338646 10.854989  7.925843 13.071727\n [645] 16.835856  4.845624  9.226108 13.642872  5.772860  8.887952  5.243541\n [652]  4.263603 23.188352 13.690002 20.874907  8.918477 10.476960  6.636883\n [659]  8.761189  4.709350 11.320123 24.507418  9.163258  8.992812  8.472868\n [666]  5.879822  4.797541 12.477772  8.286386  5.717944  5.165515  9.274063\n [673] 15.228879 16.709629  6.133527 13.981494 14.309024  4.264435 26.002135\n [680] 13.968556  7.814676  5.876766 10.549172 20.794342  5.853262  8.843604\n [687]  3.812978 17.399245  7.863833 25.094374  8.615947  7.964152 13.314466\n [694] 12.680812  4.923450  6.455609 11.434462 14.541552  8.714530 12.612795\n [701]  6.163971 15.209324  6.847874  3.844012  7.103783  7.574910 10.795806\n [708] 16.410587  5.662586 13.531550 33.414515  7.564603  5.195435 13.201671\n [715]  4.325110 10.617245 10.300937 11.935363 13.869749  8.080492 15.102606\n [722] 16.282545  5.176784  5.136219  5.285879  5.625455  7.506466  4.921103\n [729]  6.765790  7.958353  7.963147  6.934881  3.619867  6.484795 10.017483\n [736] 18.284185  8.898351  7.059117  6.588445  7.935032  6.322977 41.917574\n [743] 12.404358 14.242764  5.097891 14.854218  7.547913  8.666346 13.214126\n [750]  5.100667 12.028939 10.790179 12.294230  9.122289  6.434208  5.884426\n [757] 13.364949  8.006220 19.007541 10.488436  8.082841 13.238103  6.719256\n [764]  7.227000 16.549775  7.153885 12.443782  9.452624 25.252522  9.817281\n [771] 12.822458  9.911348  5.790378  3.705566  3.895793 21.127984 11.495832\n [778] 15.299475 19.963282 11.816459  6.252820  8.443047 15.334389  6.132213\n [785] 12.981232 10.010884  5.690510 10.145072 25.650558 12.716718  6.782282\n [792]  6.562549 16.509622  6.600271 37.759372  7.100508 14.414004  3.065307\n [799] 10.515343 13.527511  7.875056 13.066810  4.108463 11.023320 11.310364\n [806]  5.734216  4.638748 11.251885  5.755294 35.481208 15.873180 17.853320\n [813] 10.019237 10.815541  7.331981 13.915003 17.168597 27.299591  4.626265\n [820] 11.234908 15.389150  5.171982 20.390825 11.365891  5.377197  5.289994\n [827]  8.019107 12.923733  6.289761  4.747143 11.767389  8.075054  7.589202\n [834]  7.063260  7.810713  7.875881  6.130386 10.039928 23.316898  7.935942\n [841]  6.674351  3.131728 16.350651  5.097724  5.988275 14.234798 12.825203\n [848]  7.611399 37.671219  4.817731  5.256682  7.979159 17.991390 21.070383\n [855]  7.529323 16.710610  7.649365  6.302058  9.598788 12.846073 13.906178\n [862]  5.188373 25.243174 22.341673  6.530109 10.424809  5.226890  3.973538\n [869]  3.990799 13.202370  9.261346 14.617998 16.655459  7.951494  5.906118\n [876]  4.769386  8.912543 15.912557  3.118072 14.157836  6.209171  9.392903\n [883]  4.554953 13.466440  3.797291  8.291070 10.196705 10.299693 22.681143\n [890] 15.534273  7.670050  6.763429 10.566136 26.770649  6.446907 12.851007\n [897]  8.038916  5.252278 10.528966  4.640688 11.907272  8.239203 13.426491\n [904] 12.509110 11.085973  4.344476  7.568144 21.025616  8.501848 16.664800\n [911]  9.309018  6.923763  7.267968  5.632527  9.304927 10.994524  3.350223\n [918] 10.126452  7.169008  2.926619  9.687305  5.026705  9.950606 10.244122\n [925]  8.669018  9.389935 12.937114  9.959240  5.039959 12.693318 11.938860\n [932]  5.696850  8.662153 10.259350 11.582157  8.450874  4.766270  3.424745\n [939] 13.851277 12.794559 20.900590 12.841001 14.012461 15.627804 30.236454\n [946] 14.350391  5.393277  9.856223  6.522718  9.223314  4.711810  8.786792\n [953] 10.369246  8.506418 11.455688  6.973830  5.460132 14.463669  5.608424\n [960] 18.667199  7.510966  8.206467  8.737598 19.988102  6.779640 26.523068\n [967]  7.889209  8.381606  6.199563 30.781468  5.955071  9.740918 17.915503\n [974] 19.661966 11.802730  9.512840 19.829487 12.278193 21.192155 15.600610\n [981]  6.838013 19.593403 11.454369  4.072621 12.941464  6.992606 15.983477\n [988]  7.864295  9.085529 12.672088  5.475907 31.787008  6.200947  7.029389\n [995]  6.273913 10.351483 10.609119  7.566718  7.227250 13.763964\n\n\nCode\nmu1 <- log(10); sigma1= sqrt(theta^2)\nmu2 <- log(10) - .5*theta^2; sigma2= sqrt(theta^2)\n\n# ito's integral\nappdensI <- dsde1d(mod1, at = 1) # conditional density estimation at particular time\n# strat integral\nappdensS <- dsde1d(mod2, at = 1)\nplot(appdensI, dens = function(x) dlnorm(x, meanlog = mu1, sdlog = sigma1))\n\n\n\n\n\nCode\nplot(appdensS, dens = function(x) dlnorm(x, meanlog = mu2, sdlog = sigma2))\n\n\n\n\n\n\n\nCode\nplot(mod1) # show the plot values\nlines(time(mod1),apply(mod1$X,1,mean),col=2,lwd=2)\nlines(time(mod1),apply(mod1$X,1,bconfint,level=0.95)[1,],col=4,lwd=2)\nlines(time(mod1),apply(mod1$X,1,bconfint,level=0.95)[2,],col=4,lwd=2)\nlegend(\"topleft\",c(\"mean path\",paste(\"bound of\", 95,\"% confidence\")),inset = .01,col=c(2,4),lwd=2,cex=0.8)\n\n\n\n\n\nIn two dimensions, vector of drift and diffusion\n\n\\begin{aligned}\ndX = -\\frac{1}{\\mu} Xdt + \\sqrt{\\sigma} dW, \\quad X_0 = x_0\n\\end{aligned}\n\n\n\nCode\nx0=5;y0=0\nmu=3;sigma=0.5\nfx <- expression(-(x/mu),x)  \ngx <- expression(sqrt(sigma),0)\nmod2d <- snssde2d(drift=fx,diffusion=gx,Dt=0.01,M=1000,x0=c(x0,y0),method=\"smilstein\")\nmod2d\n\n\nItô Sde 2D:\n    | dX(t) = -(X(t)/mu) * dt + sqrt(sigma) * dW1(t)\n    | dY(t) = X(t) * dt + 0 * dW2(t)\nMethod:\n    | Second-order Milstein scheme\nSummary:\n    | Size of process   | N  = 1001.\n    | Number of simulation  | M  = 1000.\n    | Initial values    | (x0,y0) = (5,0).\n    | Time of process   | t in [0,10].\n    | Discretization    | Dt = 0.01.\n\n\n\n\nCode\nsummary(mod2d, at = 10)\n\n\n\n    Monte-Carlo Statistics for (X(t),Y(t)) at time t = 10\n                          X             Y\nMean              0.2114239  1.473032e+01\nVariance          0.7162004  2.452654e+01\nMedian            0.2211481  1.474951e+01\nMode              0.2740158  1.509543e+01\nFirst quartile   -0.3465095  1.120916e+01\nThird quartile    0.7881960  1.795676e+01\nMinimum          -2.5031512 -1.658424e+00\nMaximum           2.9862385  2.781259e+01\nSkewness         -0.0183102 -1.056010e-02\nKurtosis          3.1056085  2.905123e+00\nCoef-variation    4.0027928  3.362064e-01\n3th-order moment -0.0110980 -1.282691e+00\n4th-order moment  1.5930000  1.747580e+03\n5th-order moment  0.1742750 -9.018843e+02\n6th-order moment  5.9779595  1.942875e+05\n\n\n\n\nCode\n## in time\nplot(mod2d)\n\n\n\n\n\nCode\n## in plane (O,X,Y)\nplot2d(mod2d,type=\"n\") \npoints2d(mod2d,col=rgb(0,100,0,50,maxColorValue=255), pch=16)"
  },
  {
    "objectID": "stochastic/stochastic.html#guassian-markov-random-field",
    "href": "stochastic/stochastic.html#guassian-markov-random-field",
    "title": "31  Stochastic",
    "section": "31.4 Guassian Markov Random Field",
    "text": "31.4 Guassian Markov Random Field\nGMRF is a generalization of the markov chians. I want to know what data actually looks like when simulated from a GMRF. We start with a line graph, which is just a standard random walk\n\n31.4.1 Random Walk\n\n\nCode\nn <- 20 # 20 obs\nN <- 100 # nodes\n\n\n#' Random Walk\n#'\n#' @param N number of nodes\n#'\n#' @return\n#' @export\n#'\n#' @examples\nrandomwalk <- function(nodes = 10, y0 = 0, drift = 0, stddev = 1) {\n  y0 + cumsum(rnorm(nodes, mean = drift, sd = stddev))\n}\n\nset.seed(1)\nrw_mat <- replicate(1000,\n          randomwalk(10)) # 6 rows, 1000 samples\n\nrw_cov <- rw_mat %>% t() %>% cov() \nrw_prec <- rw_cov %>% solve()\nimage(rw_prec)\n\n\n\n\n\n\n\nCode\n# can try to use glasso to estimate sparse patterns in the covariance matrix\nrw_glasso <- glasso(rw_cov, rho = .02, nobs = 1000)\n\nrw_glassopath <- glassopath(rw_cov, rholist = seq(.01, .40, .01))\n\n\nm\n[1] 1\nm\n[1] 2\nm\n[1] 3\nm\n[1] 4\nm\n[1] 5\nm\n[1] 6\nm\n[1] 7\nm\n[1] 8\nm\n[1] 9\nm\n[1] 10\nm\n[1] 1\nm\n[1] 2\nm\n[1] 3\nm\n[1] 4\nm\n[1] 5\nm\n[1] 6\nm\n[1] 7\nm\n[1] 8\nm\n[1] 9\nm\n[1] 10\nm\n[1] 1\nm\n[1] 2\nm\n[1] 3\nm\n[1] 4\nm\n[1] 5\nm\n[1] 6\nm\n[1] 7\nm\n[1] 8\nm\n[1] 9\nm\n[1] 10\nrho=\n[1] 0.4\nrho=\n[1] 0.39\nrho=\n[1] 0.38\nrho=\n[1] 0.37\nrho=\n[1] 0.36\nrho=\n[1] 0.35\nrho=\n[1] 0.34\nrho=\n[1] 0.33\nrho=\n[1] 0.32\nrho=\n[1] 0.31\nrho=\n[1] 0.3\nrho=\n[1] 0.29\nrho=\n[1] 0.28\nrho=\n[1] 0.27\nrho=\n[1] 0.26\nrho=\n[1] 0.25\nrho=\n[1] 0.24\nrho=\n[1] 0.23\nrho=\n[1] 0.22\nrho=\n[1] 0.21\nrho=\n[1] 0.2\nrho=\n[1] 0.19\nrho=\n[1] 0.18\nrho=\n[1] 0.17\nrho=\n[1] 0.16\nrho=\n[1] 0.15\nrho=\n[1] 0.14\nrho=\n[1] 0.13\nrho=\n[1] 0.12\nrho=\n[1] 0.11\nrho=\n[1] 0.1\nrho=\n[1] 0.09\nrho=\n[1] 0.08\nrho=\n[1] 0.07\nrho=\n[1] 0.06\nrho=\n[1] 0.05\nrho=\n[1] 0.04\nrho=\n[1] 0.03\nrho=\n[1] 0.02\nrho=\n[1] 0.01\n\n\nCode\nrw_glassopath$wi[,,6]\n\n\n             [,1]        [,2]        [,3]         [,4]        [,5]         [,6]\n [1,]  1.53363033 -0.59705922 -0.05045755  0.000000000 -0.05281481  0.000000000\n [2,] -0.59704118  1.46328031 -0.66740241 -0.062748027 -0.01166860  0.000000000\n [3,] -0.05045859 -0.66736724  1.48804455 -0.709396648 -0.01840336  0.000000000\n [4,]  0.00000000 -0.06272291 -0.70930973  1.502769843 -0.64832439 -0.104003565\n [5,] -0.05271855 -0.01139892 -0.01848738 -0.648268462  1.39557882 -0.614267466\n [6,]  0.00000000  0.00000000  0.00000000 -0.104181741 -0.61433940  1.450875970\n [7,]  0.00000000  0.00000000  0.00000000  0.000000000  0.00000000 -0.685011604\n [8,]  0.00000000  0.00000000  0.00000000 -0.006829119 -0.03745143 -0.040873008\n [9,]  0.00000000 -0.00267496  0.00000000  0.000000000 -0.02218277 -0.012819793\n[10,]  0.00000000  0.00000000  0.00000000  0.000000000  0.00000000 -0.005073061\n             [,7]         [,8]         [,9]        [,10]\n [1,]  0.00000000  0.000000000  0.000000000  0.000000000\n [2,]  0.00000000  0.000000000 -0.002589899  0.000000000\n [3,]  0.00000000  0.000000000  0.000000000  0.000000000\n [4,]  0.00000000 -0.006867342  0.000000000  0.000000000\n [5,]  0.00000000 -0.037544676 -0.022236583  0.000000000\n [6,] -0.68516608 -0.040524630 -0.012875666 -0.005038421\n [7,]  1.44064932 -0.678296252 -0.049116046  0.000000000\n [8,] -0.67798966  1.509124167 -0.676270096 -0.090260579\n [9,] -0.04922243 -0.676158135  1.480116539 -0.719604508\n[10,]  0.00000000 -0.090276515 -0.719623596  0.810028387\n\n\nCode\napply(rw_glassopath$wi, 3, as_function(~sum(.x != 0))) # why does the number of elements increase then decrease? behavior is curious\n\n\n [1] 80 70 64 64 60 58 58 60 60 62 62 64 68 68 68 68 70 70 70 70 70 70 72 72 72\n[26] 74 74 74 74 74 74 76 78 78 78 78 78 78 78 80\n\n\n\n\nCode\nset.seed(1)\ngaus <- matrix(rnorm(50*20), ncol = 20)\ngaus_cov <- var(gaus)\ngaus_prec <- gaus_cov %>% solve()\ngaus_prec %>% Matrix() %>% image() # has a diagonal structure\n\n\n\n\n\nCode\ngaus_glasso <- glasso(gaus_cov, rho = .03, nobs = 50)\n\ngaus_glassopath <- glassopath(gaus_cov, rho = seq(.01, .7, .01))\n\n\nrho=\n[1] 0.7\nrho=\n[1] 0.69\nrho=\n[1] 0.68\nrho=\n[1] 0.67\nrho=\n[1] 0.66\nrho=\n[1] 0.65\nrho=\n[1] 0.64\nrho=\n[1] 0.63\nrho=\n[1] 0.62\nrho=\n[1] 0.61\nrho=\n[1] 0.6\nrho=\n[1] 0.59\nrho=\n[1] 0.58\nrho=\n[1] 0.57\nrho=\n[1] 0.56\nrho=\n[1] 0.55\nrho=\n[1] 0.54\nrho=\n[1] 0.53\nrho=\n[1] 0.52\nrho=\n[1] 0.51\nrho=\n[1] 0.5\nrho=\n[1] 0.49\nrho=\n[1] 0.48\nrho=\n[1] 0.47\nrho=\n[1] 0.46\nrho=\n[1] 0.45\nrho=\n[1] 0.44\nrho=\n[1] 0.43\nrho=\n[1] 0.42\nrho=\n[1] 0.41\nrho=\n[1] 0.4\nrho=\n[1] 0.39\nrho=\n[1] 0.38\nrho=\n[1] 0.37\nrho=\n[1] 0.36\nrho=\n[1] 0.35\nrho=\n[1] 0.34\nrho=\n[1] 0.33\nrho=\n[1] 0.32\nrho=\n[1] 0.31\nrho=\n[1] 0.3\nrho=\n[1] 0.29\nrho=\n[1] 0.28\nrho=\n[1] 0.27\nrho=\n[1] 0.26\nrho=\n[1] 0.25\nrho=\n[1] 0.24\nrho=\n[1] 0.23\nrho=\n[1] 0.22\nrho=\n[1] 0.21\nrho=\n[1] 0.2\nrho=\n[1] 0.19\nrho=\n[1] 0.18\nrho=\n[1] 0.17\nrho=\n[1] 0.16\nrho=\n[1] 0.15\nrho=\n[1] 0.14\nrho=\n[1] 0.13\nrho=\n[1] 0.12\nrho=\n[1] 0.11\nrho=\n[1] 0.1\nrho=\n[1] 0.09\nrho=\n[1] 0.08\nrho=\n[1] 0.07\nrho=\n[1] 0.06\nrho=\n[1] 0.05\nrho=\n[1] 0.04\nrho=\n[1] 0.03\nrho=\n[1] 0.02\nrho=\n[1] 0.01\n\n\nCode\n# count number of non-zero entries\napply(gaus_glassopath$wi, 3, as_function(~sum(.x != 0)))\n\n\n [1] 366 346 326 306 276 254 238 222 208 192 180 166 158 142 126 112 104  94  88\n[20]  76  66  62  56  48  46  42  32  26  26  24  24  24  24  24  24  22  22  22\n[39]  20  20  20  20  20  20  20  20  20  20  20  20  20  20  20  20  20  20  20\n[58]  20  20  20  20  20  20  20  20  20  20  20  20  20\n\n\nCode\ngaus_glassopath$wi[,,60] %>% image() # sparsity pattern is correct but not the scale...\n\n\n\n\n\nSampling from multivariate normal distribution, sparse precision matrix, we use sparseMVN"
  },
  {
    "objectID": "stochastic/stochastic.html#reaction-diffusion-equations",
    "href": "stochastic/stochastic.html#reaction-diffusion-equations",
    "title": "31  Stochastic",
    "section": "31.5 Reaction Diffusion Equations",
    "text": "31.5 Reaction Diffusion Equations\nThese are non-linear differential equations, and a system of them. We can start describing the reactions of SIR model as sets of nonlinear differential equations. There are a number of famous examples of these, we’ll study the Grey-Scott system, then the SIR system, and hopefully we’ll see the reaction diffusion nonlinearity around the boundaries of the different stable sets.\n\n31.5.1 Grey-Scott Model\nA really cool web simulation of the phenomena I want to recreate can be found by Karl Sims, Reaction Diffusion Tutorial. Luckily, someone else has already implemented a version of this, and we’ll just borrow their code (Fronkonstin).\n\nThe Grey-Scott Model describes the following irreversible, reactions of three compound U, V, P and P is an inert product.\n\n\\begin{align*}\nU + 2V &\\rightarrow 3V \\\\\nV &\\rightarrow P\n\\end{align*}\n\nWe will use the simulation parameters from Pearson (1993), in particular, the equations that result from this reaction diffusion is\n\n\\frac{\\partial U}{\\partial t} = D_u \\nabla^2U - UV^2 + F(1 - U) \\\\\n\\frac{\\partial V}{\\partial t} = D_v \\nabla^2V + UV^2 - (F + k)V\n\n\nD_u = 2 \\times 10^{-5}\nD_v = 10^{-5}\nperiodic boundary condition\nF and k are known as the feed and kill rates of the reactants. since concentration ranges between 0 and 1, the reaction term in the first equation is positive, and then F controls how much of reactant U is being introduced.\n\n\n\nCode\ngray_scott <- function(U0 = NULL, V0 = NULL,\n                       feed_rate = 0.0545,\n                       kill_rate = 0.062,\n                       N = 256,\n                       tN = 2000,\n                       D.u = 1,\n                       D.v = .5,\n                       save_frame_freq = 20,\n                       video_file = \"gray_scott.mp4\",\n                       pic_dir = NULL,\n                       init_strategy = c(\"random\"),\n                       seed = 1,\n                       ...) {\n  set.seed(seed)\n  pct <- proc.time()\n  init_strategy <- match.arg(init_strategy)\n  if (missing(U0) | missing(V0)) {\n    #TODO: implement different initialization strategies for different patterns\n    U0 <- matrix(1, nrow = N, ncol = N)\n    V0 <- matrix(0, nrow = N, ncol = N)\n    V0[sample(N^2, ceiling(N^2 / 20))] <- 1 # 10% of cells\n    } else if (!all(c(dim(U0), dim(V0)) == N)) {\n      rlang::abort(\"Initial Matrix must be a grid with dimension N\")\n    }\n  \n  U <- U0\n  V <- V0\n  # 9 point stencil for Laplacian\n  # yuvj420p pix format used: https://superuser.com/questions/1273920/deprecated-pixel-format-used-make-sure-you-did-set-range-correctly\n  L <- matrix(c(0.05, 0.2, 0.05, \n                0.2,  -1, 0.2,\n                0.05, 0.2, 0.05), nrow = 3)\n  \n  \n  if (missing(pic_dir))\n    pic_dir <- tempdir() \n  else \n    pic_dir <- fs::dir_create(fs::path_wd(pic_dir))\n\n  # clean directory  \n  if (length(Sys.glob(fs::path(pic_dir, \"*.jpg\"))) > 0 ) {\n    rlang::abort(sprintf(\"%s not empty, please clean out *.jpg files to prevent overwriting!\", pic_dir))\n  }\n\n  jpeg_file <- fs::path(pic_dir, sprintf(\"plot%06d.jpg\", 0))\n  jpeg(jpeg_file)\n      # setup base plot graphical parameters\n      par(bty = \"n\", oma = rep(0, 4), mar = rep(0, 4))\n      plot(raster(V0),\n           axes = F, bty = \"n\", frame.plot = F,\n           interpolate = F,\n           breaks = hist(V0, breaks = 256, plot = F)$breaks,\n           col = hcl.colors(256,  palette = \"ag_sunset\"),\n           legend = FALSE)\n      \n      dev.off()\n  \n  p <- progressr::progressor(tN / save_frame_freq)\n  for (i in 1:tN) {\n    dU <- D.u * filter2(U, L) - U * V^2 + feed_rate * (1 - U)\n    dV <- D.v * filter2(V, L) + U * V^2 - (feed_rate + kill_rate) * V\n    U <- U + dU\n    V <- V + dV\n    # save frame in temp folder\n    if (i %% save_frame_freq == 0) {\n      p(message = sprintf(\"Timestep: %g\", i))\n      jpeg_file <- fs::path(pic_dir,\n                            sprintf(\"plot%06d.jpg\", i))\n      jpeg(jpeg_file)\n      # setup base plot graphical parameters\n      par(bty = \"n\", oma = rep(0, 4), mar = rep(0, 4))\n      plot(raster(V),\n           axes = F, bty = \"n\", frame.plot = F,\n           interpolate = T,\n           breaks = hist(V, breaks = 256, plot = F)$breaks,\n           col = hcl.colors(256,  palette = \"ag_sunset\"),\n           legend = FALSE)\n      dev.off()\n    }\n  }\n  \n  # create video directory\n  fs::dir_create(fs::path_wd(fs::path_dir(video_file)))\n  \n  #TODO: check if ffmpeg available on system?\n  ffmpeg_cmd <- sprintf('ffmpeg -y -f image2 -pattern_type glob -i \"%s/*.jpg\" -framerate 60 -c:v libx264 -crf 20 -filter:v \"format=yuvj420p\" %s', pic_dir, fs::path_wd(video_file))\n  # run ffmpeg command\n  system(ffmpeg_cmd)\n  cat(paste0(\"Running command: \", ffmpeg_cmd, \"\\n\"))\n  proc.time() - pct # elapsed wall time\n}\n# relative to this script (when running commands in notebook)\nwith_progress(\n  gray_scott(video_file = \"vid/gray_scott.mp4\", pic_dir = \"gray_pic\"), handlers = handlers(\"progress\")) # for text updates\n\n\n\n\n\n\n\nGray Scott Reaction Diffusion, f=.0545, k=.062\n\n\n\n\n31.5.2 SIR Diffusion\n\n\n\nhere’s we’re trying to upsample the values that come out of the\nThe SIR image has a slightly different quality to it in that the lines around the image are thinner\n\n\n\n\n\nSIR Reaction Diffusion, f=.0545, k=.062\n\n\n\n\n31.5.3 SIS system\nI’m curious if the SIS system without vital dynamics will show any interesting patterns. We know there must be an absorbing state, but this system can also reach an endemic state based on the reproduction number. If we add diffusion into the system we should get interesting results, at the very least some cool videos.\n\n\n31.5.4 Brusselator\n\n\n31.5.5 Lorentz Attractor\n\n\n\n\nPearson, John E. 1993. “Complex Patterns in a Simple System.” Science 261 (5118): 189–92. https://doi.org/10.1126/science.261.5118.189."
  },
  {
    "objectID": "timeseries/timeseries.html",
    "href": "timeseries/timeseries.html",
    "title": "34  Time Series",
    "section": "",
    "text": "35 Introduction\nThese models have a hierarchical form, in which there is some underlying time series process, but then we also observe data on top of that. Thus, the general form of the equations look like this:\n\\begin{aligned}\n\\textbf{State Equation:}& \\\\\nx_t &= \\Phi x_{t-1} + \\Upsilon u_t +   w_t \\\\\n\\textbf{Observation Equation:}& \\\\\ny_t &= A_t x_t + \\Gamma u_t  + v_t\n\\end{aligned}\n where:\nThere are more general forms of the state space model: with correlated errors, see Durbin Koopman for more state space methods.\nIt seems now that state space models are now also being superseded by recurrent neural networks, which can model dynamical properties."
  },
  {
    "objectID": "timeseries/timeseries.html#types-of-models",
    "href": "timeseries/timeseries.html#types-of-models",
    "title": "34  Time Series",
    "section": "35.1 Types of Models",
    "text": "35.1 Types of Models\n\nAutoregression (AR)\nMoving Average (AM)\nVector Autoregression (VAR)\nHMM\n\nrelated to bayesian inference, special\n\nState Space Models\n\nvery broad space of models that includes arima models, and dynamic linear models. Can also account for linear/nonlinear gaussian/non-gaussian errors\n\nMultivariate autoregression (MAR)"
  },
  {
    "objectID": "timeseries/timeseries.html#software",
    "href": "timeseries/timeseries.html#software",
    "title": "34  Time Series",
    "section": "35.2 Software",
    "text": "35.2 Software\n\nstats\n\nKalmanLike, KalmanRun, KalmanSmooth, KalmanForecast\n\ndlm\nKFAS\n\nSee also State Space Models in R."
  },
  {
    "objectID": "timeseries/timeseries.html#theory",
    "href": "timeseries/timeseries.html#theory",
    "title": "34  Time Series",
    "section": "35.3 Theory",
    "text": "35.3 Theory\nCourses\n\nKevin Kotzé’s Time Series Analysis Course\n\nhas a great overview and description, and packaged R commands for univariate and multivariate from an economics perspective."
  },
  {
    "objectID": "timeseries/timeseries.html#base-tools",
    "href": "timeseries/timeseries.html#base-tools",
    "title": "34  Time Series",
    "section": "36.1 base tools",
    "text": "36.1 base tools\n\n\nCode\n# acf, ccf, pacf"
  },
  {
    "objectID": "timeseries/timeseries.html#ar1",
    "href": "timeseries/timeseries.html#ar1",
    "title": "34  Time Series",
    "section": "36.2 AR(1)",
    "text": "36.2 AR(1)\nAutoregressive of order 1 is the simplest time series you can have.\n\n\\begin{aligned}\nY_t = Y_{t-1} + \\varepsilon_t\n\\end{aligned}"
  },
  {
    "objectID": "timeseries/timeseries.html#simulated-examples",
    "href": "timeseries/timeseries.html#simulated-examples",
    "title": "34  Time Series",
    "section": "36.3 Simulated Examples",
    "text": "36.3 Simulated Examples\n\n\nCode\n# AR models\nsim_ar <- tibble(p = 1:3,\n                 ar = list(.7,\n                           c(.7,-.3),\n                           c(.7, -.3, .5)),\n                 n = 200) %>% \n  rowwise() %>% \n  mutate(y = list(arima.sim(list(ar = ar), n = 200)),\n         x = list(time(y)))\n\nsim_ar %>% unnest(c(x, y)) %>% \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  facet_grid(p~.)\n\n\n\n\n\n\n\nCode\n# ma processes\nsim_ma <- tibble(q = 1:3,\n                 ma = list(.7,\n                           c(.7,-.3),\n                           c(.7, -.3, .5)),\n                 n = 200) %>% \n  rowwise() %>% \n  mutate(y = list(arima.sim(list(ma = ma), n = 200)),\n         x = list(time(y))) %>% \n  ungroup()\n\nsim_ma %>% unnest(c(x, y)) %>% \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  facet_grid(q~.)"
  },
  {
    "objectID": "timeseries/timeseries.html#examples-la-county",
    "href": "timeseries/timeseries.html#examples-la-county",
    "title": "34  Time Series",
    "section": "36.4 Examples: LA County",
    "text": "36.4 Examples: LA County\nWe just examine the cardiovascular mortality from the LA Pollution study. These are average weekly cv mortality rates from astsa package.\n\n\nCode\n# ?cmort\nx <- cmort %>% as.numeric()\nx1 <- x %>% lag()\n# yt = u + beta * yt-1\ncmort_lm <- lm(x~x1)\n\ncbind(coef(cmort_lm)[1] + coef(cmort_lm)[2] * x1,\n      c(NA, fitted(cmort_lm)),\n      c(NA, predict(cmort_lm))) %>% \n  head() # fitted/predicted values match up\n\n\n       [,1]      [,2]      [,3]\n         NA        NA        NA\n2  95.73811  95.73811  95.73811\n3 100.97814 100.97814 100.97814\n4  93.04478  93.04478  93.04478\n5  95.89246  95.89246  95.89246\n6  94.19466  94.19466  94.19466\n\n\n\n\nCode\nqplot(time(cmort), x, geom = \"line\", color = \"black\") +\n  geom_line(aes(y =  c(NA, fitted(cmort_lm)), color = \"red\")) +\n  scale_color_manual(values = c(\"black\", \"red\"),\n                     label = c(\"raw\", \"fitted\"))\n\n\nDon't know how to automatically pick scale for object of type ts. Defaulting to continuous.\n\n\nWarning: Removed 1 row(s) containing missing values (geom_path)."
  },
  {
    "objectID": "timeseries/timeseries.html#example-airpassengers",
    "href": "timeseries/timeseries.html#example-airpassengers",
    "title": "34  Time Series",
    "section": "36.5 Example: AirPassengers",
    "text": "36.5 Example: AirPassengers\n\n\nCode\nAirPassengers %>% plot()\n\n\n\n\n\n\n\nCode\nsarima(AirPassengers, d = 1, p = 2, q = 0)\n\n\ninitial  value 3.522169 \niter   2 value 3.456359\niter   3 value 3.447683\niter   4 value 3.447001\niter   5 value 3.447000\niter   6 value 3.447000\niter   6 value 3.447000\niter   6 value 3.447000\nfinal  value 3.447000 \nconverged\ninitial  value 3.441124 \niter   2 value 3.441115\niter   3 value 3.441115\niter   4 value 3.441114\niter   4 value 3.441114\niter   4 value 3.441114\nfinal  value 3.441114 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2  constant\n      0.3792  -0.2314    2.4075\ns.e.  0.0823   0.0834    3.0636\n\nsigma^2 estimated as 973.4:  log likelihood = -694.99,  aic = 1397.98\n\n$degrees_of_freedom\n[1] 140\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.3792 0.0823  4.6065  0.0000\nar2       -0.2314 0.0834 -2.7767  0.0062\nconstant   2.4075 3.0636  0.7858  0.4333\n\n$AIC\n[1] 9.77605\n\n$AICc\n[1] 9.777257\n\n$BIC\n[1] 9.858927"
  },
  {
    "objectID": "timeseries/timeseries.html#vector-autoregression",
    "href": "timeseries/timeseries.html#vector-autoregression",
    "title": "34  Time Series",
    "section": "34.7 Vector Autoregression",
    "text": "34.7 Vector Autoregression\nThe simplest vector autoregressive model we can make is VAR(1), which takes the form\nWe assume that the process is stable, which implies that the function is stationary.\n\n34.7.1 Simulation\nhttps://www.r-econometrics.com/timeseries/svarintro/\nThis is an example of a “B”-model structural, VAR(1) model in which the coefficients are given. Let’s see what we can do with given coefficients.\n\ny_t = A_1y_{t-1} + B\\epsilon_t\n \nA_1 = \\begin{bmatrix} 0.3 & 0.12 & 0.69 \\\\ 0 & 0.3 & 0.48 \\\\ 0.24 & 0.24 & 0.3 \\end{bmatrix} \\text{, }\nB = \\begin{bmatrix} 1 & 0 & 0 \\\\ -0.14 & 1 & 0 \\\\ -0.06 & 0.39 & 1 \\end{bmatrix} \\text{ and } \\epsilon_t \\sim N(0, I_3)\n\n\n\nCode\n# Reset random number generator for reproducibility\nset.seed(24579)\n\ntt <- 500 # Number of time series observations\n\n# Coefficient matrix\nA_1 <- matrix(c(0.3, 0, 0.24,\n                0.12, 0.3, 0.24,\n                0.69, 0.48, 0.3), 3)\n\n# Structural coefficients\nB <- diag(1, 3)\nB[lower.tri(B)] <- c(-0.14, -0.06, 0.39)\n\n# Generate series\nseries <- matrix(rnorm(3, 0, 1), 3, tt + 1) # Raw series with zeros\nfor (i in 2:(tt + 1)){\n  series[, i] <- A_1 %*% series[, i - 1] +  B %*% rnorm(3, 0, 1)\n}\n\nseries <- ts(t(series)) # Convert to time series object\ndimnames(series)[[2]] <- c(\"S1\", \"S2\", \"S3\") # Rename variables\n\n# Plot the series\nplot.ts(series, main = \"Artificial time series\")\n\n\n\n\n\nI this case, we’ve been handed the correct model, and we can quickly note some properties of the model. We can check for stability (which implies stationarity) by looking at the eigenvalues and checking that they’re magnitude is less than 1. It’s also clear that the innovations are correlated with one another,\n\n\nCode\neigen(A_1)$values\n\n\n[1]  0.8529906  0.2503329 -0.2033235\n\n\n\n\nCode\nvar_est <- VAR(series, p = 1, type = \"none\")\n\n# estimated A_1 matrix\nas_latex(\"\\\\hat A_1 = \") + as_latex(round(Bcoef(var_est), digits = 3))\n\n# estimate structural equation for A Model\na <- diag(1, 3)\na[lower.tri(a)] <- NA\n\nsvar_est_a <- SVAR(var_est, Amat = a, max.iter = 1000)\nas_latex(\"\\\\hat A^{-1} = \") + as_latex(round(solve(svar_est_a$A), digits = 33)) # pretty close to B\n\n# estimate structural equation for B Model\nb <- diag(1, 3)\nb[lower.tri(b)] <- NA # specify restriction with NA\n\nsvar_est_b <- SVAR(var_est, Bmat = b)\nas_latex(\"\\\\hat B = \") + as_latex(round(svar_est_b$B, digits = 3))\n\n\n\\hat A_1 = \\begin{bmatrix} 0.329 &0.112 &0.681 \\\\-0.002 &0.29 &0.513 \\\\0.24 &0.252 &0.319 \\\\ \\end{bmatrix}\\hat A^{-1} = \\begin{bmatrix} 1 &0 &0 \\\\-0.181774697458286 &1 &0 \\\\-0.107710324655258 &0.313198839004628 &1 \\\\ \\end{bmatrix}\\hat B = \\begin{bmatrix} 1 &0 &0 \\\\-0.182 &1 &0 \\\\-0.108 &0.313 &1 \\\\ \\end{bmatrix}\n\n\nThe estimated A_1 is given directly by the function Bcoef on the fitted VAR model. We invert the matrix A because we need to translate this into a “B” model from the estimated “A” model, we estimated and find that it’s quite correct.\n\n\n34.7.2 Moving Average Representations\nWe can get the basic moving average representation\n\n\\begin{aligned}\ny_t = \\Phi_0 u_t + \\Phi_1 u_{t-1} + ...\n\\end{aligned}\n\nPhi is normally calculated recursively from the last observations, starting with\n\n\\Phi_0 = I_K\n\\Phi_i = \\sum_{j=1}^{i}\\Phi_{i-j}A_j\n\n\nfor the VAR(1) case, the sequence is simply \\Phi = I, A_1, A_1^2, A_1^3, \\dots.\n\n\n\nCode\n# Bcoef(var_est) # A_1\n# Bcoef(var_est) %^% 2 #A_1^2\nPhi(var_est, nstep = 2)\n\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n, , 2\n\n             [,1]      [,2]      [,3]\n[1,]  0.328807868 0.1124078 0.6809865\n[2,] -0.001530688 0.2895784 0.5127013\n[3,]  0.240186093 0.2519319 0.3186145\n\n, , 3\n\n          [,1]      [,2]      [,3]\n[1,] 0.2715060 0.2410736 0.4985175\n[2,] 0.1221972 0.2128494 0.3107789\n[3,] 0.1551162 0.1802220 0.3942445\n\n\nFor the other MA representation with orthogonal coefficients,\n\n\nCode\n# Sigmahat <- crossprod(resid(var_est))/497 # estimated covariance, 500 total obs\n# summary(var_est)$covres # i don't know how to get them to match exactly\n# P <- t(chol(Sigmahat))\n# apply(Phi(var_est, nstep = 2), 3, {\\(x) x %*% P}, simplify = FALSE) # multiply all Phi by P\nPsi(var_est, nstep = 2)\n\n\n, , 1\n\n           [,1]     [,2]      [,3]\n[1,]  0.9972189 0.000000 0.0000000\n[2,] -0.1812692 1.021112 0.0000000\n[3,] -0.1074108 0.319811 0.9953141\n\n, , 2\n\n           [,1]      [,2]      [,3]\n[1,]  0.2343721 0.3325679 0.6777954\n[2,] -0.1090877 0.4596596 0.5102988\n[3,]  0.1596280 0.3591471 0.3171215\n\n, , 3\n\n           [,1]      [,2]      [,3]\n[1,] 0.17350558 0.4055946 0.4961815\n[2,] 0.04989328 0.3167336 0.3093226\n[3,] 0.07967004 0.3101106 0.3923971\n\n\n\n\n34.7.3 Impulse Response Analysis\nThere are two types of basic impulse responses:\n\n(forecast error impulse responses) These are on the scale of the original error, Sigma.u and let c(0, 0, 1) reverberate in the system\n\n\nthese have the disadvantage that\n\n\n(orthogonal error impulse responses) These are on the scale of Sigma.eps and let P^{-1}c(0, 0, 1) reverberate in the system where Sigma.u = PP' by cholesky decomposition\n\n\n\\begin{aligned}\ny_t &= \\Phi_0 u_t + \\Phi_1 u_{t-1} + \\dots \\\\\n&= \\underbrace{\\Phi_0 P}_{\\Psi_0}\\underbrace{P^{-1}u_t}_{\\epsilon_{t}} +  \\dots\n\\end{aligned}\n\n\n(accumulated error impulse response)\n\n\n\nCode\n# First three rows of irf1\n# Phi(var_est, nstep = 2) # First columns of this\n## even more manually, with A matrix and unit impulse\n# Ahat <- Bcoef(var_est)\n# Ahat %*% cbind(c(1, 0, 0)) # A e_1\n# (Ahat %^% 2) %*% cbind(c(1, 0, 0)) #A_1^2 e_1\nirf1 <- irf(var_est, orth = FALSE, boot = FALSE)\n\n# First three rows of irf2\n# Psi(var_est, nstep= 2 )\n## more manually for VAR(1)\n# Sigmahat <- crossprod(resid(var_est))/497 # estimated covariance, 500 total obs\n# P <- t(chol(Sigmahat))\n# P  %*% cbind(c(1, 0, 0)) # P e_1\n# Ahat %*% P %*%cbind(c(1, 0, 0)) # A_1 P e_1\n# (Ahat %^% 2) %*% P %*% cbind(c(1, 0, 0)) # A_1 P e_1\nirf2 <- irf(var_est, orth = TRUE, boot = FALSE)\n# row 2 \n# P  %*% cbind(c(1, 0, 0)) + Ahat %*% P %*%cbind(c(1, 0, 0)) \nirf3 <- irf(var_est, cumulative = TRUE, boot = FALSE) # accumulates rowwise of regular irf\nplot(irf1)\nplot(irf2)\nplot(irf3) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe impulse response functions are simply ways of answering the question, “what would happen if I send a shock through the system”. For non orthogonal\nWe assume the process is mean 0, and irf_1(0) = c(1, 0, 0) irf_1(1) = A \\begin{bmatrix}1, 0, 0\\end{bmatrix}\n\n\nCode\n# varsim_orthirf <- irf(var_est, orth = TRUE, boot = FALSE)\n# varsim_orthirf$irf$S1\n# Bcoef(var_est) # gives as large matrix\n\n\n\n\nCode\n# \n# # ?Psi # MA orthogonal representation\n# Psi(var_est) # W = PD^{-1} =\n# \n# D <- diag(P)\n# W <- P %*% diag((1 / D)) # matches with Psi(1)\n# Psi(var_est)[,,1] # I don't know why these are slightly off... but using d\n# \n# Psi(var_est)\n# \n# P%*% t(P) # Sigmahat\n\n\n\n\nCode\n# P %*% diag(1/D)\n# svar_est_b$B\n# \n# solve(svar_est_a$A) # This is estimate of B matrix\n\n\n\n\nCode\n# svar_est_a$Sigma.U / 100\n# tcrossprod(svar_est_b$B) # is Sigma.U\n# tcrossprod(W) # Sigma U = PD^{-1}\n# Sigmahat\n# solve(W) %*% (svar_est_b$Sigma.U / 100) %*% t(solve(W)) # Identity, Sigma_epsilon\n# Psi(var_est)[,,2]\n# Acoef(var_est) # gives as list\n\n\n\n\n34.7.4 Example: LA County (VAR)\n\n\nCode\n# cardiovascular mortality, temperature and particulates in LA county, weekly.\nla <- cbind(cmort, tempr, part)\n\n# visualization\n# ts.plot(cmort, tempr, part, col = 1:3) # base r of autoplot \nautoplot(la, color = \"black\") + facet_grid(series~., scales = \"free_y\")\n\n\n\n\n\n\n\nCode\nla_var1 <- VAR(la, p = 1, type = \"both\") # w/ trend\nla_var2 <- VAR(la, p = 2, type = \"both\") # w/ trend\n\n# coefficient matrix\nsapply(coef(la_var1),rlang::as_function(~.x[,\"Estimate\"])) |> as.data.frame()\n\n\n               cmort        tempr         part\ncmort.l1  0.46482370 -0.244046444 -0.124774858\ntempr.l1 -0.36088790  0.486595619 -0.476526201\npart.l1   0.09941503 -0.127660994  0.581308364\nconst    73.22729188 67.585597743 67.463501275\ntrend    -0.01445884 -0.006912455 -0.004650001\n\n\nCode\n# vcov of coef estimates\nsapply(coef(la_var1), rlang::as_function(~round(.x[,\"Std. Error\"], 3))) |> as.data.frame()\n\n\n         cmort tempr   part\ncmort.l1 0.037 0.042  0.079\ntempr.l1 0.032 0.037  0.069\npart.l1  0.019 0.022  0.041\nconst    4.834 5.542 10.399\ntrend    0.002 0.002  0.004\n\n\nCode\n# estimated covariance of errors\nsummary(la_var1)$covres |> as.data.frame()\n\n\n          cmort     tempr      part\ncmort 31.171944  5.974621  16.65448\ntempr  5.974621 40.964945  42.32335\npart  16.654476 42.323345 144.26025\n\n\n\n\nCode\nla_var1_const <- VAR(la, p = 1, type = \"const\")\n# matching manaully with ar.ols\n# la_ols <- ar.ols(la, order.max = 1, demean = FALSE, intercept = TRUE)\n# \n# # matches constant VAR coefficients\n# rbind(t(la_ols$ar[1,,]),\n#       const = la_ols$x.intercept)\n\nsapply(coef(la_var1_const),rlang::as_function(~.x[,\"Estimate\"])) |> as.data.frame()\n\n\n               cmort      tempr        part\ncmort.l1  0.60149346 -0.1787076 -0.08082151\ntempr.l1 -0.30946101  0.5111817 -0.45998718\npart.l1   0.07096225 -0.1412636  0.57215788\nconst    54.94579126 58.8456142 61.58412402\n\n\n\n\nCode\nla %>% melt(c(\"time\", \"series\")) %>% \n  ggplot(aes(time, value)) + \n  geom_line(color = \"black\") + \n  geom_line(data = fitted(la_var1) %>% melt(c(\"time\", \"series\")),\n            mapping = aes(time, value, color = \"red\"), alpha= .6) +  # var1\n  geom_line(data = fitted(la_var1_const) %>% melt(c(\"time\", \"series\")),\n            mapping = aes(time, value, color = \"blue\"), alpha = .6) + # var1 const\n  geom_line(data = fitted(la_var2) %>% melt(c(\"time\", \"series\")),\n            mapping = aes(time, value, color = \"green\"), alpha = .6) + # var2\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\"),\n                     labels = c(\"var1\", \"var1_const\", \"var2\")) + \n  facet_grid(series~.)\n\n\n\n\n\nFrom looking at the models, they’re quite difficult to tell which model is fitting better than the others.\nWe can try to do some automatic VAR order selection with the function VARselect(), which uses a number of other criteria.\n\n\nCode\n# order selection\nVARselect(la) # selects 2 by BIC\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n     9      5      2      9 \n\n$criteria\n                  1           2           3           4           5           6\nAIC(n)     11.84541    11.35895    11.32363    11.28376    11.23095    11.20946\nHQ(n)      11.88523    11.42864    11.42318    11.41318    11.39023    11.39861\nSC(n)      11.94687    11.53651    11.57728    11.61351    11.63679    11.69140\nFPE(n) 139442.85952 85730.12941 82755.88760 79522.85813 75434.43083 73833.93231\n                 7           8           9          10\nAIC(n)    11.21508    11.19717    11.17261    11.17843\nHQ(n)     11.43409    11.44604    11.45134    11.48703\nSC(n)     11.77311    11.83130    11.88283    11.96475\nFPE(n) 74254.62493 72942.17171 71179.59211 71604.50745\n\n\nThe CCF plots should all be non significant. The second part of “x & y” are the ones that lead.\n\n\nCode\n# serial test, BG test for \nserial.test(la_var2, lags.pt = 12, type = \"PT.adjusted\")\n\n\nCode\nserial.test(la_var2, lags.pt = 12, type = \"BG\")\n\n\nCode\nserial.test(la_var2, lags.pt = 12, type = \"ES\")\n\n\n\n    Portmanteau Test (adjusted)\n\ndata:  Residuals of VAR object la_var2\nChi-squared = 162.35, df = 90, p-value = 4.602e-06\n\n\n    Breusch-Godfrey LM test\n\ndata:  Residuals of VAR object la_var2\nChi-squared = 151.85, df = 45, p-value = 1.656e-13\n\n\n    Edgerton-Shukur F test\n\ndata:  Residuals of VAR object la_var2\nF statistic = 3.6803, df1 = 45, df2 = 1429, p-value = 1.221e-14\n\n\nserial.test will check for portmanteau, and BG\n\n\n34.7.5 sparsevar\nSince VAR models can grow O(np^2), this is a high dimensional problem when we’re tracking many states. We can use sparsity constraints during fitting.\nThe theoretical properties of this estimator are studied by VAR\nThe SCAD penalty, and MCP penalty are used in this package\n\n\\begin{aligned}\n\\min_\\beta \\frac{1}{2} \\|y - X\\beta\\|^2_2 + p(\\beta)\n\\end{aligned}\n\n\n\nCode\nset.seed(1)\n\n# 5% of non-zero entries and a Toeplitz variance-covariance matrix with rho = 0.5.\nsim <- simulateVAR(N = 20, p = 2)\n\nfit <- fitVAR(sim$series, p = 2, threshold = TRUE)\nplotVAR(sim, fit) # recovered quite well\n\n\n\n\n\nThe top image is the true generating values of A in the vector autoregression model."
  },
  {
    "objectID": "timeseries/timeseries.html#example-ar1-with-observational-noise",
    "href": "timeseries/timeseries.html#example-ar1-with-observational-noise",
    "title": "34  Time Series",
    "section": "38.1 Example: AR(1) with observational noise",
    "text": "38.1 Example: AR(1) with observational noise\nThe state equation:\n\n\\begin{aligned}\n\\textbf{State Equation:}& \\\\\nx_t &= \\phi x_{t-1} + w_t \\\\\n\\textbf{Observation Equation:}& \\\\\ny_t &= x_t + v_t\n\\end{aligned}\n\nWhat is interesting about this case with a hierarchical data structure, is that we can show it has the same error structure as an ARMA model. Shumway Stoffer notes that even though it has the same parameterization as an ARMA model, it is often easier to think about the state model form. See example 6.3 for more details."
  },
  {
    "objectID": "timeseries/timeseries.html#kalman-filter",
    "href": "timeseries/timeseries.html#kalman-filter",
    "title": "34  Time Series",
    "section": "34.9 Kalman Filter",
    "text": "34.9 Kalman Filter\nKalman filter is a recursive, markovian updating algorithm for estimating a hidden state variable given noisy and partial observations. The common example is that we are tracking a truck by gps observations. Since the gps observations are imprecise, they will jump back and forth.\nAn excellent resource explanation with pictures is found from bzarg\n\n34.9.1 Example: Local level Model\nConsider the equations:\n\n\\begin{aligned}\n\\textbf{State Equation:}& \\\\\nx_t &= x_{t-1} + w_t \\\\\n\\textbf{Observation Equation:}& \\\\\ny_t &= x_t + v_t\n\\end{aligned}\n where both w_t, v_t \\sim N(0, 1). We can make the Kalman filter here based on our observation equation. We assume that A = 1 (in this case) and Phi = 1 are known here. In reality, we can use maximum likelihood of the innnovations (prediction errors) in order to estimate the\n\n\nCode\n# generate the data\nset.seed(1)\nn <-  50\nx0 <- rnorm(1) # random initial state\nw <- rnorm(n, 0, 1) # state level noise\nv = rnorm(n, 0, 1) # observation level noise\nx <- cumsum(c(x0, w)) # true state variables\ny <- x[-1] + v # remove initial state\nks <- Ksmooth0(num = 50, \n               y = y, \n               A = 1, \n               mu0 = 10, # set initial values\n               Sigma0 = 20, # set initial values\n               Phi = 1,\n               cQ = 1,\n               cR = 1)\n\n\n\n\nCode\n# plot all the predictions\npar(mfrow = c(3, 1),\n    mar = c(2, 4, 2, 4))\n# predictions\nplot(x[-1], main = \"Prediction\", ylim = c(-5, 10))\nlines(ks$xp[1,,])\nlines(ks$xp + 2*sqrt(ks$Pp[1,,]), lty = 2, col = 4)\nlines(ks$xp - 2*sqrt(ks$Pp[1,,]), lty = 2, col = 4) # variance matrices\n\n# filters\nplot(x[-1], main = \"Filters\", ylim = c(-5, 10), xlab = \"\")\nlines(ks$xf[1,,])\nlines(ks$xf + 2*sqrt(ks$Pf[1,,]), lty = 2, col = 4)\nlines(ks$xf - 2*sqrt(ks$Pf[1,,]), lty = 2, col = 4) # variance matrices\n\n\n# ks$xs[1,,] # smooth values\nplot(x[-1], main = \"Smooth\", ylim = c(-5, 10), xlab = \"\")\nlines(ks$xs[1,,])\nlines(ks$xs + 2*sqrt(ks$Ps[1,,]), lty = 2, col = 4)\nlines(ks$xs - 2*sqrt(ks$Ps[1,,]), lty = 2, col = 4) # variance matrices\n\n\n\n\n\n\n\nCode\n# creating table for comparison\ntibble(predict = ks$xp[1,,],\n       filter = ks$xf[1,,],\n       smooth = ks$xs[1,,],\n       predict_sd = sqrt(ks$Pp[1,,]),\n       filter_sd = sqrt(ks$Pf[1,,]),\n       smooth_sd = sqrt(ks$Ps[1,,])) %>%\n  mutate(across(predict:smooth, ~scales::number(.x, accuracy = .001)),\n         across(predict_sd:smooth_sd, ~scales::number(.x,accuracy = .01))) %>% \n  transmute(predict = glue(\"{predict} ({predict_sd})\"),\n         filter = glue(\"{filter} ({filter_sd})\"),\n         smooth = glue(\"{smooth} ({smooth_sd})\"))\n\n\n# A tibble: 50 × 3\n   predict       filter        smooth       \n   <glue>        <glue>        <glue>       \n 1 10.000 (4.58) -0.552 (0.98) -0.538 (0.77)\n 2 -0.552 (1.40) -0.807 (0.81) -0.524 (0.69)\n 3 -0.807 (1.29) -0.810 (0.79) -0.096 (0.67)\n 4 -0.810 (1.27) 0.978 (0.79)  1.048 (0.67) \n 5 0.978 (1.27)  1.490 (0.79)  1.161 (0.67) \n 6 1.490 (1.27)  0.536 (0.79)  0.628 (0.67) \n 7 0.536 (1.27)  0.209 (0.79)  0.778 (0.67) \n 8 0.209 (1.27)  1.438 (0.79)  1.699 (0.67) \n 9 1.438 (1.27)  1.283 (0.79)  2.123 (0.67) \n10 1.283 (1.27)  3.726 (0.79)  3.481 (0.67) \n# … with 40 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\n34.9.2 Estimation of State Parameters\nThere are bayesian methods for estimating the state parameters but also maximum likelihood w/ Newton Raphson or EM algorithmss\n\n\n34.9.3 Example: Nile\nAnnual flow of the river Nile from 1871 - 1970. There’s an apparent changepoint near 1898.\nI found this example going through all the state space model libraries. The orginal paper is called “JSS Journal of Statistical Software, State Space Models in R”.Specifically, we walk through the Nile example with 3 functions:\n\nstats::StructTS\ndlm:dlmMLE\nKFAS:kf\n\n\n\nCode\n# change point analyses\nts.plot(Nile)\n\n\n\n\n\n\nstats::StructTS\n\n\nCode\nnile_sts <- StructTS(Nile, \"level\")\n# nile_sts %>% tsdiag() # diagnostics of structural model\n\n# values from model.\ntibble(\n  times = time(Nile),\n  filtered = fitted(nile_sts)[,\"level\"], # filtered values\n  smoothed = tsSmooth(nile_sts)[,\"level\"]) # smoothed valuees\n\n\n# A tibble: 100 × 3\n   times filtered smoothed\n   <dbl>    <dbl>    <dbl>\n 1  1871    1120     1112.\n 2  1872    1141.    1111.\n 3  1873    1073.    1105.\n 4  1874    1117.    1114.\n 5  1875    1130.    1112.\n 6  1876    1138.    1107.\n 7  1877    1049.    1096.\n 8  1878    1098.    1112.\n 9  1879    1171.    1117.\n10  1880    1163.    1098.\n# … with 90 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\n# forecast values\npredict(nile_sts,n.ahead = 5)\n\n\n$pred\nTime Series:\nStart = 1971 \nEnd = 1975 \nFrequency = 1 \n[1] 798.3682 798.3682 798.3682 798.3682 798.3682\n\n$se\nTime Series:\nStart = 1971 \nEnd = 1975 \nFrequency = 1 \n[1] 143.5266 148.5564 153.4215 158.1370 162.7159\n\n\nCode\n# plotting forecast values, with package forecast\nplot(forecast::forecast(nile_sts, # StructST object\n                   level = c(50, 90), # CI levels\n                   h = 10), # periods of forecasting\n     xlim = c(1950, 1980))\n\n\n\n\n\n\n\ndlm::dlm\n\n\nCode\n# set up the dlm object\nnile_dlm_ll <- function(theta){\n  dlmModPoly(order = 1, dV = theta[1], dW = theta[2]) # fits local level model\n}\n\n# calls optim internally to optimize model (default BFGS)\n# could use numDeriv::hessian for numerically accurate evaluation of Hessians\nnile_dlm_mle <- dlmMLE(Nile, # data\n                   parm = c(100, 2), # initial parameters\n                   nile_dlm_ll, # model\n                   lower = rep(1e-4, 2)) \n\nnile_dlm_mle$par # similar variance parameters of local linear model\n\n\n[1] 15099.787  1468.438\n\n\n\n\nCode\nnile_dlm_ll_best <- nile_dlm_ll(nile_dlm_mle$par) # build model with best fit by optim\nW(nile_dlm_ll_best) # state randomnesss\n\n\n\\begin{bmatrix} 1468.43775833941 \\\\ \\end{bmatrix}\n\n\nCode\nV(nile_dlm_ll_best) # observation error variance\n\n\n\\begin{bmatrix} 15099.7867214723 \\\\ \\end{bmatrix}\n\n\nCode\n# can use fitted model to create smooth estimates now\n# $s has time series of smooth estimates\n# $U.S and $D.S have SVD of smoothing varriances (for std err)\nnile_dlm_ll_smooth <- dlmSmooth(Nile , nile_dlm_ll_best)\n# conf ints can be calculated by\n\n\n\n\nCode\n# calculate standard errors\nhwidth <- sqrt(unlist(dlmSvd2var(nile_dlm_ll_smooth$U.S, nile_dlm_ll_smooth$D.S))) * qnorm(0.025, lower = FALSE)\nnile_dlm_ll_smooth_ci <- cbind(nile_dlm_ll_smooth$s, as.vector(nile_dlm_ll_smooth$s) + hwidth %o% c(-1, 1))\nautoplot(nile_dlm_ll_smooth_ci) + theme(legend.position = \"\") +\n  labs(title = \"smoothed kalman with CI\") +\n  geom_point(data = tibble(time = time(Nile),\n                           flow = Nile,\n                           series = \"real\"),\n             mapping = aes(time, flow))\n\n\n\n\n\n\n\nKFAS::KFS\nKalman filtering/smoothing/simulation for linear state space models in the exponential family. KFAS uses the sequential processing method. This package uses a slightly different parameterization of their state space models.\n\n\\begin{aligned}\ny_t &= Z_t\\alpha_t + \\varepsilon_t &\\text{observation}\\\\\n\\alpha_{t+1} &= T_t \\alpha_t + R_t\\eta_t &\\text{transition}\n\\end{aligned}\n\n\n\nCode\n# build the local linear model\n# can initialize a specific model with values\n# nile_kfas_ll_model <- SSModel(Nile ~ SSMtrend(1, Q = 15000), # Q = Transition error\n#                               H = 30) # Observation error\n\n# add NA for values you want to optimize\nnile_kfas_ll_model <- SSModel(Nile ~ SSMtrend(1, Q = list(matrix(NA))), # Q = Transition error\n                              H = matrix(NA)) # Observation error\n\n\n# fit the model with wrapper to `optim`\n# -logLik(nile_kfas_ll_model) # is the objective that is optimized\nnile_kfas_ll_fit <- fitSSM(nile_kfas_ll_model,\n       c(log(var(Nile)), log(var(Nile))), # initial parameters for optim\n       method = \"BFGS\")\n\n# extract just the optimal model\nnile_kfas_ll <- nile_kfas_ll_fit$model\n\n# Filter/smooth\nnile_kfas_ll_smooth <- KFS(nile_kfas_ll,\n    filtering = \"state\",\n    smoothing = \"state\")\n\n# autoplot calls fortify, grabbing some components of the model\n# ggfortify:::autoplot.KFS\n# fortify(nile_kfas_ll_smooth)  # create df with: time, raw y, alphahat (smoothed values), raw residual = raw y - alphahat\n# nile_kfas_ll_smooth$alphahat # contains smoothed state variable estimates. for some reason \"fitted\" doesn't return these.\nautoplot(nile_kfas_ll_smooth)\n\n\n\n\n\n\n\nCode\n# these give same values\n# cbind(predict(nile_kfas_ll),\n      # nile_kfas_ll_smooth$alphahat)\n\nnile_kfas_ll_ci <- predict(nile_kfas_ll, interval = \"confidence\", level = .9)\nnile_kfas_ll_pi <- predict(nile_kfas_ll, interval = \"prediction\", level = .9)\n\n                                                                              \nlegend_labels <- c(\"lower_ci\", \"upper_ci\", \"fit\", \"lower_pi\", \"upper_pi\")\n# forecast:::autoplot.mts\nautoplot(cbind(nile_kfas_ll_ci[,-1], nile_kfas_ll_pi),\n         mapping = aes(x, y, group = series, linetype = series)) +\n  scale_linetype_manual(values = c(2, 2, 1, 3, 3), labels = legend_labels) +\n  scale_color_manual(values = c(2, 2, 1, 3, 3), labels = legend_labels) +\n  labs(y = \"Predicted Annual flow\", main = \"River Nile\")\n\n\n\n\n\n\n\nCode\n# forecasting\nnile_kfs_mean <- KFS(nile_kfas_ll,\n    filtering = c('state')) # specifying \"mean\" only estimates the smooths (alphahat)\n\ncbind(predict = nile_kfs_mean$a, # one step ahead prediction\n      filter = nile_kfs_mean$att, # filter estimates\n      smooth = nile_kfs_mean$alphahat) %>%  # smoothed\n  autoplot() + \n  coord_cartesian(ylim = c(700, 1250)) +\n  labs(title = \"predict/filter/smooth estiamtes of Nile with KFS\")"
  },
  {
    "objectID": "timeseries/timeseries.html#estimation-of-state-parameters",
    "href": "timeseries/timeseries.html#estimation-of-state-parameters",
    "title": "34  Time Series",
    "section": "38.3 Estimation of State Parameters",
    "text": "38.3 Estimation of State Parameters\nThere are bayesian methods for estimating the state parameters but also maximum likelihood w/ Newton Raphson or EM algorithmss"
  },
  {
    "objectID": "timeseries/timeseries.html#example-nile",
    "href": "timeseries/timeseries.html#example-nile",
    "title": "34  Time Series",
    "section": "38.4 Example: Nile",
    "text": "38.4 Example: Nile\nAnnual flow of the river Nile from 1871 - 1970. There’s an apparent changepoint near 1898.\nI found this example going through all the state space model libraries. The orginal paper is called “JSS Journal of Statistical Software, State Space Models in R”.Specifically, we walk through the Nile example with 3 functions:\n\nstats::StructTS\ndlm:dlmMLE\nKFAS:kf\n\n\n\nCode\n# change point analyses\nts.plot(Nile)\n\n\n\n\n\n\n38.4.1 stats::StructTS\n\n\nCode\nnile_sts <- StructTS(Nile, \"level\")\n# nile_sts %>% tsdiag() # diagnostics of structural model\n\n# values from model.\ntibble(\n  times = time(Nile),\n  filtered = fitted(nile_sts)[,\"level\"], # filtered values\n  smoothed = tsSmooth(nile_sts)[,\"level\"]) # smoothed valuees\n\n\n# A tibble: 100 × 3\n   times filtered smoothed\n   <dbl>    <dbl>    <dbl>\n 1  1871    1120     1112.\n 2  1872    1141.    1111.\n 3  1873    1073.    1105.\n 4  1874    1117.    1114.\n 5  1875    1130.    1112.\n 6  1876    1138.    1107.\n 7  1877    1049.    1096.\n 8  1878    1098.    1112.\n 9  1879    1171.    1117.\n10  1880    1163.    1098.\n# … with 90 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\nCode\n# forecast values\npredict(nile_sts,n.ahead = 5)\n\n\n$pred\nTime Series:\nStart = 1971 \nEnd = 1975 \nFrequency = 1 \n[1] 798.3682 798.3682 798.3682 798.3682 798.3682\n\n$se\nTime Series:\nStart = 1971 \nEnd = 1975 \nFrequency = 1 \n[1] 143.5266 148.5564 153.4215 158.1370 162.7159\n\n\nCode\n# plotting forecast values, with package forecast\nplot(forecast::forecast(nile_sts, # StructST object\n                   level = c(50, 90), # CI levels\n                   h = 10), # periods of forecasting\n     xlim = c(1950, 1980))\n\n\n\n\n\n\n\n38.4.2 dlm::dlm\n\n\nCode\n# set up the dlm object\nnile_dlm_ll <- function(theta){\n  dlmModPoly(order = 1, dV = theta[1], dW = theta[2]) # fits local level model\n}\n\n# calls optim internally to optimize model (default BFGS)\n# could use numDeriv::hessian for numerically accurate evaluation of Hessians\nnile_dlm_mle <- dlmMLE(Nile, # data\n                   parm = c(100, 2), # initial parameters\n                   nile_dlm_ll, # model\n                   lower = rep(1e-4, 2)) \n\nnile_dlm_mle$par # similar variance parameters of local linear model\n\n\n[1] 15099.787  1468.438\n\n\n\n\nCode\nnile_dlm_ll_best <- nile_dlm_ll(nile_dlm_mle$par) # build model with best fit by optim\nW(nile_dlm_ll_best) # state randomnesss\n\n\n         [,1]\n[1,] 1468.438\n\n\nCode\nV(nile_dlm_ll_best) # observation error variance\n\n\n         [,1]\n[1,] 15099.79\n\n\nCode\n# can use fitted model to create smooth estimates now\n# $s has time series of smooth estimates\n# $U.S and $D.S have SVD of smoothing varriances (for std err)\nnile_dlm_ll_smooth <- dlmSmooth(Nile , nile_dlm_ll_best)\n# conf ints can be calculated by\n\n\n\n\nCode\n# calculate standard errors\nhwidth <- sqrt(unlist(dlmSvd2var(nile_dlm_ll_smooth$U.S, nile_dlm_ll_smooth$D.S))) * qnorm(0.025, lower = FALSE)\nnile_dlm_ll_smooth_ci <- cbind(nile_dlm_ll_smooth$s, as.vector(nile_dlm_ll_smooth$s) + hwidth %o% c(-1, 1))\nautoplot(nile_dlm_ll_smooth_ci) + theme(legend.position = \"\") +\n  labs(title = \"smoothed kalman with CI\") +\n  geom_point(data = tibble(time = time(Nile),\n                           flow = Nile,\n                           series = \"real\"),\n             mapping = aes(time, flow))\n\n\n\n\n\n\n\n38.4.3 KFAS::KFS\nKalman filtering/smoothing/simulation for linear state space models in the exponential family. KFAS uses the sequential processing method. This package uses a slightly different parameterization of their state space models.\n\n\\begin{aligned}\ny_t &= Z_t\\alpha_t + \\varepsilon_t &\\text{observation}\\\\\n\\alpha_{t+1} &= T_t \\alpha_t + R_t\\eta_t &\\text{transition}\n\\end{aligned}\n\n\n\nCode\n# build the local linear model\n# can initialize a specific model with values\n# nile_kfas_ll_model <- SSModel(Nile ~ SSMtrend(1, Q = 15000), # Q = Transition error\n#                               H = 30) # Observation error\n\n# add NA for values you want to optimize\nnile_kfas_ll_model <- SSModel(Nile ~ SSMtrend(1, Q = list(matrix(NA))), # Q = Transition error\n                              H = matrix(NA)) # Observation error\n\n\n# fit the model with wrapper to `optim`\n# -logLik(nile_kfas_ll_model) # is the objective that is optimized\nnile_kfas_ll_fit <- fitSSM(nile_kfas_ll_model,\n       c(log(var(Nile)), log(var(Nile))), # initial parameters for optim\n       method = \"BFGS\")\n\n# extract just the optimal model\nnile_kfas_ll <- nile_kfas_ll_fit$model\n\n# Filter/smooth\nnile_kfas_ll_smooth <- KFS(nile_kfas_ll,\n    filtering = \"state\",\n    smoothing = \"state\")\n\n# autoplot calls fortify, grabbing some components of the model\n# ggfortify:::autoplot.KFS\n# fortify(nile_kfas_ll_smooth)  # create df with: time, raw y, alphahat (smoothed values), raw residual = raw y - alphahat\n# nile_kfas_ll_smooth$alphahat # contains smoothed state variable estimates. for some reason \"fitted\" doesn't return these.\nautoplot(nile_kfas_ll_smooth)\n\n\n\n\n\n\n\nCode\n# these give same values\n# cbind(predict(nile_kfas_ll),\n      # nile_kfas_ll_smooth$alphahat)\n\nnile_kfas_ll_ci <- predict(nile_kfas_ll, interval = \"confidence\", level = .9)\nnile_kfas_ll_pi <- predict(nile_kfas_ll, interval = \"prediction\", level = .9)\n\n\nlegend_labels <- c(\"lower_ci\", \"upper_ci\", \"fit\", \"lower_pi\", \"upper_pi\")\n# forecast:::autoplot.mts\nautoplot(cbind(nile_kfas_ll_ci[,-1], nile_kfas_ll_pi),\n         mapping = aes(x, y, group = series, linetype = series)) +\n  scale_linetype_manual(values = c(2, 2, 1, 3, 3), labels = legend_labels) +\n  scale_color_manual(values = c(2, 2, 1, 3, 3), labels = legend_labels) +\n  labs(y = \"Predicted Annual flow\", main = \"River Nile\") \n\n\n\n\n\n\n\nCode\n# forecasting\nnile_kfs_mean <- KFS(nile_kfas_ll,\n    filtering = c('state')) # specifying \"mean\" only estimates the smooths (alphahat)\n\ncbind(predict = nile_kfs_mean$a, # one step ahead prediction\n      filter = nile_kfs_mean$att, # filter estimates\n      smooth = nile_kfs_mean$alphahat) %>%  # smoothed\n  autoplot() + \n  coord_cartesian(ylim = c(700, 1250)) +\n  labs(title = \"predict/filter/smooth estiamtes of Nile with KFS\")"
  },
  {
    "objectID": "bootstrap/bootstrap.html#estimating-bias",
    "href": "bootstrap/bootstrap.html#estimating-bias",
    "title": "33  Bootstrap",
    "section": "33.1 Estimating Bias",
    "text": "33.1 Estimating Bias\n\n\\begin{aligned}\n\\text{Bias}_F(\\hat \\theta, \\theta) = E_F[\\hat \\theta] - \\theta\n\\end{aligned}\n\nWe note that \\hat\\theta = s(\\mathbf{x}) is function of the data \\hat\\theta = s(\\mathbf{x}), whereas \\theta = t(F) is a function of the underlying true distribution. In order to use bootstrap to estimate the bias of an estimator, we use average of bootstrap samples to estimate the expected value, while we use the empirical distribution to estimate the true parameter.\n\n33.1.1 Example (sample standard deviation)\n\n\nCode\nset.seed(1)\nx <- rnorm(30) # \"Empirical Distribution\" from normal distribution\nB <- 1000 # Number of bootstrap samples to take.\ngen_boot <- function(x) {\n  xb <- sample(x, replace = TRUE)\n  sum((xb - mean(xb))^2 / length(xb)) \n}\n\n# bootstrap expected value\nthetahat <- mean(replicate(B, gen_boot(x)))\ntheta <- sum((x - mean(x))^2 / length(x)) # Distributional \"real value\"\n\ncbind(thetahat - theta,\n      1/30) # Pretty close to theoretical \"bias\"\n\n\n           [,1]       [,2]\n[1,] -0.0298494 0.03333333\n\n\nI’m sure if you increase the number of replicates"
  },
  {
    "objectID": "causal/causal.html#propensity-scores",
    "href": "causal/causal.html#propensity-scores",
    "title": "34  Causal Statistics",
    "section": "34.1 Propensity Scores",
    "text": "34.1 Propensity Scores\nWhat are propensity scores? A propensity score is defined as the probability of a treatment assignment conditional on some background observed covariates.\nThis allows one to mimic some of the conditions of a randomized controlled trial. In a randomized control trial, one can be fairly certain that each group is balanced due to the law of large numbers.\n\n\\begin{aligned}\n\\Pr(T = 1 \\, | X=x )\n\\end{aligned}\n For more background information see Matching Methods."
  },
  {
    "objectID": "causal/causal.html#matching",
    "href": "causal/causal.html#matching",
    "title": "34  Causal Statistics",
    "section": "34.2 Matching",
    "text": "34.2 Matching\nThere are many ways to match:"
  },
  {
    "objectID": "causal/causal.html#r-implementation",
    "href": "causal/causal.html#r-implementation",
    "title": "34  Causal Statistics",
    "section": "34.3 R Implementation",
    "text": "34.3 R Implementation\nThe package MatchIt provides the function matchit to attempt matching of covariates automatically. There are many methods to matching covariates. The main page of the package can be found here. The package also uses Zelig for additional features."
  },
  {
    "objectID": "sensitivity/sensitivity.html",
    "href": "sensitivity/sensitivity.html",
    "title": "Sensitivity Analysis",
    "section": "",
    "text": "Code\nn <- 1000\nX1 <- data.frame(matrix(runif(8 * n), nrow = n))\nX2 <- data.frame(matrix(runif(8 * n), nrow = n))\n\n\n# sensitivity analysis\nx <- sobol(model = sobol.fun, X1 = X1, X2 = X2, order = 2, nboot = 100)\nprint(x)\n\n\n\nCall:\nsobol(model = sobol.fun, X1 = X1, X2 = X2, order = 2, nboot = 100)\n\nModel runs: 37000 \n\nSobol indices\n         original         bias std. error   min. c.i.  max. c.i.\nX1     0.71828456  0.002445438 0.05186076  0.61657267 0.83118519\nX2     0.11566517  0.008152420 0.06524513 -0.04495559 0.24780626\nX3     0.02119309  0.007832096 0.06405158 -0.11089865 0.14241266\nX4    -0.01045341  0.008627902 0.06551847 -0.14857049 0.11906131\nX5    -0.02044485  0.008093132 0.06421207 -0.16307713 0.09657454\nX6    -0.02169464  0.008265737 0.06414454 -0.16499003 0.09577416\nX7    -0.02070819  0.008243268 0.06420241 -0.16413294 0.09661017\nX8    -0.02119081  0.007987289 0.06423328 -0.16334437 0.09668928\nX1*X2  0.09555373 -0.007639882 0.07826432 -0.05118885 0.26557789\nX1*X3  0.03309802 -0.008589700 0.06637684 -0.08804008 0.17883732\nX1*X4  0.01956721 -0.009451668 0.06288286 -0.09443062 0.15993281\nX1*X5  0.02163437 -0.008280230 0.06413888 -0.09634617 0.16381146\nX1*X6  0.02087152 -0.008160821 0.06403248 -0.09647914 0.16346848\nX1*X7  0.02141592 -0.008132554 0.06404980 -0.09556315 0.16379275\nX1*X8  0.02234298 -0.008221968 0.06401672 -0.09424576 0.16467397\nX2*X3  0.03042921 -0.008913108 0.06571678 -0.09043714 0.17765966\nX2*X4  0.02043462 -0.008445121 0.06477597 -0.09754521 0.16774232\nX2*X5  0.02170404 -0.008148076 0.06407436 -0.09564903 0.16382602\nX2*X6  0.02100104 -0.008176939 0.06412990 -0.09665733 0.16347599\nX2*X7  0.02134876 -0.008124752 0.06409806 -0.09598199 0.16362810\nX2*X8  0.02221105 -0.008225145 0.06410079 -0.09509822 0.16434613\nX3*X4  0.02272969 -0.008321954 0.06443601 -0.09500249 0.16658228\nX3*X5  0.02182676 -0.008165834 0.06412735 -0.09575746 0.16409740\nX3*X6  0.02184411 -0.008152459 0.06409339 -0.09552783 0.16400958\nX3*X7  0.02173355 -0.008176563 0.06412144 -0.09544991 0.16402746\nX3*X8  0.02152713 -0.008169608 0.06411604 -0.09575735 0.16402055\nX4*X5  0.02166003 -0.008154140 0.06410582 -0.09569189 0.16403534\nX4*X6  0.02168870 -0.008173371 0.06412289 -0.09573416 0.16414538\nX4*X7  0.02184814 -0.008166242 0.06409887 -0.09558916 0.16414173\nX4*X8  0.02187975 -0.008164186 0.06410859 -0.09541363 0.16420010\nX5*X6  0.02170013 -0.008162384 0.06411030 -0.09565447 0.16405777\nX5*X7  0.02170264 -0.008162974 0.06410948 -0.09565215 0.16405030\nX5*X8  0.02170714 -0.008163759 0.06411016 -0.09565146 0.16406589\nX6*X7  0.02170093 -0.008162875 0.06411003 -0.09565642 0.16405360\nX6*X8  0.02169374 -0.008163356 0.06411094 -0.09566522 0.16405104\nX7*X8  0.02170280 -0.008163541 0.06410956 -0.09565216 0.16405102\n\n\nCode\n#plot(x)\n\nlibrary(ggplot2)\nggplot(x)\n\n\n\n\n\nCode\nplot(X1, sobol.fun(X1))"
  },
  {
    "objectID": "signal/signal.html#introduction-to-dsp",
    "href": "signal/signal.html#introduction-to-dsp",
    "title": "36  Signal Processing",
    "section": "36.1 Introduction to DSP",
    "text": "36.1 Introduction to DSP\nJeff Doser’s R Programming for Data Science\n\n\nCode\nx <- seq(0, 2 * pi, .1)\nstandard <- sin(x)\naltered <- 2 * sin(3 * (x-1)) + 4\ngraphData <- tibble(x, standard, altered)\nggplot(data = graphData, mapping = aes(x = x)) +\n  geom_point(aes(y = standard, color = \"sin(x)\")) +\n  geom_point(aes(y = altered, color = \"2*sin(3 * (x - 1)) + 4\"))\n\n\n\n\n\n\n\nCode\nt <- seq(0, 1, len = 100)\n\n# consider the signals\nsig <- sin(2 * pi * t)\nnoisySig <- sin(2*pi*t) + .25 * rnorm(length(t))\nggplot() + \n  geom_line(mapping = aes(t, sig)) +\n  geom_line(mapping = aes(t, noisySig), color = \"red\")\n\n\n\n\n\n\n\nCode\n# butter(filter order, critical frequency 0<w<1)\n# w is like the bandwidth\nbutterFilter <- butter(2, .1)\nrecoveredSig <- signal::filter(butterFilter, noisySig)\nallSignals <- data.frame(t, sig, noisySig, recoveredSig)\n\nallSignals %>% ggplot(aes(t)) +\n  geom_line(aes(y = sig, color = \"Original\")) +\n  geom_line(aes(y = noisySig, color = \"noisy\")) +\n  geom_line(aes(y = recoveredSig, color = \"Recovered\")) +\n  labs(x = \"Time\", y = \"Signal\")"
  },
  {
    "objectID": "signal/signal.html#savitzky-golay-smoother",
    "href": "signal/signal.html#savitzky-golay-smoother",
    "title": "36  Signal Processing",
    "section": "36.2 Savitzky-Golay Smoother",
    "text": "36.2 Savitzky-Golay Smoother\n\n\nCode\n# discrete signal\nt <- 1:40\nsignal <- c(rep(0,15), rep(10, 10), rep(0, 15))\n\n\n\n\nCode\n# shiny app for interactive\nui <- fluidPage(\n  titlePanel(\"Interactive sgolayfilt\"),\n  sliderInput(\"sg_p\", \"p\",min = 3, max = 10, value = 3),\n  sliderInput(\"sg_n\", \"n\",min = 5, max = 21, value = 7, step = 2),\n  plotOutput(\"plot\")\n)\nserver <- function(input, output){\n  sg <- reactive({sgolayfilt(x, input$sg_p, input$sg_n)})\n  output$plot <- renderPlot({\n    ggplot(mapping = aes(x = t)) + geom_point(aes(y = signal)) +\n      geom_line(aes(y = sg()))\n  }, res = 96)\n  output$product <- renderText({\n    input$sg_p\n  })\n}\n\nshinyApp(ui = ui, server = server)\n\n\n\n\nCode\nbf <- butter(5,1/3) # butter filter\nsg <- sgolayfilt(x) # sgolayfilt\nplot(sg, type=\"l\")\nlines(filtfilt(rep(1, 5)/5,1,x), col = \"red\") # averaging filter\nlines(filtfilt(bf,x), col = \"blue\")   # butterworth\npoints(x, pch = \"x\")        # original data"
  },
  {
    "objectID": "color/color.html#viewing-color-palettes",
    "href": "color/color.html#viewing-color-palettes",
    "title": "Appendix A — Colors in R",
    "section": "A.1 Viewing Color Palettes",
    "text": "A.1 Viewing Color Palettes\n\n\nCode\nhcl.pals(type = \"sequential\")\n\n\n [1] \"Grays\"         \"Light Grays\"   \"Blues 2\"       \"Blues 3\"      \n [5] \"Purples 2\"     \"Purples 3\"     \"Reds 2\"        \"Reds 3\"       \n [9] \"Greens 2\"      \"Greens 3\"      \"Oslo\"          \"Purple-Blue\"  \n[13] \"Red-Purple\"    \"Red-Blue\"      \"Purple-Orange\" \"Purple-Yellow\"\n[17] \"Blue-Yellow\"   \"Green-Yellow\"  \"Red-Yellow\"    \"Heat\"         \n[21] \"Heat 2\"        \"Terrain\"       \"Terrain 2\"     \"Viridis\"      \n[25] \"Plasma\"        \"Inferno\"       \"Rocket\"        \"Mako\"         \n[29] \"Dark Mint\"     \"Mint\"          \"BluGrn\"        \"Teal\"         \n[33] \"TealGrn\"       \"Emrld\"         \"BluYl\"         \"ag_GrnYl\"     \n[37] \"Peach\"         \"PinkYl\"        \"Burg\"          \"BurgYl\"       \n[41] \"RedOr\"         \"OrYel\"         \"Purp\"          \"PurpOr\"       \n[45] \"Sunset\"        \"Magenta\"       \"SunsetDark\"    \"ag_Sunset\"    \n[49] \"BrwnYl\"        \"YlOrRd\"        \"YlOrBr\"        \"OrRd\"         \n[53] \"Oranges\"       \"YlGn\"          \"YlGnBu\"        \"Reds\"         \n[57] \"RdPu\"          \"PuRd\"          \"Purples\"       \"PuBuGn\"       \n[61] \"PuBu\"          \"Greens\"        \"BuGn\"          \"GnBu\"         \n[65] \"BuPu\"          \"Blues\"         \"Lajolla\"       \"Turku\"        \n[69] \"Hawaii\"        \"Batlow\"       \n\n\nCode\nhcl.colors(256, palette = \"ag_sunset\")\n\n\n  [1] \"#4B1D91\" \"#4D1C92\" \"#4F1C92\" \"#511B92\" \"#531B93\" \"#551B93\" \"#571A93\"\n  [8] \"#591A94\" \"#5B1A94\" \"#5C1994\" \"#5E1995\" \"#601895\" \"#611895\" \"#631895\"\n [15] \"#651796\" \"#661796\" \"#681696\" \"#6A1697\" \"#6B1697\" \"#6D1597\" \"#6E1597\"\n [22] \"#701598\" \"#711498\" \"#731498\" \"#741498\" \"#761399\" \"#771399\" \"#791399\"\n [29] \"#7A1299\" \"#7C1299\" \"#7D129A\" \"#7F119A\" \"#80119A\" \"#81119A\" \"#83109A\"\n [36] \"#84109B\" \"#86109B\" \"#87109B\" \"#88109B\" \"#8A109B\" \"#8B0F9B\" \"#8C0F9B\"\n [43] \"#8E0F9C\" \"#8F0F9C\" \"#900F9C\" \"#910F9C\" \"#930F9C\" \"#940F9C\" \"#950F9C\"\n [50] \"#960F9C\" \"#980F9C\" \"#990F9C\" \"#9A109C\" \"#9B109C\" \"#9D109C\" \"#9E109D\"\n [57] \"#9F119D\" \"#A0119D\" \"#A1119D\" \"#A3119D\" \"#A4129D\" \"#A5129D\" \"#A6139D\"\n [64] \"#A7139C\" \"#A9149C\" \"#AA149C\" \"#AB159C\" \"#AC159C\" \"#AD169C\" \"#AE169C\"\n [71] \"#AF179C\" \"#B0179C\" \"#B2189C\" \"#B3199C\" \"#B4199C\" \"#B51A9C\" \"#B61B9B\"\n [78] \"#B71B9B\" \"#B81C9B\" \"#B91D9B\" \"#BA1D9B\" \"#BB1E9B\" \"#BC1F9A\" \"#BD209A\"\n [85] \"#BE209A\" \"#BF219A\" \"#C0229A\" \"#C12399\" \"#C22499\" \"#C32499\" \"#C42599\"\n [92] \"#C52698\" \"#C62798\" \"#C72898\" \"#C82897\" \"#C92997\" \"#CA2A97\" \"#CB2B96\"\n [99] \"#CC2C96\" \"#CD2D96\" \"#CE2E95\" \"#CF2E95\" \"#D02F95\" \"#D13094\" \"#D23194\"\n[106] \"#D23293\" \"#D33393\" \"#D43493\" \"#D53592\" \"#D63692\" \"#D73691\" \"#D83791\"\n[113] \"#D93890\" \"#D93990\" \"#DA3A8F\" \"#DB3B8F\" \"#DC3C8E\" \"#DD3D8E\" \"#DE3E8D\"\n[120] \"#DE3F8D\" \"#DF408C\" \"#E0418B\" \"#E1428B\" \"#E2438A\" \"#E24489\" \"#E34589\"\n[127] \"#E44588\" \"#E54687\" \"#E54787\" \"#E64886\" \"#E74985\" \"#E84A85\" \"#E84B84\"\n[134] \"#E94C83\" \"#EA4D82\" \"#EB4E82\" \"#EB4F81\" \"#EC5080\" \"#EC527F\" \"#EC537E\"\n[141] \"#ED557E\" \"#ED567D\" \"#ED587C\" \"#ED597B\" \"#EE5A7B\" \"#EE5C7A\" \"#EE5D79\"\n[148] \"#EE5E78\" \"#EE6078\" \"#EF6177\" \"#EF6276\" \"#EF6475\" \"#EF6575\" \"#EF6674\"\n[155] \"#F06873\" \"#F06972\" \"#F06A72\" \"#F06B71\" \"#F06D70\" \"#F16E6F\" \"#F16F6F\"\n[162] \"#F1706E\" \"#F1716D\" \"#F1736D\" \"#F1746C\" \"#F1756B\" \"#F2766B\" \"#F2776A\"\n[169] \"#F27969\" \"#F27A69\" \"#F27B68\" \"#F27C68\" \"#F27D67\" \"#F27F66\" \"#F28066\"\n[176] \"#F28165\" \"#F28265\" \"#F38364\" \"#F38464\" \"#F38563\" \"#F38763\" \"#F38862\"\n[183] \"#F38962\" \"#F38A61\" \"#F38B61\" \"#F38C61\" \"#F38D60\" \"#F38E60\" \"#F38F5F\"\n[190] \"#F3915F\" \"#F3925F\" \"#F3935F\" \"#F3945E\" \"#F3955E\" \"#F3965E\" \"#F3975E\"\n[197] \"#F3985E\" \"#F3995E\" \"#F39A5E\" \"#F39B5D\" \"#F39D5D\" \"#F39E5D\" \"#F29F5D\"\n[204] \"#F2A05E\" \"#F2A15E\" \"#F2A25E\" \"#F2A35E\" \"#F2A45E\" \"#F2A55E\" \"#F2A65F\"\n[211] \"#F2A75F\" \"#F2A85F\" \"#F1A960\" \"#F1AA60\" \"#F1AB61\" \"#F1AC61\" \"#F1AD62\"\n[218] \"#F1AE62\" \"#F1AF63\" \"#F1B063\" \"#F0B164\" \"#F0B265\" \"#F0B465\" \"#F0B566\"\n[225] \"#F0B667\" \"#F0B768\" \"#EFB869\" \"#EFB96A\" \"#EFBA6A\" \"#EFBB6B\" \"#EFBC6C\"\n[232] \"#EEBD6E\" \"#EEBE6F\" \"#EEBE70\" \"#EEBF71\" \"#EDC072\" \"#EDC173\" \"#EDC275\"\n[239] \"#EDC376\" \"#EDC477\" \"#ECC579\" \"#ECC67A\" \"#ECC77C\" \"#ECC87D\" \"#EBC97F\"\n[246] \"#EBCA81\" \"#EBCB82\" \"#EACC84\" \"#EACD86\" \"#EACE88\" \"#EACF8A\" \"#E9D08C\"\n[253] \"#E9D18F\" \"#E8D191\" \"#E8D295\" \"#E7D39A\"\n\n\nCode\nplot(0, \n     type = \"n\",\n     axes = F,\n     xlab = \"\", ylab = \"\",\n     xlim = c(0, 1),\n     ylim = c(0, 1),\n     xaxs = \"i\",\n     yaxs = \"i\",\n     omi = rep(0, 4),\n     oma = rep(0, 4),\n     mar = rep(0, 4),\n     frame.plot = F)\nrect(0, .5, .8, 1, col = terrain.colors(256))\n\n\n\n\n\n\n\nCode\ngrid.raster(seq(0, 1, .01))\n\n\n\n\n\n\n\nCode\n# you can color the matrix directly as well, and generate the raster that way\nredGradient <- matrix(hcl(0, 80, seq(50, 80, 10)),\n                      nrow=4, ncol=5)\n\ngrid.raster(redGradient)"
  },
  {
    "objectID": "stochastic/ctmc.html#r-package-overview",
    "href": "stochastic/ctmc.html#r-package-overview",
    "title": "32  CTMC",
    "section": "32.1 R Package Overview",
    "text": "32.1 R Package Overview\n\n32.1.1 Adaptive Tau\nThere are 3 functions featured in in adaptive tau\n\nssa.exact - the gillespie algorithm for exact transitions\nssa.adaptivetau - implements the Cao adaptive tau algorithm in C++\nssa.maketrans - create transition matrix\n\n\n\nCode\nlvrates <- function(x, params, t) {\n  with(params, {\n    return(c(preygrowth*x[\"prey\"],      ## prey growth rate\n             x[\"prey\"]*x[\"pred\"]*eat,   ## prey death / predator growth rate\n             x[\"pred\"]*preddeath))      ## predator death rate\n  })\n}\nparams=list(preygrowth=10, eat=0.01, preddeath=10);\nr=ssa.exact(c(prey = 1000, pred = 500),\n            matrix(c(1,0, -2,1, 0,-1), nrow=2), lvrates, params, tf=2)\n\nmatplot(r[,\"time\"], r[,c(\"prey\",\"pred\")], type='l', xlab='Time', ylab='Counts')\nlegend(\"topleft\", legend=c(\"prey\", \"predator\"), lty=1:2, col=1:2)\n\n\n\n\n\n\n\n32.1.2 GillespieSSA2\nGillespieSSA2 is an improvement on the original that runs many times faster for simulation. It is also a method for simulating CTMC.\n\n\nCode\ninitial_state <- c(prey = 1000, predators = 1000)\nparams <- c(c1 = 10, c2 = 0.01, c3 = 10)\nreactions <- list(\n  #        propensity function     effects                       name for reaction\n  reaction(~c1 * prey,             c(prey = +1),                 \"prey_up\"),\n  reaction(~c2 * prey * predators, c(prey = -1, predators = +1), \"predation\"),\n  reaction(~c3 * predators,        c(predators = -1),            \"pred_down\")\n)\n\nout <-\n  ssa(\n    initial_state = initial_state,\n    reactions = reactions,\n    params = params,\n    method = ssa_exact(),\n    final_time = 5,\n    census_interval = .001,\n    verbose = TRUE\n  )\n\n\nRunning SSA exact with console output every 1 seconds\nwalltime: 0, sim_time: 0\nSSA finished!\n\n\nCode\nplot_ssa(out)"
  },
  {
    "objectID": "epidemiology/epidemiology.html#epimodel",
    "href": "epidemiology/epidemiology.html#epimodel",
    "title": "10  Epidemiology",
    "section": "10.2 EpiModel",
    "text": "10.2 EpiModel\nThis section is walking through the EpiModel Tutorial\nLargely, the package is split into\n\nDeterministic Compartmental Models (DCM) : macroview\nIndividual Contact Models (ICM) : microscopic analogues to DCMS\nNetwork Models : dynamically changing models here\n\n\n10.2.1 First DCM Model\nThe model is\n\n\\begin{aligned}\n\\frac{dS}{dt} &= -\\frac{\\beta c I}{N}S \\\\\n\\frac{dI}{dt} &= \\frac{\\beta c I}{N}S\n\\end{aligned}\n\nwhere \\beta and c are the infection and contact rate. This is a logistic differential equation\n\n\nCode\nparam <- param.dcm(inf.prob = .2, act.rate = .25) # infection rate, and contact rate\ninit <- init.dcm(s.num = 500, i.num = 1) # initial population\ncontrol <- control.dcm(type = \"SI\", nsteps = 500) # SI, SIS, SIR\n\n# dcm fitting uses deSolve\nmod <- dcm(param, init, control) # class dcm\nsummary(mod, at = 200) # summary at specific time\n\n\nEpiModel Summary\n=======================\nModel class: dcm\n\nSimulation Summary\n-----------------------\nModel type: SI\nNo. runs: 1\nNo. time steps:\nNo. groups: 1\n\nModel Statistics\n------------------------------\nTime: 200    Run: 1 \n------------------------------ \n                n    pct\nSuscept.   11.677  0.023\nInfect.   489.323  0.977\nTotal     501.000  1.000\nS -> I      0.557     NA\n------------------------------ \n\n\n\n\nCode\n# Exact solution of SI model is a logistic function\nsi_init <- binomial()$linkfun(500/501) # initial constant (intercept)\nbeta <- param$inf.prob * param$act.rate # .05 in this example\neta <- 0:499 * beta - si_init\nexact_result <- binomial()$linkinv(eta) * 501\n\nas.data.frame(mod) |> add_column(exact.i = exact_result) |> \n  pivot_longer(c(\"s.num\", \"i.num\", \"exact.i\")) |> \n  ggplot(aes(x = time, y = value, color = name, linetype = name)) +\n  geom_line() +\n  scale_linetype_manual(values = c(4, 2, 1)) +\n  labs(title = \"Solutions of SI model\")\n\n\n\n\n\nIt’s clear that the model matches with our analytical solution.\n\n\n10.2.2 SIR w/ Vital dynamics\nWe can also model leaving rates to all S, I, and R, as well as a birth rate into S.\n\n\nCode\nparam <- param.dcm(inf.prob = 0.2, act.rate = 1, rec.rate = 1/20,\n                   a.rate = 1/95, ds.rate = 1/100, di.rate = 1/80, dr.rate = 1/100)\ninit <- init.dcm(s.num = 1000, i.num = 1, r.num = 0)\ncontrol <- control.dcm(type = \"SIR\", nsteps = 500, dt = 0.5)\n\n# model\nmod <- dcm(param, init, control)\n\ncomp_plot(mod, at = 50, digits = 1) # summary of the model at particular moment\n\n\n\n\n\n\n\nCode\npar(mar = c(3.2, 3, 2, 1), mgp = c(2, 1, 0), mfrow = c(1, 2))\nplot(mod, popfrac = FALSE, alpha = 0.5,\n     lwd = 4, main = \"Compartment Sizes\")\nplot(mod, y = \"si.flow\", lwd = 4, col = \"firebrick\",\n     main = \"Disease Incidence\", legend = \"n\")\n\n\n\n\n\nThe tutorial ends by stating multiple parameters, and multiple groups can be modeled."
  },
  {
    "objectID": "stochastic/ctmc.html#exact-cmtc-network-sis",
    "href": "stochastic/ctmc.html#exact-cmtc-network-sis",
    "title": "32  CTMC",
    "section": "32.2 Exact CMTC Network SIS",
    "text": "32.2 Exact CMTC Network SIS\nThis section recreates the figures from the mieghan paper\n\n\nCode\n# N <- 3\n# create adjacency graph of line graph\nline_adj <- function(N) {\n  toeplitz(c(0, 1, rep(0, N-2)))\n}\n\ncomplete_adj <- function(N) {\n  A <- matrix(1, N, N)\n  diag(A) <- 0\n  A\n}\n\nsis_make_Q <- function(A, delta, beta) {\n  if (nrow(A) > 10) rlang::abort(\"please dont\")\n  N <- nrow(A)\n  num_states <- 2^N\n  states <- matrix(as.integer(intToBits(0:(num_states - 1))), nrow = 32)[N:1,]\n  num_I <- states |> colSums()\n  chr_states <- apply(MARGIN = 2, states, FUN = {\\(x) paste0(x, collapse = \"\")})\n  possible_trans <- which(adist(chr_states) == 1, arr.ind = TRUE)\n  \n  assign_rate <- function(x, delta, beta, num_I){\n    if (num_I[x[1]] > num_I[x[2]]) return(delta)\n    else {\n      threat_node <- which((states[, x[2]] - states[,x[1]]) == 1)\n      (states[,x[1]] %*% A)[threat_node] * beta\n    }\n  }\n  \n  trans_rate <- apply(possible_trans,\n                      MARGIN = 1,\n                      FUN = assign_rate,\n                      delta = delta, beta = beta, num_I = num_I)\n  Q_el <- cbind(possible_trans,\n      trans_rate)\n  Q <- matrix(0, num_states, num_states)\n  Q[Q_el[,1:2 ]] <- Q_el[,3]\n  # set diags\n  diag(Q) <- -rowSums(Q)\n  Q\n}\n\n# Figure 5\n# 4 largest eigenvalues of complete graph\n\n# top 4 eigen values\nQ_top_4 <- function(tau, A) {\n  delta <- 5e-3\n  beta <- tau * delta\n  Q <- sis_make_Q(A, delta, beta)\n  sort(Re(eigen(Q)$values), decreasing = T)[1:4]\n}\n\n# Plot figure -------------------------------------------------------------\n\ntaus <- seq(.01, .5, .02) # parameter\n\nA <- complete_adj(5)\nK5 <- taus |> map(Q_top_4, A = A)\nK5_mat <- do.call(rbind, K5)\nK5_tib <- K5_mat |> as_tibble(.name_repair = ~as.character(1:4)) |> \n  add_column(N= 5, taus = taus, .before = 1)\n\nA <- complete_adj(8)\nK8 <- taus |> map(Q_top_4, A = A)\nK8_mat <- Re(do.call(rbind, K8)) # real values\nK8_tib <- K8_mat |> as_tibble(.name_repair = ~as.character(1:4)) |> \n  add_column(N= 8,taus = taus, .before = 1)\n\nA <- complete_adj(10)\nK10 <- taus |> map(Q_top_4, A = A)\nK10_mat <- do.call(rbind, K10)\nK10_tib <- K10_mat |> as_tibble(.name_repair = ~as.character(1:4)) |> \n  add_column(N= 10, taus = taus, .before = 1)\n\nbind_rows(K5_tib, K8_tib, K10_tib) |> \n  pivot_longer(cols = 3:6, names_to = \"rank\")|> \n  ggplot(aes(x = taus, y = value, group = interaction(N, rank), color = factor(N))) +\n  geom_line() +\n  labs(x = \"Tau\",\n       y = \"Eigenvalue\",\n       color = \"N\") +\n  facet_wrap(~N) +\n  theme_test()\n\n\n\n\n\n\n\nCode\n# Figure 7\ntaus <- c(0.05, .1, .15, .2, .25, .3, .4, .5, .6, .7, 1, 2)\n\nby_size <- function(N) {\n  A <- line_adj(N)\n  Q_2 <- function(tau) {\n    delta <- 5e-3\n    beta <- delta * tau\n    Q <- sis_make_Q(A, delta, beta)\n    sort(Re(eigen(Q, only.values = TRUE)$values), decreasing = T)[2]\n  }\n  taus |> map_dbl(Q_2)\n}\n\n\nNs <- 2:10\n\nline_2 <- Ns |> map(by_size) |> as_tibble(.name_repair = ~Ns |> as.character())\nspectral_line <- 2:10 |> map_dbl(~1/max(abs(eigen(line_adj(.x))$values))) # spectral radius\n\ng1 <- line_2 |> add_column(tau = taus, .before = 1) |> \n  pivot_longer(cols = -1, names_to = \"N\", values_to = \"L2\") |> \n  ggplot(aes(x = as.integer(N), y = L2, color = tau, group = tau)) +\n  geom_line() + \n  theme(legend.position = \"none\")\n\ng2 <- qplot(x = 2:10, spectral_line, xlab = \"N\", ylab = \"crit. tau\")  + geom_line()\n\nplot_grid(g1, g2, labels = \"auto\")\n\n\n\n\n\nCool, we’ve replicated the two main plots in the papers\n\n\nCode\n# bifurcation on line graphs?\ntaus <- seq(.01, .5, .02) # parameter\n\nA <- line_adj(5)\nL5 <- taus |> map(Q_top_4, A = A)\nL5_mat <- do.call(rbind, L5)\nL5_tib <- L5_mat |> as_tibble(.name_repair = ~as.character(1:4)) |> \n  add_column(N= 5, taus = taus, .before = 1)\n\nA <- line_adj(8)\nL8 <- taus |> map(Q_top_4, A = A)\nL8_mat <- Re(do.call(rbind, L8)) # real values\nL8_tib <- L8_mat |> as_tibble(.name_repair = ~as.character(1:4)) |> \n  add_column(N= 8,taus = taus, .before = 1)\n\nA <- line_adj(10)\nL10 <- taus |> map(Q_top_4, A = A)\nL10_mat <- do.call(rbind, L10)\nL10_tib <- L10_mat |> as_tibble(.name_repair = ~as.character(1:4)) |> \n  add_column(N= 10, taus = taus, .before = 1)\n\nbind_rows(L5_tib, L8_tib, L10_tib) |> \n  pivot_longer(cols = 3:6, names_to = \"rank\")|> \n  ggplot(aes(x = taus, y = value, group = interaction(N, rank), color = factor(N))) +\n  geom_line() +\n  labs(x = \"Tau\",\n       y = \"Eigenvalue\",\n       color = \"N\") +\n  facet_wrap(~N) +\n  theme_test()\n\n\n\n\n\nIt doesn’t seem like there’s anything here, but what about .5 to 2? It seems there’s no bifurcating patterns on these graphs because we can already see the top 4 values."
  },
  {
    "objectID": "chaos/chaos.html#lyapanov-exponent",
    "href": "chaos/chaos.html#lyapanov-exponent",
    "title": "7  Chaos",
    "section": "7.1 Lyapanov exponent",
    "text": "7.1 Lyapanov exponent\nGenerally measures the overall sensitivity to initial conditions. Suppose we start two trajectories, x and y and let \\delta(0) = x(0)y(0)\n\n\nCode\nlogistic_map <- function(x0, n, r) {\n  x <- vector(mode = \"numeric\", length = n + 1)\n  x[1] <- x0\n  logistic <- function(x) r * x*(1 - x)\n  for (i in 1:n) {\n    x[i+1] <- logistic(x[i])\n  }\n  # lya only if n > 200\n  \n  lya <- ifelse(n > 200,\n                mean(log(abs(r - 2 * r * x[100:length(x)]))),\n                NA)\n  structure(list(x = x, lya = lya, n = n, r = r),\n            class = \"chaos\")\n}\n\nx <- logistic_map(.1, 500, 3.5)\n\nplot.chaos <- function(chaos_obj) {\n  x <- chaos_obj$x\n  r <- chaos_obj$r\n  plot(c(0, 1), c(0, 1), type = \"n\",\n       xlab = \"\",\n       ylab = \"\")\n  \n  abline(a = 0, b = 1, lty = 2)\n  \n  y <- seq(0,1, .01)\n  lines(y, r * y * (1 - y))\n  segments(x[1], 0, x[1], x[2])\n  segments(x[1], x[2], x[2], x[2])\n  \n  s <- seq(2, length(x) - 2)\n  alphas <- exp(seq(0, -5, length.out = length(s)))\n  segments(x[s], x[s], x[s], x[s+1], col = alpha(\"black\", alpha = alphas))\n  segments(x[s], x[s+1], x[s+1], x[s+1], col = alpha(\"black\", alpha = alphas))\n}\nplot(x)\n\n\n\n\n\n\n\nCode\nrs <- seq(0, 4, .0001)\nxn <- rs |> map_dbl(~logistic_map(runif(1, .1, .9), 100, .x)$x[101])\nplot(rs, xn, pch = \".\", xlab = \"r\", ylab = \"x\", main = \"Bifurcation of Logistic Map\")\n\n\n\n\n\nWe’ll try to look at the Lyaponov plot here.\n\n\nCode\nrs <- seq(0, 4.5, .01)\nlyas <- rs |> map_dbl(~logistic_map(.1, 500, .x)$lya)\n\nplot(rs, lyas, pch = \".\", cex = 2, main = \"Lyaponov Exponent Plot\",xlab = \"r\", ylab = \"Lyaponov Exponent\")\nabline(h = 0, col = 2, lty = 2)"
  },
  {
    "objectID": "timeseries/timeseries.html#introduction",
    "href": "timeseries/timeseries.html#introduction",
    "title": "34  Time Series",
    "section": "34.1 Introduction",
    "text": "34.1 Introduction\n\n34.1.1 Types of Models\n\nAutoregression (AR)\nMoving Average (AM)\nVector Autoregression (VAR)\nHMM\n\nrelated to bayesian inference, special\n\nState Space Models\n\nvery broad space of models that includes arima models, and dynamic linear models. Can also account for linear/nonlinear gaussian/non-gaussian errors\n\nMultivariate autoregression (MAR)\n\n\n\n34.1.2 Software\n\nstats\n\nKalmanLike, KalmanRun, KalmanSmooth, KalmanForecast\n\ndlm\nKFAS\n\nSee also State Space Models in R.\n\n\n34.1.3 Theory\nCourses\n\nKevin Kotzé’s Time Series Analysis Course\n\nhas a great overview and description, and packaged R commands for univariate and multivariate from an economics perspective."
  },
  {
    "objectID": "timeseries/timeseries.html#univariate-time-series",
    "href": "timeseries/timeseries.html#univariate-time-series",
    "title": "34  Time Series",
    "section": "34.2 Univariate time series",
    "text": "34.2 Univariate time series\nThe general pattern for univariate time series analysis is as follows\n\nfigure out how to make stationary\n\n\nby trend modeling (exogenous), or differencing\n“Breusch-Godfrey” : up to p order ar\n“Durbin-Watson” : first order ar testing\n\n\n\n\n\n34.2.1 base tools\n\n\nCode\n# acf, ccf, pacf, arima, arima.sim\n\n\n\n\n34.2.2 AR(1)\nAutoregressive of order 1 is the simplest time series you can have.\n\n\\begin{aligned}\nY_t = Y_{t-1} + \\varepsilon_t\n\\end{aligned}\n\n\n\n34.2.3 Simulated Examples\nthis section we’ll simulate a number of AR and MA models so that you get a better sense of what they look like.\n\n\nCode\n# AR models\nsim_ar <- tibble(p = 1:3,\n                 ar = list(.7,\n                           c(.7,-.3),\n                           c(.7, -.3, .5)),\n                 n = 200) %>% \n  rowwise() %>% \n  mutate(y = list(arima.sim(list(ar = ar), n = 200)),\n         x = list(time(y)))\n\nsim_ar %>% unnest(c(x, y)) %>% \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  facet_grid(p~.)\n\n\n\n\n\nAR Model Simulations\n\n\n\n\n\n\nCode\n# MA processes\nsim_ma <- tibble(q = 1:3,\n                 ma = list(.7,\n                           c(.7,-.3),\n                           c(.7, -.3, .5)),\n                 n = 200) %>% \n  rowwise() %>% \n  mutate(y = list(arima.sim(list(ma = ma), n = 200)),\n         x = list(time(y))) %>% \n  ungroup()\n\nsim_ma %>% unnest(c(x, y)) %>% \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  facet_grid(q~.)\n\n\n\n\n\nMA Model Simulations\n\n\n\n\n\n\n34.2.4 Analyzing simulations\n\n\nCode\n# ar1\nset.seed(1)\nar1 <- arima.sim(list(order = c(1, 0, 0), ar = .2), 100)\n\ninvisible(capture.output(\n  mod_sar1 <- sarima(ar1, p = 1, d = 0, q = 0) # asta\n  ))\n\n\n\n\n\nCode\nmod_Ar1 <- Arima(ar1, order = c(1, 0, 0)) # forecast\nmod_ar1 <- arima(ar1, order = c(1, 0, 0)) # base\n\nbind_rows(\n  tidy(mod_Ar1) |> add_column(model = \"forecast::Arima\", .before = 1),\n  tidy(mod_ar1) |> add_column(model = \"stats::arima\", .before = 1),\n  mod_sar1$ttable[, 1:2] |> \n  as_tibble(rownames = \"term\", .name_repair = ~c(\"estimate\", \"std.error\")) |>\n  add_column(model = \"astsa::sarima\", .before = 1)\n) |> arrange(desc(term), model) |> \n  kable() |> kable_minimal()\n\n\n\n\n \n  \n    model \n    term \n    estimate \n    std.error \n  \n \n\n  \n    astsa::sarima \n    xmean \n    0.1021000 \n    0.1125000 \n  \n  \n    forecast::Arima \n    intercept \n    0.1020681 \n    0.1125445 \n  \n  \n    stats::arima \n    intercept \n    0.1020681 \n    0.1125445 \n  \n  \n    astsa::sarima \n    ar1 \n    0.2171000 \n    0.0978000 \n  \n  \n    forecast::Arima \n    ar1 \n    0.2170632 \n    0.0977980 \n  \n  \n    stats::arima \n    ar1 \n    0.2170632 \n    0.0977980 \n  \n\n\n\n\n\n\n\nCode\n# arx1\nset.seed(1)\nx <- sort(runif(400))\nbeta <- 2\narx1 <- arima.sim(list(ar=.3), 400) + x * beta\n\nmod_Arx1 <- Arima(arx1, order = c(1, 0, 0), xreg = x)\nmod_arx1 <- arima(arx1, order = c(1, 0, 0), xreg = x)\ninvisible(capture.output(\n  mod_sarx1 <- sarima(arx1, p = 1, d = 0, q = 0, xreg = x)\n))\n\n\n\n\n\nCode\nbind_rows(\n  mod_Arx1 |> tidy()|> add_column(model = \"forecast::Arima\", .before = 1),\n  mod_arx1 |> tidy() |> add_column(model = \"stats::arima\", .before = 1),\n  mod_sarx1$ttable[,1:2] |>\n    as_tibble(rownames = \"term\", .name_repair = ~c(\"estimate\", \"std.error\")) |>\n    add_column(model = \"astsa::sarima\", .before = 1)\n) |> arrange(desc(term)) |> \n  kbl() |> kable_minimal()\n\n\n\n\n \n  \n    model \n    term \n    estimate \n    std.error \n  \n \n\n  \n    forecast::Arima \n    xreg \n    1.9302542 \n    0.2520409 \n  \n  \n    astsa::sarima \n    xreg \n    1.9303000 \n    0.2520000 \n  \n  \n    stats::arima \n    x \n    1.9302542 \n    0.2520409 \n  \n  \n    forecast::Arima \n    intercept \n    0.0275105 \n    0.1427769 \n  \n  \n    stats::arima \n    intercept \n    0.0275105 \n    0.1427769 \n  \n  \n    astsa::sarima \n    intercept \n    0.0275000 \n    0.1428000 \n  \n  \n    forecast::Arima \n    ar1 \n    0.2709738 \n    0.0481873 \n  \n  \n    stats::arima \n    ar1 \n    0.2709738 \n    0.0481873 \n  \n  \n    astsa::sarima \n    ar1 \n    0.2710000 \n    0.0482000 \n  \n\n\n\n\n\nIt’s pretty clear from both of these ### Diagnostics\nThere are hypothesis tests, and graphical checks for stationarity\nunit root tests are for stability/stationarity:\n\nAugmented Dickey Fulley Test\n\nwant low p value\n\nKPSS Test\n\nwant high p value for stationarity\n\n\n\n\n34.2.5 Examples: LA County\nWe just examine the cardiovascular mortality from the LA Pollution study. These are average weekly cv mortality rates from astsa package.\n\n\nCode\n# ?cmort\nx <- cmort %>% as.numeric()\nx1 <- x %>% lag()\n# yt = u + beta * yt-1\ncmort_lm <- lm(x~x1)\n\ncbind(coef(cmort_lm)[1] + coef(cmort_lm)[2] * x1,\n      c(NA, fitted(cmort_lm)),\n      c(NA, predict(cmort_lm))) %>% \n  head() # fitted/predicted values match up\n\n\n\\begin{bmatrix} NA &NA &NA \\\\95.7381108162687 &95.7381108162684 &95.7381108162687 \\\\100.978136273105 &100.978136273105 &100.978136273105 \\\\93.0447840350672 &93.0447840350672 &93.0447840350672 \\\\95.8924561905496 &95.8924561905496 &95.8924561905496 \\\\94.1946570734599 &94.1946570734598 &94.1946570734599 \\\\ \\end{bmatrix}\n\n\n\n\nCode\nqplot(time(cmort), x, geom = \"line\", color = \"black\") +\n  geom_line(aes(y =  c(NA, fitted(cmort_lm)), color = \"red\")) +\n  scale_color_manual(values = c(\"black\", \"red\"),\n                     label = c(\"raw\", \"fitted\"))\n\n\nDon't know how to automatically pick scale for object of type ts. Defaulting to continuous.\n\n\nWarning: Removed 1 row(s) containing missing values (geom_path).\n\n\n\n\n\n\n\n34.2.6 Example: AirPassengers\n\n\nCode\nAirPassengers %>% plot()\n\n\n\n\n\n\n\nCode\nsarima(AirPassengers, d = 1, p = 2, q = 0)\n\n\ninitial  value 3.522169 \niter   2 value 3.456359\niter   3 value 3.447683\niter   4 value 3.447001\niter   5 value 3.447000\niter   6 value 3.447000\niter   6 value 3.447000\niter   6 value 3.447000\nfinal  value 3.447000 \nconverged\ninitial  value 3.441124 \niter   2 value 3.441115\niter   3 value 3.441115\niter   4 value 3.441114\niter   4 value 3.441114\niter   4 value 3.441114\nfinal  value 3.441114 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2  constant\n      0.3792  -0.2314    2.4075\ns.e.  0.0823   0.0834    3.0636\n\nsigma^2 estimated as 973.4:  log likelihood = -694.99,  aic = 1397.98\n\n$degrees_of_freedom\n[1] 140\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.3792 0.0823  4.6065  0.0000\nar2       -0.2314 0.0834 -2.7767  0.0062\nconstant   2.4075 3.0636  0.7858  0.4333\n\n$AIC\n[1] 9.77605\n\n$AICc\n[1] 9.777257\n\n$BIC\n[1] 9.858927"
  },
  {
    "objectID": "timeseries/timeseries.html#state-space-model",
    "href": "timeseries/timeseries.html#state-space-model",
    "title": "34  Time Series",
    "section": "34.8 State Space Model",
    "text": "34.8 State Space Model\nThese models have a hierarchical form, in which there is some underlying time series process, but then we also observe data on top of that. Thus, the general form of the equations look like this:\n\n\\begin{aligned}\n\\textbf{State Equation:}& \\\\\nx_t &= \\Phi x_{t-1} + \\Upsilon u_t +   w_t \\\\\n\\textbf{Observation Equation:}& \\\\\ny_t &= A_t x_t + \\Gamma u_t  + v_t\n\\end{aligned}\n where:\n\n\\Upsilon u_t - is time varying exogenous variables\n\\Upsilon u_t - is time varying exogenous variables\nx_t is state at time t\n\\Phi x_t is state at time t describes how x evolves\nw_t is state noise with w_t \\sim N(0, Q)\nv_t is measurement noise with v_t \\sim N(0, R)\n\nThere are more general forms of the state space model: with correlated errors, see Durbin Koopman for more state space methods.\nIt seems now that state space models are now also being superseded by recurrent neural networks, which can model dynamical properties.\n\n34.8.1 Example: AR(1) with observational noise\nThe state equation:\n\n\\begin{aligned}\n\\textbf{State Equation:}& \\\\\nx_t &= \\phi x_{t-1} + w_t \\\\\n\\textbf{Observation Equation:}& \\\\\ny_t &= x_t + v_t\n\\end{aligned}\n\nWhat is interesting about this case with a hierarchical data structure, is that we can show it has the same error structure as an ARMA model. Shumway Stoffer notes that even though it has the same parameterization as an ARMA model, it is often easier to think about the state model form. See example 6.3 for more details."
  },
  {
    "objectID": "epidemiology/epidemiology.html#icm",
    "href": "epidemiology/epidemiology.html#icm",
    "title": "10  Epidemiology",
    "section": "10.3 ICM",
    "text": "10.3 ICM\nFeatures:\n\nDiscrete Time (dcm was continuous time)\nParameters are random draws from poisson, normal or binomial distributions\na unit is an individual\n\n\n\nCode\nparam <- param.icm(inf.prob = 0.2, act.rate = 0.25)\ninit <- init.icm(s.num = 500, i.num = 1)\ncontrol <- control.icm(type = \"SI\", nsims = 10, nsteps = 300)\nmod <- icm(param, init, control)\n\nsummary(mod, at = 125)\n\n\nEpiModel Summary\n=======================\nModel class: icm\n\nSimulation Details\n-----------------------\nModel type: SI\nNo. simulations: 10\nNo. time steps: 300\nNo. groups: 1\n\nModel Statistics\n------------------------------\nTime: 125 \n------------------------------ \n           mean       sd    pct\nSuscept.  338.7  132.051  0.676\nInfect.   162.3  132.051  0.324\nTotal     501.0    0.000  1.000\nS -> I      3.1    2.183     NA\n------------------------------ \n\n\n\n\nCode\nplot(mod)\n\n\n\n\n\nThe averages are shown"
  },
  {
    "objectID": "epidemiology/epidemiology.html#network-models",
    "href": "epidemiology/epidemiology.html#network-models",
    "title": "10  Epidemiology",
    "section": "10.4 Network Models",
    "text": "10.4 Network Models\nTutorial for Network Models can be found in original paper or in Workshop Materials\n\nnetwork models are modeled with tergm\n\nfunctions commonly used\n\nnetest : estimates generative model, ergm and stergm\nnetdx : simulaties replications over time from model in netest. Diagnostics for whether empirically observed one matches the simulation\nnetsim : stochastic epidemic process. Independent models, dynamics first simulated, then the epidemic simulation over the realization. For dependent models, both are updated in each time step.\n\n\n\nCode\nnw <- network_initialize(n = 500)\nformation <- ~edges + concurrent + degrange(from = 4)\ntarget.stats <- c(175, 110, 0) # 175 edges, d\ncoef.diss <- dissolution_coefs(dissolution = ~offset(edges), duration = 50)\n\n# estimate network\nest <- netest(nw, formation, target.stats, coef.diss)\n\n\nObserved statistic(s) deg4+ are at their smallest attainable values. Their coefficients will be fixed at -Inf.\n\n\nStarting maximum pseudolikelihood estimation (MPLE):\n\n\nEvaluating the predictor and response matrix.\n\n\nMaximizing the pseudolikelihood.\n\n\nFinished MPLE.\n\n\nStarting Monte Carlo maximum likelihood estimation (MCMLE):\n\n\nIteration 1 of at most 60:\n\n\nOptimizing with step length 1.0000.\n\n\nThe log-likelihood improved by 0.0028.\n\n\nConvergence test p-value: 0.0041. Converged with 99% confidence.\nFinished MCMLE.\nThis model was fit using MCMC.  To examine model diagnostics and check\nfor degeneracy, use the mcmc.diagnostics() function.\n\n\ndissolution coef are constraints on estimation, for how long the ties last.\n\n\nCode\n# parallel::detectCores() # 8\ndx <- netdx(est, nsims = 10, nsteps = 1000,\n            nwstats.formula = ~edges + meandeg + degree(0:4) + concurrent,\n            keep.tedgelist = TRUE, ncores = 8)\n\n\n\nNetwork Diagnostics\n-----------------------\n- Simulating 10 networks\n- Calculating formation statistics\n\n\nCode\nprint(dx)\n\n\nEpiModel Network Diagnostics\n=======================\nDiagnostic Method: Dynamic\nSimulations: 10\nTime Steps per Sim: 1000\n\nFormation Diagnostics\n----------------------- \n           Target Sim Mean Pct Diff Sim SE Z Score SD(Sim Means) SD(Statistic)\nedges         175  173.343   -0.947  1.148  -1.443         3.702        13.198\nmeandeg        NA    0.693       NA  0.005      NA         0.015         0.053\ndegree0        NA  273.966       NA  1.328      NA         3.876        13.920\ndegree1        NA  118.547       NA  0.456      NA         1.417         9.159\ndegree2        NA   94.322       NA  0.622      NA         2.050        10.451\ndegree3        NA   13.165       NA  0.214      NA         0.965         3.869\ndegree4        NA    0.000       NA    NaN      NA         0.000         0.000\nconcurrent    110  107.487   -2.284  0.785  -3.199         2.757        11.922\n\nDuration Diagnostics\n----------------------- \n      Target Sim Mean Pct Diff Sim SE Z Score SD(Sim Means) SD(Statistic)\nedges     50   49.887   -0.226  0.365  -0.309         1.319         3.949\n\nDissolution Diagnostics\n----------------------- \n      Target Sim Mean Pct Diff Sim SE Z Score SD(Sim Means) SD(Statistic)\nedges   0.02     0.02   -0.116      0  -0.217             0         0.011\n\n\n\n\nCode\nplot(dx)\n\n\n\n\n\nCode\npar(mfrow = c(1, 2))\n# when duration.imputed = TRUE, edges are censored at time 1, so we must estimate them.\nplot(dx, type = \"duration\", duration.imputed = FALSE) # check the duration of how long edges last, right around 50 time steps.\nplot(dx, type = \"dissolution\") # as a percentage across all time\n\n\n\n\n\nThis gives the timed edge list, keep.tedgelist = TRUE option\n\nonset when edge started\nterminus when edge stopped\ntail, head from who to who\n\n\n\nCode\ntel <- as.data.frame(dx, sim = 1)\nhead(tel, 20)\n\n\n   onset terminus tail head onset.censored terminus.censored duration edge.id\n1      0        6    4  118           TRUE             FALSE        6       1\n2      0        9    4  481           TRUE             FALSE        9       2\n3      0        3    8  244           TRUE             FALSE        3       3\n4      0        2    8  344           TRUE             FALSE        2       4\n5      0       32   12   31           TRUE             FALSE       32       5\n6      0        5   12  162           TRUE             FALSE        5       6\n7      0       42   12  354           TRUE             FALSE       42       7\n8      0       56   15  246           TRUE             FALSE       56       8\n9      0       56   15  311           TRUE             FALSE       56       9\n10     0       90   17  129           TRUE             FALSE       90      10\n11     0       26   21   53           TRUE             FALSE       26      11\n12     0      107   21  347           TRUE             FALSE      107      12\n13     0        8   23  176           TRUE             FALSE        8      13\n14     0       37   23  464           TRUE             FALSE       37      14\n15     0       82   24  347           TRUE             FALSE       82      15\n16     0       14   24  442           TRUE             FALSE       14      16\n17     0       93   25  345           TRUE             FALSE       93      17\n18     0       37   29  116           TRUE             FALSE       37      18\n19     0      177   30   46           TRUE             FALSE      177      19\n20     0       27   30  440           TRUE             FALSE       27      20\n\n\n\n\nCode\nparam <- param.net(inf.prob = 0.4, act.rate = 2, rec.rate = 0.1)\ninit <- init.net(i.num = 10)\ncontrol <- control.net(type = \"SIS\", nsims = 5, nsteps = 500)\n\nsim <- netsim(est, param, init, control)\n\n\n\nStarting Network Simulation...\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 2/500\nPrevalence: 17\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 3/500\nPrevalence: 20\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 4/500\nPrevalence: 20\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 5/500\nPrevalence: 23\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 6/500\nPrevalence: 21\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 7/500\nPrevalence: 24\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 8/500\nPrevalence: 29\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 9/500\nPrevalence: 31\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 10/500\nPrevalence: 29\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 11/500\nPrevalence: 34\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 12/500\nPrevalence: 36\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 13/500\nPrevalence: 32\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 14/500\nPrevalence: 30\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 15/500\nPrevalence: 36\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 16/500\nPrevalence: 40\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 17/500\nPrevalence: 35\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 18/500\nPrevalence: 33\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 19/500\nPrevalence: 39\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 20/500\nPrevalence: 40\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 21/500\nPrevalence: 42\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 22/500\nPrevalence: 47\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 23/500\nPrevalence: 47\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 24/500\nPrevalence: 49\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 25/500\nPrevalence: 48\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 26/500\nPrevalence: 51\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 27/500\nPrevalence: 52\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 28/500\nPrevalence: 50\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 29/500\nPrevalence: 52\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 30/500\nPrevalence: 52\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 31/500\nPrevalence: 59\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 32/500\nPrevalence: 61\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 33/500\nPrevalence: 71\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 34/500\nPrevalence: 73\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 35/500\nPrevalence: 74\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 36/500\nPrevalence: 71\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 37/500\nPrevalence: 74\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 38/500\nPrevalence: 83\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 39/500\nPrevalence: 83\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 40/500\nPrevalence: 84\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 41/500\nPrevalence: 84\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 42/500\nPrevalence: 93\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 43/500\nPrevalence: 98\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 44/500\nPrevalence: 91\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 45/500\nPrevalence: 98\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 46/500\nPrevalence: 102\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 47/500\nPrevalence: 103\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 48/500\nPrevalence: 102\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 49/500\nPrevalence: 103\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 50/500\nPrevalence: 111\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 51/500\nPrevalence: 112\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 52/500\nPrevalence: 114\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 53/500\nPrevalence: 113\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 54/500\nPrevalence: 116\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 55/500\nPrevalence: 116\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 56/500\nPrevalence: 107\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 57/500\nPrevalence: 114\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 58/500\nPrevalence: 114\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 59/500\nPrevalence: 115\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 60/500\nPrevalence: 117\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 61/500\nPrevalence: 118\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 62/500\nPrevalence: 116\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 63/500\nPrevalence: 110\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 64/500\nPrevalence: 112\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 65/500\nPrevalence: 118\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 66/500\nPrevalence: 123\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 67/500\nPrevalence: 122\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 68/500\nPrevalence: 113\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 69/500\nPrevalence: 121\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 70/500\nPrevalence: 119\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 71/500\nPrevalence: 123\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 72/500\nPrevalence: 123\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 73/500\nPrevalence: 122\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 74/500\nPrevalence: 133\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 75/500\nPrevalence: 138\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 76/500\nPrevalence: 140\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 77/500\nPrevalence: 135\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 78/500\nPrevalence: 142\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 79/500\nPrevalence: 145\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 80/500\nPrevalence: 134\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 81/500\nPrevalence: 135\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 82/500\nPrevalence: 138\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 83/500\nPrevalence: 137\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 84/500\nPrevalence: 139\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 85/500\nPrevalence: 140\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 86/500\nPrevalence: 143\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 87/500\nPrevalence: 137\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 88/500\nPrevalence: 142\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 89/500\nPrevalence: 142\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 90/500\nPrevalence: 141\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 91/500\nPrevalence: 139\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 92/500\nPrevalence: 142\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 93/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 94/500\nPrevalence: 143\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 95/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 96/500\nPrevalence: 148\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 97/500\nPrevalence: 141\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 98/500\nPrevalence: 146\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 99/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 100/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 101/500\nPrevalence: 145\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 102/500\nPrevalence: 148\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 103/500\nPrevalence: 147\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 104/500\nPrevalence: 146\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 105/500\nPrevalence: 145\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 106/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 107/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 108/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 109/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 110/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 111/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 112/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 113/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 114/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 115/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 116/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 117/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 118/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 119/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 120/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 121/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 122/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 123/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 124/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 125/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 126/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 127/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 128/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 129/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 130/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 131/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 132/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 133/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 134/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 135/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 136/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 137/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 138/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 139/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 140/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 141/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 142/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 143/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 144/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 145/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 146/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 147/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 148/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 149/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 150/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 151/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 152/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 153/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 154/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 155/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 156/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 157/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 158/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 159/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 160/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 161/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 162/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 163/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 164/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 165/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 166/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 167/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 168/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 169/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 170/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 171/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 172/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 173/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 174/500\nPrevalence: 148\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 175/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 176/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 177/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 178/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 179/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 180/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 181/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 182/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 183/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 184/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 185/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 186/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 187/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 188/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 189/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 190/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 191/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 192/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 193/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 194/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 195/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 196/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 197/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 198/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 199/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 200/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 201/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 202/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 203/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 204/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 205/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 206/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 207/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 208/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 209/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 210/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 211/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 212/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 213/500\nPrevalence: 146\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 214/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 215/500\nPrevalence: 148\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 216/500\nPrevalence: 146\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 217/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 218/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 219/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 220/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 221/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 222/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 223/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 224/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 225/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 226/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 227/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 228/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 229/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 230/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 231/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 232/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 233/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 234/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 235/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 236/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 237/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 238/500\nPrevalence: 148\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 239/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 240/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 241/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 242/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 243/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 244/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 245/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 246/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 247/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 248/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 249/500\nPrevalence: 149\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 250/500\nPrevalence: 144\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 251/500\nPrevalence: 146\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 252/500\nPrevalence: 147\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 253/500\nPrevalence: 142\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 254/500\nPrevalence: 143\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 255/500\nPrevalence: 140\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 256/500\nPrevalence: 142\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 257/500\nPrevalence: 139\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 258/500\nPrevalence: 136\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 259/500\nPrevalence: 135\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 260/500\nPrevalence: 134\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 261/500\nPrevalence: 138\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 262/500\nPrevalence: 140\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 263/500\nPrevalence: 143\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 264/500\nPrevalence: 143\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 265/500\nPrevalence: 149\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 266/500\nPrevalence: 146\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 267/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 268/500\nPrevalence: 145\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 269/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 270/500\nPrevalence: 147\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 271/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 272/500\nPrevalence: 149\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 273/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 274/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 275/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 276/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 277/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 278/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 279/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 280/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 281/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 282/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 283/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 284/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 285/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 286/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 287/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 288/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 289/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 290/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 291/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 292/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 293/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 294/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 295/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 296/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 297/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 298/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 299/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 300/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 301/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 302/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 303/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 304/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 305/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 306/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 307/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 308/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 309/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 310/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 311/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 312/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 313/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 314/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 315/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 316/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 317/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 318/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 319/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 320/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 321/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 322/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 323/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 324/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 325/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 326/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 327/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 328/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 329/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 330/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 331/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 332/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 333/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 334/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 335/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 336/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 337/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 338/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 339/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 340/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 341/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 342/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 343/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 344/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 345/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 346/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 347/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 348/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 349/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 350/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 351/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 352/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 353/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 354/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 355/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 356/500\nPrevalence: 146\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 357/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 358/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 359/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 360/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 361/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 362/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 363/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 364/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 365/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 366/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 367/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 368/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 369/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 370/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 371/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 372/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 373/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 374/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 375/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 376/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 377/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 378/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 379/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 380/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 381/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 382/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 383/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 384/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 385/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 386/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 387/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 388/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 389/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 390/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 391/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 392/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 393/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 394/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 395/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 396/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 397/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 398/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 399/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 400/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 401/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 402/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 403/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 404/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 405/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 406/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 407/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 408/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 409/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 410/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 411/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 412/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 413/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 414/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 415/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 416/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 417/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 418/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 419/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 420/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 421/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 422/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 423/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 424/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 425/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 426/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 427/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 428/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 429/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 430/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 431/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 432/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 433/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 434/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 435/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 436/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 437/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 438/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 439/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 440/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 441/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 442/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 443/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 444/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 445/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 446/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 447/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 448/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 449/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 450/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 451/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 452/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 453/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 454/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 455/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 456/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 457/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 458/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 459/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 460/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 461/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 462/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 463/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 464/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 465/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 466/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 467/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 468/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 469/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 470/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 471/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 472/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 473/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 474/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 475/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 476/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 477/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 478/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 479/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 480/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 481/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 482/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 483/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 484/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 485/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 486/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 487/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 488/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 489/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 490/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 491/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 492/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 493/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 494/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 495/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 496/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 497/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 498/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 499/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 1/5\nTimestep: 500/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 2/500\nPrevalence: 13\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 3/500\nPrevalence: 13\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 4/500\nPrevalence: 16\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 5/500\nPrevalence: 20\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 6/500\nPrevalence: 18\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 7/500\nPrevalence: 20\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 8/500\nPrevalence: 19\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 9/500\nPrevalence: 18\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 10/500\nPrevalence: 19\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 11/500\nPrevalence: 22\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 12/500\nPrevalence: 23\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 13/500\nPrevalence: 24\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 14/500\nPrevalence: 28\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 15/500\nPrevalence: 33\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 16/500\nPrevalence: 35\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 17/500\nPrevalence: 37\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 18/500\nPrevalence: 41\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 19/500\nPrevalence: 43\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 20/500\nPrevalence: 45\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 21/500\nPrevalence: 43\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 22/500\nPrevalence: 42\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 23/500\nPrevalence: 43\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 24/500\nPrevalence: 49\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 25/500\nPrevalence: 50\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 26/500\nPrevalence: 53\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 27/500\nPrevalence: 54\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 28/500\nPrevalence: 52\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 29/500\nPrevalence: 51\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 30/500\nPrevalence: 52\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 31/500\nPrevalence: 54\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 32/500\nPrevalence: 60\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 33/500\nPrevalence: 63\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 34/500\nPrevalence: 65\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 35/500\nPrevalence: 61\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 36/500\nPrevalence: 63\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 37/500\nPrevalence: 66\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 38/500\nPrevalence: 69\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 39/500\nPrevalence: 76\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 40/500\nPrevalence: 73\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 41/500\nPrevalence: 73\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 42/500\nPrevalence: 67\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 43/500\nPrevalence: 71\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 44/500\nPrevalence: 73\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 45/500\nPrevalence: 81\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 46/500\nPrevalence: 85\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 47/500\nPrevalence: 86\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 48/500\nPrevalence: 90\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 49/500\nPrevalence: 89\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 50/500\nPrevalence: 101\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 51/500\nPrevalence: 97\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 52/500\nPrevalence: 99\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 53/500\nPrevalence: 104\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 54/500\nPrevalence: 111\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 55/500\nPrevalence: 112\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 56/500\nPrevalence: 119\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 57/500\nPrevalence: 121\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 58/500\nPrevalence: 123\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 59/500\nPrevalence: 127\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 60/500\nPrevalence: 129\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 61/500\nPrevalence: 134\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 62/500\nPrevalence: 145\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 63/500\nPrevalence: 139\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 64/500\nPrevalence: 147\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 65/500\nPrevalence: 141\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 66/500\nPrevalence: 145\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 67/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 68/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 69/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 70/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 71/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 72/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 73/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 74/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 75/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 76/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 77/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 78/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 79/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 80/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 81/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 82/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 83/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 84/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 85/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 86/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 87/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 88/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 89/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 90/500\nPrevalence: 149\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 91/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 92/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 93/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 94/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 95/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 96/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 97/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 98/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 99/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 100/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 101/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 102/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 103/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 104/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 105/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 106/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 107/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 108/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 109/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 110/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 111/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 112/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 113/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 114/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 115/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 116/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 117/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 118/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 119/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 120/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 121/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 122/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 123/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 124/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 125/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 126/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 127/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 128/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 129/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 130/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 131/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 132/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 133/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 134/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 135/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 136/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 137/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 138/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 139/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 140/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 141/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 142/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 143/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 144/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 145/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 146/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 147/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 148/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 149/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 150/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 151/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 152/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 153/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 154/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 155/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 156/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 157/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 158/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 159/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 160/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 161/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 162/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 163/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 164/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 165/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 166/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 167/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 168/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 169/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 170/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 171/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 172/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 173/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 174/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 175/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 176/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 177/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 178/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 179/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 180/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 181/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 182/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 183/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 184/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 185/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 186/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 187/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 188/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 189/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 190/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 191/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 192/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 193/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 194/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 195/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 196/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 197/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 198/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 199/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 200/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 201/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 202/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 203/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 204/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 205/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 206/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 207/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 208/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 209/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 210/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 211/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 212/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 213/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 214/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 215/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 216/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 217/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 218/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 219/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 220/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 221/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 222/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 223/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 224/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 225/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 226/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 227/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 228/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 229/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 230/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 231/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 232/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 233/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 234/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 235/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 236/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 237/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 238/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 239/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 240/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 241/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 242/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 243/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 244/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 245/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 246/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 247/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 248/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 249/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 250/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 251/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 252/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 253/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 254/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 255/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 256/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 257/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 258/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 259/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 260/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 261/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 262/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 263/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 264/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 265/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 266/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 267/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 268/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 269/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 270/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 271/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 272/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 273/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 274/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 275/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 276/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 277/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 278/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 279/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 280/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 281/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 282/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 283/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 284/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 285/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 286/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 287/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 288/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 289/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 290/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 291/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 292/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 293/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 294/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 295/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 296/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 297/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 298/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 299/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 300/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 301/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 302/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 303/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 304/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 305/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 306/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 307/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 308/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 309/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 310/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 311/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 312/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 313/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 314/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 315/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 316/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 317/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 318/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 319/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 320/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 321/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 322/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 323/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 324/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 325/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 326/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 327/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 328/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 329/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 330/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 331/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 332/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 333/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 334/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 335/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 336/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 337/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 338/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 339/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 340/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 341/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 342/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 343/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 344/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 345/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 346/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 347/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 348/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 349/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 350/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 351/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 352/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 353/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 354/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 355/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 356/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 357/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 358/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 359/500\nPrevalence: 149\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 360/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 361/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 362/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 363/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 364/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 365/500\nPrevalence: 149\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 366/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 367/500\nPrevalence: 149\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 368/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 369/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 370/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 371/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 372/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 373/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 374/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 375/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 376/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 377/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 378/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 379/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 380/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 381/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 382/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 383/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 384/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 385/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 386/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 387/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 388/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 389/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 390/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 391/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 392/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 393/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 394/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 395/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 396/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 397/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 398/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 399/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 400/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 401/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 402/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 403/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 404/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 405/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 406/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 407/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 408/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 409/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 410/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 411/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 412/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 413/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 414/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 415/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 416/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 417/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 418/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 419/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 420/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 421/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 422/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 423/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 424/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 425/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 426/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 427/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 428/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 429/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 430/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 431/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 432/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 433/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 434/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 435/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 436/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 437/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 438/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 439/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 440/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 441/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 442/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 443/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 444/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 445/500\nPrevalence: 205\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 446/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 447/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 448/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 449/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 450/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 451/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 452/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 453/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 454/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 455/500\nPrevalence: 199\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 456/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 457/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 458/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 459/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 460/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 461/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 462/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 463/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 464/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 465/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 466/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 467/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 468/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 469/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 470/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 471/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 472/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 473/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 474/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 475/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 476/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 477/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 478/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 479/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 480/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 481/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 482/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 483/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 484/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 485/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 486/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 487/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 488/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 489/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 490/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 491/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 492/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 493/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 494/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 495/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 496/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 497/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 498/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 499/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 2/5\nTimestep: 500/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 2/500\nPrevalence: 14\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 3/500\nPrevalence: 18\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 4/500\nPrevalence: 20\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 5/500\nPrevalence: 22\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 6/500\nPrevalence: 23\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 7/500\nPrevalence: 24\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 8/500\nPrevalence: 25\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 9/500\nPrevalence: 27\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 10/500\nPrevalence: 29\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 11/500\nPrevalence: 29\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 12/500\nPrevalence: 27\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 13/500\nPrevalence: 26\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 14/500\nPrevalence: 27\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 15/500\nPrevalence: 27\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 16/500\nPrevalence: 30\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 17/500\nPrevalence: 32\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 18/500\nPrevalence: 28\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 19/500\nPrevalence: 33\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 20/500\nPrevalence: 37\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 21/500\nPrevalence: 34\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 22/500\nPrevalence: 37\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 23/500\nPrevalence: 40\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 24/500\nPrevalence: 45\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 25/500\nPrevalence: 46\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 26/500\nPrevalence: 46\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 27/500\nPrevalence: 49\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 28/500\nPrevalence: 50\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 29/500\nPrevalence: 53\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 30/500\nPrevalence: 59\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 31/500\nPrevalence: 61\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 32/500\nPrevalence: 71\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 33/500\nPrevalence: 74\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 34/500\nPrevalence: 78\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 35/500\nPrevalence: 73\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 36/500\nPrevalence: 74\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 37/500\nPrevalence: 73\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 38/500\nPrevalence: 74\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 39/500\nPrevalence: 68\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 40/500\nPrevalence: 67\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 41/500\nPrevalence: 72\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 42/500\nPrevalence: 80\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 43/500\nPrevalence: 75\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 44/500\nPrevalence: 77\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 45/500\nPrevalence: 72\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 46/500\nPrevalence: 78\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 47/500\nPrevalence: 80\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 48/500\nPrevalence: 82\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 49/500\nPrevalence: 86\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 50/500\nPrevalence: 89\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 51/500\nPrevalence: 92\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 52/500\nPrevalence: 87\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 53/500\nPrevalence: 88\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 54/500\nPrevalence: 91\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 55/500\nPrevalence: 91\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 56/500\nPrevalence: 98\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 57/500\nPrevalence: 104\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 58/500\nPrevalence: 102\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 59/500\nPrevalence: 109\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 60/500\nPrevalence: 106\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 61/500\nPrevalence: 111\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 62/500\nPrevalence: 117\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 63/500\nPrevalence: 117\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 64/500\nPrevalence: 119\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 65/500\nPrevalence: 117\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 66/500\nPrevalence: 123\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 67/500\nPrevalence: 126\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 68/500\nPrevalence: 138\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 69/500\nPrevalence: 132\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 70/500\nPrevalence: 125\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 71/500\nPrevalence: 134\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 72/500\nPrevalence: 137\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 73/500\nPrevalence: 135\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 74/500\nPrevalence: 145\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 75/500\nPrevalence: 142\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 76/500\nPrevalence: 149\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 77/500\nPrevalence: 147\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 78/500\nPrevalence: 149\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 79/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 80/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 81/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 82/500\nPrevalence: 147\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 83/500\nPrevalence: 144\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 84/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 85/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 86/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 87/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 88/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 89/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 90/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 91/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 92/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 93/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 94/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 95/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 96/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 97/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 98/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 99/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 100/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 101/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 102/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 103/500\nPrevalence: 147\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 104/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 105/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 106/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 107/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 108/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 109/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 110/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 111/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 112/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 113/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 114/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 115/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 116/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 117/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 118/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 119/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 120/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 121/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 122/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 123/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 124/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 125/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 126/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 127/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 128/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 129/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 130/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 131/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 132/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 133/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 134/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 135/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 136/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 137/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 138/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 139/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 140/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 141/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 142/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 143/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 144/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 145/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 146/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 147/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 148/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 149/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 150/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 151/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 152/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 153/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 154/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 155/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 156/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 157/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 158/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 159/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 160/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 161/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 162/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 163/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 164/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 165/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 166/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 167/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 168/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 169/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 170/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 171/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 172/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 173/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 174/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 175/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 176/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 177/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 178/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 179/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 180/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 181/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 182/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 183/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 184/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 185/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 186/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 187/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 188/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 189/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 190/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 191/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 192/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 193/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 194/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 195/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 196/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 197/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 198/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 199/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 200/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 201/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 202/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 203/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 204/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 205/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 206/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 207/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 208/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 209/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 210/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 211/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 212/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 213/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 214/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 215/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 216/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 217/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 218/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 219/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 220/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 221/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 222/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 223/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 224/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 225/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 226/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 227/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 228/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 229/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 230/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 231/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 232/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 233/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 234/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 235/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 236/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 237/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 238/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 239/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 240/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 241/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 242/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 243/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 244/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 245/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 246/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 247/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 248/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 249/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 250/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 251/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 252/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 253/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 254/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 255/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 256/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 257/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 258/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 259/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 260/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 261/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 262/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 263/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 264/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 265/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 266/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 267/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 268/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 269/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 270/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 271/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 272/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 273/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 274/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 275/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 276/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 277/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 278/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 279/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 280/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 281/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 282/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 283/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 284/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 285/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 286/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 287/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 288/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 289/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 290/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 291/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 292/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 293/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 294/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 295/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 296/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 297/500\nPrevalence: 139\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 298/500\nPrevalence: 146\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 299/500\nPrevalence: 144\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 300/500\nPrevalence: 140\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 301/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 302/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 303/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 304/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 305/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 306/500\nPrevalence: 149\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 307/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 308/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 309/500\nPrevalence: 148\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 310/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 311/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 312/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 313/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 314/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 315/500\nPrevalence: 144\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 316/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 317/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 318/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 319/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 320/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 321/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 322/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 323/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 324/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 325/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 326/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 327/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 328/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 329/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 330/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 331/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 332/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 333/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 334/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 335/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 336/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 337/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 338/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 339/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 340/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 341/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 342/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 343/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 344/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 345/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 346/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 347/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 348/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 349/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 350/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 351/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 352/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 353/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 354/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 355/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 356/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 357/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 358/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 359/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 360/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 361/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 362/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 363/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 364/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 365/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 366/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 367/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 368/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 369/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 370/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 371/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 372/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 373/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 374/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 375/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 376/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 377/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 378/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 379/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 380/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 381/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 382/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 383/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 384/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 385/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 386/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 387/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 388/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 389/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 390/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 391/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 392/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 393/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 394/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 395/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 396/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 397/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 398/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 399/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 400/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 401/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 402/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 403/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 404/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 405/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 406/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 407/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 408/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 409/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 410/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 411/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 412/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 413/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 414/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 415/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 416/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 417/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 418/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 419/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 420/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 421/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 422/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 423/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 424/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 425/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 426/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 427/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 428/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 429/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 430/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 431/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 432/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 433/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 434/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 435/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 436/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 437/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 438/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 439/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 440/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 441/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 442/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 443/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 444/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 445/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 446/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 447/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 448/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 449/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 450/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 451/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 452/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 453/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 454/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 455/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 456/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 457/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 458/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 459/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 460/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 461/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 462/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 463/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 464/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 465/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 466/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 467/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 468/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 469/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 470/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 471/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 472/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 473/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 474/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 475/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 476/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 477/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 478/500\nPrevalence: 200\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 479/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 480/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 481/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 482/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 483/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 484/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 485/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 486/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 487/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 488/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 489/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 490/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 491/500\nPrevalence: 202\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 492/500\nPrevalence: 208\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 493/500\nPrevalence: 201\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 494/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 495/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 496/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 497/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 498/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 499/500\nPrevalence: 201\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 3/5\nTimestep: 500/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 2/500\nPrevalence: 10\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 3/500\nPrevalence: 10\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 4/500\nPrevalence: 14\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 5/500\nPrevalence: 19\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 6/500\nPrevalence: 19\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 7/500\nPrevalence: 19\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 8/500\nPrevalence: 20\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 9/500\nPrevalence: 21\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 10/500\nPrevalence: 27\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 11/500\nPrevalence: 26\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 12/500\nPrevalence: 29\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 13/500\nPrevalence: 30\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 14/500\nPrevalence: 31\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 15/500\nPrevalence: 30\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 16/500\nPrevalence: 37\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 17/500\nPrevalence: 41\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 18/500\nPrevalence: 43\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 19/500\nPrevalence: 44\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 20/500\nPrevalence: 41\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 21/500\nPrevalence: 44\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 22/500\nPrevalence: 45\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 23/500\nPrevalence: 47\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 24/500\nPrevalence: 52\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 25/500\nPrevalence: 47\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 26/500\nPrevalence: 51\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 27/500\nPrevalence: 54\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 28/500\nPrevalence: 52\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 29/500\nPrevalence: 60\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 30/500\nPrevalence: 60\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 31/500\nPrevalence: 62\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 32/500\nPrevalence: 60\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 33/500\nPrevalence: 65\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 34/500\nPrevalence: 66\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 35/500\nPrevalence: 61\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 36/500\nPrevalence: 65\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 37/500\nPrevalence: 67\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 38/500\nPrevalence: 73\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 39/500\nPrevalence: 69\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 40/500\nPrevalence: 76\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 41/500\nPrevalence: 76\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 42/500\nPrevalence: 85\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 43/500\nPrevalence: 88\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 44/500\nPrevalence: 84\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 45/500\nPrevalence: 86\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 46/500\nPrevalence: 91\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 47/500\nPrevalence: 88\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 48/500\nPrevalence: 95\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 49/500\nPrevalence: 95\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 50/500\nPrevalence: 101\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 51/500\nPrevalence: 99\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 52/500\nPrevalence: 99\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 53/500\nPrevalence: 102\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 54/500\nPrevalence: 106\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 55/500\nPrevalence: 101\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 56/500\nPrevalence: 107\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 57/500\nPrevalence: 107\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 58/500\nPrevalence: 106\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 59/500\nPrevalence: 114\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 60/500\nPrevalence: 114\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 61/500\nPrevalence: 117\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 62/500\nPrevalence: 117\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 63/500\nPrevalence: 119\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 64/500\nPrevalence: 126\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 65/500\nPrevalence: 129\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 66/500\nPrevalence: 128\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 67/500\nPrevalence: 125\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 68/500\nPrevalence: 131\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 69/500\nPrevalence: 130\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 70/500\nPrevalence: 135\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 71/500\nPrevalence: 141\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 72/500\nPrevalence: 146\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 73/500\nPrevalence: 144\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 74/500\nPrevalence: 149\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 75/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 76/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 77/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 78/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 79/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 80/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 81/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 82/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 83/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 84/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 85/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 86/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 87/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 88/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 89/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 90/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 91/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 92/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 93/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 94/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 95/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 96/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 97/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 98/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 99/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 100/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 101/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 102/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 103/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 104/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 105/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 106/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 107/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 108/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 109/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 110/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 111/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 112/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 113/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 114/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 115/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 116/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 117/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 118/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 119/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 120/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 121/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 122/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 123/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 124/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 125/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 126/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 127/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 128/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 129/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 130/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 131/500\nPrevalence: 201\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 132/500\nPrevalence: 201\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 133/500\nPrevalence: 200\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 134/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 135/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 136/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 137/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 138/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 139/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 140/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 141/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 142/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 143/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 144/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 145/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 146/500\nPrevalence: 203\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 147/500\nPrevalence: 206\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 148/500\nPrevalence: 199\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 149/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 150/500\nPrevalence: 209\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 151/500\nPrevalence: 201\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 152/500\nPrevalence: 217\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 153/500\nPrevalence: 202\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 154/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 155/500\nPrevalence: 202\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 156/500\nPrevalence: 212\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 157/500\nPrevalence: 211\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 158/500\nPrevalence: 210\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 159/500\nPrevalence: 209\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 160/500\nPrevalence: 210\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 161/500\nPrevalence: 214\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 162/500\nPrevalence: 219\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 163/500\nPrevalence: 216\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 164/500\nPrevalence: 214\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 165/500\nPrevalence: 223\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 166/500\nPrevalence: 223\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 167/500\nPrevalence: 216\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 168/500\nPrevalence: 224\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 169/500\nPrevalence: 210\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 170/500\nPrevalence: 208\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 171/500\nPrevalence: 202\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 172/500\nPrevalence: 217\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 173/500\nPrevalence: 212\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 174/500\nPrevalence: 206\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 175/500\nPrevalence: 218\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 176/500\nPrevalence: 217\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 177/500\nPrevalence: 211\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 178/500\nPrevalence: 209\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 179/500\nPrevalence: 219\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 180/500\nPrevalence: 218\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 181/500\nPrevalence: 206\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 182/500\nPrevalence: 205\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 183/500\nPrevalence: 199\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 184/500\nPrevalence: 200\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 185/500\nPrevalence: 200\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 186/500\nPrevalence: 199\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 187/500\nPrevalence: 203\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 188/500\nPrevalence: 209\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 189/500\nPrevalence: 202\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 190/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 191/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 192/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 193/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 194/500\nPrevalence: 205\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 195/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 196/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 197/500\nPrevalence: 199\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 198/500\nPrevalence: 209\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 199/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 200/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 201/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 202/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 203/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 204/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 205/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 206/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 207/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 208/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 209/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 210/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 211/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 212/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 213/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 214/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 215/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 216/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 217/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 218/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 219/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 220/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 221/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 222/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 223/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 224/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 225/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 226/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 227/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 228/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 229/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 230/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 231/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 232/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 233/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 234/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 235/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 236/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 237/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 238/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 239/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 240/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 241/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 242/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 243/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 244/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 245/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 246/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 247/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 248/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 249/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 250/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 251/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 252/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 253/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 254/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 255/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 256/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 257/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 258/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 259/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 260/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 261/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 262/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 263/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 264/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 265/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 266/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 267/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 268/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 269/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 270/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 271/500\nPrevalence: 204\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 272/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 273/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 274/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 275/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 276/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 277/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 278/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 279/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 280/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 281/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 282/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 283/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 284/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 285/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 286/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 287/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 288/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 289/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 290/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 291/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 292/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 293/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 294/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 295/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 296/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 297/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 298/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 299/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 300/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 301/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 302/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 303/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 304/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 305/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 306/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 307/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 308/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 309/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 310/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 311/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 312/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 313/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 314/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 315/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 316/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 317/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 318/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 319/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 320/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 321/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 322/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 323/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 324/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 325/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 326/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 327/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 328/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 329/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 330/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 331/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 332/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 333/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 334/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 335/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 336/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 337/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 338/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 339/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 340/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 341/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 342/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 343/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 344/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 345/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 346/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 347/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 348/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 349/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 350/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 351/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 352/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 353/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 354/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 355/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 356/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 357/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 358/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 359/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 360/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 361/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 362/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 363/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 364/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 365/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 366/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 367/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 368/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 369/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 370/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 371/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 372/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 373/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 374/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 375/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 376/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 377/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 378/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 379/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 380/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 381/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 382/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 383/500\nPrevalence: 200\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 384/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 385/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 386/500\nPrevalence: 206\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 387/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 388/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 389/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 390/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 391/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 392/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 393/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 394/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 395/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 396/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 397/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 398/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 399/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 400/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 401/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 402/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 403/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 404/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 405/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 406/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 407/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 408/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 409/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 410/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 411/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 412/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 413/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 414/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 415/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 416/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 417/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 418/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 419/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 420/500\nPrevalence: 201\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 421/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 422/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 423/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 424/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 425/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 426/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 427/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 428/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 429/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 430/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 431/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 432/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 433/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 434/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 435/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 436/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 437/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 438/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 439/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 440/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 441/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 442/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 443/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 444/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 445/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 446/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 447/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 448/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 449/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 450/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 451/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 452/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 453/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 454/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 455/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 456/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 457/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 458/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 459/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 460/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 461/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 462/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 463/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 464/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 465/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 466/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 467/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 468/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 469/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 470/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 471/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 472/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 473/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 474/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 475/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 476/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 477/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 478/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 479/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 480/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 481/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 482/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 483/500\nPrevalence: 146\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 484/500\nPrevalence: 149\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 485/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 486/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 487/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 488/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 489/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 490/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 491/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 492/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 493/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 494/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 495/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 496/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 497/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 498/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 499/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 4/5\nTimestep: 500/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 2/500\nPrevalence: 13\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 3/500\nPrevalence: 14\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 4/500\nPrevalence: 16\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 5/500\nPrevalence: 18\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 6/500\nPrevalence: 18\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 7/500\nPrevalence: 17\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 8/500\nPrevalence: 20\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 9/500\nPrevalence: 22\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 10/500\nPrevalence: 22\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 11/500\nPrevalence: 21\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 12/500\nPrevalence: 23\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 13/500\nPrevalence: 28\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 14/500\nPrevalence: 28\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 15/500\nPrevalence: 26\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 16/500\nPrevalence: 21\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 17/500\nPrevalence: 23\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 18/500\nPrevalence: 23\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 19/500\nPrevalence: 24\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 20/500\nPrevalence: 27\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 21/500\nPrevalence: 33\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 22/500\nPrevalence: 31\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 23/500\nPrevalence: 34\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 24/500\nPrevalence: 33\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 25/500\nPrevalence: 38\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 26/500\nPrevalence: 38\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 27/500\nPrevalence: 37\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 28/500\nPrevalence: 39\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 29/500\nPrevalence: 37\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 30/500\nPrevalence: 43\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 31/500\nPrevalence: 40\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 32/500\nPrevalence: 45\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 33/500\nPrevalence: 50\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 34/500\nPrevalence: 44\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 35/500\nPrevalence: 48\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 36/500\nPrevalence: 46\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 37/500\nPrevalence: 50\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 38/500\nPrevalence: 53\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 39/500\nPrevalence: 53\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 40/500\nPrevalence: 56\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 41/500\nPrevalence: 60\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 42/500\nPrevalence: 68\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 43/500\nPrevalence: 69\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 44/500\nPrevalence: 74\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 45/500\nPrevalence: 76\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 46/500\nPrevalence: 79\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 47/500\nPrevalence: 82\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 48/500\nPrevalence: 87\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 49/500\nPrevalence: 87\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 50/500\nPrevalence: 93\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 51/500\nPrevalence: 90\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 52/500\nPrevalence: 97\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 53/500\nPrevalence: 102\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 54/500\nPrevalence: 101\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 55/500\nPrevalence: 100\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 56/500\nPrevalence: 104\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 57/500\nPrevalence: 104\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 58/500\nPrevalence: 115\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 59/500\nPrevalence: 108\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 60/500\nPrevalence: 115\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 61/500\nPrevalence: 116\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 62/500\nPrevalence: 124\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 63/500\nPrevalence: 119\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 64/500\nPrevalence: 117\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 65/500\nPrevalence: 117\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 66/500\nPrevalence: 121\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 67/500\nPrevalence: 117\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 68/500\nPrevalence: 118\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 69/500\nPrevalence: 121\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 70/500\nPrevalence: 119\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 71/500\nPrevalence: 117\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 72/500\nPrevalence: 124\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 73/500\nPrevalence: 125\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 74/500\nPrevalence: 129\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 75/500\nPrevalence: 131\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 76/500\nPrevalence: 135\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 77/500\nPrevalence: 140\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 78/500\nPrevalence: 144\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 79/500\nPrevalence: 148\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 80/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 81/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 82/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 83/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 84/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 85/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 86/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 87/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 88/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 89/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 90/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 91/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 92/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 93/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 94/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 95/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 96/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 97/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 98/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 99/500\nPrevalence: 144\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 100/500\nPrevalence: 149\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 101/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 102/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 103/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 104/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 105/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 106/500\nPrevalence: 150\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 107/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 108/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 109/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 110/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 111/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 112/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 113/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 114/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 115/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 116/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 117/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 118/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 119/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 120/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 121/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 122/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 123/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 124/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 125/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 126/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 127/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 128/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 129/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 130/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 131/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 132/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 133/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 134/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 135/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 136/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 137/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 138/500\nPrevalence: 156\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 139/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 140/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 141/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 142/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 143/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 144/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 145/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 146/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 147/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 148/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 149/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 150/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 151/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 152/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 153/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 154/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 155/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 156/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 157/500\nPrevalence: 202\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 158/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 159/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 160/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 161/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 162/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 163/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 164/500\nPrevalence: 199\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 165/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 166/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 167/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 168/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 169/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 170/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 171/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 172/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 173/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 174/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 175/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 176/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 177/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 178/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 179/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 180/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 181/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 182/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 183/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 184/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 185/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 186/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 187/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 188/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 189/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 190/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 191/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 192/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 193/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 194/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 195/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 196/500\nPrevalence: 202\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 197/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 198/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 199/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 200/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 201/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 202/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 203/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 204/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 205/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 206/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 207/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 208/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 209/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 210/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 211/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 212/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 213/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 214/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 215/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 216/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 217/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 218/500\nPrevalence: 202\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 219/500\nPrevalence: 199\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 220/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 221/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 222/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 223/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 224/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 225/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 226/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 227/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 228/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 229/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 230/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 231/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 232/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 233/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 234/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 235/500\nPrevalence: 194\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 236/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 237/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 238/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 239/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 240/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 241/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 242/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 243/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 244/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 245/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 246/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 247/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 248/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 249/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 250/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 251/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 252/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 253/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 254/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 255/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 256/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 257/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 258/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 259/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 260/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 261/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 262/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 263/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 264/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 265/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 266/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 267/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 268/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 269/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 270/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 271/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 272/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 273/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 274/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 275/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 276/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 277/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 278/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 279/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 280/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 281/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 282/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 283/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 284/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 285/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 286/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 287/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 288/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 289/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 290/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 291/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 292/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 293/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 294/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 295/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 296/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 297/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 298/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 299/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 300/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 301/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 302/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 303/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 304/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 305/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 306/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 307/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 308/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 309/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 310/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 311/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 312/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 313/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 314/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 315/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 316/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 317/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 318/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 319/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 320/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 321/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 322/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 323/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 324/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 325/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 326/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 327/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 328/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 329/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 330/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 331/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 332/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 333/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 334/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 335/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 336/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 337/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 338/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 339/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 340/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 341/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 342/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 343/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 344/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 345/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 346/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 347/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 348/500\nPrevalence: 158\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 349/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 350/500\nPrevalence: 152\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 351/500\nPrevalence: 154\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 352/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 353/500\nPrevalence: 151\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 354/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 355/500\nPrevalence: 155\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 356/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 357/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 358/500\nPrevalence: 159\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 359/500\nPrevalence: 157\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 360/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 361/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 362/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 363/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 364/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 365/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 366/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 367/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 368/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 369/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 370/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 371/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 372/500\nPrevalence: 164\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 373/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 374/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 375/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 376/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 377/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 378/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 379/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 380/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 381/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 382/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 383/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 384/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 385/500\nPrevalence: 168\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 386/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 387/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 388/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 389/500\nPrevalence: 160\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 390/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 391/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 392/500\nPrevalence: 161\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 393/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 394/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 395/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 396/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 397/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 398/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 399/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 400/500\nPrevalence: 153\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 401/500\nPrevalence: 163\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 402/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 403/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 404/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 405/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 406/500\nPrevalence: 162\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 407/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 408/500\nPrevalence: 169\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 409/500\nPrevalence: 167\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 410/500\nPrevalence: 166\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 411/500\nPrevalence: 170\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 412/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 413/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 414/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 415/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 416/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 417/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 418/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 419/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 420/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 421/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 422/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 423/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 424/500\nPrevalence: 165\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 425/500\nPrevalence: 172\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 426/500\nPrevalence: 171\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 427/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 428/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 429/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 430/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 431/500\nPrevalence: 174\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 432/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 433/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 434/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 435/500\nPrevalence: 179\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 436/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 437/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 438/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 439/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 440/500\nPrevalence: 183\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 441/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 442/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 443/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 444/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 445/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 446/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 447/500\nPrevalence: 176\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 448/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 449/500\nPrevalence: 180\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 450/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 451/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 452/500\nPrevalence: 192\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 453/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 454/500\nPrevalence: 202\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 455/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 456/500\nPrevalence: 199\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 457/500\nPrevalence: 204\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 458/500\nPrevalence: 199\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 459/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 460/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 461/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 462/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 463/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 464/500\nPrevalence: 196\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 465/500\nPrevalence: 184\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 466/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 467/500\nPrevalence: 201\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 468/500\nPrevalence: 189\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 469/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 470/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 471/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 472/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 473/500\nPrevalence: 200\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 474/500\nPrevalence: 198\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 475/500\nPrevalence: 195\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 476/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 477/500\nPrevalence: 193\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 478/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 479/500\nPrevalence: 197\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 480/500\nPrevalence: 188\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 481/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 482/500\nPrevalence: 190\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 483/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 484/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 485/500\nPrevalence: 173\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 486/500\nPrevalence: 177\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 487/500\nPrevalence: 186\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 488/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 489/500\nPrevalence: 187\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 490/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 491/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 492/500\nPrevalence: 181\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 493/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 494/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 495/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 496/500\nPrevalence: 175\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 497/500\nPrevalence: 182\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 498/500\nPrevalence: 178\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 499/500\nPrevalence: 185\nPopulation Size: 500\n----------------------------\f\nEpidemic Simulation\n----------------------------\nSimulation: 5/5\nTimestep: 500/500\nPrevalence: 191\nPopulation Size: 500\n----------------------------"
  },
  {
    "objectID": "meta/meta.html#meta-regression",
    "href": "meta/meta.html#meta-regression",
    "title": "18  Meta Analysis",
    "section": "18.8 meta Regression",
    "text": "18.8 meta Regression\n\n\nCode\n# 3 level hierarchical mixed model for variance studies\n?rma.mv() # mods = ~a + b + c\n\n\n\n\n\n\nDingman, Harvey F., and Norman C. Perry. 1956. “A Comparison of the Accuracy of the Formula for the Standard Error of Pearson “r” with the Accuracy of Fisher’s z-Transformation.” The Journal of Experimental Education 24 (4): 319–21. https://doi.org/10.1080/00220973.1956.11010555.\n\n\nHald, Anders. 2008. A History of Parametric Statistical Inference from Bernoulli to Fisher, 1713-1935. Springer Science & Business Media.\n\n\nKnapp, Guido, and Joachim Hartung. 2003. “Improved tests for a random effects meta-regression with a single covariate.” Statistics in Medicine 22 (17): 2693–2710. https://doi.org/10.1002/sim.1482.\n\n\nMcMurdie, Paul J., and Susan Holmes. 2014. “Waste Not, Want Not: Why Rarefying Microbiome Data Is Inadmissible.” PLOS Computational Biology 10 (4): e1003531. https://doi.org/10.1371/journal.pcbi.1003531.\n\n\nSheppard, W. F., and Andrew Russell Forsyth. 1899. “III. On the Application of the Theory of Error to Cases of Normal Distribution and Normal Correlation.” Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 192 (January): 101–67. https://doi.org/10.1098/rsta.1899.0003.\n\n\nSidik, Kurex, and Jeffrey N. Jonkman. 2005. “A note on variance estimation in random effects meta-regression.” Journal of Biopharmaceutical Statistics 15 (5): 823–38. https://doi.org/10.1081/BIP-200067915."
  },
  {
    "objectID": "meta/meta.html#subgroup-analysis",
    "href": "meta/meta.html#subgroup-analysis",
    "title": "18  Meta Analysis",
    "section": "18.7 Subgroup Analysis",
    "text": "18.7 Subgroup Analysis\nOnce you have the lists of effect sizes and standard errors associated with each of the studies, now you can combine them “generically” with metagen. The “gen” part stands for generic,\n\n\nCode\nm.gen <- metagen(TE = TE,\n                 seTE = seTE,\n                 studlab = Author, # study labels (optional)\n                 data = ThirdWave, # study information (optional)\n                 sm = \"SMD\", # underlying effect\n                 fixed = FALSE,\n                 random = TRUE,\n                 method.tau = \"REML\",\n                 hakn = TRUE,\n                 title = \"Third Wave Psychotherapies\")\n\n# with prediction interval\nupdate.meta(m.gen, prediction = TRUE)\n\n\nReview:     Third Wave Psychotherapies\n\nNumber of studies combined: k = 18\n\n                             SMD            95%-CI    t  p-value\nRandom effects model (HK) 0.5771 [ 0.3782; 0.7760] 6.12 < 0.0001\nPrediction interval              [-0.0572; 1.2115]              \n\nQuantifying heterogeneity:\n tau^2 = 0.0820 [0.0295; 0.3533]; tau = 0.2863 [0.1717; 0.5944]\n I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 45.50   17  0.0002\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp (HK) adjustment for random effects model (df = 17)\n- Prediction interval based on t-distribution (HTS) (df = 16)\n\n\nAssumes that there’s a random effect, and the heterogeneity is the variance of the true effects from each of the studies.\n\n\nCode\n# need to refit with rma for gosh plot\nm.rma <- rma(yi = m.gen$TE,\n             sei = m.gen$seTE,\n             method = m.gen$method.tau,\n             test = \"knha\")\nsummary(m.rma)\n\n\n\nRandom-Effects Model (k = 18; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n -8.8668   17.7337   21.7337   23.4001   22.5908   \n\ntau^2 (estimated amount of total heterogeneity): 0.0820 (SE = 0.0458)\ntau (square root of estimated tau^2 value):      0.2863\nI^2 (total heterogeneity / total variability):   64.63%\nH^2 (total variability / sampling variability):  2.83\n\nTest for Heterogeneity:\nQ(df = 17) = 45.5026, p-val = 0.0002\n\nModel Results:\n\nestimate      se    tval  df    pval   ci.lb   ci.ub      \n  0.5771  0.0943  6.1222  17  <.0001  0.3782  0.7760  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# gosh(m.rma) # computationally intensive, but used for sensitivity of outliers.\n\n\nWhen doing random effect models, there’s just 2 sources of variance to consider, the sample (within study) and the heterogeneity (between study). together they’re considered the total variance, and there are various metrics to quantify if there is between study heterogeneity.\nWhen doing subgroup analysis, you can start to take into account study characteristics and try to correct for them. In this case, maybe we’d like to split it by two. Careful because they lack power, and often not enough to give differences.\n\n\nCode\nupdate.meta(m.gen, \n            subgroup = RiskOfBias, \n            tau.common = FALSE) # heterogeneity in the subgroups\n\n\nReview:     Third Wave Psychotherapies\n\nNumber of studies combined: k = 18\n\n                             SMD           95%-CI    t  p-value\nRandom effects model (HK) 0.5771 [0.3782; 0.7760] 6.12 < 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0820 [0.0295; 0.3533]; tau = 0.2863 [0.1717; 0.5944]\n I^2 = 62.6% [37.9%; 77.5%]; H = 1.64 [1.27; 2.11]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 45.50   17  0.0002\n\nResults for subgroups (random effects model (HK)):\n                    k    SMD           95%-CI  tau^2    tau     Q   I^2\nRiskOfBias = high   7 0.8126 [0.2835; 1.3417] 0.2423 0.4922 25.89 76.8%\nRiskOfBias = low   11 0.4300 [0.2770; 0.5830] 0.0099 0.0997 13.42 25.5%\n\nTest for subgroup differences (random effects model (HK)):\n                    Q d.f. p-value\nBetween groups   2.84    1  0.0917\n\nDetails on meta-analytical method:\n- Inverse variance method\n- Restricted maximum-likelihood estimator for tau^2\n- Q-Profile method for confidence interval of tau^2 and tau\n- Hartung-Knapp (HK) adjustment for random effects model (df = 17)\n\n\nThere’s the extra section in the middle, in which we can see separate analysis for the two groups. This logic can be extended to meta regression."
  },
  {
    "objectID": "timeseries/timeseries.html#arch",
    "href": "timeseries/timeseries.html#arch",
    "title": "34  Time Series",
    "section": "34.5 ARCH",
    "text": "34.5 ARCH\nArch models have variance that depend on the previous timepoint as well. That is, an ARCH(1) model has dependence on the previous error term.\n\n\\begin{aligned}\ny_t &= b'x_t + \\varepsilon_t \\\\\n\\varepsilon_t &\\overset{i.i.d.}{\\sim} N(0, \\sigma^2_t) \\\\\n\\text{Var}(\\varepsilon_t) = \\sigma^2_t &= \\omega + \\alpha_1\\varepsilon_{t-1}^2\n\\end{aligned}\n\nWe can write the zero centered process a little easier,\n\n\\begin{aligned}\ny_t &= \\sigma_t\\epsilon_t \\\\\n\\sigma_t^2 &= \\omega + \\alpha_1y^2_{t-1} \\\\\n\\varepsilon_t &\\overset{i.i.d}{\\sim} N(0, 1)\n\\end{aligned}\n\nLet’s simulate the ARCH model and see the basic characteristics, we’re simulating a zero mean\n\n\\begin{aligned}\n\\sigma^2_t = 5 + .5 y^2_{t-1}\n\\end{aligned}\n\n\n\nCode\n# helper functions to simulate arch/garch models\nsim_arch <- function(y0 = 2, omega = 5, alpha1 = .5, n = 100) {\n  y <- vector(\"numeric\", n)\n  y[1] <- y0\n  inn <- rnorm(n)\n  for (i in 2:n) {\n    y[i] <- inn[i] * sqrt(omega + alpha1 * y[i-1]^2) # arch\n  }\n  y\n}\n\n\n\n\nCode\nset.seed(1)\ny <- sim_arch(n=1000)\nplot(y, type = \"l\", main = \"Time Series Plot\")\nAcf(y, main = \"ACF y\")\nPacf(y, main = \"PACF y\")\nAcf(y^2, main = \"ACF y^2\")\nPacf(y^2, main = \"PACF y^2\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the time series plot, we can see why this is stochastically volatile plot, since there are regions of the time series that go way more wild when the values start to get high. The characteristics of an ARCH(1) model, are that the series y look like white noise, and y^2 look like an AR(1) process. There’s a spike in the PACF model for lag 1, and a geometric decay in the acf plot of y^2 indicating AR(1).\n\n\nCode\ninvisible(capture.output(\n  mod_arch <- garch(y, order = c(0, 1))\n))\nsummary(mod_arch)\n\n\n\nCall:\ngarch(x = y, order = c(0, 1))\n\nModel:\nGARCH(0,1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.88500 -0.66301 -0.03251  0.65554  3.55454 \n\nCoefficient(s):\n    Estimate  Std. Error  t value Pr(>|t|)    \na0   5.77142     0.38941   14.821  < 2e-16 ***\na1   0.44892     0.05722    7.846 4.22e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 0.0090504, df = 2, p-value = 0.9955\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.076017, df = 1, p-value = 0.7828\n\n\nThe above is the output for fitting an ARCH(1) model, and we can see that the estimates are quite close to one another for the parameters that are controlling the variance of the function. Also notice that we have almost 1000 values that we had to feed in too though, it seems that estimating the variance coefficients is quite inefficient."
  },
  {
    "objectID": "timeseries/timeseries.html#garch",
    "href": "timeseries/timeseries.html#garch",
    "title": "34  Time Series",
    "section": "34.6 GARCH",
    "text": "34.6 GARCH\nA garch model will use the last error value, as well as the last variance for the error model.\n\n\\begin{aligned}\ny_t &= b'x_t + \\varepsilon_t \\\\\n\\varepsilon_t &\\overset{i.i.d.}{\\sim} N(0, \\sigma^2_t) \\\\\n\\text{Var}(\\varepsilon_t) = \\sigma^2_t &= \\omega + \\alpha_1\\varepsilon_{t-1}^2 + \\beta_1\\sigma_{t-1}^2\n\\end{aligned}\n\nThe following we are simulating\n\n\\begin{aligned}\nVar(\\varepsilon_t) = \\sigma^2 = .1 + .1\\varepsilon_{t-1}^2 + .8 \\sigma^2_{t-1}\n\\end{aligned}\n\n\n\nCode\n# see also fGarch::garchSim\nsim_garch <- function(y0 = 2, omega = .1, alpha1 = .1, beta1 = .8, n = 100, xt = 0, b = .4) {\n  if (xt != 0 & length(xt) != n) rlang::abort(\"length of xt should be length of simulation\")\n  y <- vector(\"numeric\", n)\n  err <- vector(\"numeric\", n)\n  sds <- vector(\"numeric\", n)\n  y[1] <- y0\n  inn <- rnorm(n)\n  for (i in 2:n) {\n    sds[i] <- sqrt(omega + alpha1 * err[i-1]^2 + beta1 * sds[i-1]^2)\n    err[i] <- rnorm(1, mean = 0, sd = sds[i])\n    y[i] <- xt * b + err[i]\n    # y[i] <- inn[i] * (sqrt(a + b * y[i-1]^2 + c * inn[i-1]^2)) # garch\n  }\n  y\n}\n# spec <- garchSpec(model = list())\n# y <- garchSim(spec, n = 100)\ninvisible(capture.output(\n  mod_garch <- fGarch::garchFit(formula = ~garch(1, 1), y, include.mean = FALSE)\n))\n\n\n\n\nCode\n# garch\nset.seed(1)\ny <- sim_garch(n=500)\nplot(y, type = \"l\")\nAcf(y) # supposed to be white noise\nPacf(y)\nAcf(y^2) # supposed to be arma pattern\nPacf(y^2) # MA(2) maybe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# fitting the garch model\ninvisible(capture.output(\n  mod_garch <- fGarch::garchFit(~garch(1, 1), y,  include.mean = FALSE)\n))\nsummary(mod_garch)\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n fGarch::garchFit(formula = ~garch(1, 1), data = y, include.mean = FALSE) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x7ff80af103d0>\n [data = y]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n  omega   alpha1    beta1  \n0.14254  0.11326  0.77157  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nomega    0.14254     0.07242    1.968  0.04904 *  \nalpha1   0.11326     0.04074    2.780  0.00544 ** \nbeta1    0.77157     0.08115    9.508  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n -751.9354    normalized:  -1.503871 \n\nDescription:\n Tue Jan 31 12:37:01 2023 by user:  \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  0.9215242 0.6308027 \n Shapiro-Wilk Test  R    W      0.9976149 0.7017248 \n Ljung-Box Test     R    Q(10)  16.02394  0.09894901\n Ljung-Box Test     R    Q(15)  27.95652  0.0218428 \n Ljung-Box Test     R    Q(20)  38.01825  0.00881023\n Ljung-Box Test     R^2  Q(10)  8.714543  0.5593918 \n Ljung-Box Test     R^2  Q(15)  22.62514  0.09242972\n Ljung-Box Test     R^2  Q(20)  29.21913  0.08353343\n LM Arch Test       R    TR^2   13.13471  0.3593258 \n\nInformation Criterion Statistics:\n     AIC      BIC      SIC     HQIC \n3.019742 3.045029 3.019670 3.029665 \n\n\n\nJarque-Bera Test - null is normally distributed residuals\nLjung-Box - is portmanteau test for autocorrelation, testing for a set of lags.\nLM arch test - fit the model, and examine the square of the residuals for length of ARCH lags. Null is that there is no AR trend in the squared residuals. See ARCH wiki\n\n\n\nCode\ncoef(mod_garch) |> kbl() |> kable_minimal(full_width = FALSE)\n\n\n\n\n \n  \n      \n    x \n  \n \n\n  \n    omega \n    0.1425396 \n  \n  \n    alpha1 \n    0.1132581 \n  \n  \n    beta1 \n    0.7715676"
  },
  {
    "objectID": "timeseries/timeseries.html#exponential-smoothing",
    "href": "timeseries/timeseries.html#exponential-smoothing",
    "title": "34  Time Series",
    "section": "34.3 Exponential smoothing",
    "text": "34.3 Exponential smoothing\n\nets() is probably the function you want\n\n\n\nCode\ndata(oil)\nfc <- ses(oil, h = 5, alpha = .1) # can leave out the alpha to estimate alpha instead\n\n# oil |> autoplot() +\n#   geom_line(data = data.frame(time = index(fitted(fc)),\n#                        fitted = fitted(fc)),\n#             mapping = aes(time, fitted),\n#             color = \"red\")\n\nautoplot(fc) + \n  autolayer(fitted(fc)) +\n  theme(legend.position =\"none\")"
  },
  {
    "objectID": "timeseries/timeseries.html#tsrepr",
    "href": "timeseries/timeseries.html#tsrepr",
    "title": "34  Time Series",
    "section": "34.4 TSrepr",
    "text": "34.4 TSrepr\nThis R package is designed to give smaller dimension representations of time series. If there are n observations of the time series, this package would like to represent that as p. parameters. This roughly equates to nonparametric regression of the signals. We’ll investigate how these work by following along in the tutorial given:\n\n\nCode\ndata(\"elec_load\")\n\nelec <- as.numeric(elec_load[1,])\n \nggplot(data.frame(Time = 1:length(elec), Value = elec), aes(Time, Value)) +\n  geom_line() +\n  theme_bw()\n\n\n\n\n\n\n\nCode\n# DWT with level of 2^3\ndata_dwt <- repr_dwt(elec, level = 3) # discrete wavelet transform\n# first 84 DFT coefficients are extracted and then inverted\ndata_dft <- repr_dft(elec, coef = 84)\n# first 84 DCT coefficients are extracted and then inverted\ndata_dct <- repr_dct(elec, coef = 84)\n# Classical PAA\ndata_paa <- repr_paa(elec, q = 8, func = mean)\n\n\n\n\nCode\ndata_plot <- data.frame(Value = c(data_dwt, data_dft, data_dct, data_paa),\n                        Time = rep(1:length(data_dwt), 4),\n                        Method = factor(rep(c(\"DWT\", \"DFT\", \"DCT\", \"PAA\"),\n                                            each = length(data_dwt))))\n \nggplot(data_plot, aes(Time, Value)) +\n  geom_line(aes(color = Method), alpha = 0.80, size = 0.8) +\n  geom_line(mapping = aes(Time, Value), data = data.frame(Time = 1:length(elec), Value = elec)) + \n  theme_bw()"
  },
  {
    "objectID": "timeseries/timeseries.html#arimax",
    "href": "timeseries/timeseries.html#arimax",
    "title": "34  Time Series",
    "section": "34.2 ARIMAX",
    "text": "34.2 ARIMAX\nThe general pattern for univariate time series analysis is as follows\n\nfigure out how to make stationary\n\n\nby trend modeling (exogenous), or differencing\n“Breusch-Godfrey” : up to p order ar\n“Durbin-Watson” : first order ar testing\n\n\n\n\n\n34.2.1 base tools\n\n\nCode\n# acf, ccf, pacf, arima, arima.sim\n\n\n\n\n34.2.2 AR(1) and MA(1)\nAutoregressive of order 1 is the simplest time series you can have.\n\n\\begin{aligned}\nY_t = \\alpha Y_{t-1} + \\varepsilon_t\n\\end{aligned}\n\n\n\n34.2.3 Simulated Examples\nthis section we’ll simulate a number of AR and MA models so that you get a better sense of what they look like.\n\n\nCode\n# AR models\nsim_ar <- tibble(p = 1:3,\n                 ar = list(.7,\n                           c(.7,-.3),\n                           c(.7, -.3, .5)),\n                 n = 200) %>% \n  rowwise() %>% \n  mutate(y = list(arima.sim(list(ar = ar), n = 200)),\n         x = list(time(y)))\n\nsim_ar %>% unnest(c(x, y)) %>% \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  facet_grid(p~.)\n\n\n\n\n\nAR Model Simulations\n\n\n\n\n\n\nCode\n# MA processes\nsim_ma <- tibble(q = 1:3,\n                 ma = list(.7,\n                           c(.7,-.3),\n                           c(.7, -.3, .5)),\n                 n = 200) %>% \n  rowwise() %>% \n  mutate(y = list(arima.sim(list(ma = ma), n = 200)),\n         x = list(time(y))) %>% \n  ungroup()\n\nsim_ma %>% unnest(c(x, y)) %>% \n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  facet_grid(q~.)\n\n\n\n\n\nMA Model Simulations\n\n\n\n\n\n\n34.2.4 Analyzing simulations\n\n\nCode\n# ar1\nset.seed(1)\nar1 <- arima.sim(list(order = c(1, 0, 0), ar = .2), 100)\n\ninvisible(capture.output(\n  mod_sar1 <- sarima(ar1, p = 1, d = 0, q = 0) # asta\n  ))\n\n\n\n\n\nCode\nmod_Ar1 <- Arima(ar1, order = c(1, 0, 0)) # forecast\nmod_ar1 <- arima(ar1, order = c(1, 0, 0)) # base\n\nbind_rows(\n  tidy(mod_Ar1) |> add_column(model = \"forecast::Arima\", .before = 1),\n  tidy(mod_ar1) |> add_column(model = \"stats::arima\", .before = 1),\n  mod_sar1$ttable[, 1:2] |> \n  as_tibble(rownames = \"term\", .name_repair = ~c(\"estimate\", \"std.error\")) |>\n  add_column(model = \"astsa::sarima\", .before = 1)\n) |> arrange(desc(term), model) |> \n  kable() |> kable_minimal()\n\n\n\n\n \n  \n    model \n    term \n    estimate \n    std.error \n  \n \n\n  \n    astsa::sarima \n    xmean \n    0.1021000 \n    0.1125000 \n  \n  \n    forecast::Arima \n    intercept \n    0.1020681 \n    0.1125445 \n  \n  \n    stats::arima \n    intercept \n    0.1020681 \n    0.1125445 \n  \n  \n    astsa::sarima \n    ar1 \n    0.2171000 \n    0.0978000 \n  \n  \n    forecast::Arima \n    ar1 \n    0.2170632 \n    0.0977980 \n  \n  \n    stats::arima \n    ar1 \n    0.2170632 \n    0.0977980 \n  \n\n\n\n\n\n\n\nCode\n# arx1\nset.seed(1)\nx <- sort(runif(400))\nbeta <- 2\narx1 <- arima.sim(list(ar=.3), 400) + x * beta\n\nmod_Arx1 <- Arima(arx1, order = c(1, 0, 0), xreg = x)\nmod_arx1 <- arima(arx1, order = c(1, 0, 0), xreg = x)\ninvisible(capture.output(\n  mod_sarx1 <- sarima(arx1, p = 1, d = 0, q = 0, xreg = x)\n))\n\n\n\n\n\nCode\nbind_rows(\n  mod_Arx1 |> tidy()|> add_column(model = \"forecast::Arima\", .before = 1),\n  mod_arx1 |> tidy() |> add_column(model = \"stats::arima\", .before = 1),\n  mod_sarx1$ttable[,1:2] |>\n    as_tibble(rownames = \"term\", .name_repair = ~c(\"estimate\", \"std.error\")) |>\n    add_column(model = \"astsa::sarima\", .before = 1)\n) |> arrange(desc(term)) |> \n  kbl() |> kable_minimal()\n\n\n\n\n \n  \n    model \n    term \n    estimate \n    std.error \n  \n \n\n  \n    forecast::Arima \n    xreg \n    1.9302542 \n    0.2520409 \n  \n  \n    astsa::sarima \n    xreg \n    1.9303000 \n    0.2520000 \n  \n  \n    stats::arima \n    x \n    1.9302542 \n    0.2520409 \n  \n  \n    forecast::Arima \n    intercept \n    0.0275105 \n    0.1427769 \n  \n  \n    stats::arima \n    intercept \n    0.0275105 \n    0.1427769 \n  \n  \n    astsa::sarima \n    intercept \n    0.0275000 \n    0.1428000 \n  \n  \n    forecast::Arima \n    ar1 \n    0.2709738 \n    0.0481873 \n  \n  \n    stats::arima \n    ar1 \n    0.2709738 \n    0.0481873 \n  \n  \n    astsa::sarima \n    ar1 \n    0.2710000 \n    0.0482000 \n  \n\n\n\n\n\nIt’s pretty clear from both of these tests, of the AR1 model and the AR1X model that they estimate very similarly,\n\n\n34.2.5 Diagnostics\nThere are hypothesis tests, and graphical checks for stationarity\nunit root tests are for stability/stationarity:\n\nAugmented Dickey Fulley Test\n\nwant low p value\n\nKPSS Test\n\nwant high p value for stationarity\n\n\n\n\n34.2.6 Examples: LA County\nWe just examine the cardiovascular mortality from the LA Pollution study. These are average weekly cv mortality rates from astsa package.\n\n\nCode\n# ?cmort\nx <- cmort %>% as.numeric()\nx1 <- x %>% lag()\n# yt = u + beta * yt-1\ncmort_lm <- lm(x~x1)\n\ncbind(coef(cmort_lm)[1] + coef(cmort_lm)[2] * x1,\n      c(NA, fitted(cmort_lm)),\n      c(NA, predict(cmort_lm))) %>% \n  head() # fitted/predicted values match up\n\n\n\\begin{bmatrix} NA &NA &NA \\\\95.7381108162687 &95.7381108162684 &95.7381108162687 \\\\100.978136273105 &100.978136273105 &100.978136273105 \\\\93.0447840350672 &93.0447840350672 &93.0447840350672 \\\\95.8924561905496 &95.8924561905496 &95.8924561905496 \\\\94.1946570734599 &94.1946570734598 &94.1946570734599 \\\\ \\end{bmatrix}\n\n\n\n\nCode\nqplot(time(cmort), x, geom = \"line\", color = \"black\") +\n  geom_line(aes(y =  c(NA, fitted(cmort_lm)), color = \"red\")) +\n  scale_color_manual(values = c(\"black\", \"red\"),\n                     label = c(\"raw\", \"fitted\"))\n\n\nDon't know how to automatically pick scale for object of type ts. Defaulting to continuous.\n\n\nWarning: Removed 1 row(s) containing missing values (geom_path).\n\n\n\n\n\n\n\n34.2.7 Example: AirPassengers\n\n\nCode\nAirPassengers %>% plot()\n\n\n\n\n\n\n\nCode\nsarima(AirPassengers, d = 1, p = 2, q = 0)\n\n\ninitial  value 3.522169 \niter   2 value 3.456359\niter   3 value 3.447683\niter   4 value 3.447001\niter   5 value 3.447000\niter   6 value 3.447000\niter   6 value 3.447000\niter   6 value 3.447000\nfinal  value 3.447000 \nconverged\ninitial  value 3.441124 \niter   2 value 3.441115\niter   3 value 3.441115\niter   4 value 3.441114\niter   4 value 3.441114\niter   4 value 3.441114\nfinal  value 3.441114 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2  constant\n      0.3792  -0.2314    2.4075\ns.e.  0.0823   0.0834    3.0636\n\nsigma^2 estimated as 973.4:  log likelihood = -694.99,  aic = 1397.98\n\n$degrees_of_freedom\n[1] 140\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.3792 0.0823  4.6065  0.0000\nar2       -0.2314 0.0834 -2.7767  0.0062\nconstant   2.4075 3.0636  0.7858  0.4333\n\n$AIC\n[1] 9.77605\n\n$AICc\n[1] 9.777257\n\n$BIC\n[1] 9.858927"
  },
  {
    "objectID": "categorical/categorical.html#evaluating-results",
    "href": "categorical/categorical.html#evaluating-results",
    "title": "5  Categorical Data Analysis",
    "section": "5.6 Evaluating Results",
    "text": "5.6 Evaluating Results\nThere are a lot of multiclass metrics that are difficult to keep straight, especially as it comes to confusion matrix."
  },
  {
    "objectID": "mixed/mixed.html#exploring-rxode",
    "href": "mixed/mixed.html#exploring-rxode",
    "title": "19  Mixed Models",
    "section": "29.1 Exploring RxODE",
    "text": "29.1 Exploring RxODE\nRxODE is a package that simulates nonlinear mixed models with ODE language"
  },
  {
    "objectID": "matrix/matrix.html#matrix-differentials",
    "href": "matrix/matrix.html#matrix-differentials",
    "title": "17  Matrix Math",
    "section": "17.6 Matrix differentials",
    "text": "17.6 Matrix differentials\n\n17.6.1 Example: Trace\nMatrix differentials are confusing as hell, but normally we want to optimize some functions by linearizing around some point. That is if we perturb the matrix at some particular point, what is the effect that we’ll see?\nThe animals of the derivative zoo are plenty. We can define the derivative of a function that is a combination of scalar, vector, matrix arguments with scalar, vector, matrix results. This makes the concept of the differential (and derivative) confusing because the layout of where the derivatives are all different. What’s worse, is that authors generally don’t agree on the form that these derivatives should take,\nLet’s take the example of a matrix argument, with a scalar output. For the first example, let’s take a linear function and show how the derivative can give us an approximation of the perturbation. Suppose f(X) = \\mathrm{tr}(AX) where A\\in\\mathbb{R}^{m\\times n} is some given constant matrix, and X \\in \\mathbb{R}^{n\\times p}.\n\n\\mathrm{D}f = \\frac{\\mathrm{d}\\, f}{\\mathrm{d}X} = \\frac{\\mathrm{d}\\,\\mathrm{tr}(AX)}{\\mathrm{d}X} = \\begin{cases}\nA & \\text{numerator layout} \\\\\nA' & \\text{denominator layout} \\\\\nA' & \\text{mixed layout} \\\\\n(\\mathrm{vec}A')' & \\text{vectorized layout}\n\\end{cases}\n This notation reads “the derivative of f with respect to X”, and I think it’s quite bad. We note that the differential is much more clear about the effect of a perturbation.\n\n\\begin{aligned}\n\\mathrm{d}\\,\\mathrm{tr}(AX) =  \\mathrm{tr} (A\\,\\mathrm{d}X)\n\\end{aligned}\n\nLet’s let A = \\begin{bmatrix}4 & 2 \\\\ 3 & 2\\end{bmatrix} and just so I can plot the surface, X = \\begin{bmatrix}x & 2 \\\\ y & -1\\end{bmatrix}. If I vary both x and y, I can plot some surface with components (x, y, f(X)).\n\n\nCode\nA <- matrix(c(4, 2, \n              3, 2), ncol = 2, byrow = TRUE)\nsum(diag(A)) # trace\n\n\n[1] 6\n\n\nCode\n# Just so we can visualize the surface of \nx <- seq(-3, 3, .1)\ny <- seq(-3, 3, .1)\n\nf <- Vectorize(function(x, y) {\n  X <- matrix(c(x, 2,\n                y, -1), ncol = 2, byrow = TRUE)\n  sum(diag(A %*% X))\n})\n\n# f(1, 3)\n# f(1, 4)\n# f(2, 3)\n# f(2, 4)\n# outer(1:2, 3:4, FUN = f) # columns iterate first arg\n\nZ <- outer(x, y, f)\n\n# it's a linear function, as in it only depends on the matrix A\nfplot <- plotly::plot_ly(x = ~x, y = ~y, scene = \"scene\") |> add_surface(z = Z)\nconfig(fplot, displaylogo = FALSE) |> layout(scene = list(camera = list(eye = list(x = -1.25, y = -1.25, z = .5))))\n\n\n\n\n\n\nSuppose we want to know f(X + H) using our nicely calculated derivatives, we should have some taylor series looking approximation, linearization\n\n\\begin{aligned}\nf(X + U) = f(X) + \\fbox{?(U)?} + R(H)\n\\end{aligned}\n where R is the remainder dependent on U that goes to 0 faster than linear. Roughly we know that \\fbox{?U?}, should be a function of the step U. Most of the forms of Taylor’s theorem I’ve seen don’t work here because U is a matrix.\n\nf(x + a) = f(x) + af'(x) + R(a) (scalar -> scalar)\nf(\\mathbf{x} + \\mathbf{a}) = f(\\mathbf{x}) + \\mathbf{a}'\\nabla f(x) + R(\\mathbf{a}) (vector -> scalar)\nf(\\mathbf{x} + \\mathbf{a}) = f(\\mathbf{x}) + \\mathbf{a} \\cdot \\nabla f(x) + R(\\mathbf{a}) (vector -> scalar)\nf(\\mathbf{x} + \\mathbf{a}) = f(\\mathbf{x}) + \\langle \\nabla f(\\mathbf{x}), \\mathbf{a}\\rangle + R(\\mathbf{a}) (vector -> scalor)\nf(X + U) = f(\\mathbf{x}) + \\langle D f(X), U\\rangle + R(\\mathbf{a}) (matrix -> scalar)\n\nSince \\mathrm{tr} is linear in this case, we can figure out what the answer should be,\n\n\\begin{aligned}\n\\mathrm{tr}(A(X + H)) &= \\mathrm{tr}(AX) + \\mathrm{tr}(AH) \\\\\n&=  \\mathrm{tr}(AX) + \\langle A', H\\rangle_F \\\\\n&=  \\mathrm{tr}(AX)+  \\mathrm{vec(A')}'\\mathrm{vec}(H)\n\\end{aligned}\n\nwhere \\langle A, H\\rangle is the standard inner product on real valued matrices (frobenius innner product).\nThe easiest method I see is just substitude H for \\mathrm{d}X in the differential equation, so something like:\n\n\\begin{aligned}\nf(X + U) = f(X) + \\mathrm{d}f\\big|_{\\mathrm{d}X = U} \\\\\nf(X + U) = f(X) + \\langle Df, U\\rangle\n\\end{aligned}\n The inner product formulation also works quite well and gets us the right answer when using the denominator or mixed layout of derivative.\n\n\n17.6.2 Example: Determinant\nNow let’s walk through an example that is not a linear function, something more interesting like the determinant of the gram matrix.\n\n\\begin{aligned}\ng(X) &=|X'X| \\\\\n\\mathrm{d}g &= 2|X'X|\\,\\mathrm{tr}\\left[\\mathrm{(X'X)^{-1}X'\\mathrm{d}X}\\right]\n\\end{aligned}\n The linear approximation is thus,\n\ng(X + U)\\approx |X'X| + 2|X'X|\\mathrm{tr}\\left[(X'X)^{-1}X'U\\right]\n while the quadratic approximation is\n\n\\begin{aligned}\ng(X+U)&\\approx g(X) + \\mathrm{d}g(X) + \\frac{1}{2}\\mathrm{d}^2g(X) \\\\\n&\\approx |X'X| + 2|X'X|\\mathrm{tr}\\left[(X'X)^{-1}X'U\\right] + \\\\ &\\qquad \\left(2|X'X|\\mathrm{tr}\\left[(X'X)^{-1}X'U\\right]\\right)\\mathrm{tr}\\left[(X'X)^{-1}X'U\\right] + \\\\&\\qquad\n|X'X|\\mathrm{tr}\\left[-(X'X)^{-1}(X'U +U'X)(X'X)^{-1}X'U + U'(X'X)^{-1}U\\right]\n\\end{aligned}\n\nJust so we can plot the thing, again we’re constraining the matrix to be X=\\begin{bmatrix}x & 2 \\\\ -1 & y\\end{bmatrix}. Let’s do the approximation at x=y=1, so X = \\begin{bmatrix}1 & 2 \\\\ -1 & 1\\end{bmatrix}, and extend the approximation by dx = dy = 1 in each direction, so U=\\begin{bmatrix}dx & 0 \\\\ 0 & dy\\end{bmatrix}.\n\n\nCode\nA <- matrix(c(4, 2, \n              3, 2), ncol = 2, byrow = TRUE)\ng <- Vectorize(function(x, y) {\n  X <- matrix(c(x, 2,\n                -1, y), ncol = 2, byrow = T)\n  det(t(X) %*% X)\n})\n\nx <- seq(-2.5,2.5, .1)\ny <- seq(-2.5,2.5, .1)\n\nZ <- outer(x, y, g)\n# X <- matrix(c(0, 2,\n#               -1, 0), ncol = 2, byrow = T)\n# g(.1, -.1)\n\n# plot without any approximations\nfplot <- plotly::plot_ly(x = ~x, y = ~y, scene = \"dflkj\") |> add_surface(z = Z, showlegend = F)\n# scene <-  list(camera = list(eye = list(x = -1.25, y = -1.25, z = .5)))\n# config(fplot, displaylogo = FALSE) |> layout(scene = scene)\n\n# Linear approximation\ndg <- Vectorize(function(x, y, dx, dy) {\n  X <- matrix(c(x, 2,\n                -1, y), ncol = 2, byrow = T)\n  U <- matrix(c(dx, 0,\n                0, dy), ncol = 2, byrow = T)\n  2 * det(crossprod(X)) *sum(diag(solve(crossprod(X)) %*% t(X) %*% U))\n})\ndx <- seq(-1, 1, .1)\ndy <- seq(-1, 1, .1)\n\ndg_surf <- outer(dx, dy, dg, x = 1, y = 1)\n\napprox_Z <- g(1, 1) + dg_surf # linear approximation\n\nfplot1 <- plotly::plot_ly(x = ~x, y = ~y, scene = \"scene\") |> \n  add_surface(z = Z, showscale=F) |> \n   add_surface(x = ~1 + dx, y =~ 1 + dy, z = approx_Z, \n               inherit = FALSE, \n               # cmin = min(Z), cmax = max(Z),\n               colorscale = list(c(0, 1), c(\"#FFFAEB\", \"#FF2704\")),\n               showscale = FALSE,\n               scene = \"scene\")\n\n# quadratic approximation\nd2g <- Vectorize(function(x,y, dx, dy) {\n  # don't look at this its ugly :P\n  X <- matrix(c(x, 2,\n                -1, y), ncol = 2, byrow = T)\n  U <- matrix(c(dx, 0,\n                0, dy), ncol = 2, byrow = T)\n  xx <- crossprod(X)\n  xx1 <- solve(crossprod(X))\n  xx1x <- xx1 %*% t(X)\n  xx1xU <- xx1x %*% U\n  2 * dg(x,y,dx,dy) * sum(diag(xx1xU)) + 2 * det(xx) * sum(diag(\n    -xx1xU  %*% xx1xU - xx1 %*% t(U)%*% X %*% xx1xU + U %*% xx1 %*% U\n  ))\n})\ndx <- seq(-1, 1, .1)\ndy <- seq(-1, 1, .1)\ndg2_surf <- outer(dx, dy, d2g, x = 1, y = 1)\n\napprox_Z2 <- g(1, 1) + dg_surf + dg2_surf / 2 # quadratic approximation\nfplot2 <- plot_ly(x= ~x, y = ~y, scene = \"scene2\") |>\n  add_surface(z = Z, showscale = FALSE) |> \n  add_surface(x = ~1 + dx, y =~ 1 + dy, z = approx_Z2, \n               inherit = FALSE, \n               # cmin = min(Z), cmax = max(Z),\n               colorscale = list(c(0, 1), c(\"#FFFAEB\", \"#FF2704\")),\n              showscale = FALSE,\n              scene = \"scene2\")\n\n# custom grid style\naxx <- list(\n  gridcolor='rgb(255, 255, 255)',\n  zerolinecolor='rgb(255, 255, 255)',\n  showbackground=TRUE,\n  backgroundcolor='rgb(230, 230,230)'\n)\nall_plot <- subplot(fplot1, fplot2, nrows = 1) |>\n  config(displaylogo = FALSE) |>\n  layout(title = \"Approximations to det(X'X)\",\n         # scene = list(domain = list(x = c(0, .5), y = c(0, 1)),\n         #              xaxis = axx, yaxis = axx, zaxis = axx,\n         #              aspectmode = \"cube\",\n         #              camera = list(eye = list(x = .3, y = -2.3, z = .3))),\n         # scene2 = list(domain = list(x = c(.5, 1), y = c(0, 1)),\n         #               xaxis = axx, yaxis = axx, zaxis = axx,\n         #               aspectmode = \"cube\",\n         #               camera = list(eye = list(x = .3, y = -2.3, z = .3))),\n         annotations = list(\n           list(text = \"Linear Approximation\",\n                x = .25,\n                y = .9,\n                showarrow = FALSE,\n                xanchor = \"center\",\n                yanchor = \"bottom\"),\n           list(text = \"Quadratic Approximation\",\n                x = .75,\n                y = .9,\n                showarrow = FALSE,\n                xanchor = \"center\",\n                yanchor = \"bottom\")\n         ))\n# library(htmlwidgets)\n# saveWidget(all_plot, file = \"det_approx.html\", selfcontained = FALSE, libdir = \"plotly_lib\")\nfplot # |> layout(dflkj = list(domain = list(x = .3, y=-2.3, z=.3))) # no idea why this one is skipped\n\n\n\n\n\n\n\n\nCode\nall_plot\n\n\nWarning: 'layout' objects don't have these attributes: 'NA'\nValid attributes include:\n'_deprecated', 'activeshape', 'annotations', 'autosize', 'autotypenumbers', 'calendar', 'clickmode', 'coloraxis', 'colorscale', 'colorway', 'computed', 'datarevision', 'dragmode', 'editrevision', 'editType', 'font', 'geo', 'grid', 'height', 'hidesources', 'hoverdistance', 'hoverlabel', 'hovermode', 'images', 'legend', 'mapbox', 'margin', 'meta', 'metasrc', 'modebar', 'newshape', 'paper_bgcolor', 'plot_bgcolor', 'polar', 'scene', 'selectdirection', 'selectionrevision', 'separators', 'shapes', 'showlegend', 'sliders', 'spikedistance', 'template', 'ternary', 'title', 'transition', 'uirevision', 'uniformtext', 'updatemenus', 'width', 'xaxis', 'yaxis', 'barmode', 'bargap', 'mapType'\n\n\n\n\n\n\n\n\n17.6.3 Example: Matrix Powers\nThis is an example of a matrix value, with matrix argument. Suppose we want to find some linear approximation of the matrix power function:\n\n\\begin{aligned}\nh &= X^3 \\\\\n\\mathrm{d}h &= (\\mathrm{d}X)XX + X (\\mathrm{d}X)X + XX(\\mathrm{d}X) \\\\\n\\mathrm{d}^2h &= 2\\left[(\\mathrm{d}X)(\\mathrm{d}X)X + (\\mathrm{d}X) X (\\mathrm{d}X) + X(\\mathrm{d}X)(\\mathrm{d}X)\\right] \\\\\n\\mathrm{d}^3h &= 6(dX)^3\n\\end{aligned}\n\n\n\nCode\n# plot options\npar(mar = c(0, 0, 2, 0))\n\nmat_pow3 <- Vectorize(function(x, y) {\n  X <- matrix(c(x, 2,\n                y, -1), ncol = 2, byrow = T)\n  X %^% 3\n}, SIMPLIFY = FALSE)\n\nx <- seq(-1, 1, .1)\ny <- seq(-1, 1, .1)\n\ndh <- Vectorize(function(x, y, dx, dy) {\n  X <- matrix(c(x, 2,\n                y, -1), ncol = 2, byrow = T)\n  U <- matrix(c(dx, 0,\n                dy, 0), ncol = 2, byrow = T)\n  xx <- X %*% X\n  U %*% xx + X %*% U %*% X + xx %*% U\n}, SIMPLIFY = FALSE)\nZ <- outer(x, y, mat_pow3) # The matrix surface\n# norm(Z[[1,3]], type = \"F\")\n\nZ_frob <- mapply(norm, Z, type = \"F\") # apply frobenius\ndim(Z_frob) <- dim(Z)\npersp(x, y,Z_frob, phi = 30, theta = 45, ticktype = \"detailed\", shade = .4, col = \"cadetblue1\", main = expression(\"\\u2016\"~h(X)~\"\\u2016\"[F]))\ndy <- dx <- seq(-1, 1, .1)\n\n# dh(1, 1, .5, .3)\ndh_surf <- outer(dx, dy, dh, x = 0, y = 0)\n\n# linear approximations\napprox_Z <- mapply(function(x, y) list(x - (mat_pow3(0, 0)[[1]] + y)), Z, dh_surf)\ndim(approx_Z) <- dim(Z)\n\n# frobenius residuals of linear approximation\nres_linear_frob <- mapply(norm, approx_Z, type = \"F\")\ndim(res_linear_frob) <- dim(Z)\npersp(dx, dy, res_linear_frob, phi = 20, theta = 50, ticktype = \"detailed\", nticks = 3,  zlim = c(-.01, 8), main = expression(\"\\u2016\"~h(X) - h[1](X)~\"\\u2016\"[F]))\n# quadratic approximation\ndh2 <- Vectorize(function(x, y, dx, dy) {\n  X <- matrix(c(x, 2,\n                y, -1), ncol = 2, byrow = T)\n  U <- matrix(c(dx, 0,\n                dy, 0), ncol = 2, byrow = T)\n  UU <- U %*% U\n  2 * (UU %*% X + U %*% X %*% U + X %*% UU)\n}, SIMPLIFY = FALSE)\n\ndh2_surf <- outer(dx, dy, dh2, x=0, y = 0)\n\n# h(X + H) - [h(X) + dh(X) + d^2h(X) / 2]\napprox_Z2 <- mapply(function(x, y, z) list(x - (mat_pow3(0, 0)[[1]] + y + z/2)), Z, dh_surf, dh2_surf)\n\nres_quad_frob <- mapply(norm, approx_Z2, type = \"F\")\ndim(res_quad_frob) <- dim(Z)\npersp(dx, dy, res_quad_frob, phi = 20, theta = 50, ticktype = \"detailed\", nticks = 3,  zlim = c(-.01, 8), main = expression(\"\\u2016\"~h(X) - h[2](X)~\"\\u2016\"[F]))\n# mapply(rep, 1:4, 4:1)\n\n# cubic approx? \ndh3 <- Vectorize(function(x, y, dx, dy) {\n  X <- matrix(c(x, 2,\n                y, -1), ncol = 2, byrow = T)\n  U <- matrix(c(dx, 0,\n                dy, 0), ncol = 2, byrow = T)\n  6 * U%^%3\n}, SIMPLIFY = FALSE)\n\ndh3_surf <- outer(dx, dy, dh3, x=0, y = 0)\napprox_Z3 <- mapply(function(x, y, z, w) list(x - (mat_pow3(0, 0)[[1]] + y + z/2 + w/6)), Z, dh_surf, dh2_surf, dh3_surf)\nres_cubic_frob <- mapply(function(x) round(norm(x, type = \"F\"), 10), approx_Z3)\ndim(res_cubic_frob) <- dim(Z)\npersp(dx, dy, res_cubic_frob, phi = 20, theta = 50, ticktype = \"detailed\", nticks = 3, zlim = c(-.01, 8), mar = c(0, 0, 0, 0), main = expression(\"\\u2016\"*h(X) - h[3](X)*\"\\u2016\"[F]))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPanel 1 shows the Frobenius norm of the matrix power, which is some quantification of how the matrices are changing. Then in subsequent panels, we show that\n\n\n\n\nAnderson, William N., and Thomas D. Morley. 1985. “Eigenvalues of the Laplacian of a Graph.” Linear and Multilinear Algebra 18 (2): 141–45. https://doi.org/10.1080/03081088508817681.\n\n\nChung, Fan R. K. 1997. Spectral Graph Theory. American Mathematical Soc. https://books.google.com?id=YUc38_MCuhAC.\n\n\nHong, Yuan, Jin-Long Shu, and Kunfu Fang. 2001. “A Sharp Upper Bound of the Spectral Radius of Graphs.” Journal of Combinatorial Theory, Series B 81 (2): 177–83. https://doi.org/10.1006/jctb.2000.1997.\n\n\nMcKay, Brendan D., and Adolfo Piperno. 2013. “Practical Graph Isomorphism, II.” arXiv. https://doi.org/10.48550/arXiv.1301.1493.\n\n\nNewman, Mark. 2018. Networks. 2nd ed. Oxford, New York: Oxford University Press.\n\n\nNilli, A. 1991. “On the Second Eigenvalue of a Graph.” Discrete Mathematics 91 (2): 207–10. https://doi.org/10.1016/0012-365X(91)90112-F."
  },
  {
    "objectID": "nonlinear/nonlinear.html#standard-nonlinear-models",
    "href": "nonlinear/nonlinear.html#standard-nonlinear-models",
    "title": "24  Nonlinear",
    "section": "24.1 Standard Nonlinear Models",
    "text": "24.1 Standard Nonlinear Models\nUse nls mostly, but it can be quite clunky to use. Consider more robust tool"
  },
  {
    "objectID": "nonlinear/nonlinear.html#dose-response-curves",
    "href": "nonlinear/nonlinear.html#dose-response-curves",
    "title": "24  Nonlinear",
    "section": "24.2 Dose Response Curves",
    "text": "24.2 Dose Response Curves\nDose Response curves are popular in the physical sciences and many biological methods also require some form of calibration.\nFor example, spectroscopy is common to determine the concentration of a particular analyte in a solution. This involves shining a light onto the chemical.\n\ntransmittance : T = I / I_0 light intensity measured by the initial light intensity supplied\nabsorbance : \\log T the amount of light absorbed, the signal\n\nThe models that you can fit are numerous\n\nLog Logistic\nWeibull\n\nWe follow along a blog for this section.\nryegrass is the dataset here, which just has 2 variables\n\n\nCode\nryegrass |> ggplot(aes(x = conc, y = rootl)) +\n  geom_point() + \n  geom_smooth(se = FALSE, color = \"red\", size = .5, method = \"loess\", formula = y~x)\n\n\n\n\n\nThe most popular model to fit to dose responses with a log logistic function\n\n\nCode\nmod <- drm(rootl~conc, data = ryegrass, fct=LL.4(names = c(\"Slope\", \"Lower Limit\", \"Upper Limit\", \"ED50\")))\nplot(mod, type = \"all\") # average\n\n\n\n\n\nCode\nED(mod, c(10, 20, 50), interval = \"delta\") # effective dose, with delta method intervals\n\n\n\nEstimated effective doses\n\n       Estimate Std. Error   Lower   Upper\ne:1:10  1.46371    0.18677 1.07411 1.85330\ne:1:20  1.92109    0.17774 1.55032 2.29186\ne:1:50  3.05795    0.18573 2.67053 3.44538\n\n\nIf we want to work on a percentage scale 0-100, we can transform our data and then fix some parameters of the log logistic function that’s being fit.\n\n\nCode\nmean_rootl_control <- mean(ryegrass$rootl[ryegrass$conc == 0])\nryegrass_relative <- ryegrass |> mutate(per_response = rootl/ mean_rootl_control * 100)\nmod_fixed <- drm(per_response~conc, data = ryegrass_relative,\n                 fct = LL.3(fixed = c(NA, 100, NA),\n                            names = c(\"Slope\", \"Upper Limit\", \"ED50\"))) # LL3 lower bound is 0, and we fix upper bound to be 100\nplot(mod_fixed, main = \"LL.3(fixed = c(NA, 100, NA))\", type = \"all\")\n\n\n\n\n\nWe use mselect to choose a model from a list of models. maED to do model averaging\n\n\nCode\n# Model selection\nmselect(mod, fctList = list(W1.3(fixed=c(NA, 100, NA)),W1.4(), W2.3(fixed=c(NA, 100, NA)), W2.4(),  LL.4()),linreg=TRUE) |> \n  suppressWarnings() # not sure why warnings are produced \n\n\n         logLik        IC  Lack of fit    Res var\nW2.4  -15.91352  41.82703 9.450713e-01  0.2646283\nLL.4  -16.15514  42.31029 8.664830e-01  0.2700107\nLL.4  -16.15514  42.31029 8.664830e-01  0.2700107\nW1.4  -17.46720  44.93439 4.505676e-01  0.3012075\nCubic -25.53428  61.06856           NA  0.5899609\nQuad  -35.11558  78.23116           NA  1.2485122\nLin   -50.47554 106.95109           NA  4.2863247\nW1.3  -74.48596 154.97191 1.808628e-17 31.6993835\nW2.3  -74.49140 154.98280 1.801677e-17 31.7137648\n\n\nCode\n# here the interpretation is we can calculate effective dose for 10 and 50, using the averages of all the models included in the list\nmaED(mod, \n     list(W2.4(),\n          LL.4(),\n          LL.3(fixed=c(NA, 100, NA)),\n          W1.4()),\n     c(10, 50), \n     interval=\"kang\")\n\n\n            ED10        ED50       Weight\nLL.4    1.463706    3.057955 2.822797e-01\nW2.4    1.628278    2.996913 3.594322e-01\nLL.4    1.463706    3.057955 2.822797e-01\nLL.3 2218.795584 2239.859777 9.641731e-26\nW1.4    1.405979    3.088964 7.600843e-02\n\n\n       Estimate    Lower    Upper\ne:1:10 1.518470 1.153434 1.883507\ne:1:50 3.038371 2.644522 3.432220"
  },
  {
    "objectID": "nonlinear/nonlinear.html#nonlinear-mixed-models",
    "href": "nonlinear/nonlinear.html#nonlinear-mixed-models",
    "title": "24  Nonlinear",
    "section": "24.3 Nonlinear Mixed Models",
    "text": "24.3 Nonlinear Mixed Models\nThe package landscape in nonlinear mixed models is\n\n24.3.1 Exploring RxODE\nRxODE is a package that simulates nonlinear mixed models with ODE language"
  },
  {
    "objectID": "spatial/spatial.html#using-gls-in-spatial-context",
    "href": "spatial/spatial.html#using-gls-in-spatial-context",
    "title": "32  Spatial Statistics",
    "section": "38.1 Using GLS in spatial context",
    "text": "38.1 Using GLS in spatial context\nThis section roughly follows Rossiter’s Tutorial, Demonstrating Generalized Least Squares\nThis example dataset studies spatial relationship of 20 rows and 25 columns\n\nr : rows\nc : columns\nstraw :\ngrain : something else\n\n\n\nCode\n# version with sf package is easier\nmhw <- read.csv(here(\"spatial/data/mhw.csv\"))\n\nha2ac <- 1 / 2.471054  # hectares per acre\nft2m <- .3048      # metres per foot\nfield.area <- 10000*ha2ac\nplot_len <- sqrt(field.area)/20 # length of plot\nplot_wid <- sqrt(field.area)/25\nn_row <- length(unique(mhw$r))\nn_col <- length(unique(mhw$c))\n# creates plotting grid, with real measurement units, ie, creating coordinates of observations\nsx <- seq(plot_wid/2, plot_wid/2+(n_col-1)*plot_wid, length=n_col)\nsy <- seq(plot_len/2+(n_row-1)*plot_len, plot_len/2, length=n_row)\nxy <- expand.grid(x=sx, y=sy)\n\nmhw_sp <- mhw\ncoordinates(mhw_sp) <- xy\nmhw_sf <- st_as_sf(mhw_sp) # spatial points object\n\n# alternatively: convert data frame to sf object\n# mhw_sf <- st_as_sf(bind_cols(mhw, xy), coords = c(\"x\", \"y\")) # specify the coords, which are points in space\n\n# mhw_sf[, \"grain\"] |> plot() # looks decent\n# mhw_sf |> ggplot() +\n#   geom_sf(aes(color = grain))\n\n# plot(mhw_sf[,\"grain\"]) # straightforward plotting with sf\n# plot of the raw data\n\nmhw_sf |> ggplot() +\n  geom_sf(aes(color = straw)) +\n  scale_color_viridis_c(option = \"rocket\") + #\n  theme_half_open()\n\n\n\n\n\nCode\nmhw_lm <- lm(straw ~ grain, data = mhw_sf)\nmhw_sf |> ggplot() +\n  geom_sf(aes(size = abs(mhw_lm$residuals)), \n          color = ifelse(sign(mhw_lm$residuals) > 0, \"black\", \"red\")) +\n  theme_half_open() # with residuals\n\n\n\n\n\nWe’ve created two plots here, one in which we plot the raw data of the straw out in the field, and another in which we are regressing straw against grain and plotting the residuals.\nNow we make a variogram of the residuals. Just a reminder, the components of a variogram are\n\n\n\n\n\n\n\nCode\nvr <- gstat::variogram(mhw_lm$residuals ~ 1, data = mhw_sf) # location = coordinates(mhw_sf)\nvrmf_sph <- fit.variogram(vr, model = vgm(\"Sph\")) # psill, model, range, nugget\nvrmf_exp <- fit.variogram(vr, model = vgm(\"Exp\"))\nvrmf_exp_ols <- fit.variogram(vr, model = vgm(\"Exp\"), fit.method = 6)\nvrmf_exp_1 <- fit.variogram(vr, model = vgm(\"Exp\"), fit.method = 1)\nvrmf_exp_reml <- fit.variogram.reml(straw~grain, data = mhw_sp, model = vgm(.1, \"Exp\", .4, .2))\n# ?plot.gstatVariogram\n# par(mfrow =c(2,1))\n# plot(vr, plot.numbers = T, model = vrmf_sph) # show the plot numbers\n# plot(vr, plot.numbers = T, model = vrmf_exp) # show the plot numbers\n\nplot(vr$dist, vr$gamma, ylim = c(0, .5), pch = 16, main = \"Different Methods of Fitting Exp Form Function\")\nlines(variogramLine(vrmf_exp_1, maxdist = 30)$dist, variogramLine(vrmf_exp_1, maxdist = 30)$gamma, col = 2)\nlines(variogramLine(vrmf_exp, maxdist = 30)$dist, variogramLine(vrmf_exp, maxdist = 30)$gamma, col = 1)\nlines(variogramLine(vrmf_exp_reml, maxdist = 30)$dist, variogramLine(vrmf_exp_reml, maxdist = 30)$gamma, col = 3) # weird, not a good fit\nlines(variogramLine(vrmf_exp_ols, maxdist = 30)$dist, variogramLine(vrmf_exp_ols, maxdist = 30)$gamma, col = 4)\nlegend(\"bottomright\", legend = c(\"Exp\", \"Exp(N weight)\", \"Exp(reml)\", \"Exp(OLS\"), col = c(1,2, 3, 4), lty = c(1, 1, 1, 1))\n\n\n\n\n\nThe actual parameters for the fit are:\n\n\nCode\nvrmf_exp\n\n\n  model     psill    range\n1   Nug 0.1350031 0.000000\n2   Exp 0.2375852 3.299723\n\n\n\n\n\n\n\n\n“range” here means the “range parameter, which is slightly different than the common use of the term”range” to denote when the variogram levels off. The reason it’s called the range parameter is because it does control when the variogram levels off, but in general we should not expect the range parameter and the range to be the same.\n\n\n\nBy default the spatial dependence is calculated out to a distance of 25, which is 1/3 of the diagonal of the bounding box, but the dependence seems to start to level out 10 or 12. so we can specify the cutoff of the variogram model as well.\nNow we try fitting with nlme, the values are slightly different with nlme, and we’ll try to resolve the differences:\n\n\nCode\nmhw_xy <- bind_cols(mhw, xy) # data frame with position location\nmhw_gls_exp <- gls(straw~grain,\n                   correlation = corExp(form = ~x + y, nugget = TRUE),\n                   data = mhw_xy,\n                   method = \"ML\")\n\nsummary(mhw_gls_exp)\n\n\nGeneralized least squares fit by maximum likelihood\n  Model: straw ~ grain \n  Data: mhw_xy \n       AIC      BIC    logLik\n  848.6456 869.7186 -419.3228\n\nCorrelation Structure: Exponential spatial correlation\n Formula: ~x + y \n Parameter estimate(s):\n    range    nugget \n3.0636363 0.2036605 \n\nCoefficients:\n               Value  Std.Error   t-value p-value\n(Intercept) 1.711027 0.24707543  6.925119       0\ngrain       1.217055 0.06062506 20.075114       0\n\n Correlation: \n      (Intr)\ngrain -0.967\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-3.06739304 -0.62992714 -0.01324196  0.65067470  4.75093676 \n\nResidual standard error: 0.6162918 \nDegrees of freedom: 500 total; 498 residual\n\n\n\n\nCode\n# lets try to manually recreate these calculations\ngls_cor_exp <- mhw_gls_exp$modelStruct$corStruct\ngls_cor_exp[1:2] # the parameterization does not match\n\n\n[1]  1.119603 -1.363571\n\n\nThe parameterization of the corExp is a little weird, in the sense that the printed range and nugget do not match the “values” held in the vector for the corStruct. nlme:::coef.corSpatial shows that the nugget is calculated by x_2 / (1 + x_2) and the range is calculated as exp(range)\n\n\nCode\nrng <- exp(gls_cor_exp[1]) # range\nnug <- exp(gls_cor_exp[2]) / (1 + exp(gls_cor_exp[2]))# nugget\ncbind(c(rng, nug))\n\n\n          [,1]\n[1,] 3.0636363\n[2,] 0.2036605\n\n\nNow the range and nugget parameter match! The correlation function is\n\\text{correlation} = (1 - n)exp(-r / d) where d is the rang and r is the distance.\n\n\nCode\nD <- st_distance(mhw_sf)\n\ncorMatrix(gls_cor_exp)[1:5, 1:5] |> \n  round(digits = 3) # the correlation matrix is scaled so that the variance is 1\nS <- (1 - nug) * (exp(-D / rng)) + diag(rep(nug, nrow(D))) # correlation + the nugget at 0 distance\nS[1:5, 1:5] |> round(digits = 3) # how do the estimates actually differ\n\n\n      [,1]  [,2]  [,3]  [,4]  [,5]\n[1,] 1.000 0.347 0.151 0.066 0.029\n[2,] 0.347 1.000 0.347 0.151 0.066\n[3,] 0.151 0.347 1.000 0.347 0.151\n[4,] 0.066 0.151 0.347 1.000 0.347\n[5,] 0.029 0.066 0.151 0.347 1.000\n      1     2     3     4     5\n1 1.000 0.347 0.151 0.066 0.029\n2 0.347 1.000 0.347 0.151 0.066\n3 0.151 0.347 1.000 0.347 0.151\n4 0.066 0.151 0.347 1.000 0.347\n5 0.029 0.066 0.151 0.347 1.000\n\n\nThe correlations match from the models, but the fitting is slightly different in these cases, wonder how the variance structures are fit. I also can’t seem to get multiple models to show up on the same plot\n\n\nCode\n# ?Variogram.gls\nmhw_gls_vg <- Variogram(mhw_gls_exp, maxDist = 30, metric = \"euclidean\") # defaults to pearson residuals (standardized)\n# corExp(value = c(log(0.2375852), 3.299723), nugget = T)\n# ?plot.variogramModel\nattr(mhw_gls_vg, \"modelVariog\") <- Variogram(Initialize(corSpher(form = ~x +y, nugget = T), data = mhw_xy), maxDist = 30)\nplot(mhw_gls_vg, showModel = FALSE, smooth = FALSE) \n\n\n\n\n\nCode\nplot(mhw_gls_vg)\n\n\n\n\n\nIt seems like I’m not the only one having difficulty reproducing how nlme produces it’s best variogram fit for the given specification. SO Matching geor,"
  },
  {
    "objectID": "spatial/spatial.html#sf-package",
    "href": "spatial/spatial.html#sf-package",
    "title": "32  Spatial Statistics",
    "section": "38.2 sf package",
    "text": "38.2 sf package\nAll sf functions start with “st” which make them easy to spot"
  }
]